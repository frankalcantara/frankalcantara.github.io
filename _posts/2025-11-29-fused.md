---
layout: post
title: Fused Multiply-Add (FMA) – A Instrução que Dobrou o Pico Teórico das CPUs Modernas e Revoluciona a Computação Numérica em 2025
author: Grok
categories:
    - artigo
    - Matemática
    - disciplina
tags:
    - algoritmos
    - C++
    - Python
    - performance
    - hardware
    - FMA
    - computação numérica
    - MLIR
    - GPU
    - TPU
    - LLVM
image: assets/images/fma.webp
rating: 5
description: Um estudo sobre a operação fused multiply-add (FMA), sua importância matemática fundamental, impacto brutal na performance real de CPUs desde Haswell/Zen até Arrow Lake/Zen 5, uso prático em Python e C++23, e como o MLIR transforma **FMA** em magia absoluta em GPUs, TPUs e ASICs em 2025.
date: 2025-11-29T12:00:00.000Z
preview: FMA não é apenas uma instrução – é a razão pela qual metade do poder da sua CPU moderna estaria jogada fora se você não a estiver usando. Este artigo explica por que **FMA** é obrigatório em código numérico sério em 2025.
lastmod: 2025-11-30T21:31:27.431Z
keywords:
    - FMA
    - fused multiply-add
    - precisão ponto flutuante
    - IEEE 754-2008
    - AVX2
    - AVX-512
    - Haswell
    - Zen
    - MLIR
    - Torch.compile
    - XLA
    - Triton
    - performance 2025
    - computação numérica
published: false
draft: false
schema:
    type: Article
    headline: Fused Multiply-Add (FMA) em 2025 – Precisão Perfeitamente Arredondada, Dobro do Throughput e a Base da Aceleração Moderna
    description: Análise profunda da operação fused multiply-add, sua exigência no IEEE 754-2008, impacto revolucionário na performance de CPUs desde 2013, uso automático em Python/NumPy/PyTorch/JAX, controle explícito em C++23, e como o ecossistema MLIR (Torch-Inductor, XLA, IREE, Triton) transforma **FMA** em performance sobre-humana em GPU/TPU/ASICs.
    author:
        type: Person
        name: Grok
    datePublished: 2025-11-29
    dateModified: 2025-11-29
    publisher:
        type: Organization
        name: xAI
    image: https://frankalcantara.com/assets/images/fma.webp
    keywords:
        - FMA
        - fused multiply-add
        - IEEE 754
        - AVX-512
        - MLIR
        - Torch.compile
        - performance numérica
        - GPU
        - TPU
        - Zen 5
        - Arrow Lake
    wordCount: 4420
    inLanguage: pt-BR
    license: https://creativecommons.org/licenses/by-sa/4.0/
    mainEntityOfPage:
        type: WebPage
        id: https://grok.x.ai/2025/11/29/fma-revolucao-computacao-numerica-2025.html
slug: fma-revolucao-computacao-numerica-2025
---

Se a esforçada leitora olhar a expressão `a * b + c` vai pensar que se trata de uma expressão banal. Talvez, aos olhos da aritmética básica, até seja. Mas esta banalidade mudou completamente mudou completamente o panorama da computação numérica na última década depois que o hardware passou a executar essa operação, **fused multiply-add** (FMA), com um único arredondamento final. A partir deste ponto, não só obtemos o resultado matematicamente correto (exatamente o que o padrão IEEE 754-2008 exige), como literalmente **dobramos** o throughput de ponto flutuante da CPU.

>Em 2025, ignorar **FMA** é o mesmo que rodar com meia CPU desligada.

Este artigo explica por que **FMA** é a instrução mais importante da aritmética de ponto flutuante moderna, como usá-la (ou deixar que o compilador use por você) em Python e C++23, e, principalmente, como o ecossistema MLIR em 2025 transforma essa operação simples no coração absoluto da performance em GPUs, TPUs e nas novas unidades matriciais (AMX/SME).

## Por Que **FMA** É Matematicamente Superior

Em aritmética de ponto flutuante tradicional, a operação `a * b + c` é realizada em duas etapas distintas, introduzindo dois pontos de injeção de erro:

1. `temp = a * b` $\rightarrow$ arredonda para o formato (perde precisão).
2. `result = temp + c` $\rightarrow$ arredonda novamente.

Formalmente, isso é representado como:

$$RN(RN(a \times b) + c)$$

Onde $RN$ é a operação de *Round to Nearest*. Dois arredondamentos implicam um erro total de arredondamento que pode chegar a quase 1 ulp (0.5 + 0.5), enquanto o **FMA** garante no máximo 0.5 ulp (*unit in the last place*) em casos de cancelamento.

Com FMA, a operação é atômica:

- A multiplicação $a \times b$ é realizada com precisão interna infinita (ou estendida o suficiente pelo hardware).
- A adição com $c$ ocorre sobre esse produto exato.
- Ocorre apenas **um** arredondamento no final.

Matematicamente:

$$RN(a \times b + c)$$

O resultado garante um erro máximo de 0.5 ulp. Isso é tão importante que o padrão IEEE 754-2008 tornou o **FMA** **obrigatório** para conformidade total. Em algoritmos sensíveis, como o *Kahan Summation*, produtos escalares longos ou resolução de sistemas lineares mal condicionados, o **FMA** previne o cancelamento catastrófico que destruiria a precisão em operações separadas.

## O Salto Brutal de Performance nas CPUs (2013–2025)

A evolução do hardware foi ditada pela capacidade de despachar FMAs.

1. **A Era Vetorial (AVX2/AVX-512):**
    Uma instrução **FMA** AVX-512 realiza 32 operações de ponto flutuante (16 multiplicações + 16 adições) por ciclo em 512 bits. Antes do FMA, o pico teórico era metade disso.
2. **A Era Matricial (2025 - AMX e SME):**
    Em 2025, com Intel Granite Rapids (AMX - *Advanced Matrix Extensions*) e ARM (SME - *Scalable Matrix Extension*), o **FMA** deixou de ser apenas vetorial para se tornar matricial. O hardware agora despacha "tiles" inteiros de operações FMA.

Em código denso (GEMM, DNNs, CFD), processadores modernos como o Apple M4 Max ou AMD EPYC Turin atingem 95–98% do pico teórico apenas porque suas unidades de execução são saturadas com instruções FMA. Quem não usa **FMA** em 2025 está desperdiçando, no mínimo, 50% da capacidade de computação do silício.

## Python em 2025 – **FMA** Grátis (Via Bibliotecas)

Em Python, a estratégia é delegar. Bibliotecas como NumPy, PyTorch e JAX ligam-se a backends (OpenBLAS, MKL, oneDNN) que utilizam instruções **FMA** agressivamente.

```python
import numpy as np

# O backend (MKL/Accelerate) detecta o padrão e usa instruções vetoriais FMA
# ou matriciais (AMX) dependendo do hardware.
def benchmark_fma():
    N = 10_000_000
    a = np.random.rand(N).astype(np.float32)
    b = np.random.rand(N).astype(np.float32)
    
    # Dot product é o caso clássico de uso de FMA
    # Resultado computado com acumulação de alta precisão
    return np.dot(a, b) 
```

O PyTorch 2.5+ com `torch.compile(mode="max-autotune")` utiliza o **Torch-Inductor** (baseado em MLIR) para gerar kernels que fundem operações de multiplicação e adição em instruções **FMA** nativas, seja em CPU (AVX-512) ou GPU (FFMA/HFMA).

## C++23 – Controle Total: Instrução vs. Contração

Em C++, é fundamental distinguir entre forçar a instrução e permitir a otimização.

1.  **`std::fma` (Instrução Explícita):** Força o comportamento de um único arredondamento. É vital para precisão numérica, mas pode inibir certas otimizações de pipeline se o compilador não conseguir vetorizar a chamada intrínseca eficientemente.
2.  **Contração de FP (Otimização):** Permite que o compilador funda `a * b + c` em uma instrução **FMA** quando seguro.

```cpp
#include <cmath>
#include <vector>

// 1. Abordagem Explícita (Foco em Precisão Numérica)
// Garante RN(a*b + c). Útil para Math Libraries.
void compute_precise(const std::vector<float>& a, const std::vector<float>& b, std::vector<float>& c) {
    for(size_t i = 0; i < a.size(); ++i) {
        c[i] = std::fma(a[i], b[i], c[i]); 
    }
}

// 2. Abordagem de Performance (C++23 Moderno)
// Use flags: -O3 -march=native -ffp-contract=fast
// O compilador (GCC 15/Clang 19) detecta o padrão e emite vfmadd231ps (AVX) 
//Na verdade o compilador emite instruções do tipo vfmadd*ps (AVX/AVX-512) ou instruções bf16/fp8 AMX/SME
// ou instruções equivalentes AMX/SME, vetorizando o loop agressivamente.
float dot_product_fast(const float* a, const float* b, size_t n) {
    float s = 0.0f;
    for (size_t i = 0; i < n; ++i) {
        s += a[i] * b[i]; // O compilador funde isso automaticamente
    }
    return s;
}
```

Em 2025, para *High Performance Computing* (HPC), a recomendação é utilizar a contração automática (`-ffp-contract=fast`) para permitir que o autovectorizador do LLVM ou GCC utilize todo o registro vetorial. Use `std::fma` apenas quando a precisão do último bit for mais valiosa que o throughput bruto.

## MLIR – A Mágica do Compilador Modular (2025)

No ecossistema moderno (Mojo, IREE, XLA, Triton), o **FMA** é tratado como uma operação de primeira classe dentro da representação intermediária (IR). O MLIR permite elevar o nível de abstração, desacoplando a matemática da arquitetura.

O pipeline de compilação típico para um acelerador de IA segue o fluxo:
`linalg` (álgebra) $\rightarrow$ `vector` (tiling) $\rightarrow$ `llvm/ptx` (hardware).

É no dialeto `vector` que a fusão se torna explícita antes de tocar o assembly:

```llvm
// Exemplo realista de MLIR em 2025 (dialeto linalg → vector)
// Isso representa um trecho típico de um kernel de dot product ou GEMM pequeno
// que os passes modernos do MLIR (Torch-Inductor, IREE, Triton, etc.)
// automaticamente fundem em vector.fma ou instruções matriciais (AMX, Tensor Core, etc.)

func.func @dot_product_fma(%a: vector<16xf32>, %b: vector<16xf32>, %c: f32) -> f32 {
  // Contrato de multiplicação + adição com redução (exactamente o padrão a[i]*b[i] + c)
  // iterator_types = ["reduction"] indica que a dimensão está sendo reduzida
  %result = vector.contract {
      indexing_maps = [
        affine_map<(d0) -> (d0)>,   // a
        affine_map<(d0) -> (d0)>,   // b  
        affine_map<(d0) -> ()>      // c (acumulador escalar)
      ],
      iterator_types = ["reduction"],
      kind = #vector.kind<add>
    } %a, %b, %c : vector<16xf32>, vector<16xf32>, f32 into f32

  return %result : f32
}

// O pipeline de otimização típico em 2025 faz:
//   mlir-opt --convert-linalg-to-vector 
//            --vectorize 
//            --vector-contraction-to-fma   ← este passe aqui funde automaticamente em vector.fma
//            --convert-vector-to-llvm
// 
// Resultado final no alvo:
//   • CPU AVX-512 → sequência de vfmadd231ps
//   • AMX (Intel) → tile de bf16/int8 FMA
//   • CUDA → mma.sync ou wgmma (dependendo do tipo)
//   • TPU → instrução systolic array (mar de FMAs)
```

Compiladores como o **Triton** usam essa infraestrutura para mapear operações de matriz diretamente para Tensor Cores em GPUs NVIDIA (instruções `mma.sync`) ou unidades AMX em CPUs Intel, sem que o programador precise escrever assembly ou intrínsecos complexos.

## Conclusão – 2025

Se você escreve código numérico hoje:

1.  **Protótipo:** Python/JAX/Torch.compile (FMA + aceleração automática).
2.  **Performance Crítica em CPU:** C++ com `-march=native -ffp-contract=fast`.
3.  **Hardware Exótico/Custom:** Use stacks baseadas em MLIR.

FMA não é mais uma "otimização". É a unidade atômica sobre a qual a infraestrutura de IA e simulação física de 2025 foi construída.