---
layout: post
title: Talvez você deva estudar código intermediário
author: Frank
categories: |-
    Matemática
    programação
    compiladores
    artigo
tags: |-
    Algoritmos
    C++
    Compiladores
image: assets/images/cpp.webp
featured: false
rating: 5
description: Explicando a estrutura Heap como implementada em C++23.
date: 2025-10-14T11:21:10.761Z
preview: Um estudo detalhado da estrutura heap em c++ como base para um artigo do livro sobre computação competitiva
keywords: |-
    Algoritmos
    **MLIR**
    compiladores
    nvidia
    cuda
    tpu
    lpu
    rocm
toc: true
published: true
lastmod: 2025-10-29T22:31:38.668Z
draft: 2025-10-14T11:22:19.148Z
---

A performance na era da Inteligência Artificial (IA) não é determinada somente pelo silício de ponta, mas de forma crítica pela **inteligência e eficiência da sua infraestrutura de software de compilação**. Este texto explora a arquitetura de compilação para aceleradores de _Machine Learning_ (ML) e _High Performance Computing_ (HPC), destacando como o Multi-Level IR (**MLIR**) emergiu como um novo padrão para a portabilidade e otimização de código heterogêneo, transformando a dinâmica competitiva do mercado de aceleradores.

O futuro da computação acelerada será definido não apenas pela inovação em silício, mas pela sofisticação da infraestrutura de software que traduz intenção algorítmica em execução eficiente. O **MLIR** representa uma transformação estrutural — de um ecossistema fragmentado e proprietário para uma arquitetura modular, aberta e extensível.

Para desenvolvedores, pesquisadores e organizações, compreender e dominar esse stack de compilação não é mais opcional: é a competência crítica que determinará a capacidade de extrair performance máxima de hardware heterogêneo e de navegar um mercado de aceleradores cada vez mais diversificado e competitivo.

>A verdadeira inovação não acontece apenas no hardware — ela acontece na camada de abstração que permite que o hardware seja plenamente utilizado.

### O Contexto Histórico: A Hegemonia da NVIDIA e o PTX

Para compreender a importância do **MLIR**, é necessário primeiro entender o modelo que dominou o mercado de aceleradores por mais de uma década. A NVIDIA e o Google estabeleceram seus mercados por meio de uma estratégia de integração vertical, na qual o hardware e o software formam um ecossistema coeso e proprietário.

#### PTX: A Interface Estável da NVIDIA

O **PTX (Parallel Thread Execution)** é o pilar fundamental da arquitetura de software da NVIDIA. Ele funciona como uma linguagem de montagem virtual — ou, mais precisamente, uma Intermediate Representation (IR) e baixo nível — que oferece uma **interface estável e retrocompatível** entre compiladores de alto nível e o hardware diversificado das GPUs NVIDIA.

A função primordial do PTX é a abstração: ele permite que frameworks de ML e compiladores como o nvcc (para CUDA C++) gerem código que funciona em diferentes gerações de GPUs sem necessidade de recompilação. O PTX encapsula as minúcias das microarquiteturas, oferecendo um alvo de compilação consistente.

#### O Ciclo PTX-SASS: O Limite do Controle

Entretanto, o PTX não é o código final executado pela GPU. O driver proprietário da NVIDIA contém um compilador Just-In-Time (JIT) que realiza a etapa crítica de tradução:

```shell
PTX (Formato Aberto / Interface) → Driver NVIDIA (Compilador JIT Fechado) → **SASS** (Código Nativo Proprietário)
```

O **SASS (Streaming Assembler)** é o _assembly_ nativo real da arquitetura NVIDIA, mantido em segredo e inacessível aos desenvolvedores. Este modelo de "caixa-preta" impõe limitações fundamentais:

* **Teto de Otimização**: desenvolvedores ficam dependentes da qualidade do compilador JIT embutido no driver, sem controle direto sobre a geração do código de máquina final.
* **Opacidade no Tuning**: embora ferramentas como o **Nsight Compute** permitam inferir e perfilar o **SASS** gerado, o controle direto sobre sua produção é limitado, criando uma barreira ao tuning agressivo específico para hardware.

Este modelo funcionou excepcionalmente bem para a NVIDIA, mas criou um problema fundamental: a **dependência de um ecossistema fechado** que dificulta a portabilidade e a inovação em arquiteturas alternativas.

### A Evolução: MLIR como Solução Estrutural

O **MLIR (Multi-Level Intermediate Representation)** representa uma mudança de paradigma na forma como pensamos sobre compiladores para hardware heterogêneo. Desenvolvido originalmente no Google e posteriormente adotado pela comunidade **LLVM**, o **MLIR** não é apenas mais uma IR, mas uma **infraestrutura de compilador modular e extensível**.

#### A Arquitetura Multi-Nível e o Sistema de Dialetos

O principal avanço conceitual do **MLIR** é permitir que código seja representado em **múltiplos níveis de abstração simultaneamente**. Isso é alcançado por meio do conceito de **dialetos** — conjuntos especializados de operações e tipos específicos para diferentes domínios e níveis de abstração.

Uma pipeline **MLIR** típica para ML opera da seguinte forma:

$$
\text{Framework ML} \xrightarrow{\text{Frontend}} \text{MLIR Alto Nível} \xrightarrow{\text{Otimizações}} \text{MLIR Nível Intermediário} \xrightarrow{\text{Lowering}} \text{MLIR Baixo Nível} \xrightarrow{\text{Backend}} \text{PTX/Assembly}
$$

Exemplos de dialetos em diferentes níveis:

* **Alto Nível**: StableHLO/MHLO (grafos de computação de ML)
* **Nível Intermediário**: Linalg (álgebra linear orientada a laços)
* **Baixo Nível**: GPU, Vector, **LLVM** (próximos ao hardware)

#### A Solução para a Fragmentação Quadrática

Antes do **MLIR**, o desenvolvimento de compiladores para hardware heterogêneo enfrentava o problema da **fragmentação quadrática**: cada combinação de $N$ frontends (PyTorch, TensorFlow, JAX) com $M$ backends (NVIDIA, AMD, TPU, CPU) exigia implementação específica de otimizações.

O modelo de dialetos do **MLIR** tenta resolver este problema modularizando o processo :

$$
\text{Complexidade de Desenvolvimento} \propto N + M \quad \text{(ao invés de } N \times M \text{)}
$$

As otimizações são implementadas como **passes** que transformam operações entre dialetos. Uma otimização de fusão de loops definida no dialeto Linalg pode ser aplicada a código proveniente de qualquer framework que se rebaixe (lower) para esse dialeto, eliminando a necessidade de reimplementação para cada combinação frontend-backend.

### O Impacto Estrutural: Otimização Holística e Portabilidade

A arquitetura do **MLIR** habilita capacidades fundamentalmente novas no desenvolvimento de compiladores para ML.

#### Otimizações Inter-Domínio

Em compiladores tradicionais, otimizações específicas (fusão de kernels, alocação de memória, vetorização) operam em níveis isolados. O **MLIR** permite que otimizações que exigem **visibilidade de diferentes níveis de abstração** ocorram antes que o código perca suas informações semânticas de alto nível.

Por exemplo, decisões sobre fusão de operações de tensor podem ser tomadas no nível Linalg, preservando a semântica matemática, antes de serem rebaixadas para laços explícitos (dialeto SCF) e finalmente para código específico de hardware (dialeto GPU). Esta abordagem permite:

* **Transformações informadas por contexto**: Otimizações podem considerar tanto características de alto nível (padrões de acesso a dados) quanto de baixo nível (hierarquia de memória) simultaneamente.
* **Preservação de informação semântica**: Diferentemente do PTX, que já representa um nível fixo de abstração, o **MLIR** mantém múltiplas representações durante o processo de compilação.

#### Representação Abstrata do Hardware

O **MLIR** oferece ferramentas para modelar características de hardware (como hierarquia de cache, largura de banda de memória, unidades de execução) em seus dialetos de baixo nível. Isso facilita a implementação de **otimizações hardware-específicas** de forma controlada e explícita, sem a necessidade de reverter para _assembly_ manual ou depender de heurísticas opacas de compiladores JIT.

### A Convergência dos Ecossistemas: Google, NVIDIA e OpenXLA

O desenvolvimento do **MLIR** catalisou uma convergência notável entre ecossistemas que eram historicamente paralelos e isolados.

#### A Arquitetura do Google: XLA e TPUs

O Google, ao desenvolver suas **TPUs (Tensor Processing Units)** e o compilador **XLA (Accelerated Linear Algebra)**, criou um stack que reflete estruturalmente o modelo da NVIDIA, mas com implementação proprietária:

| Camada | NVIDIA Stack | Google Stack | Função |
| :--- | :--- | :--- | :--- |
| **IR de Interface** | PTX | XLA HLO / StableHLO | Representação intermediária estável |
| **Código Nativo** | **SASS** | TPU _assembly_ | _assembly_ proprietário |
| **Compilação Final** | Driver NVIDIA (JIT) | TensorFlow Runtime | Tradução para código de máquina |
| **Profiling** | Nsight Compute | TensorFlow Profiler | Análise de performance |

#### A Iniciativa OpenXLA: Padronização Vendor-Neutral

A transformação crítica ocorreu quando o Google abriu o XLA por meio da iniciativa **OpenXLA**, adotando o **MLIR** como base e criando o dialeto **StableHLO** como representação intermediária padronizada.

**StableHLO** é projetado para ser uma **representação estável e vendor-neutral** de grafos de computação de ML, com a garantia de que um modelo compilado será funcionalmente idêntico em qualquer backend que suporte o dialeto:

$$
\text{StableHLO} \xrightarrow[\text{via **MLIR**}]{\text{Portabilidade}} \left\{ \begin{array}{l} \text{TPU (Google)} \\ \text{GPU NVIDIA} \\ \text{GPU AMD (ROCm)} \\ \text{LPU Groq} \\ \text{Outros ASICs} \end{array} \right.
$$

Esta abordagem **desacopla o desenvolvimento de modelos do hardware específico**, enfraquecendo o vendor lock-in que caracterizou o ecossistema CUDA/PTX.

#### IREE: Runtime Universal

O **IREE (Intermediate Representation Execution Environment)** demonstra o potencial completo do **MLIR** para desacoplar compilação de execução. IREE permite que código **MLIR** seja compilado ahead-of-time para múltiplos backends e executado em qualquer ambiente, desde servidores de datacenter até dispositivos edge, provendo um runtime universal de alta performance.

### As Limitações Persistentes: O Modelo Proprietário da NVIDIA

Apesar da convergência em torno do **MLIR**, o modelo de integração vertical da NVIDIA ainda impõe restrições fundamentais. O ciclo PTX-SASS permanece como um gargalo para otimização de baixo nível:

* **Dependência do JIT Proprietário**: Mesmo com pipelines **MLIR** sofisticadas, o código destinado a GPUs NVIDIA deve eventualmente ser traduzido para PTX e passar pelo compilador JIT fechado do driver.
* **Opacidade no _assembly_ Final**: O **SASS** permanece inacessível, limitando a capacidade de desenvolvedores e pesquisadores de realizar otimizações manuais específicas para microarquitetura.
* **Controle sobre Inovação**: A NVIDIA mantém controle sobre a última milha da compilação, o que lhe dá vantagem competitiva mas cria fricção para inovação independente.

Este modelo contrasta com a abordagem do **MLIR**/OpenXLA, que promove transparência e controle em todas as camadas de abstração até o ponto onde o hardware específico exige código proprietário.

### Panorama de Mercado: A Fragmentação Acelerada e o Papel do Software

O mercado de aceleradores está passando por diversificação acelerada, com a ascensão de Application-Specific Integrated Circuits (ASICs) e plataformas abertas desafiando a hegemonia da NVIDIA.

#### Alternativas Emergentes

* **AMD e ROCm**: A AMD oferece o **ROCm (Radeon Open Compute Platform)** como plataforma aberta, com o **HIP (Heterogeneous-Compute Interface for Portability)** fornecendo compatibilidade com CUDA. O foco está em supercomputação e HPC, onde a transparência e controle sobre o stack completo são valorizados.

* **Integração Vertical na Nuvem**: Provedores de nuvem investem pesadamente em hardware customizado:
  * **Google (TPU)**: Otimizado para cargas de trabalho de ML do Google
  * **AWS (Inferentia/Trainium)**: Focado em inferência e treinamento com melhor custo-benefício
  * **Azure (Maia)**: Integração com o ecossistema Microsoft

  Esta **integração vertical** visa otimizar a pilha completa (hardware + software de compilação) para cargas de trabalho específicas, oferecendo ganhos de eficiência e custo.

* **Arquiteturas Inovadoras:**
  * **Groq**: Arquitetura determinística para latência ultra-baixa em inferência
  * **Cerebras**: Wafer-scale computing para treinamento massivamente paralelo
  * **Graphcore**: IPUs com modelo de memória diferenciado

Cada uma dessas alternativas exige flexibilidade no stack de compilação para que suas inovações arquiteturais sejam plenamente exploradas. O **MLIR** é a infraestrutura que habilita essa experimentação.

#### A Nova Dinâmica Competitiva

O **MLIR** **democratizou o desenvolvimento de backends** para novos aceleradores. Um novo fornecedor de hardware não precisa mais construir um compilador full-stack do zero — pode desenvolver um backend **MLIR** que consome StableHLO e gera código para seu ASIC, reduzindo drasticamente a barreira de entrada.

### Tendências e Visão Estratégica: O Futuro da Computação Acelerada

A competição no ecossistema de aceleradores está migrando das camadas superiores (frameworks e linguagens) para a infraestrutura de compilação. O mapa estratégico do stack de software moderno pode ser visualizado assim:

```shell
┌────────────────────────────────────────────┐
│  Aplicações (GenAI, Modelos de Linguagem)  │
├────────────────────────────────────────────┤
│  Frameworks (PyTorch, TensorFlow, JAX)     │
├────────────────────────────────────────────┤
│  **Compiler Stack (MLIR, OpenXLA)**        │ ← O Foco da Inovação
├────────────────────────────────────────────┤
│  Hardware (NVIDIA, TPU, AMD, Groq, ...)    │
└────────────────────────────────────────────┘
```

#### O Domínio do Stack de Compilação

O **MLIR** e o OpenXLA estão se estabelecendo como a **"Língua Franca"** para unificar stacks de compilação díspares. Essa convergência tem implicações profundas:

* **Portabilidade Real**: Modelos treinados podem ser executados em múltiplos hardwares sem reescrita, desde que existam backends **MLIR** adequados.
* **Inovação Acelerada**: Pesquisadores podem experimentar com novas técnicas de otimização (fusão de operações, quantização, pruning) de forma hardware-agnóstica.
* **Redução do Vendor Lock-in**: A dependência de ecossistemas proprietários diminui à medida que representações intermediárias abertas se tornam padrão.

#### A Abstração como Vantagem Competitiva

A capacidade de **abstrair a complexidade do hardware** mantendo alta performance é a chave para o princípio "escreva uma vez, execute em qualquer lugar" sem sacrificar eficiência. O stack de compilação moderno permite que a performance se torne uma função da qualidade do backend e das otimizações aplicadas, não apenas da capacidade de reescrever código em linguagens proprietárias diferentes.

