---
layout: post
title: Um Mundo Inteiro em uma Grade
author: Frank
categories:
    - artigo
    - Matem√°tica
    - Intelig√™ncia Artificial
tags:
    - algoritmos
    - C++
    - intelig√™ncia artificial
    - resolu√ß√£o de problemas
image: assets/images/gw2.webp
featured: false
rating: 5
description: Aprenda de forma did√°tica os fundamentos de RL, desde MDPs at√© pol√≠ticas √≥timas, e construa sua base em IA.
date: 2025-02-09T11:51:59.153Z
preview: O Reinforcement Learning (RL) com o Grid World! üöÄ Um guia pr√°tico e did√°tico para iniciantes constru√≠rem uma base s√≥lida em IA.  Domine MDPs e algoritmos essenciais de RL explorando este ambiente simples e fundamental.  Ideal para quem busca entender RL de verdade!
keywords: |-
    Reinforcement Learning (RL)
    Reinforcement Learning
    Grid World
    Mundo em Grade
    Processo de Decis√£o de Markov (MDP)
    Tomada de Decis√£o Sequencial
    Propriedade de Markov
    Agente
    Ambiente
    Estados
    A√ß√µes
    Recompensas
    Din√¢mica do Sistema
    Fun√ß√£o de Transi√ß√£o
    Pol√≠tica (Policy)
    Pol√≠tica √ìtima
    Explora√ß√£o vs Explota√ß√£o
    Algoritmos de RL
    Q-Learning
    SARSA
    Recompensa por Passo (Living Reward)
    Estados Terminais
    Planejamento em RL
    Modelagem MDP
toc: true
published: true
lastmod: 2025-05-06T11:04:17.933Z
---

A esfor√ßada leitora, se realmente quiser entender as t√©cnicas e algoritmos de *Reinforcement Learning* - **RL**, deve come√ßar com problemas simples. N√£o √© qualquer problema. Problemas que permitam construir uma compreens√£o s√≥lida dos princ√≠pios estruturantes desta tecnologia. √â aqui que o **Grid World** brilha e se destaca.

A simplicidade do **Grid World** reside em n√£o ser um desafio do mundo real, como dirigir um carro, ou jogar um jogo de estrat√©gia. Em vez disso, este problema usa um mundo representado por um ambiente formado por uma grade bidimensional, discreta e simplificada. Um mundo cuidadosamente projetado para isolar e destacar os conceitos mais importantes do **RL**. N√£o √© raro que a simplicidade deste problema fa√ßa com que ele seja escolhido para testar novos algoritmos, novas ideias.

Historicamente, o **Grid World** e problemas similares, foram utilizados para o desenvolvimento e valida√ß√£o dos primeiros algoritmos de **RL**, como **Q-Learning** e **SARSA**. Al√©m disso, este √© um problema de decis√£o sequencial.

A √™nfase na natureza **sequencial** dos Processos de Decis√£o de Markov n√£o √© por acaso. Esta caracter√≠stica distingue os **MDPs** de outros modelos de decis√£o e reflete mais fielmente a realidade dos problemas que encontramos tanto na natureza quanto em sistemas artificiais.

A atenta leitora pode, simplesmente, imaginar um jogador de xadrez. Cada movimento que ele faz n√£o pode ser considerado isoladamente. A posi√ß√£o de uma pe√ßa determinar√° n√£o apenas o resultado imediato, mas tamb√©m moldar√° todas as possibilidades de jogadas futuras. Esta √© a ess√™ncia de um problema de decis√£o sequencial, no qual cada escolha se entrela√ßa com um futuro estoc√°stico.

Em contraste com problemas de decis√£o √∫nica, dom√≠nio da programa√ß√£o linear, em que podemos simplesmente otimizar uma determinada solu√ß√£o em busca de um resultado imediato, os problemas sequenciais exigem uma compreens√£o profunda das rela√ß√µes temporais. Uma decis√£o aparentemente sub√≥tima em um determinado momento pode ser parte de uma estrat√©gia superior se observada ao longo do tempo. Este conceito permite que a esfor√ßada leitora possa entender por que os **MDPs** s√£o t√£o poderosos na modelagem de problemas do mundo real.

A depend√™ncia temporal em problemas sequenciais cria uma teia complexa de causa e efeito. Cada a√ß√£o n√£o apenas gera uma recompensa imediata, mas tamb√©m transporta o sistema para um novo estado, que por sua vez determina quais a√ß√µes estar√£o dispon√≠veis no futuro e quais recompensas ser√£o poss√≠veis. Esta cadeia de influ√™ncias torna o planejamento em **MDPs** um desafio particularmente interessante. Neste cen√°rio, o trabalho de Markov permite estabelecer que o estado atual cont√©m toda a informa√ß√£o necess√°ria para tomar decis√µes √≥timas, permitindo navegar pela complexidade dos problemas sequenciais sem nos perdermos no labirinto do passado.

A criativa leitora deve imaginar cada estado como sendo uma fotografia completa do sistema, capturando tudo que precisamos saber para seguir adiante. Esta caracter√≠stica sequencial tamb√©m distingue os **MDPs**de outros paradigmas de aprendizado de m√°quina.

Enquanto problemas de classifica√ß√£o ou regress√£o lidam com mapeamentos est√°ticos de entradas para sa√≠das, os **MDPs** for√ßam a considera√ß√£o das consequ√™ncias de longo prazo das decis√µes tomadas. Introduzindo quest√µes sobre o equil√≠brio entre explora√ß√£o e explota√ß√£o, sobre como aprender com experi√™ncias passadas e sobre como planejar para um futuro incerto.

> Explora√ß√£o (*exploration*) refere-se a buscar e descobrir novas possibilidades/a√ß√µes enquanto explota√ß√£o (*exploitation*) refere-se a utilizar o conhecimento j√° adquirido para maximizar recompensas.

Em ess√™ncia, a natureza sequencial dos **MDPs**n√£o √© apenas uma caracter√≠stica t√©cnica do modelo. Trata-se de um reflexo de como decis√µes se desdobram no tempo real. Cada escolha √© um elo em uma cadeia cont√≠nua de causa e efeito. Esta compreens√£o permite que qualquer um que busque aplicar **MDPs**a problemas pr√°ticos ou te√≥ricos.

> All models are wrong, but some are useful. George Box.

O **MDP** permite a cria√ß√£o de modelos, apenas modelos. Nada mais, nada menos. Dessa forma, existe uma imperfei√ß√£o inerente aos processos estoc√°sticos envolvidos. Antes de continuarmos, podemos concordar com algumas defini√ß√µes que ser√£o √∫teis ao longo do processo de estudo.

1. **Agente**: no contexto do **MDP** um agente √© a entidade que interage com o ambiente e toma decis√µes. O agente √© o componente que observa o estado atual do ambiente, escolhe a√ß√µes com base em sua pol√≠tica (*strategy/policy*) e recebe recompensas como resultado dessas a√ß√µes. A esfor√ßada leitora pode pensar no agente como um tomador de decis√µes que est√° constantemente aprendendo e se adaptando para maximizar suas recompensas ao longo do tempo.

    >Uma "pol√≠tica" (policy em ingl√™s) √© simplesmente um mapeamento que diz qual a√ß√£o tomar em cada estado poss√≠vel do ambiente. √â como um conjunto de regras de decis√£o - para cada situa√ß√£o que voc√™ encontra, a pol√≠tica te diz o que fazer.
    >
    >A "pol√≠tica √≥tima" seria o conjunto de decis√µes que leva √†s melhores recompensas poss√≠veis. 

2. **Estados**: um estado em um **MDP** √© uma fotografia completa do sistema em um dado momento. Como se a sagaz leitora pudesse pausar o tempo e capturar toda informa√ß√£o necess√°ria a tomada de decis√£o. **A caracter√≠stica fundamental de um estado em um MDP √© que ele possui a propriedade de Markov**. Isso significa que o estado atual cont√©m toda a informa√ß√£o necess√°ria para decidir a pr√≥xima a√ß√£o, sem precisar de informa√ß√µes anteriores. Por exemplo: em um jogo de xadrez, o estado seria a posi√ß√£o atual de todas as pe√ßas no tabuleiro. N√£o importa qual sequ√™ncia de movimentos montou o tabuleiro. Apenas a configura√ß√£o atual ser√° relevante na previs√£o do acontecer√° a seguir.

3. **A√ß√µes**: as a√ß√µes s√£o as diferentes escolhas dispon√≠veis ao agente em cada estado. A perspicaz leitora deve entender que as a√ß√µes s√£o os √∫nicos elementos que o agente pode controlar. Em um **MDP**, em cada estado, existe um conjunto de a√ß√µes poss√≠veis, denominado $A(s)$, no qual $s$ √© o estado atual. Ainda no xadrez, as a√ß√µes seriam os movimentos legais poss√≠veis para as pe√ßas na posi√ß√£o atual. Se voc√™ tem um cavalo em $b1$, uma a√ß√£o poss√≠vel seria mov√™-lo para $c3$.

4. **Recompensas**: as recompensas s√£o como sinais num√©ricos que guiam o agente em dire√ß√£o ao comportamento desejado. A fun√ß√£o de recompensa $R(s,a,s')$ mapeia cada par estado-a√ß√£o para um valor num√©rico. Este valor representa o *qu√£o bom* foi tomar aquela a√ß√£o naquele estado. A recompensa √© imediata e n√£o considera consequ√™ncias futuras da a√ß√£o. Voltando ao exemplo do xadrez, capturar uma rainha poderia dar uma recompensa alta imediata $(+9)$, mesmo que isso possa levar a uma posi√ß√£o t√°tica desfavor√°vel no futuro.

5. **Din√¢mica**: a din√¢mica do sistema, tamb√©m chamada de fun√ß√£o de transi√ß√£o $P(s' \vert  s,a)$, descreve como o mundo evolui em resposta √†s a√ß√µes do agente. Esta fun√ß√£o determina a probabilidade de alcan√ßar um estado $s'$ quando tomamos a a√ß√£o a no estado $s$. A propriedade mais importante da din√¢mica em um **MDP** √© que ela depende apenas do estado atual e da a√ß√£o escolhida.

6. **Planejamento**: no planejamento, assume-se que todos os componentes s√£o conhecidos. O objetivo do agente, tomador de decis√£o, √© encontrar uma pol√≠tica, isto √©, um mapeamento do hist√≥rico de observa√ß√µes de estados para a√ß√µes, que maximize alguma fun√ß√£o objetivo da recompensa.

### Um Pouco de Sanidade N√£o Faz Mal a Ningu√©m

At√© este momento, utilizamos a nota√ß√£o tradicional da teoria da probabilidade, com $X_n$ representando estados e

$$P_{ij}$$

ou

$$P(X_{n+1} = j \vert  X_n = i)$$

representando probabilidades de transi√ß√£o. Esta nota√ß√£o serviu bem ao prop√≥sito de estabelecer as bases matem√°ticas dos processos estoc√°sticos e das *Cadeias de Markov*. No entanto, ao entrarmos no dom√≠nio do **Reinforcement Learning - RL**, adotaremos a nota√ß√£o padr√£o desta √°rea.

Em **RL**, representamos estados como $s$ e $s'$, ao inv√©s de $X_n$ e $X_{n+1}$. Esta mudan√ßa n√£o √© apenas uma prefer√™ncia est√©tica, reflete uma sutileza importante: em **RL**, frequentemente estamos mais preocupados com a rela√ß√£o entre estados e menos preocupados com sua evolu√ß√£o temporal expl√≠cita. A nota√ß√£o $s$ e $s'$ enfatiza a transi√ß√£o de um estado para outro, independentemente do momento espec√≠fico em que isso ocorre.

As probabilidades de transi√ß√£o tamb√©m ganham um novo elemento: a a√ß√£o $a$. Assim, $P(s' \vert  s,a)$ substitui nossa nota√ß√£o anterior $P(X_{n+1} = j \vert  X_n = i)$. Esta mudan√ßa destaca o papel central do agente no processo de decis√£o. *N√£o estamos mais apenas observando transi√ß√µes, mas ativamente influenciando-as atrav√©s de a√ß√µes*.

Esta nova nota√ß√£o permite expressar conceitos de **RL** de forma natural e intuitiva. Por exemplo: *uma pol√≠tica* $\pi(s)$ mapeia estados para a√ß√µes, e a fun√ß√£o valor $V^\pi(s)$ representa o valor esperado de longo prazo de estar em um estado $s$ seguindo a pol√≠tica $\pi$. Estas ideias, embora poss√≠veis de expressar na nota√ß√£o anterior, tornam-se mais claras e diretas com a nota√ß√£o padr√£o de *RL*.

Deste ponto em diante, adotaremos esta nota√ß√£o padr√£o de RL. Entretanto, √© importante que a atenta leitora mantenha em mente que esta nota√ß√£o representa os mesmos conceitos que [desenvolvemos anteriormente](https://frankalcantara.com/entendendo-mdp/), apenas expressos de uma forma mais adequada ao estudo de Reinforcement Learning.

## **Grid World**: Um Problema de Processo de Decis√£o de Markov (MDP)

No cora√ß√£o do **Grid World** reside o conceito de **Processo de Decis√£o de Markov - MDP**. Um **MDP** fornece uma estrutura matem√°tica formal para modelar *tomadas de decis√£o sequenciais em ambientes estoc√°sticos*[^1], nos quais o resultado de uma a√ß√£o √© probabil√≠stico. Em termos mais simples, dizemos que o **Grid World** √©, portanto, um exemplo pr√°tico e intuitivo de um **MDP**.

[^1]: Um processo ou sistema estoc√°stico √© aquele que envolve aleatoriedade ou probabilidade.

Imagine um tabuleiro retangular, como um tabuleiro de xadrez simplificado. Sem as casas de cores diferentes. Este √© o nosso **Grid World**. Cujo exemplo, pode ser visto na Figura 1.

![uma grade quadriculada com um agente, recompensa e puni√ß√£o](/assets/images/gw1.webp)

_Figura 1: Exemplo de **Grid World** mostrando um agente, recompensa e puni√ß√£o[^2]._{: class="legend"}

[^2]: Wikimedia Commons contributors, "File:Figure-red.png," Wikimedia Commons, https://commons.wikimedia.org/w/index.php?title=File:Figure-red.png&oldid=452927028 (accessed fevereiro 8, 2025).

Antes de prosseguirmos pode ser produtivo detalhar os componentes deste mundo:

1. **C√©lulas como Estados**: cada c√©lula dentro da grade representa um estado distinto no qual o agente pode se encontrar. A posi√ß√£o do agente na grade define completamente o estado do ambiente.

2. **Paredes como Limita√ß√µes**: algumas c√©lulas podem ser designadas como paredes, atuando como obst√°culos intranspon√≠veis. Paredes restringem o movimento do agente, adicionando um elemento de desafio e realismo ao ambiente.

3. **Movimento Estoc√°stico**: quando o agente decide se mover (por exemplo: para o norte), o resultado n√£o ser√° determin√≠stico. Introduzimos caracter√≠sticas estoc√°sticas ao movimento:

- Em $80\%$ das vezes, o agente se move na dire√ß√£o desejada.

- Em $10\%$ das vezes, o agente se move para cada lado perpendicular √† dire√ß√£o desejada. Por exemplo: se tentar ir para o norte, pode ir para o leste ou oeste com $10\%$ de probabilidade cada. Como pode ser visto na Figura 2.

![Mostra o agente e setas proporcionais a probabilidade do movimento](/assets/images/gw3.webp)

_Figura 2: Visualiza√ß√£o do efeito estoc√°stico do movimento._{: class="legend"}

   Se a a√ß√£o resultante levar o agente a colidir com uma parede, o agente permanece no estado atual. Essa regra garante que o agente n√£o possa *atravessar paredes*.

   Esta caracter√≠stica estoc√°stica no movimento for√ßa o agente a lidar com a incerteza e planejar suas a√ß√µes considerando as poss√≠veis consequ√™ncias n√£o determin√≠sticas.

### Sistema de Recompensas: Incentivando o Comportamento Desejado

Para que o agente aprenda a navegar no **Grid World** de forma inteligente, vamos criar e fornecer um sistema de recompensas que guie seu aprendizado. Para tanto, vamos definir o nosso sistema de recompensas segundo as seguintes regras:

1. **Recompensa por vida ou passo**: A cada passo que o agente d√° na grade, exceto nos estados terminais, ele recebe uma pequena recompensa, tamb√©m chamada de *recompensa passo* (em ingl√™s, *living reward*). Essa recompensa pode ser positiva ou negativa:

- uma recompensa por passo negativa, como no exemplo fornecido, $-0.03$ incentiva o agente a alcan√ßar os estados terminais o mais r√°pido poss√≠vel, *viver no ambiente tem um custo*;

- uma recompensa por passo positiva, incentiva o agente a permanecer no ambiente o m√°ximo poss√≠vel, viver no ambiente sua recompensa, se n√£o houvesse estados terminais atrativos;

- recompensas nos Estados Terminais: certos estados na grade s√£o designados como estados terminais. Ao alcan√ßar um estado terminal, o epis√≥dio de aprendizado se encerra, e o agente recebe uma grande recompensa, que pode ser: (a) positiva: representando um objetivo bem-sucedido, como alcan√ßar um estado de meta ou coletar um recurso valioso (+1 na Figura 1); (b) negativa: Representando um resultado indesejado, como cair em um estado de armadilha ou falhar na tarefa (-1 na Figura 1).

O sistema de recompensas define o objetivo do agente no **Grid World**. O agente deve aprender a sequenciar suas a√ß√µes de forma a maximizar a recompensa acumulada ao longo do tempo, considerando tanto as recompensas imediatas (recompensa por passo) quanto as recompensas futuras (recompensas nos estados terminais).

### Estrutura Formal do MDP no **Grid World**

Agora, que a anal√≠tica leitora entendeu os conceitos, podemos mapear os componentes do **Grid World** na estrutura formal de um **MDP**:

1. **Estados ($S$)**: o conjunto de todos os poss√≠veis estados √© representado por todas as c√©lulas da grade. Na Figura 1, nosso mundo tem $12$ estados poss√≠veis.

2. **A√ß√µes ($A$)**: o conjunto de a√ß√µes poss√≠veis para o agente em cada estado consiste nos movimentos direcionais: ${Norte, Sul, Leste, Oeste}$.

3. **Fun√ß√£o de Transi√ß√£o ($P$)**: a fun√ß√£o de transi√ß√£o $P(s' \vert  s,a)$ define a probabilidade de, estando no estado $s$ e executando a a√ß√£o $a$, o agente transite para o estado $s'$. No nosso **Grid World**, essa fun√ß√£o √© determinada pelas regras de movimento estoc√°stico que definimos anteriormente ($80\%$ na dire√ß√£o desejada, $10\%$ para os lados, permanecer no mesmo estado se colidir com a parede).

4. **Fun√ß√£o de Recompensa ($R$)**: a fun√ß√£o de recompensa $R(s,a,s‚Ä≤)$ define a recompensa que o agente recebe ao transitar do estado $s$ para o estado $s‚Ä≤$ ap√≥s executar a a√ß√£o $a$. No **Grid World**, isso engloba tanto as recompensas de passo, positivas e negativas, quanto as recompensas nos estados terminais.

5. **Estado Inicial** $(s_0)$: um estado espec√≠fico na grade √© designado como o estado inicial, Neste estado o agente come√ßa cada epis√≥dio de aprendizado.

6. **Estados Terminais ($S_terminal$):**: um ou mais estados s√£o designados como estados terminais. Ao alcan√ßar um estado terminal, o epis√≥dio se encerra.

O **Grid World**, como um **MDP**, assume a *propriedade de Markov*. Essa propriedade simplifica o problema de aprendizado e tomada de decis√£o. Ela *afirma que o futuro depende apenas do estado presente, e n√£o do hist√≥rico passado de estados e a√ß√µes*.

Em termos pr√°ticos no **Grid World**, isso significa que a probabilidade de o agente se mover para um pr√≥ximo estado e a recompensa que ele receber√° dependem apenas da c√©lula em que ele se encontra e da a√ß√£o que ele escolher executar. O caminho que o agente percorreu para chegar ao estado atual e seu hist√≥rico de recompensas s√£o irrelevantes para prever o futuro. Esta simplicidade sustenta o desenvolvimento de algoritmos eficientes de Reinforcement Learning.

> A propriedade de Markov permite que os algoritmos de RL tomem decis√µes baseadas no estado atual, sem a necessidade de manter um hist√≥rico complexo de toda a trajet√≥ria do agente.

## Grid World: Defini√ß√£o em L√≥gica de Primeira Ordem

Embora a formula√ß√£o de **MDP** seja amplamente utilizada e eficaz para modelar o **Grid World**, podemos tamb√©m descrever este ambiente de forma mais formal e rigorosa utilizando a **L√≥gica de Primeira Ordem - FOL**.

Essa abordagem n√£o √© comum e, talvez seja in√©dita. Ent√£o, se n√£o for afeito √†s aventuras. Pule essa se√ß√£o. Eu vou arriscar esta defini√ß√£o porque tenho ambi√ß√µes maiores que simplesmente entender **RL** e, enquanto explico um, vou criando bases de pesquisa. Mas, as vozes na minha cabe√ßa, e o [DuckduckGo](https://duckduckgo.com/) dizem que n√£o estou s√≥ [^3], [^4], [^5].

[^3]: BADREDDINE, Samy; SPRANGER, Michael. Injecting Prior Knowledge for Transfer Learning into Reinforcement Learning Algorithms using Logic Tensor Networks. *arXiv*, 2019. Dispon√≠vel em: <https://arxiv.org/abs/1906.06576>. Acesso em: 9 fev. 2025.

[^4]: BAREZ, Fazl; HASANBIEG, Hosien; ABBATE, Alesandro. System III: Learning with Domain Knowledge for Safety Constraints. arXiv, 2023. Dispon√≠vel em: https://arxiv.org/pdf/2304.11593. Acesso em: 9 fev. 2025.

[^5]: PANELLA, Amulya; JIN, Ruijun; XU, Zheng; CHEN, Yunhuo; CAMPBELL, Roy H.; HELLERSTEIN, Joseph L.; PAPADOPOULOS, Dennis. SUM11: A Scalable Unsupervised Multi-source Summarization System for News Articles. *University of Illinois at Chicago*, s.d. Dispon√≠vel em: <https://www.cs.uic.edu/~apanella/papers/sum11.pdf>. Acesso em: 9 fev. 2025.

A **FOL** permite definir o dom√≠nio do problema, as a√ß√µes poss√≠veis e o modelo de transi√ß√£o de forma declarativa e precisa. Esta abordagem pode ser particularmente √∫til para racioc√≠nio formal sobre o ambiente e para conectar o **RL** com outras √°reas da intelig√™ncia artificial, como o Planejamento Automatizado e os processos de racionaliza√ß√£o.

### Defini√ß√£o do Dom√≠nio

A defini√ß√£o de um mundo, Em **FOL**, come√ßa pela defini√ß√£o dos termos primitivos e axiomas que descrevem este mundo. Neste caso, para **Grid World**, podemos come√ßar por:

1. **Termos Primitivos**: definimos os seguintes predicados para descrever os elementos b√°sicos do **Grid World**:

    - $\text{Celula}(x,y)$: este predicado √© verdadeiro se a posi√ß√£o $(x,y)$ representa uma c√©lula v√°lida na grade;
    - $\text{Agente(x,y,t)}$: este predicado, √© verdadeiro se o agente est√° localizado na c√©lula $(x,y)$ no instante de tempo $t$.
    - $\text{Estado}(x,y,tipo)$: define o tipo de c√©lula na posi√ß√£o $(x,y)$. O tipo pode ser $\text{vazio}$, $\text{parede}$, $\text{inicio}$, $\text{terminal}^+$ (terminal positivo) ou $\text{terminal}^-$ (terminal negativo).

2. **Axiomas B√°sicos**: estabelecemos agora alguns axiomas que restringem e definem o dom√≠nio do **Grid World**:

    - Axioma 1: tipo de c√©lula √önico: para cada c√©lula $(x,y)$, existe um e apenas um tipo associado a ela. Formalmente, teremos:

       $$\forall x,y \exists! tipo \text{Estado}(x,y,tipo) \land tipo \in \\text{vazio, parede, inicio, terminal}^+, \text{terminal}^-\}$$

        Este axioma garante que cada c√©lula tem um tipo bem definido e que esse tipo pertence ao conjunto de tipos poss√≠veis.

    - Axioma 2: posi√ß√£o √∫nica do agente: em qualquer instante de tempo $t$, o agente pode estar em uma e apenas uma posi√ß√£o. Ou seja, teremos:

        $$\forall t \exists! x,y \;\text{Agente}(x,y,t) $$

        Este axioma assegura que o agente n√£o pode estar em m√∫ltiplas posi√ß√µes simultaneamente.

Um mundo, qualquer mundo, que define um problema espec√≠fico precisa da defini√ß√£o das a√ß√µes que, por ventura, podem ser implementadas neste mundo. Em **FOL**, tamb√©m podemos formalizar predicados referentes √†s a√ß√µes que o agente pode executar no **Grid World**.

1. **A√ß√µes Primitivas**: definimos o conjunto de a√ß√µes $A$ utilizando um predicado $Acao(a)$ que, pode ser definido como:

    $$\text{Acao}(a) \leftrightarrow a \in \{\text{Norte, Sul, Leste, Oeste}\}$$

    Este predicado define que as a√ß√µes primitivas dispon√≠veis para o agente s√£o mover-se para $\text{Norte}$, $\text{Sul}$, $\text{Leste}$ ou $\text{Oeste}$.

2. **Axiomas de A√ß√£o**: introduzimos o predicado $\text{Executavel}(a,x,y)$ para definir quando uma a√ß√£o a √© execut√°vel a partir da c√©lula $(x,y)$:

    $$\text{Executavel}(a,x,y) \leftrightarrow \exists x',y' \;[\text{Adjacente}(x,y,x',y',a) \land \neg \text{Estado}(x',y',\text{parede})]$$

    Na qual $\text{Adjacente}(x,y,x',y',a)$ √© um predicado definido para ser verdadeiro se $(x',y')$ √© adjacente a $(x,y)$ na dire√ß√£o da a√ß√£o $a$. Este axioma estabelece que uma a√ß√£o √© execut√°vel se, na dire√ß√£o pretendida, n√£o houver uma parede na c√©lula adjacente.

3. **Modelo de Transi√ß√£o**: podemos definir um modelo de transi√ß√£o em **FOL** tanto para o caso determin√≠stico quanto para o caso estoc√°stico.

    - **Caso Determin√≠stico**: se uma a√ß√£o a √© execut√°vel no estado $(x,y)$ no tempo $t$, ent√£o existe um √∫nico estado sucessor $(x',y')$ no tempo $t+1$:

      $$\forall x,y,t,a \; [\text{Agente}(x,y,t) \land \text{Executavel}(a,x,y)] \rightarrow $$

      $$ \exists! x',y' \; [\text{Adjacente}(x,y,x',y',a) \land \text{Agente}(x',y',t+1)]$$

      Este axioma descreve a transi√ß√£o de estado para o caso em que o movimento √© sempre bem-sucedido na dire√ß√£o desejada, caso a a√ß√£o seja execut√°vel.

    - **Caso Estoc√°stico**: para modelar o caso estoc√°stico, como no nosso **Grid World** original, precisamos usar probabilidades. Por exemplo: para a a√ß√£o $Norte$, podemos definir as probabilidades de transi√ß√£o da seguinte forma:

         - Probabilidade de mover para $\text{Norte}$ (dire√ß√£o desejada): $0.8$;

         - Probabilidade de mover para $\text{Leste}$ (perpendicular): $0.1$;

         - Probabilidade de mover para $\text{Oeste}$ (perpendicular): $0.1$.

        Estas transi√ß√µes podem ser representadas probabilisticamente em **FOL** utilizando, como exemplo considere as seguintes distribui√ß√µes de probabilidade condicionais, para a a√ß√£o $Norte$:

        $$P(\text{Agente}(x,y+1,t+1) \vert  \text{Agente}(x,y,t) \land \text{Acao}(\text{Norte})) = 0.8$$

        $$P(\text{Agente}(x+1,y,t+1) \vert  \text{Agente}(x,y,t) \land \text{Acao}(\text{Norte})) = 0.1 $$

        $$P(\text{Agente}(x-1,y,t+1) \vert  \text{Agente}(x,y,t) \land \text{Acao}(\text{Norte})) = 0.1 $$

        Note que estas probabilidades somam $1.0$:

        $$\sum_{s'} P(\text{Agente}(s',t+1) \vert  \text{Agente}(s,t) \land \text{Acao}(a)) = 1.0$$

        Esta √© uma propriedade fundamental de qualquer distribui√ß√£o de probabilidade e deve ser mantida para todas as a√ß√µes do agente.

        Estas equa√ß√µes probabil√≠sticas, que fogem um pouco do formalismo da l√≥gica de primeira ordem, definem o modelo de transi√ß√£o estoc√°stico para a a√ß√£o $Norte$. Caber√° a esfor√ßada leitora criar modelos semelhantes para as a√ß√µes $Sul$, $Leste$ e $Oeste$.

Para completar o modelo de transi√ß√£o, precisamos tamb√©m definir o comportamento quando o agente encontra uma parede:

$$P(\text{Agente}(x,y,t+1) \vert  \text{Agente}(x,y,t) \land \text{Acao}(\text{Norte}) \land \text{Estado}(x,y+1,\text{parede})) = 1.0$$

Esta equa√ß√£o especifica que quando h√° uma parede no estado de destino, o agente permanece em sua posi√ß√£o atual com probabilidade 1.0. Equa√ß√µes similares devem ser definidas para as outras dire√ß√µes.

A fun√ß√£o objetivo, que guia o aprendizado do agente, tamb√©m pode ser definida formalmente.

1. **Recompensa Imediata**: a recompensa imediata $R(x,y)$, recebida ao alcan√ßar uma c√©lula $(x,y)$ pode ser definida em fun√ß√£o do tipo de c√©lula:

    $$R(x,y) = \begin{cases}
       +1 & \text{se } \text{Estado}(x,y,\text{terminal}^+) \\
       -1 & \text{se } \text{Estado}(x,y,\text{terminal}^-) \\
       r_{\text{vida}} & \text{caso contr√°rio}
       \end{cases}$$

    Esta fun√ß√£o recompensa atribui valores diferentes dependendo se a c√©lula √© um terminal positivo, terminal negativo ou uma c√©lula comum (vazia ou de in√≠cio), utilizando a recompensa de passo $r_{\text{vida}}$. Neste caso, simplificamos a fun√ß√£o de recompensa $R(s,a,s‚Ä≤)$ criando um caso especial que considera apenas a posi√ß√£o no mundo.

2. **Fun√ß√£o Valor**: a fun√ß√£o valor $V^\pi(x,y)$ para uma pol√≠tica $\pi$ pode ser definida como o valor esperado do retorno acumulado a partir do estado inicial $(x,y)$ seguindo a pol√≠tica $\pi$:

    $$V^\pi(x,y) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R(x_t,y_t) \vert  (x_0,y_0)=(x,y), \pi \right]$$

    Esta equa√ß√£o representa a defini√ß√£o padr√£o da fun√ß√£o valor em *Reinforcement Learning*, agora expressa na nossa formaliza√ß√£o em **FOL**.

Finalmente, o **Grid World**, formalizado em L√≥gica de Primeira Ordem, ser√° definido por meio da tupla:

$$ GW = \langle \mathcal{L}, \Sigma, M \rangle $$

Na qual, teremos:

- $\mathcal{L}$ √© a linguagem de primeira ordem que definimos, incluindo os predicados $\text{Celula}$, $\text{Agente}$, $\text{Estado}$, $\text{Acao}$, $\text{Executavel}$ e $\text{Adjacente$.

- $\Sigma$ √© o conjunto de axiomas do dom√≠nio que estabelecemos (Axiomas B√°sicos e Axiomas de A√ß√£o).

- $M$ √© o modelo de transi√ß√£o estoc√°stico, definido pelas probabilidades de transi√ß√£o para cada a√ß√£o (como exemplificado para a a√ß√£o $Norte$).

Este sistema, $GW$, captura formalmente a ess√™ncia do **Grid World** utilizando L√≥gica de Primeira Ordem. √â importante notar que este sistema preserva a propriedade de Markov, mesmo na formula√ß√£o em **FOL**:

$$ P(s_{t+1} \vert  s_t, a_t, s_{t-1}, a_{t-1}, ..., s_0) = P(s_{t+1} \vert  s_t, a_t) $$

Embora a linguagem de descri√ß√£o seja diferente (**FOL** vs. nota√ß√£o de **MDP** tradicional), a propriedade de Markov, que simplifica o aprendizado, continua v√°lida. A formaliza√ß√£o em **FOL** oferece uma perspectiva alternativa ao estudo do **Grid World** e, por extens√£o, para a funda√ß√£o te√≥rica dos Processos de Decis√£o de Markov e do Reinforcement Learning.

## O Desafio: Encontrar a Pol√≠tica √ìtima

O objetivo central no **Grid World** continua sendo o mesmo: que o agente aprenda uma pol√≠tica √≥tima. *Uma pol√≠tica √© uma fun√ß√£o que mapeia cada estado para uma a√ß√£o*. Uma pol√≠tica √≥tima, denotada como $\pi^‚àó(s)$, √© aquela que, para cada estado $s$, seleciona a a√ß√£o, $a$, que maximiza a soma esperada de recompensas acumuladas a partir daquele estado. A Figura 3 ilustra o conceito de pol√≠tica √≥tima e sub√≥tima, mostrando como o fator de desconto $\gamma$ pode afetar o valor das recompensas futuras.

![Pol√≠ticas e recompensas descontadas](/assets/images/gw4.webp)

_Figura 3: Ilustra√ß√£o da compara√ß√£o entre uma pol√≠tica √≥tima e uma sub√≥tima, mostrando o efeito do fator de desconto nas recompensas ao longo do caminho._{: class="legend"}

Em nosso exemplo, um mundo de $4\times 3$ estados, o agente deve aprender a pol√≠tica que o guia de forma eficiente a partir do estado $Inicio$ at√© o estado terminal $+1$, evitando o estado terminal $-1$ e minimizando a recompensa de passo negativa acumulada ao longo do caminho. N√£o custa lembrar que os valores de recompensa $+1$ e $-1$ s√£o apenas exemplos.

**Pr√≥ximos Passos**: Solucionando o **Grid World** com Algoritmos de RL

Agora que definimos o **Grid World** tanto na perspectiva de **MDP** quanto na de L√≥gica de Primeira Ordem, o pr√≥ximo passo natural √© explorar como podemos usar algoritmos de *Reinforcement Learning* para encontrar a pol√≠tica √≥tima para um agente navegando neste ambiente. Se tudo correr bem, nas se√ß√µes seguintes, investigaremos algoritmos como *Programa√ß√£o Din√¢mica*, *Monte Carlo*, *Diferen√ßa Temporal (TD)* e *Aprendizado por Q-Learning*, e demonstraremos como eles podem ser aplicados para resolver o problema do **Grid World** e desvendar os segredos do Reinforcement Learning.

## Resumo da Nota√ß√£o Utilizada

### Nota√ß√£o Matem√°tica

- **Estados**: Representados por $s$ ou $s'$. O conjunto de todos os estados poss√≠veis √© denotado por $S$.
- **A√ß√µes**: Representadas por $a$. O conjunto de a√ß√µes poss√≠veis √© denotado por $A$.
- **Fun√ß√£o de Transi√ß√£o**: Denotada por $P(s' \vert  s, a)$, que representa a probabilidade de transitar para o estado $s'$ ao tomar a a√ß√£o $a$ no estado $s$.
- **Fun√ß√£o de Recompensa**: Denotada por $R(s, a, s')$, que mapeia cada par estado-a√ß√£o para um valor num√©rico representando a recompensa imediata.
- **Pol√≠tica**: Denotada por $\pi(s)$, que mapeia estados para a√ß√µes. A pol√≠tica √≥tima √© denotada por $\pi^*(s)$.
- **Fun√ß√£o Valor**: Denotada por $V^\pi(s)$, que representa o valor esperado de longo prazo de estar em um estado $s$ seguindo a pol√≠tica $\pi$.

### Nota√ß√£o em L√≥gica de Primeira Ordem (FOL)

- **Predicados**:
  - $\text{Celula}(x, y)$: Verdadeiro se $(x, y)$ √© uma c√©lula v√°lida na grade.
  - $\text{Agente}(x, y, t)$: Verdadeiro se o agente est√° na c√©lula $(x, y)$ no instante $t$.
  - $\text{Estado}(x, y, tipo)$: Define o tipo de c√©lula na posi√ß√£o $(x, y)$.
  - $\text{Acao}(a)$: Verdadeiro se $a$ √© uma a√ß√£o v√°lida.
  - $\text{Executavel}(a, x, y)$: Verdadeiro se a a√ß√£o $a$ √© execut√°vel a partir da c√©lula $(x, y)$.
  - $\text{Adjacente}(x, y, x', y', a)$: Verdadeiro se $(x', y')$ √© adjacente a $(x, y)$ na dire√ß√£o da a√ß√£o $a$.

- **Axiomas**:
  - Axioma 1: Tipo de c√©lula √∫nico.
  - Axioma 2: Posi√ß√£o √∫nica do agente.

- **Modelo de Transi√ß√£o**:
  - Probabilidades de transi√ß√£o s√£o representadas por $P(\text{Agente}(x', y', t+1) \vert  \text{Agente}(x, y, t), \text{Acao}(a))$.

### Exemplos de Uso

- **Fun√ß√£o de Transi√ß√£o**:
  
  $$P(s' \vert  s, a) = \text{Probabilidade de transitar para } s' \text{ ao tomar a a√ß√£o } a \text{ no estado } s$$

- **Fun√ß√£o de Recompensa**:
  
  $$R(s, a, s') = \text{Recompensa imediata ao transitar de } s \text{ para } s' \text{ ap√≥s executar a a√ß√£o } a$$

- **Pol√≠tica**:
  
  $$\pi(s) = \text{A√ß√£o a ser tomada no estado } s$$

- **Fun√ß√£o Valor**:
  
  $$V^\pi(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t) \vert  s_0 = s, \pi \right]$$
