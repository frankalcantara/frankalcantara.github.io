[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linguagens Formais e Autômatos",
    "section": "",
    "text": "1 Disciplina de Linguagens Formais\nEste é o material de suporte para a disciplina de Linguagens Formais, que abrange conceitos fundamentais de teoria da computação, autômatos, gramáticas e linguagens formais.\nAlém deste texto o aluno deve consultar os livros e artigos recomendados no plano de ensino da disciplina listados a seguir:\nBibliografia Básica:\n\nAHO, A. V. et al. Compiladores: princípios, técnicas e ferramentas. 2º. ed. Boston, MA, USA: Pearson Education Inc., 2007.\nSANTOS, Pedro Reis; LANGLOIS, Thibault. Compiladores da Teoria à Prática. Rio de Janeiro: Ltc, 2018.\nLOUDEN, Kenneth C.. Compiladores: princípios e práticas. São Paulo: Cenage Learning, 2004.\nJOSÉ NETO, João. Introdução à compilação. 2. ed. Rio de Janeiro: Elsevier, 2016.\nHOPCROFT, J.E.; MOTWANI, R.; ULLMAN, J.D. Introdução à teoria de autômatos, linguagens e computação. Ed. Campus, 2002.\nLOUDEN, K. C.; Compiladores Princípios e práticas. Editora Thompson, 2004.\n\nBibliografia Complementar:\n\nLEWIS, R. L., PAPADIMITRIOU C. H.; Elementos de Teoria da Computação, 2a Edição. Editora Bookman, 2000.\nKOZEN, D. C.; Automata and Computability. Editora Springer, 1997.\nDIVERIO, T. A.; Teoria da Computação, 2a Edição. Editora Sagra Luzzatto, 2004.\nNETO, J. J.; Introdução à Compilação. Editora LTC, 1987.\nSETZER, V. W.; A construção de um compilador, 2a Edição. Editora Campus, 1985.\nSETHI, R.; Programming Languages Concepts & Constructs, 2a Edição. Editora Addison Wesley, 1997.\nPRICE, A. M., TOSCANI S. S.; Implementação de Linguagens de Programação: Compiladores, 3a Edição. Editora Bookman, 2008.\n\nBibliografia de Apoio:\n\nDIJKSTRA, E. W. A Case against the GO TO Statement. Commun. ACM , v. 3, n. 1, p. 147-148, Mar. 1968.\nALCANTARA, FRANK COELHO DE. Compiladores e Interpretadores. 1. ed. Curitiba: IESDE BRASIL S.A., 2022. v. 1. 170p",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Disciplina de Linguagens Formais</span>"
    ]
  },
  {
    "objectID": "01-lexico.html",
    "href": "01-lexico.html",
    "title": "2  Analisadores Léxicos",
    "section": "",
    "text": "2.1 Máquinas de Estado Finito: História e Conceito\nOs estudos da Ciência da Computação são frequentemente organizados em uma hierarquia de capacidade computacional. Uma hierarquia em que a Máquina de Estados Finitos ocupa a posição de modelo mais fundamental e, em certos aspectos, o mais simples. Uma Máquina de Estado Finito é um modelo para sistemas que possuem uma quantidade finita e limitada de memória.\nO modelo que chamamos de Máquina de Estado Finito é composto por um conjunto finito de estados, um estado inicial, um conjunto de transições que definem como a máquina muda de estado com base em entradas discretas, e um conjunto de estados de aceitação que determinam se uma cadeia de símbolos de entrada é aceita ou rejeitada.\nA origem da máquina de estado finito está no produto de um esforço colaborativo que envolveu biólogos, psicólogos, matemáticos, engenheiros e alguns dos primeiros cientistas da computação, todos unidos por um interesse comum: modelar o processo do pensamento humano, seja no cérebro ou em uma máquina. Isso, curiosamente, liga as máquinas de estados finitos às ferramentas de Inteligência Artificial e Aprendizagem de Máquina. A história das Máquinas de Estados Finitos começa com a tentativa de entender o funcionamento do cérebro humano e como este processa informações.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analisadores Léxicos</span>"
    ]
  },
  {
    "objectID": "01-lexico.html#máquinas-de-estado-finito-história-e-conceito",
    "href": "01-lexico.html#máquinas-de-estado-finito-história-e-conceito",
    "title": "2  Analisadores Léxicos",
    "section": "",
    "text": "2.1.1 O Ponto de Partida: O Modelo de McCulloch e Pitts\nO trabalho pioneiro que deu origem à teoria dos autômatos foi o artigo de 1943 dos neurofisiologistas Warren McCulloch e Walter Pitts, intitulado A Logical Calculus Immanent in Nervous Activity(1)1.\nMcCulloch e Pitts propuseram um modelo matemático para o neurônio biológico, caracterizando-o como uma unidade de processamento binária. No modelo de McCulloch e Pitts, um neurônio dispararia, produzindo um sinal de saída, se, e somente se, um número fixo de sinapses de entrada fosse excitado dentro de um período de tempo discreto, excedendo assim um limiar predefinido (1)]. Este é um modelo discreto e binário, no qual cada neurônio pode estar em um conjunto finito de estados, por exemplo, disparando ou quieto, e cuja transição de estado é governada por entradas também discretas. O modelo de McCulloch e Pitts representa a primeira formalização conhecida de um Autômato Finito. O artigo demonstrou que uma rede destes neurônios formava um sistema capaz de computações lógicas complexas, como as operações da lógica proposicional, indicando que comportamentos complexos poderiam emergir de componentes simples e finitos.\n\n\n\n\n\n\nNote\n\n\n\nUm Autômato Finito é uma Máquina de Estados Finitos do tipo reconhecedor cuja função é reconhecer padrões em cadeias de símbolos, ou seja, determinar se um string de entrada pertence a uma determinada linguagem. Este modelo é fundamental para a teoria da computação e serve como base para muitos conceitos mais avançados, como autômatos de pilha e máquinas de Turing. Todo Autômato Finito é uma Máquina de Estados Finitos. Mas, nem toda Máquina de Estados Finitos é um Autômato Finito. A diferença está no propósito: enquanto os Autômatos Finitos são projetados para reconhecer linguagens, as Máquinas de Estados Finitos podem ser usadas para modelar qualquer sistema que possa ser descrito por estados discretos e as transições entre eles.\n\n\nA partir da inspiração biológica do modelo de McCulloch e Pitts, o conceito foi rapidamente abstraído para um modelo matemático puro, que ficou conhecido como Máquina de Estados Finitos. No domínio da linguagem computacional, uma Máquina de Estado Finito, em sua forma mais interessante para nosso propósito, funcionando como um artefato capaz de identificar uma linguagem, será formalmente definida como uma 5-tupla:\n\\[M=(Q, \\Sigma, \\delta, q_0, F)\\]\nna qual:\n\n\\(Q\\) é o conjunto de estados;\n\\(\\Sigma\\) é o alfabeto;\n\\(\\delta\\) é a função de transição;\n\\(q_0\\) é o estado inicial;\n\\(F\\) é o conjunto de estados de aceitação.\n\nO conceito de estado é o coração da Máquina de Estado Finito.\nUm estado é uma abstração que resume todo o histórico de entradas que a máquina processou até um determinado momento, contendo apenas a informação necessária para decidir sobre as futuras transições e saídas. Uma Máquina de Estados Finitos transita de um estado para outro com base na entrada atual. Este comportamento é frequentemente visualizado por meio de um diagrama de transição de estados. Este diagrama é um grafo em que os vértices representam os estados e as arestas direcionadas e rotuladas representam as transições. Como pode ser visto na Figure 2.1.\nUma cadeia de entrada será aceita pela Máquina de Estado Finito se, começando no estado inicial \\(q_0\\), a sequência de transições correspondente à cadeia de entrada termina em um dos estados de aceitação que estejam no conjunto \\(F\\), no caso da Figure 2.1, o estado de aceitação é \\(q_2\\).\n\n\n\n\n\n\nFigure 2.1: Diagrama de transição de uma máquina de estados finitos.\n\n\n\nA origem da Máquina de Estado Finito ilustra um dos ciclos virtuosos de inovação da Ciência da Computação. O processo começou com a observação de um sistema complexo do mundo real, o cérebro, e continua até os dias de hoje. McCulloch e Pitts não estavam tentando inventar um modelo de computação, tentavam entender a lógica da atividade nervosa. O passo de gênio subsequente, liderado por figuras como Stephen Kleene, foi reconhecer que o princípio computacional subjacente, um sistema com memória finita que muda de estado com base em entradas discretas, pode ser divorciado de sua inspiração biológica.\nEste divórcio foi o passo que permitiu a generalização do conceito e sua aplicação a domínios completamente diferentes. Domínios que abrangem desde o design de circuitos digitais até a análise de textos em um compilador.\n\n\n2.1.2 A Era da Formalização — Kleene, Moore e Mealy\nA década de 1950 marcou um período de intensa formalização e expansão da teoria dos Autômatos Finitos. Três figuras destacam-se nesta época: Stephen Kleene, que estabeleceu a ligação fundamental entre autômatos e uma nova notação de padrões, e Edward F. Moore e George H. Mealy, que estenderam o modelo para além do simples reconhecimento, dotando-o da capacidade de produzir saídas.\nSeguindo as ideias pioneiras de McCulloch e Pitts, o matemático americano Stephen Cole Kleene publicou em 1956 seu artigo Representation of Events in Nerve Nets and Finite Automata(2)2. Seu trabalho estava inserido em um contexto mais amplo de investigação sobre os limites da computação, que funções podem ser computadas e que problemas são decidíveis.\nA contribuição mais notável de Kleene neste artigo foi a criação das expressões regulares (REGEX). Kleene introduziu esta notação algébrica como uma forma concisa e poderosa para descrever conjuntos de sequências de entrada, ou eventos, que hoje conhecemos como Linguagens Regulares. Na sua obra, Kleene definiu formalmente as três operações fundamentais que formam a base de todas as expressões regulares:\n\nUnião (Alternância): representada por \\(+\\), \\(|\\) ou \\(\\cup\\), dependendo da área da matemática onde a união é usada, denota uma escolha entre dois padrões.\nConcatenação: representada pela justaposição de dois padrões, denota a sequência de um padrão seguido por outro. A concatenação é representada simplesmente pela junção dos símbolos, como em \\(ab\\), que denota a sequência do símbolo \\(a\\) seguido pelo símbolo \\(b\\). Ou pelo uso da notação do produto escalar \\(\\cdot\\), como em \\(a \\cdot b\\).\n\nFechamento de Kleene (Kleene Star): representado pelo asterisco \\(*\\), denota zero ou mais ocorrências do padrão precedente. Considerando o padrão \\(p\\), o fechamento de Kleene é denotado por \\(p^*\\) e representa a linguagem que contém todas as cadeias que podem ser formadas concatenando zero ou mais cópias de \\(p\\). Por exemplo, o fechamento de Kleene do símbolo \\(a\\) é a linguagem \\(\\{\\epsilon, a, aa, aaa, \\ldots\\}\\), no qual \\(\\epsilon\\) representa a cadeia vazia.\n\nEstas operações, aplicadas a símbolos de um alfabeto, formam a Álgebra de Kleene, um sistema formal que se tornou onipresente na ciência da computação, com aplicações que vão desde a verificação de programas até a análise de algoritmos.\n\n2.1.2.1 O Teorema de Kleene: A Grande Unificação\nA genialidade do trabalho de Kleene não reside apenas na invenção das expressões regulares, mas na prova de sua profunda ligação com os Autômatos Finitos. O Teorema de Kleene estabelece uma equivalência fundamental: a classe de linguagens que podem ser descritas por expressões regulares é precisamente a mesma classe de linguagens que podem ser reconhecidas por Autômatos Finitos. A prova deste teorema é construtiva e, por isso, de enorme importância prática.\n\n\n\n\n\n\nNote\n\n\n\nA curiosa leitora deve notar que uma prova construtiva é um tipo de prova matemática que não apenas demonstra a existência de um objeto matemático, mas também fornece um método explícito para construí-lo ou encontrá-lo. Neste caso, em vez de provar que algo deve existir por meio de uma contradição, a prova construtiva mostra o objeto.\nExemplo: Provar que entre quaisquer dois números racionais distintos, \\(x\\) e \\(y\\), existe um outro número racional, \\(z\\).\nProva por Construção:\nAssumindo \\(x, y \\in \\mathbb{Q}\\) e \\(x &lt; y\\).\n\nConstrução: Vamos construir \\(z\\) pegando a média de \\(x\\) e \\(y\\): \\[z = \\frac{x+y}{2}\\]\nVerificação:\n\nComo a soma e a divisão de números racionais resulta em um número racional, \\(z\\) é garantidamente um número racional (\\(z \\in \\mathbb{Q}\\)).\nComo \\(x &lt; y\\), pode-se provar que \\(x &lt; z &lt; y\\).\n\n\nA prova funciona porque vértices construímos um valor específico para \\(z\\) e demonstramos que ele satisfaz as condições exigidas.\n\n\nNo caso do Teorema de Kleene, a prova construtiva é dividida em duas partes fundamentais, cada uma correspondendo a uma direção da equivalência.\n\nDe Expressão Regular para Autômato Finito: esta parte da prova demonstra que, para qualquer expressão regular, é possível construir um Autômato Finito, especificamente, um Autômato Finito Não-Determinístico (AFN) com transições épsilon, ou AFN*-\\(\\epsilon\\), que aceita a mesma linguagem. A construção deste Autômato é indutiva sobre a estrutura da expressão regular. Começa-se com autômatos simples para os casos base, a linguagem vazia \\(\\emptyset\\), a linguagem contendo a cadeia vazia \\(\\{\\epsilon\\}\\), e a linguagem contendo um único símbolo \\(\\{a\\}\\)) e depois mostram-se métodos para combinar autômatos existentes para corresponder às operações de união, concatenação e fechamento de Kleene.\nDe Autômato Finito para Expressão Regular: a segunda parte demonstra que, para qualquer Autômato Finito, é possível derivar uma expressão regular que descreve a linguagem que ele aceita. Este processo é mais complexo e envolve a eliminação progressiva de estados do autômato, enquanto as etiquetas das transições são substituídas por expressões regulares cada vez mais complexas que representam os caminhos que foram eliminados.\n\nO Teorema de Kleene é a pedra angular teórica que sustenta os programas geradores de Analisadores Léxicos modernos como o Lex (3) e o Flex (4). O Teorema de Kleene garante que os programadores podem usar a notação declarativa e legível das expressões regulares para especificar os padrões dos tokens, com a confiança de que estas especificações podem ser automaticamente compiladas por um sistema eficiente de reconhecimento, o Autômato Finito.\nSim, a leitora entendeu corretamente. Existem programas capazes de gerar Analisadores Léxicos a partir de especificações da linguagem. O Lex e o Flex são exemplos clássicos desses programas. Porém, o que queremos neste livro é entender linguagens formais e compiladores, e não apenas usar ferramentas prontas.\n\n\n2.1.2.2 A Introdução de Saídas: Máquinas Transdutoras\nOs autômatos de Kleene, tal como os de McCulloch e Pitts, eram artefatos para o reconhecimento, ou aceitação de padrões. Sua única função era emitir um veredito binário: o string de entrada pertence ou não à linguagem. Todavia, muitas aplicações, desde circuitos de controle a sistemas de Inteligência Artificial, necessitam gerar uma sequência de saídas em resposta às entradas.\nAs Máquinas de Estados Finitos que produzem saídas são chamadas de transdutores de estados finitos. Um Transdutor de Estados Finitos estende a definição da Máquina de Estado Finito para uma 6-tupla,\n\\[(Q, \\Sigma, \\Gamma, \\delta, \\lambda, q_0)\\]\nna qual \\(\\Gamma\\) é um alfabeto finito de símbolos de saída e \\(\\lambda\\) é uma função de saída.\nEm meados da década de 1950, G.H. Mealy e E.F. Moore, trabalhando de forma independente, propuseram dois modelos distintos para os Transdutores de Estados Finitos, generalizando a teoria para englobar máquinas muito mais poderosas.\nNo seu artigo “Gedanken-experiments on Sequential Machines”(5)3, Edward F. Moore introduziu um modelo de transdutor no qual a saída é determinada exclusivamente pelo estado atual da máquina. A função de saída é, portanto, definida como:\n\\[\\lambda: Q \\to \\Gamma\\]\nEsta definição implica que a saída é estável enquanto a máquina permanece em um determinado estado. Isso quer dizer que uma mudança na saída só ocorre quando há uma transição para um novo estado. Em implementações de hardware, isto significa que as saídas são síncronas com as transições de estado, que por sua vez são frequentemente sincronizadas por um sinal de relógio, um clock. Nos diagramas de estado, a saída de uma máquina de Moore é tipicamente associada ao próprio estado, sendo escrita dentro do círculo que o representa. Uma máquina de Moore tende a necessitar de mais estados do que uma máquina de Mealy para realizar a mesma tarefa. Neste caso, um estado pode ser necessário apenas para gerar uma saída específica.\nUm ano antes, George H. Mealy, em seu artigo “A Method for Synthesizing Sequential Circuits”(6)4 publicado no Bell System Technical Journal, propôs um modelo alternativo. Em uma máquina de Mealy, a saída depende tanto do estado atual como da entrada atual. A função de saída é definida como\n\\[\\lambda: Q \\times \\Sigma \\to \\Gamma\\]\nComo a saída pode mudar instantaneamente com uma mudança na entrada, mesmo sem uma transição de estado, as saídas de uma máquina de Mealy são consideradas assíncronas. Isto pode permitir uma resposta mais rápida do sistema, mas também introduz a possibilidade de problemas de temporização em circuitos sequenciais. Nos diagramas de estado, a saída é associada à transição, sendo escrita no arco da transição, tipicamente separada da entrada por uma barra (ex.: a/b indica que a máquina transita de um estado a outro com a entrada a e produz a saída b). Esta característica permite que as máquinas de Mealy sejam mais compactas, frequentemente necessitando de menos estados do que a máquina de Moore equivalente.\nA tabela Table 5.1 resume as diferenças entre os dois modelos.\n\n\n\nTable 2.1: Comparação Detalhada entre Máquinas de Moore e Mealy\n\n\n\n\n\n\n\n\n\n\nCaracterística\nMáquina de Moore\nMáquina de Mealy\n\n\n\n\nFunção de Saída (\\(\\lambda\\))\n\\(\\lambda: Q \\to \\Gamma\\)\n\\(\\lambda: Q \\times \\Sigma \\to \\Gamma\\)\n\n\nDependência da Saída\nApenas do estado atual\nDo estado atual e da entrada atual\n\n\nTiming da Saída\nSíncrona com o estado\nAssíncrona com a entrada\n\n\nRepresentação em Diagrama\nAssociada ao vértice do estado\nAssociada ao arco da transição\n\n\nNúmero de Estados\nGeralmente necessita de mais estados\nGeralmente necessita de menos estados\n\n\nAplicação Típica\nAtivação de um conjunto de ações estáveis em um estado\nDesencadear eventos ou sinais em resposta a transições\n\n\n\n\n\n\nA esforçada leitora deve ter em mente que o Teorema de Kleene não é apenas um resultado matemático elegante; ele estabelece uma dicotomia que serve como alicerce para grande parte da engenharia de software moderna. Essa dicotomia se manifesta de forma clara:\n\nAs expressões regulares são uma linguagem declarativa: descrevem o que é o padrão;\nOs Autômatos Finitos são um modelo operacional: descrevem como reconhecer o padrão.\n\nA genialidade do Teorema de Kleene, amável leitora, reside na garantia de que podemos traduzir informações, sem perdas, do mundo declarativo, que é mais fácil para os humanos, para o mundo operacional, que é eficiente para as máquinas. Ou vice-versa.\nEste padrão, traduzir uma especificação de alto nível em uma implementação de baixo nível, é a descrição sintética do que um compilador faz. Além disso, o trabalho de Kleene pode ser visto como o arquétipo da poderosa ideia de separação de preocupações, um princípio de design tecnológico que permite criar abstrações elegantes sem sacrificar a performance.\n\n\n\n2.1.3 O Poder do Não-Determinismo\nEnquanto Kleene, Moore e Mealy solidificavam e expandiam a teoria dos autômatos determinísticos, uma nova ideia despontava no horizonte. Esta ideia, o não-determinismo, parecia à primeira vista conceder um poder quase mágico às máquinas, mas acabaria por se revelar uma das ferramentas conceituais mais úteis para simplificar o design e a teoria dos autômatos.\nEm 1959, Michael O. Rabin e Dana Scott publicaram seu artigo clássico e profundamente influente, Finite Automata and Their Decision Problems(7). Por este trabalho, que introduziu formalmente o conceito de Autômato Finito Não-Determinístico, foram agraciados com o Prêmio Turing, a mais alta honra da ciência da computação.\nUm Autômato Finito Não-Determinístico relaxa as restrições de um Autômato Finito Determinístico de três formas cruciais:\n\nMúltiplas Transições: para um dado estado e um símbolo de entrada, um Autômato Finito Não-Determinístico pode ter zero, uma ou múltiplas transições possíveis. A função de transição mapeia para um conjunto de estados, não para um único estado. Formalmente, \\(\\delta: Q \\times \\Sigma \\to \\mathcal{P}(Q)\\), em que \\(\\mathcal{P}(Q)\\) é o conjunto das partes de \\(Q\\).\nTransições-Épsilon (\\(\\epsilon\\)-transitions): um Autômato Finito Não-Determinístico pode mudar de estado sem consumir qualquer símbolo da entrada. Estas transições, rotuladas com \\(\\epsilon\\), permitem que a máquina salte espontaneamente entre estados. A função de transição é então \\(\\delta: Q \\times (\\Sigma \\cup \\{\\epsilon\\}) \\to \\mathcal{P}(Q)\\).\nTransições Ausentes: para um determinado par, estado e símbolo de entrada, o conjunto de próximos estados pode ser o conjunto vazio, significando que a computação nesse ramo termina.\n\nO modelo de computação de um Autômato Finito Não-Determinístico permite a existência de múltiplas transições possíveis a partir de um mesmo estado para um mesmo símbolo de entrada. Essa característica resulta em um modelo de computação inerentemente paralelo, no qual o Autômato Finito Não-Determinístico explora todos os caminhos de transição possíveis simultaneamente ao processar uma cadeia de entrada. A cadeia é considerada aceita se pelo menos um destes caminhos terminar em um estado de aceitação após a leitura de toda a cadeia.\n\n2.1.3.1 A Construção do Conjunto das Partes (Powerset Construction)\nA conclusão mais surpreendente e poderosa do artigo de Rabin e Scott é que, apesar de sua aparente flexibilidade e poder acrescido, os Autômatos Finitos Não-Determinísticos não são mais expressivos do que os Autômatos Finitos Determinísticos. Isto é uma forma elegante de dizer que para qualquer Autômato Finito Não-Determinístico, existe um Autômato Finito Determinístico equivalente que reconhece exatamente a mesma linguagem.\nA prova da equivalência entre Autômatos Finitos Não-Determinísticos e Determinísticos, mais uma vez, construtiva, por meio de um algoritmo conhecido como a construção do conjunto das partes (powerset construction). A ideia central desse algoritmo é simular o comportamento paralelo do Autômato Finito Não-Determinístico de forma determinística. Para tanto, cada estado no Autômato Finito Determinístico construído corresponde a um conjunto de estados nos quais o Autômato Finito Não-Determinístico poderia estar em um mesmo momento. O algoritmo da construção do conjunto das partes funciona da seguinte forma:\n\nEstado Inicial do Autômato Finito Determinístico: o estado inicial do Autômato Finito Determinístico (AFD) é o conjunto de todos os estados do Autômato Finito Não-Determinístico que são alcançáveis a partir do estado inicial do Autômato Finito Não-Determinístico usando apenas transições-\\(\\epsilon\\). Este conjunto é conhecido como o fechamento-\\(\\epsilon\\) (epsilon-closure) do estado inicial do Autômato Finito Não-Determinístico.\nTransições do Autômato Finito Determinístico: para cada estado do Autômato Finito Determinístico, que corresponde a um conjunto de estados do Autômato Finito Não-Determinístico, e para cada símbolo do alfabeto, a transição do Autômato Finito Determinístico é calculada em duas etapas: primeiro, determina-se o conjunto de todos os estados do Autômato Finito Não-Determinístico que podem ser alcançados a partir do conjunto atual ao ler esse símbolo; segundo, calcula-se o fechamento-\\(\\epsilon\\) desse novo conjunto. O resultado é o novo estado do Autômato Finito Determinístico.\nEstados de Aceitação do Autômato Finito Determinístico: um estado no Autômato Finito Determinístico é um estado de aceitação se seu conjunto correspondente de estados do Autômato Finito Não-Determinístico contiver pelo menos um dos estados de aceitação originais do Autômato Finito Não-Determinístico.\n\nEste processo é repetido até que não sejam gerados novos estados no Autômato Finito Determinístico. Como o número de subconjuntos de um conjunto finito de estados é finito, embora potencialmente grande, o processo garante a terminação.\nA existência da construção do conjunto das partes estabelece uma relação de compromisso (trade-off) entre Autômatos Finitos Não-Determinísticos e Determinísticos, que é de extrema importância na prática da engenharia de compiladores.\n\nConstrução e Tamanho: os Autômatos Finitos Não-Determinísticos são geralmente muito mais fáceis e intuitivos de construir diretamente a partir de uma especificação de uma linguagem, como uma expressão regular. Um Autômato Finito Não-Determinístico para uma dada linguagem é tipicamente muito menor, em termos de número de estados e transições, do que o Autômato Finito Determinístico equivalente. A conversão de um Autômato Finito Não-Determinístico com \\(n\\) estados para um Autômato Finito Determinístico pode, no pior dos casos, resultar em um Autômato Finito Determinístico com até \\(2^n\\) estados, um fenômeno conhecido como explosão de estados.\nSimulação e Eficiência: por outro lado, os Autômatos Finitos Determinísticos são muito mais fáceis de simular. Para uma cadeia de entrada de comprimento \\(k\\), um Autômato Finito Determinístico executa exatamente \\(k\\) transições, resultando em uma execução em tempo linear. A simulação direta de um Autômato Finito Não-Determinístico é mais complexa. Esta simulação pode exigir o rastreamento de múltiplos caminhos de computação em paralelo, tornando-a mais lenta.\n\nA tabela Table 2.2 resume esses compromissos práticos.\n\n\n\nTable 2.2: Autômato Finito Não-Determinístico vs. Autômato Finito Determinístico.\n\n\n\n\n\n\n\n\n\n\nCritério\nAutômato Finito Não-Determinístico\nAutômato Finito Determinístico\n\n\n\n\nFunção de Transição\n\\(\\delta: Q \\times (\\Sigma \\cup \\{\\epsilon\\}) \\to \\mathcal{P}(Q)\\)\n\\(\\delta: Q \\times \\Sigma \\to Q\\)\n\n\nTransições-\\(\\epsilon\\)\nPermitidas\nNão permitidas\n\n\nNúmero de Estados\nGeralmente pequeno (\\(n\\))\nPotencialmente grande (até \\(2^n\\))\n\n\nFacilidade de Construção\nAlta (fácil de construir a partir de RE)\nBaixa (difícil de construir diretamente)\n\n\nVelocidade de Execução\nLenta (simulação direta)\nRápida (execução em tempo linear)\n\n\nUtilização na Prática\nPasso intermédio na compilação de RE\nProduto final para analisadores léxicos eficientes\n\n\n\n\n\n\nO não-determinismo, tal como introduzido por Rabin e Scott, não é um recurso computacional físico. Nenhuma máquina real adivinha o caminho correto, ou verifica todos os caminhos no mesmo instante. Em vez disso, o não-determinismo é uma poderosa ferramenta de abstração conceitual. O não-determinismo permite que os projetistas humanos pensem em termos de possibilidades: _existe algum caminho que leve à aceitação? Em vez de se prenderem aos detalhes físicos de uma única computação determinística. Além disso, a prova de que Autômatos Finitos Não-Determinísticos são equivalentes a Autômatos Finitos Determinísticos é mais do que um resultado teórico; é uma licença para usar a abstração mais conveniente, geralmente o Autômato Finito Não-Determinístico, para o design e a especificação, com a garantia de que ela pode ser convertida em uma forma eficiente e fisicamente executável, geralmente o Autômato Finito Determinístico.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analisadores Léxicos</span>"
    ]
  },
  {
    "objectID": "01-lexico.html#sec-analisador-lexico",
    "href": "01-lexico.html#sec-analisador-lexico",
    "title": "2  Analisadores Léxicos",
    "section": "2.2 Autômatos Finitos: O Analisador Léxico",
    "text": "2.2 Autômatos Finitos: O Analisador Léxico\nA teoria dos Autômatos Finitos, com suas provas de equivalência e algoritmos de conversão, poderia ter permanecido um tópico de interesse puramente acadêmico. No entanto, encontrou uma aplicação prática tão perfeita na construção de compiladores que se tornou um exemplo de como a teoria fundamental pode transformar a engenharia de software. Esta aplicação é a análise léxica.\nComo eu disse antes, a primeira fase de um compilador é o analisador léxico, também conhecido como scanner ou lexer. Sua tarefa é ler um fluxo de caracteres criados a partir do código fonte, e identificar conjuntos significativos de símbolos para uma dada linguagem, os lexemas. Para cada lexema, o analisador léxico produzirá um token, que é tipicamente uma estrutura de dados consistindo em uma classe de token (ex.: IDENTIFICADOR, NÚMERO, PALAVRA-CHAVE), o valor do lexema, a cadeia de símbolos real, e a posição do lexema no código fonte. Por exemplo, a linha de código if (i == j) seria transformada em uma sequência de tokens como (IF, “if”, 0, 0), (LPAREN, “(”, 0, 3), (ID, “i”, 0, 4), (EQ, “==”, 0, 6), (ID, “j”, 0, 9), (RPAREN, “)”, 0, 10). O fragmento de código Listing 2.1 ilustra uma estrutura de armazenamento para tokens.\n\n\n\nListing 2.1\n\n\n#include &lt;string&gt;\n#include &lt;string_view&gt;\n#include &lt;array&gt;\n\n// Enumeração para os tipos de token\nenum class TokenType {\n    // Palavras-chave\n    IF,\n    // outras palavras-chave como ELSE, WHILE, FOR, etc.\n    \n    // Identificadores e literais\n    IDENTIFICADOR,\n    NUMERO,\n    STRING_LITERAL,\n    \n    // Operadores\n    EQ,          // ==\n    // outros operadores como NEQ, LT, GT, LE, GE, ADD, SUB, MUL, DIV\n\n    // Delimitadores\n    LPAREN,      // (\n    RPAREN,      // )\n    // outros delimitadores como LBRACE, RBRACE, SEMICOLON, COMMA    \n    // Especiais\n    END_OF_FILE,\n    INVALID\n};\n\n// Estrutura para representar um token\nstruct Token {\n    TokenType tipo;           // Classe do token\n    std::string lexema;       // Valor do lexema (cadeia de caracteres real)\n    int linha;               // Número da linha\n    int posicao;             // Posição na linha\n    \n    // Construtor\n    Token(TokenType t, std::string_view lex, int lin, int pos)\n        : tipo(t), lexema(lex), linha(lin), posicao(pos) {}\n    \n    // Construtor padrão\n    Token() : tipo(TokenType::INVALID), linha(0), posicao(0) {}\n};\n\n// Exemplo de array de _tokens_ para o fragmento \"if (i == j)\"\nstd::array&lt;Token, 6&gt; exemplo__tokens_ = {{\n    Token(TokenType::IF, \"if\", 1, 0),\n    Token(TokenType::LPAREN, \"(\", 1, 3),\n    Token(TokenType::IDENTIFICADOR, \"i\", 1, 4),\n    Token(TokenType::EQ, \"==\", 1, 6),\n    Token(TokenType::IDENTIFICADOR, \"j\", 1, 9),\n    Token(TokenType::RPAREN, \")\", 1, 10)\n}};\n\n\n\nA questão fundamental que deve estar incomodando a curiosa leitora é: como reconhecer estes padrões?\nOs padrões que definem os tokens na maioria das linguagens de programação, como identificadores, números inteiros, e palavras-chave, são quase invariavelmente descritos por Linguagens Regulares. Como estabelecido pelo Teorema de Kleene, as Linguagens Regulares são precisamente aquelas que podem ser reconhecidas por Autômatos Finitos. Portanto, as Máquinas de Estados Finitos são o modelo computacional adequado e suficiente para a tarefa da análise léxica. Parece simples, contudo, na prática, a análise léxica enfrenta ambiguidades que devem ser resolvidas por regras claras:\n\nRegra da Correspondência Mais Longa (Maximal Munch): se uma porção do texto de entrada pode corresponder a múltiplos padrões de comprimentos diferentes, o analisador léxico escolhe sempre a correspondência mais longa possível. Por exemplo, no string &gt;=, o lexer não irá parar no token &gt;; ele continuará a ler para reconhecer o identificador &gt;=. Isto requer que o algoritmo tenha capacidade de lookahead, do inglês para olhar para a frente, lendo caracteres para além do final de um potencial lexema para garantir que não há uma correspondência mais longa.\nPrioridade das Regras: se dois padrões correspondem a um lexema do mesmo comprimento, por exemplo if poderia ser uma palavra-chave ou um identificador, a ambiguidade é resolvida dando prioridade à regra que aparece primeiro no arquivo de especificação das regras do analisador léxico. Por esta razão, as regras para palavras-chave são sempre listadas antes, e têm precedência, sobre a regra geral para identificadores.\nTratamento de Erros: se uma sequência de caracteres não corresponder a nenhum padrão conhecido, o analisador léxico deve sinalizar um erro. Normalmente, isso é feito emitindo um token especial, como INVALID, e registrando a posição do erro.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analisadores Léxicos</span>"
    ]
  },
  {
    "objectID": "01-lexico.html#extensões-modernas-e-conclusão",
    "href": "01-lexico.html#extensões-modernas-e-conclusão",
    "title": "2  Analisadores Léxicos",
    "section": "2.3 Extensões Modernas e Conclusão",
    "text": "2.3 Extensões Modernas e Conclusão\nEmbora o modelo clássico da máquina de estado finito seja perfeitamente adequado para a análise léxica, sua simplicidade pode tornar-se uma limitação ao modelar sistemas mais complexos. A história das Máquinas de Estados Finitos não termina com sua aplicação em compiladores; ela continua com extensões que procuram gerir a complexidade em domínios como sistemas reativos e de controle.\nUma das principais dificuldades ao aplicar Máquinas de Estados Finitos a sistemas complexos é o problema da explosão de estados. Se um sistema é composto por múltiplas variáveis ou componentes, o número total de estados na Máquina de Estado Finito que o descreve pode crescer exponencialmente, tornando o modelo intratável e incompreensível. Por exemplo, um sistema com \\(n\\) variáveis, cada uma podendo assumir \\(Z\\) valores, pode ter até \\(Z^n\\) estados possíveis. Uma Máquina de Estado Finito plana, na qual todos os estados existem ao mesmo nível, não escala bem para esses cenários.\nO termo máquina de estados finitos plana não é uma definição padrão na teoria formal da computação, mas pode ter algumas interpretações dependendo do contexto. Exemplo de representação plana:\nEstados: \\(Q = \\{q_0, q_1, q_2, q_3\\}\\) Transições: \\(q_0 \\xrightarrow{a} q_1, q_1 \\xrightarrow{b} q_2, q_2 \\xrightarrow{c} q_3\\)\nPara resolver esta limitação, David Harel introduziu em 1987 os StateCharts, uma poderosa extensão visual e formal das Máquinas de Estados Finitos. Os StateCharts enriquecem o formalismo clássico com duas noções fundamentais:\n\nHierarquia (Aninhamento de Estados ou Decomposição-OU): os estados podem conter subestados. Isto permite refinar o comportamento de um superestado em vários subestados mais detalhados, organizando a complexidade de forma hierárquica.\nConcorrência (Estados Ortogonais ou Decomposição-E): os StateCharts permitem que o sistema esteja em múltiplos estados de subsistemas diferentes no mesmo instante. Isto é ideal para modelar sistemas compostos por componentes paralelos e independentes, reduzindo drasticamente o número de estados explícitos necessários em comparação com uma máquina de estados finitos plana.\n\nExemplo hierárquico:\n\\[\\text{Estado\\_Principal} \\begin{cases}\n\\text{Subestado\\_A} \\{q_0, q_1\\} \\\\\n\\text{Subestado\\_B} \\{q_2, q_3\\}\n\\end{cases}\\]\nPara facilitar a compreensão da abstração dos StateCharts, considere um sistema de um player como mostrado na Figure 2.2.\n\n\n\n\n\n\nFigure 2.2\n\n\n\nEmbora as Máquinas de Estados Finitos clássicas sejam suficientes para a análise léxica, os StateCharts demonstram como o modelo fundamental pode ser estendido para lidar com a complexidade inerente a sistemas de controle de software, protocolos de comunicação e interfaces de usuário, mostrando a versatilidade e a relevância contínua do paradigma de estados.\n\n2.3.1 Um Olhar no Horizonte: Não-Determinismo e a Fronteira Quântica\nA amável leitora, ao se deparar com o não-determinismo de um Autômato Finito Não-Determinístico, pode se perguntar sobre outros modelos de computação que exploram múltiplos caminhos. Onde a computação quântica se encaixa nisso?\nA conexão teórica existe nos Autômatos Finitos Quânticos(8). Contudo, é fundamental distinguir os conceitos:\n\nNão-Determinismo Clássico: a máquina explora múltiplos caminhos computacionais em paralelo. Imagine seguir todas as saídas de um labirinto ao mesmo tempo. A cadeia de entrada é aceita se pelo menos um caminho leva a um estado de aceitação.\nParalelismo Quântico: a máquina evolui em um único estado de superposição quântica. Os estados não são caminhos separados, mas sim componentes de um vetor de estado em um espaço de Hilbert complexo. O resultado depende das probabilidades das amplitudes quânticas no momento da medição.\n\nEssa distinção leva a uma consequência surpreendente: os modelos mais básicos de Autômatos Finitos Quânticos são, na verdade, menos poderosos que os Autômatos Finitos Não-Determinísticos clássicos. Os modelos quânticos não conseguem reconhecer todas as Linguagens Regulares. A razão está nas restrições da mecânica quântica, como a reversibilidade das operações.\nOnde está a vantagem, então? Na eficiência de estados. Para certas linguagens específicas, um Autômato Finito Quântico pode ser exponencialmente mais compacto que qualquer autômato clássico. O exemplo canônico é a linguagem \\(L_p = \\{a^k \\mid k \\text{ é múltiplo de } p\\}\\), para um \\(p\\) primo. Um Autômato Finito Determinístico (AFD) precisa de \\(p\\) estados, enquanto um AFQ pode reconhecer a mesma linguagem com apenas \\(O(\\log p)\\) estados.\nEmbora a aplicação prática de Autômatos Finitos Quânticos em analisadores léxicos seja um campo puramente especulativo, eles demonstram como os modelos fundamentais que estudamos neste capítulo continuam a inspirar as fronteiras mais avançadas da teoria da computação.\n\n\n\n\n[1] MCCULLOCH, W. S.; PITTS, W. A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, v. 5, n. 4, p. 115–133, 1943. \n\n\n[2] KLEENE, S. C. Representation of events in nerve nets and finite automata. Automata studies, v. 34, p. 3–41, 1956. \n\n\n[3] LESK, M. E. Lex – A Lexical Analyzer Generator. Murray Hill, NJ: Bell Laboratories, 1975. \n\n\n[4] PAXSON, V. Flex, a fast lexical analyzer generator. [s.l: s.n.]. \n\n\n[5] MOORE, E. F. Gedanken-experiments on sequential machines. In: SHANNON, C. E.; MCCARTHY, J. (Eds.). Automata studies. Princeton: Princeton University Press, 1956. p. 129–154. \n\n\n[6] MEALY, G. H. A method for synthesizing sequential circuits. The Bell System Technical Journal, v. 34, n. 5, p. 1045–1064, Sep. 1955. \n\n\n[7] RABIN, M. O.; SCOTT, D. Finite Automata and Their Decision Problems. IBM Journal of Research and Development, v. 3, n. 2, p. 114–125, 1959. \n\n\n[8] TIAN, Y. et al. Experimental demonstration of quantum finite automaton. npj Quantum Information, v. 5, 2019.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analisadores Léxicos</span>"
    ]
  },
  {
    "objectID": "01-lexico.html#footnotes",
    "href": "01-lexico.html#footnotes",
    "title": "2  Analisadores Léxicos",
    "section": "",
    "text": "em tradução livre, “Um Cálculo Lógico Immanente na Atividade Nervosa”.↩︎\nem tradução livre, “Representação de Eventos em Redes Nervosas e Autômatos Finitos”.↩︎\nem tradução livre, “Experimentos Mentais sobre Máquinas Sequenciais”.↩︎\nem tradução livre, “Um Método para Sintetizar Circuitos Sequenciais”.↩︎",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analisadores Léxicos</span>"
    ]
  },
  {
    "objectID": "01a-lexico.html",
    "href": "01a-lexico.html",
    "title": "3  Alfabetos, Linguagens e Strings: Fundamentos Matemáticos",
    "section": "",
    "text": "3.1 Alfabetos: Os Blocos Fundamentais\nUm alfabeto constitui o artefato fundamental da teoria de linguagens formais. Matematicamente, um alfabeto é um conjunto finito não-vazio de símbolos, denotado convencionalmente pela letra grega \\(\\Sigma\\), o sigma maiúsculo. Neste contexto, um alfabeto será definido como:\n\\[\\Sigma = \\{a_1, a_2, \\ldots, a_n\\} \\text{ tal que } |\\Sigma| = n &lt; \\infty \\text{ e } n \\geq 1\\]\nComo esse é o nosso primeiro contato a cuidadosa leitora deve observar que a definição elegante e formal de alfabeto pode ser decomposta em partes mais simples:\nA natureza dos símbolos que compõem um alfabeto é irrelevante para a teoria matemática subjacente. Um símbolo pode ser uma letra, um dígito, um caractere especial, ou qualquer entidade atômica que possa ser distinguida de outras. A única exigência é que os símbolos sejam mutuamente distintos e que o conjunto seja finito.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Alfabetos, Linguagens e Strings: Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "01a-lexico.html#alfabetos-os-blocos-fundamentais",
    "href": "01a-lexico.html#alfabetos-os-blocos-fundamentais",
    "title": "3  Alfabetos, Linguagens e Strings: Fundamentos Matemáticos",
    "section": "",
    "text": "\\(\\Sigma = \\{a_1, a_2, \\ldots, a_n\\}\\): o alfabeto é um conjunto formado por \\(n\\) símbolos distintos (\\(a_1\\) até \\(a_n\\)).\n\\(|\\Sigma| = n\\): o tamanho do alfabeto (número de símbolos) é \\(n\\).\n\\(n &lt; \\infty\\): o alfabeto é finito (não possui infinitos símbolos).\n\\(n \\geq 1\\): o alfabeto é não vazio, ou seja, contém pelo menos um símbolo.\n\n\n\n3.1.1 Exemplos de Alfabetos\nA teoria ganha vida por meio de exemplos concretos. Considere os seguintes alfabetos frequentemente utilizados na ciência da computação:\n\nAlfabeto binário: \\(\\Sigma_{\\text{bin}} = \\{0, 1\\}\\). Este alfabeto fundamental aparece em representações binárias, lógica booleana e circuitos digitais. A cardinalidade deste alfabeto é \\(|\\Sigma_{\\text{bin}}| = 2\\).\nAlfabeto das letras minúsculas: \\(\\Sigma_{\\text{abc}} = \\{a, b, c, \\ldots, z\\}\\). Usado para modelar identificadores em linguagens de programação e texto em linguagem natural. Sua cardinalidade é \\(|\\Sigma_{\\text{abc}}| = 26\\).\nAlfabeto para representação de números decimais: \\(\\Sigma_{\\text{dec}} = \\{+, -, ., 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\\}\\). Fundamental para a representação de números em notação decimal. Incluindo números reais, positivos e negativos, considerando o símbolo ‘.’ como separador decimal. A cardinalidade é \\(|\\Sigma_{\\text{dec}}| = 13\\).\nAlfabeto ASCII para Expressões Aritméticas: \\(\\Sigma_{\\text{arit}} = \\{a, b, \\ldots, z, 0, 1, \\ldots, 9, +, -, *, /\\}\\). Este exemplo define um subconjunto concreto e útil do ASCII, adequado para modelar identificadores de uma única letra e operações aritméticas simples. O uso de \\(\\ldots\\) (elipses) aqui é uma notação concisa para representar todos os caracteres dentro de um intervalo sequencial bem definido, neste caso, as letras minúsculas de ‘a’ a ‘z’ e os dígitos de ‘0’ a ‘9’. Ao contrário de uma definição com uma cardinalidade ambígua, esta abordagem permite um cálculo exato do tamanho do alfabeto considerando letras = \\({a, b, \\ldots, z}\\) (26), dígitos = \\({0, 1, \\ldots, 9}\\) (10), operadores = \\({+, -, *, /}\\) seria:\n\n\\[|\\Sigma_{\\text{arit}}| = \\text{(letras)} + \\text{(dígitos)} + \\text{(operadores)} = 26 + 10 + 4 = 40\\]\n\n\n3.1.2 Propriedades Matemáticas dos Alfabetos\nA definição formal de alfabeto como conjunto finito implica que precisaremos considerar três propriedades importantes:\n\nFinitude: todo alfabeto possui um número finito de símbolos. Esta restrição não é uma limitação prática, mas sim uma necessidade teórica que garante a decidibilidade de muitos problemas computacionais. Decidibilidade, neste contexto, refere-se à capacidade de determinar se uma string pertence a uma linguagem definida sobre um alfabeto.\nDistinção: cada símbolo em um alfabeto é único e distinguível. Formalmente, se \\(a, b \\in \\Sigma\\) e \\(a \\neq b\\), então \\(a\\) e \\(b\\) são símbolos diferentes.\nNão-ordenação inerente: um alfabeto, sendo um conjunto matemático, não possui uma ordenação natural. Qualquer ordenação, como a ordem alfabética, é uma convenção externa imposta para conveniência.\n\n\n\n3.1.3 Exercícios 1\n\nDetermine a cardinalidade dos seguintes alfabetos:\n\n\\(\\Sigma_1 = \\{a, b, c, +, -, *, /, (, )\\}\\);\n\\(\\Sigma_2 = \\{0, 1, 2, \\ldots, 9, A, B, C, D, E, F\\}\\) (hexadecimal);\n\\(\\Sigma_3 = \\{\\text{verdadeiro}, \\text{falso}, \\land, \\lor, \\neg, (, )\\}\\).\n\nConstrua alfabetos apropriados para os seguintes contextos:\n\nExpressões lógicas booleanas simples com variáveis \\(p\\), \\(q\\), \\(r\\);\nNúmeros em notação científica (ex: \\(1.23 \\times 10^{-4}\\));\nCoordenadas cartesianas no formato \\((x, y)\\).\n\nDetermine quais dos seguintes conjuntos são alfabetos válidos segundo a definição formal. Justifique sua resposta:\n\n\\(A = \\emptyset\\);\n\\(B = \\{\\epsilon\\}\\);\n\\(C = \\{1, 2, 3, \\ldots\\}\\);\n\\(D = \\{a, b, a\\}\\).\n\nDado o conjunto de strings \\(S = \\{\\text{if}, \\text{then}, \\text{else}, \\text{fi}\\}\\), determine o menor alfabeto \\(\\Sigma\\) tal que todas as strings em \\(S\\) possam ser formadas usando símbolos de \\(\\Sigma\\).\nCompare os alfabetos \\(\\Sigma_A = \\{0, 1\\}\\) e \\(\\Sigma_B = \\{a, b, c\\}\\) em termos de:\n\nCardinalidade;\nNúmero de strings de comprimento 3 que podem ser formadas;\nAplicabilidade para representar números binários.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Alfabetos, Linguagens e Strings: Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "01a-lexico.html#strings-sequências-sobre-alfabetos",
    "href": "01a-lexico.html#strings-sequências-sobre-alfabetos",
    "title": "3  Alfabetos, Linguagens e Strings: Fundamentos Matemáticos",
    "section": "3.2 Strings: Sequências Sobre Alfabetos",
    "text": "3.2 Strings: Sequências Sobre Alfabetos\nUma string (ou cadeia) sobre um alfabeto \\(\\Sigma\\) é uma sequência finita de símbolos escolhidos de \\(\\Sigma\\). Formalmente, uma string \\(w\\) de comprimento \\(n\\) pode ser representada como:\n\\[w = a_1a_2\\ldots a_n \\text{ tal que } a_i \\in \\Sigma \\text{ para } i = 1, 2, \\ldots, n\\]\nNa qual, temos:\n\n\\(w = a_1a_2\\ldots a_n\\): uma string \\(w\\) é uma sequência finita de símbolos concatenados.\n\\(a_i \\in \\Sigma \\quad \\text{para} \\quad i = 1, 2, \\ldots, n\\): cada elemento \\(a_i\\) na string pertence ao alfabeto \\(\\Sigma\\).\nA string possui comprimento \\(|w| = n\\) (número de símbolos na sequência). Por exemplo, se \\(w = abc\\), então \\(|w| = 3\\).\n\n\n3.2.1 A string Vazia\nUm caso especial é a string vazia, denotada por \\(\\epsilon\\), letra grega épsilon minúsculo. Esta string existe, não contém símbolos e possui comprimento zero:\n\\[\\epsilon = \\text{string vazia tal que } |\\epsilon| = 0\\]\nA string vazia desempenha um papel análogo ao número zero na aritmética, servindo como elemento neutro para a operação de concatenação de strings.\n\n\n3.2.2 Operações com Strings\nAssim como a aritmética define operações como adição e multiplicação sobre o conjunto dos números, a teoria das linguagens formais estabelece operações para manipular e combinar strings. Essas operações formam uma espécie de álgebra sobre as strings, permitindo-nos construir sequências complexas a partir de componentes mais simples de maneira formal e precisa. A seguir, exploraremos as operações mais importantes, começando pela mais fundamental de todas, que serve como base para quase todas as outras: a concatenação.\n\n3.2.2.1 Concatenação\nA operação fundamental com strings é a concatenação. Para duas strings \\(x\\) e \\(y\\), a concatenação \\(xy\\) é a string formada pela justaposição de \\(x\\) seguida por \\(y\\).\nDefinição formal: se \\(x = a_1a_2\\ldots a_m\\) e \\(y = b_1b_2\\ldots b_n\\), então:\n\\[xy = a_1a_2\\ldots a_mb_1b_2\\ldots b_n\\]\nAlternativamente, a concatenação pode ser representada com a mesma notação do produto escalar. Assim, a concatenação de \\(x\\) e \\(y\\) pode ser denotada como \\(x \\cdot y\\).\nPropriedades da concatenação:\n\nAssociatividade: para strings \\(x\\), \\(y\\), \\(z\\), temos \\((xy)z = x(yz)\\).\nElemento neutro: para qualquer string \\(w\\), \\(w\\epsilon = \\epsilon w = w\\).\nNão-comutatividade: em geral, \\(xy \\neq yx\\).\n\n\n\n3.2.2.2 Potências de Strings\nPara uma string \\(w\\) e um inteiro não-negativo \\(n\\), a potência \\(w^n\\) é definida recursivamente:\n\\[w^0 = \\epsilon\\] \\[w^{n+1} = w^n \\cdot w \\text{ para } n \\geq 0\\]\nPor exemplo, se \\(w = ab\\), então \\(w^3 = ababab\\).\nAntes de avançarmos para operações com linguagens, a atenta leitora deve distinguir a operação de potência de uma string da operação de Fechamento de Kleene, que será detalhada adiante. Enquanto a potência \\(w^n\\) aplica-se a uma única string \\(w\\) para produzir como resultado outra única string, o Fechamento de Kleene, denotado por um asterisco, como em \\(L^*\\), é uma operação que se aplica a um conjunto de strings, uma linguagem, ou a um conjunto de símbolos, um alfabeto. O resultado do Fechamento de Kleene não é uma string, mas sim um novo conjunto de strings, geralmente infinito, representando todas as combinações possíveis de elementos do conjunto original. A potência é, portanto, uma operação sobre elementos individuais, enquanto o Fechamento de Kleene é uma operação sobre conjuntos.\n\n\n3.2.2.3 Reverso de Strings\nO reverso de uma string \\(w\\), denotado por \\(w^R\\), é a string obtida invertendo a ordem dos símbolos:\nDefinição recursiva:\n\n\\(\\epsilon^R = \\epsilon\\);\n\\((wa)^R = aw^R\\) para \\(w \\in \\Sigma^*\\) e \\(a \\in \\Sigma\\).\n\nPor exemplo, se \\(w = abc\\), então \\(w^R = cba\\).\n\n\n\n3.2.3 Substrings, Prefixos e Sufixos: Entendendo as Partes de uma String\nAo trabalhar com texto e linguagens de programação, raramente tratamos uma string como uma unidade indivisível. Quase sempre precisamos inspecionar, extrair ou analisar suas partes internas. Os conceitos de substring, prefixo e sufixo nos dão um vocabulário formal e preciso para descrever essas partes.\nEste vocabulário formado de termos para as partes em que podemos dividir strings será relevante quando: estivermos estudando analisadores léxicos e sintáticos. Estes analisadores frequentemente operam sobre essas partes para identificar padrões, validar estruturas e extrair informações significativas; criando ferramentas de busca e análise de texto, onde precisamos localizar ocorrências de padrões dentro de grandes volumes de texto; ou desenvolvendo algoritmos que manipulam ou processam strings de maneira eficiente; ou ainda, ao implementar funcionalidades de autocompletar em editores de texto e IDEs, onde o sistema sugere palavras ou comandos com base no que o usuário já digitou.\n\n3.2.3.1 Substring\nUma substring é simplesmente um pedaço ou um segmento contínuo de caracteres que está dentro de outra string. Formalmente dizemos que uma string $v$ é uma substring de $w$ se for possível encontrar duas outras strings, $x$ (a parte que vem antes) e $y$ (a parte que vem depois), de forma que $w = xvy$. As strings $x$ ou $y$ podem, inclusive, ser vazias.\nExemplo: Para a string $w = compilador$, as palavras pila, com, dor e compilador são todas substrings válidas. No caso de pila, $x = com$ e $y = dor$.\n\n\n3.2.3.2 Prefixo\nUm prefixo é um trecho que começa exatamente no início de uma string. Formalmente dizemos que uma string $v$ é um prefixo de $w$ se houver uma string $y$ (o resto da string) tal que $w = vy$.\nExemplo: Para $w = compilador$, alguns prefixos são c, com, compi e a própria string compilador.\n\n3.2.3.2.1 Prefixo Próprio\nDizemos que uma string é um prefixo próprio de outra string se for um prefixo que não seja a própria string inteira. Formalmente, um prefixo $v$ de $w$ é dito próprio se $v \\neq w$.\nExemplo: Para $w = compilador$, compi é um prefixo próprio, mas compilador não é.\n\n\n\n3.2.3.3 Sufixo\nUm sufixo é um trecho que termina exatamente no final de uma string. Formalmente dizemos que uma string $v$ é um sufixo de $w$ se houver uma string $x$ (a parte inicial) tal que $w = xv$.\nExemplo: Para $w = compilador$, alguns sufixos são r, dor, lador e a própria string compilador.\n\n3.2.3.3.1 Sufixo Próprio\nDizemos que uma string é um sufixo próprio se for qualquer sufixo que não seja a própria string inteira. Formalmente, um sufixo $v$ de $w$ é dito próprio se `\\(v \\neq w\\).\nExemplo: Para $w = compilador$, lador é um sufixo próprio, mas compilador não é.\nDefinir formalmente as partes de uma string é fundamental para a ciência da computação. Estas partes são a base para inúmeras operações e algoritmos:\n\n\n\n\n3.2.4 Exercícios 2\n\nSejam \\(x = ab\\) e \\(y = cd\\). Calcule:\n\n\\(xy\\) e \\(yx\\);\n\\(x^3\\) e \\(y^2\\);\n\\((xy)^2\\) e \\(x^2y^2\\);\n\\(|x^n|\\) em função de \\(n\\).\n\nPara as strings dadas, determine seus reversos:\n\n\\(w_1 = abcde\\);\n\\(w_2 = palíndromo\\);\n\\(w_3 = \\epsilon\\) (string vazia);\nProve que \\((\\epsilon)^R = \\epsilon\\).\n\nDemonstre as seguintes propriedades usando strings específicas:\n\nAssociatividade: \\((xy)z = x(yz)\\) para \\(x = a\\), \\(y = bc\\), \\(z = d\\);\nElemento neutro: \\(w\\epsilon = \\epsilon w = w\\) para \\(w = abc\\);\nNão-comutatividade: encontre \\(x\\) e \\(y\\) tais que \\(xy \\neq yx\\).\n\nPara a string \\(w = compilador\\):\n\nListe todos os prefixos próprios;\nListe todos os sufixos próprios;\nIdentifique todas as substrings de comprimento 4;\nDetermine quantos prefixos e sufixos \\(w\\) possui no total.\n\nSeja \\(w = aba\\). Calcule:\n\n\\((w^R)^2\\);\n\\((w^2)^R\\);\n\\(w^R w\\);\nVerifique se \\((w^2)^R = (w^R)^2\\).",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Alfabetos, Linguagens e Strings: Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "01a-lexico.html#linguagens-conjuntos-de-strings",
    "href": "01a-lexico.html#linguagens-conjuntos-de-strings",
    "title": "3  Alfabetos, Linguagens e Strings: Fundamentos Matemáticos",
    "section": "3.3 Linguagens: Conjuntos de Strings",
    "text": "3.3 Linguagens: Conjuntos de Strings\nUma linguagem sobre um alfabeto \\(\\Sigma\\) é simplesmente um conjunto de strings formadas a partir de símbolos de \\(\\Sigma\\). Formalmente:\n\\[L \\subseteq \\Sigma^*\\]\nna qual \\(\\Sigma^*\\) denota o conjunto de todas as strings possíveis sobre \\(\\Sigma\\), incluindo a string vazia \\(\\epsilon\\).\n\n3.3.1 O Conjunto Universal \\(\\Sigma^*\\)\nO conjunto \\(\\Sigma^*\\) (lê-se sigma estrela ou fechamento de Kleene de \\(\\Sigma\\)) é definido como:\n\\[\\Sigma^* = \\Sigma^0 \\cup \\Sigma^1 \\cup \\Sigma^2 \\cup \\Sigma^3 \\cup \\ldots\\]\nna qual:\n\n\\(\\Sigma^0 = \\{\\epsilon\\}\\);\n\\(\\Sigma^1 = \\Sigma = \\{a_1, a_2, \\ldots, a_n\\}\\);\n\\(\\Sigma^2 = \\{xy \\mid x, y \\in \\Sigma\\}\\);\n\\(\\Sigma^k = \\{w \\mid |w| = k \\text{ e } w \\text{ é string sobre } \\Sigma\\}\\).\n\nDessa forma, \\(\\Sigma^k\\) é o conjunto de todas as strings de comprimento \\(k\\) formadas por símbolos de \\(\\Sigma\\)\nPara o alfabeto binário \\(\\Sigma = \\{0, 1\\}\\):\n\\[\\Sigma^* = \\{\\epsilon, 0, 1, 00, 01, 10, 11, 000, 001, 010, 011, 100, 101, 110, 111, \\ldots\\}\\]\n\n\n3.3.2 Exemplos de Linguagens\n\nLinguagem vazia: \\(L_{\\emptyset} = \\emptyset\\). A linguagem que não contém nenhuma string. É importante distinguir da linguagem que contém apenas a string vazia.\nLinguagem contendo apenas a string vazia: \\(L_{\\epsilon} = \\{\\epsilon\\}\\). Uma linguagem com exatamente um elemento, a string vazia.\nLinguagem de todas as strings binárias: \\(L_{\\text{todas}} = \\{0, 1\\}^*\\). Esta linguagem contém todas as strings possíveis sobre o alfabeto binário.\nLinguagem de strings binárias de comprimento par: \\[L_{\\text{par}} = \\{w \\in \\{0, 1\\}^* \\mid |w| \\text{ é par}\\}\\]\n\nLinguagem de identificadores válidos: para modelar identificadores em uma linguagem de programação: \\[L_{\\text{id}} = \\{w \\in \\{a, \\ldots, z, A, \\ldots, Z, 0, \\ldots, 9, \\_\\}^* \\mid w \\text{ inicia com letra ou } \\_\\}\\]\n\n\n3.3.3 Operações com Linguagens\nDa mesma forma que operamos com strings individuais, podemos realizar operações com linguagens inteiras para construir novas linguagens a partir de outras. Uma vez que uma linguagem é, por definição, um conjunto de strings (\\(L \\subseteq \\Sigma^*\\)), ela herda naturalmente as operações fundamentais da teoria dos conjuntos. Adicionalmente, estenderemos as operações que vimos para strings, como a concatenação, para que se apliquem a conjuntos inteiros de strings. Iniciaremos nossa exploração com as operações que derivam diretamente da natureza das linguagens como conjuntos.\n\n3.3.3.1 União de Linguagens\nA união representa a operação mais natural e intuitiva entre linguagens, capturando a essência da escolha ou alternativa. Quando unimos duas linguagens, criamos uma nova linguagem que aceita qualquer string que pertença a pelo menos uma das linguagens originais. É como estabelecer uma regra de aceitação do tipo ou isto, ou aquilo, ou ambos.\nEsta operação herda diretamente da teoria dos conjuntos, mas seu significado em linguagens formais transcende a mera manipulação de conjuntos, tornando-se uma ferramenta fundamental para expressar alternativas em especificações de linguagens.\nUnião: A união de duas linguagens \\(L_1\\) e \\(L_2\\) é:\n\\[L_1 \\cup L_2 = \\{w \\mid w \\in L_1 \\text{ ou } w \\in L_2\\}\\]\nA palavra ou aqui é inclusiva: uma string pertence à união se está em \\(L_1\\), ou em \\(L_2\\), ou em ambas.\n\n3.3.3.1.1 Exemplos de União\nExemplo 1: Linguagens de tokens Simples\n\n\\(L_{\\text{em um}} = \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\\}\\) (dígitos);\n\\(L_{\\text{op}} = \\{+, -, *, /\\}\\) (operadores);\n\\(L_{\\text{token}} = L_{\\text{em um}} \\cup L_{\\text{op}} = \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, +, -, *, /\\}\\).\n\nEsta união modela os tokens básicos de uma calculadora simples.\nExemplo 2: Linguagens de Comprimentos Específicos\n\n\\(L_{\\text{par}} = \\{w \\in \\{a, b\\}^* \\mid |w| \\text{ é par}\\}\\);\n\\(L_{\\text{ímpar}} = \\{w \\in \\{a, b\\}^* \\mid |w| \\text{ é ímpar}\\}\\);\n\n\\(L_{\\text{par}} \\cup L_{\\text{ímpar}} = \\{a, b\\}^*\\) (todas as strings sobre \\(\\{a, b\\}\\)).\n\nExemplo 3: Linguagens Sobrepostas\n\n\\(L_1 = \\{a, ab, abc\\}\\);\n\\(L_2 = \\{ab, bc, c\\}\\);\n\\(L_1 \\cup L_2 = \\{a, ab, abc, bc, c\\}\\).\n\nNote que \\(ab\\) aparece em ambas as linguagens, mas na união aparece apenas uma vez, seguindo a definição de conjunto.\n\n\n3.3.3.1.2 Propriedades Algébricas da União\nA união satisfaz propriedades fundamentais que a tornam uma operação bem-comportada:\n1. Comutatividade: \\[L_1 \\cup L_2 = L_2 \\cup L_1\\]\nA ordem das linguagens na união não importa.\n2. Associatividade: \\[(L_1 \\cup L_2) \\cup L_3 = L_1 \\cup (L_2 \\cup L_3)\\]\nPodemos agrupar uniões de qualquer forma, permitindo escrever \\(L_1 \\cup L_2 \\cup L_3\\) sem ambiguidade.\n3. Elemento Neutro: \\[L \\cup \\emptyset = \\emptyset \\cup L = L\\]\nA linguagem vazia não adiciona elementos a qualquer união.\n4. Idempotência: \\[L \\cup L = L\\]\nUnir uma linguagem consigo mesma não a modifica.\n5. Absorção: \\[L \\cup \\Sigma^* = \\Sigma^*\\]\nUnir qualquer linguagem com a linguagem universal resulta na linguagem universal.\n\n\n3.3.3.1.3 Aplicações Práticas da União\n1. Definição de Alfabetos Estendidos: em linguagens de programação, frequentemente definimos categorias de caracteres como:\n\nLetras: \\(L_{\\text{letras}} = \\{a, b, \\ldots, z, A, B, \\ldots, Z\\}\\);\nDígitos: \\(L_{\\text{dígitos}} = \\{0, 1, \\ldots, 9\\}\\);\n\nAlfanuméricos: \\(L_{\\text{alfanum}} = L_{\\text{letras}} \\cup L_{\\text{dígitos}}\\).\n\n2. Validação de Formatos Alternativos: para aceitar diferentes formatos de data:\n\n\\(L_{\\text{br}} = \\{dd/mm/aaaa\\}\\) (formato brasileiro);\n\\(L_{\\text{us}} = \\{mm/dd/aaaa\\}\\) (formato americano);\n\\(L_{\\text{iso}} = \\{aaaa-mm-dd\\}\\) (formato ISO);\n\\(L_{\\text{data}} = L_{\\text{br}} \\cup L_{\\text{us}} \\cup L_{\\text{iso}}\\).\n\n3. Tratamento de Variações Lexicais: em processamento de linguagem natural:\n\n\\(L_{\\text{sim}} = \\{\\text{sim}, \\text{yes}, \\text{oui}, \\text{sí}\\}\\);\n\\(L_{\\text{não}} = \\{\\text{não}, \\text{no}, \\text{non}\\}\\);\n\\(L_{\\text{resposta}} = L_{\\text{sim}} \\cup L_{\\text{não}}\\).\n\n\n\n\n3.3.3.2 Concatenação de Linguagens\nA concatenação captura a essência da composição sequencial: formar novas strings justapondo elementos de duas linguagens em uma ordem específica. É como estabelecer uma regra de construção do tipo primeiro algo de \\(L_1\\), depois algo de \\(L_2\\).\nEsta operação estende naturalmente a concatenação de strings individuais para conjuntos inteiros de strings, criando um mecanismo fundamental para a construção de linguagens complexas a partir de componentes mais simples.\nConcatenação: A concatenação de duas linguagens \\(L_1\\) e \\(L_2\\) é:\n\\[L_1 \\cdot L_2 = \\{xy \\mid x \\in L_1 \\text{ e } y \\in L_2\\}\\]\nO resultado é o conjunto de todas as strings possíveis formadas escolhendo-se uma string de \\(L_1\\) seguida por uma string de \\(L_2\\).\n\n3.3.3.2.1 Exemplos de Concatenação\nExemplo 1: construção de Identificadores\n\n\\(L_{\\text{letra}} = \\{a, b, c, \\ldots, z\\}\\);\n\\(L_{\\text{dígito}} = \\{0, 1, 2, \\ldots, 9\\}\\);\n\\(L_{\\text{letra}} \\cdot L_{\\text{dígito}} = \\{a0, a1, \\ldots, a9, b0, b1, \\ldots, z9\\}\\).\n\nEsta concatenação gera todos os identificadores de exatamente dois caracteres que começam com uma letra seguida de um dígito.\nExemplo 2: Construção Passo a Passo\n\n\\(L_1 = \\{a, bb\\}\\);\n\\(L_2 = \\{c, dd\\}\\);\n\\(L_1 \\cdot L_2 = \\{ac, add, bbc, bbdd\\}\\).\n\nCálculo detalhado:\n\n\\(a \\in L_1, c \\in L_2 \\Rightarrow ac \\in L_1 \\cdot L_2\\)\n\\(a \\in L_1, dd \\in L_2 \\Rightarrow add \\in L_1 \\cdot L_2\\)\n\n\\(bb \\in L_1, c \\in L_2 \\Rightarrow bbc \\in L_1 \\cdot L_2\\)\n\\(bb \\in L_1, dd \\in L_2 \\Rightarrow bbdd \\in L_1 \\cdot L_2\\)\n\nExemplo 3: Concatenação com Linguagem Unitária\n\n\\(L = \\{hello, hi\\}\\);\n\\(\\{!\\} = \\{!\\}\\);\n\\(L \\cdot \\{!\\} = \\{hello!, hi!\\}\\).\n\n\n\n3.3.3.2.2 Propriedades Algébricas da Concatenação\nA concatenação possui um conjunto distinto de propriedades que a diferenciam significativamente da união:\n1. Associatividade: \\[(L_1 \\cdot L_2) \\cdot L_3 = L_1 \\cdot (L_2 \\cdot L_3)\\]\nPodemos agrupar concatenações de qualquer forma, permitindo escrever \\(L_1 \\cdot L_2 \\cdot L_3\\) sem ambiguidade.\n2. Elemento Neutro: \\[L \\cdot \\{\\epsilon\\} = \\{\\epsilon\\} \\cdot L = L\\]\nA linguagem contendo apenas a string vazia atua como elemento neutro para a concatenação.\n3. Elemento Anulador: \\[L \\cdot \\emptyset = \\emptyset \\cdot L = \\emptyset\\]\nA linguagem vazia anula qualquer concatenação.\n4. Não-Comutatividade: \\[L_1 \\cdot L_2 \\neq L_2 \\cdot L_1 \\text{ (em geral)}\\]\nA ordem na concatenação importa fundamentalmente. Do exemplo anterior:\n\n\\(L_1 \\cdot L_2 = \\{ac, add, bbc, bbdd\\}\\);\n\\(L_2 \\cdot L_1 = \\{ca, cbb, dda, ddbb\\}\\);\n\n5. Distributividade sobre União: \\[L_1 \\cdot (L_2 \\cup L_3) = L_1 \\cdot L_2 \\cup L_1 \\cdot L_3\\] \\[(L_1 \\cup L_2) \\cdot L_3 = L_1 \\cdot L_3 \\cup L_2 \\cdot L_3\\]\nEsta propriedade é fundamental para simplificação de expressões regulares.\n\n\n3.3.3.2.3 Casos Especiais da Concatenação\n1. Concatenação com a Linguagem Universal: \\[L \\cdot \\Sigma^* = \\{xy \\mid x \\in L, y \\in \\Sigma^*\\}\\]\nResulta em todas as strings que começam com algum elemento de \\(L\\).\n2. Autoconcatenação: \\[L \\cdot L = L^2 = \\{xy \\mid x, y \\in L\\}\\]\nBase para a definição de potências de linguagens.\n3. Concatenação de Linguagens Infinitas:\nSe \\(L_1\\) tem \\(m\\) elementos e \\(L_2\\) tem \\(n\\) elementos, então \\(|L_1 \\cdot L_2| \\leq mn\\), com igualdade quando todas as concatenações resultam em strings distintas.\n\n\n3.3.3.2.4 Aplicações Práticas da Concatenação\n1. Construção de Padrões Estruturados: Para validar endereços de email (versão muito simplificada):\n\n\\(L_{\\text{usuário}} = \\{[a-zA-Z0-9]+\\}\\);\n\\(L_{\\text{domínio}} = \\{[a-zA-Z0-9.-]+\\}\\);\n\\(L_{\\text{ext}} = \\{[a-zA-Z]{2,}\\}\\);\n\\(L_{\\text{email}} = L_{\\text{usuário}} \\cdot \\{@\\} \\cdot L_{\\text{domínio}} \\cdot \\{.\\} \\cdot L_{\\text{ext}}\\).\n\n2. Análise Sintática: Em linguagens de programação:\n\n\\(L_{\\text{tipo}} = \\{int, float, string\\}\\);\n\\(L_{\\text{id}} = \\{[a-zA-Z][a-zA-Z0-9]*\\}\\);\n\\(L_{\\text{declaração}} = L_{\\text{tipo}} \\cdot \\{\\) \\(\\} \\cdot L_{\\text{id}}\\).\n\n3. Protocolos de Comunicação:\n\n\\(L_{\\text{header}} = \\{HTTP/1.1, HTTP/2.0\\}\\);\n\\(L_{\\text{método}} = \\{GET, POST, PUT, DELETE\\}\\);\n\\(L_{\\text{requisição}} = L_{\\text{método}} \\cdot \\{\\) \\(\\} \\cdot L_{\\text{URL}} \\cdot \\{\\) \\(\\} \\cdot L_{\\text{header}}\\).\n\n\n\n3.3.3.2.5 Autômatos e Complexidade\nA concatenação de linguagens finitas tem complexidade \\(O(|L_1| \\times |L_2|)\\) no caso geral, mas pode ser otimizada quando as linguagens têm estruturas específicas. Esta consideração é importante em implementações práticas de analisadores léxicos e sintáticos.\nNa teoria de autômatos, a concatenação corresponde à construção sequencial: aceitar uma string em \\(L_1 \\cdot L_2\\) significa processar uma parte inicial com um autômato para \\(L_1\\) e a parte restante com um autômato para \\(L_2\\). Esta intuição será fundamental para construções como a de Thompson para expressões regulares.\nA concatenação, portanto, não é apenas uma operação matemática, mas a operação que captura a essência da sequencialidade na computação e na especificação de linguagens.\n\n\n\n3.3.3.3 Interseção e Complemento\nAlém das operações que derivam diretamente da teoria dos conjuntos (união) e das que estendem conceitos de strings (concatenação), duas operações adicionais desempenham papéis fundamentais na teoria de linguagens formais: a interseção e o complemento. Estas operações, embora não sejam diretamente expressáveis por expressões regulares simples, são cruciais para compreender o poder e as limitações das Linguagens Regulares.\nInterseção: a interseção de duas linguagens \\(L_1\\) e \\(L_2\\) será dada por:\n\\[L_1 \\cap L_2 = \\{w \\mid w \\in L_1 \\text{ e } w \\in L_2\\}\\]\nA interseção captura as strings que pertencem simultaneamente a ambas as linguagens. Esta operação é particularmente útil para construir linguagens que devem satisfazer múltiplas condições.\nExemplo prático: Considere uma linguagem de programação onde queremos definir identificadores que:\n\nComecem com letra: \\(L_1 = \\{w \\in \\{a, \\ldots, z, 0, \\ldots, 9\\}^* \\mid w \\text{ inicia com letra}\\}\\)\nTenham comprimento par: \\(L_2 = \\{w \\in \\{a, \\ldots, z, 0, \\ldots, 9\\}^* \\mid |w| \\text{ é par}\\}\\)\n\nA interseção \\(L_1 \\cap L_2\\) contém exatamente os identificadores que começam com letra e têm comprimento par.\nComplemento: O complemento de uma linguagem \\(L\\) sobre um alfabeto \\(\\Sigma\\) é:\n\\[\\overline{L} = \\Sigma^* - L = \\{w \\in \\Sigma^* \\mid w \\notin L\\}\\]\nO complemento de \\(L\\) contém todas as strings possíveis sobre \\(\\Sigma\\) exceto aquelas que pertencem a \\(L\\). Esta operação é fundamental para expressar condições negativas.\nExemplo prático: Se \\(L\\) é a linguagem de todas as strings binárias que contêm a substring \\(01\\), então \\(\\overline{L}\\) é a linguagem de todas as strings binárias que não contêm \\(01\\) como substring — exatamente o padrão que vimos nos exemplos anteriores ser descrito pela expressão \\(1^*0^*\\).\nNa prática, interseção e complemento aparecem frequentemente em:\n\nValidação de dados: Verificar que uma entrada satisfaz múltiplas condições simultaneamente\nAnálise de segurança: Definir padrões de tráfego permitido como complemento de padrões maliciosos\nProcessamento de linguagens naturais: Filtrar textos que pertencem a uma categoria mas não a outra\nOtimização de compiladores: Análise de fluxo de dados onde certas condições devem ser simultaneamente satisfeitas\n\nEstas operações, embora mais abstratas que concatenação e união, formam uma base teórica sólida que será fundamental quando explorarmos autômatos finitos e técnicas mais avançadas de análise de linguagens.\n\n\n3.3.3.4 Potências de Linguagens\nPara uma linguagem \\(L\\) e um inteiro não-negativo \\(n\\):\n\\[L^0 = \\{\\epsilon\\}\\] \\[L^{n+1} = L^n \\cdot L \\text{ para } n \\geq 0\\]\n\n\n3.3.3.5 Fechamento de Kleene e Fechamento Positivo\nEntre todas as operações com linguagens, o fechamento de Kleene destaca-se como uma das mais poderosas e fundamentais. Nomeada em homenagem ao matemático Stephen Kleene, esta operação captura a essência da repetição arbitrária — um conceito central tanto na teoria formal quanto nas aplicações práticas da computação.\nIntuitivamente, o fechamento de Kleene de uma linguagem \\(L\\) representa todas as possíveis concatenações de zero ou mais elementos de \\(L\\). É como se perguntássemos: Que strings podemos formar se nos for permitido escolher elementos de \\(L\\) quantas vezes quisermos, incluindo a possibilidade de não escolher nenhum?\nFechamento de Kleene: O fechamento de Kleene de uma linguagem \\(L\\) é:\n\\[L^* = L^0 \\cup L^1 \\cup L^2 \\cup L^3 \\cup \\ldots = \\bigcup_{i=0}^{\\infty} L^i\\]\nEsta definição revela uma estrutura elegante: \\(L^*\\) é construído pela união de todas as potências possíveis de \\(L\\), começando de \\(L^0 = \\{\\epsilon\\}\\) (a linguagem contendo apenas a string vazia) e estendendo-se infinitamente.\nFechamento Positivo: O fechamento positivo de \\(L\\) é uma variação que exclui a possibilidade de não escolher nada:\n\\[L^+ = L^1 \\cup L^2 \\cup L^3 \\cup \\ldots = \\bigcup_{i=1}^{\\infty} L^i = L \\cdot L^*\\]\nA diferença fundamental é que \\(L^+\\) exige pelo menos uma escolha de \\(L\\), enquanto \\(L^*\\) permite zero escolhas.\nA relação entre fechamento de Kleene e o fechamento positivo depende de um fator: se a string vazia \\(\\epsilon\\) pertence ou não à linguagem original \\(L\\).\nCaso 1: Se \\(\\epsilon \\notin L\\), então: \\[L^+ = L^* - \\{\\epsilon\\}\\]\nNeste caso, como \\(\\epsilon \\notin L\\), ela só pode aparecer em \\(L^*\\) por meio de \\(L^0 = \\{\\epsilon\\}\\). Todas as outras potências \\(L^i\\) para \\(i \\geq 1\\) não contêm \\(\\epsilon\\). Estas potências que não contêm \\(\\epsilon\\) são formadas por concatenações de elementos de \\(L\\) que não incluem \\(\\epsilon\\).\nCaso 2: Se \\(\\epsilon \\in L\\), então: \\[L^+ = L^*\\]\nNeste caso, se \\(\\epsilon \\in L = L^1\\), então \\(\\epsilon \\in L^+\\). Como \\(\\epsilon\\) também está em \\(L^*\\) (via \\(L^0\\)), e ambos contêm todas as outras potências, os conjuntos são idênticos.\n\n3.3.3.5.1 Exemplos\nExemplo 1: \\(L = \\{a\\}\\)\n\n\\(L^* = \\{\\epsilon, a, aa, aaa, aaaa, \\ldots\\} = \\{a^n \\mid n \\geq 0\\}\\)\n\\(L^+ = \\{a, aa, aaa, aaaa, \\ldots\\} = \\{a^n \\mid n \\geq 1\\}\\)\nComo \\(\\epsilon \\notin L\\), temos \\(L^+ = L^* - \\{\\epsilon\\}\\)\n\nExemplo 2: \\(L = \\{ab, cd\\}\\)\n\n\\(L^* = \\{\\epsilon, ab, cd, abab, abcd, cdab, cdcd, ababab, \\ldots\\}\\)\n\\(L^+ = \\{ab, cd, abab, abcd, cdab, cdcd, ababab, \\ldots\\}\\)\nNovamente, \\(L^+ = L^* - \\{\\epsilon\\}\\)\n\nExemplo 3: \\(L = \\{\\epsilon, a\\}\\)\n\n\\(L^* = L^+ = \\{\\epsilon, a, aa, aaa, \\ldots\\} = \\{a^n \\mid n \\geq 0\\}\\)\nComo \\(\\epsilon \\in L\\), ambos os fechamentos são idênticos\n\n\n\n3.3.3.5.2 Propriedades Matemáticas Fundamentais\nOs fechamentos de Kleene satisfazem várias propriedades algébricas importantes:\n1. Idempotência do Fechamento de Kleene: \\[(L^*)^* = L^*\\]\nIntuição: Aplicar o fechamento de Kleene duas vezes não adiciona novos elementos. Se já temos todas as concatenações possíveis de \\(L\\), concatenar essas concatenações não produz nada novo.\n2. Inclusão da Linguagem Original: \\[L \\subseteq L^+ \\subseteq L^*\\]\n3. Elemento Neutro: \\[L^* = \\{\\epsilon\\} \\cup L^+\\]\n4. Relação com Concatenação: \\[L^* = \\{\\epsilon\\} \\cup L \\cdot L^*\\]\nEsta última propriedade fornece uma definição recursiva alternativa para o fechamento de Kleene, extremamente útil em demonstrações formais e construções de autômatos.\n\n\n3.3.3.5.3 Casos Especiais Importantes\nLinguagem Vazia: Para \\(L = \\emptyset\\): \\[\\emptyset^* = \\{\\epsilon\\}\\] \\[\\emptyset^+ = \\emptyset\\]\nJustificativa: Como não podemos escolher elementos de \\(\\emptyset\\), a única concatenação possível é a de zero elementos, resultando em \\(\\epsilon\\).\nLinguagem com string Vazia: Para \\(L = \\{\\epsilon\\}\\): \\[\\{\\epsilon\\}^* = \\{\\epsilon\\}\\] \\[\\{\\epsilon\\}^+ = \\{\\epsilon\\}\\]\nJustificativa: Concatenar \\(\\epsilon\\) consigo mesmo qualquer número de vezes sempre resulta em \\(\\epsilon\\).\n\n\n3.3.3.5.4 Aplicações Práticas em Ciência da Computação\n1. Análise Léxica: o padrão [a-zA-Z][a-zA-Z0-9]* para identificadores utiliza o fechamento de Kleene implicitamente, onde [a-zA-Z0-9]* representa \\(\\Sigma^*\\) para o alfabeto alfanumérico.\n2. Expressões Regulares em Editores de Texto: o operador * em expressões regulares corresponde diretamente ao fechamento de Kleene, permitindo buscar padrões com repetições arbitrárias.\n3. Protocolos de Comunicação: muitos protocolos definem mensagens como sequências de unidades básicas, onde o fechamento de Kleene modela a repetição arbitrária dessas unidades.\n4. Análise de Algoritmos: estruturas como loops infinitos ou processos iterativos são naturalmente modelados usando fechamentos de Kleene de operações básicas.\n\n\n3.3.3.5.5 Conexão com Autômatos Finitos\nO fechamento de Kleene possui uma interpretação natural em termos de autômatos finitos: \\(L^*\\) corresponde à linguagem aceita por um autômato que pode reiniciar infinitas vezes após aceitar uma string de \\(L\\). Esta conexão será fundamental quando explorarmos a construção de Thompson para converter expressões regulares em autômatos.\n\n\n3.3.3.5.6 Fechamento como Operação Universal\nUma perspectiva interessante é que o fechamento de Kleene pode ser visto como a menor solução para a equação: \\[X = \\{\\epsilon\\} \\cup L \\cdot X\\]\nEsta caracterização como ponto fixo mínimo conecta os fechamentos de Kleene com áreas avançadas da ciência da computação, incluindo semântica denotacional e análise de programas.\nO fechamento de Kleene, portanto, não é apenas uma operação matemática abstrata, mas uma ferramenta conceitual poderosa que captura a essência da computação iterativa e da geração de linguagens por meio de repetição controlada.\n\n\n\n3.3.3.6 Propriedades e Fechamento\nUma propriedade notável das Linguagens Regulares é que elas são fechadas sob ambas as operações:\nTeorema (Fechamento das Linguagens Regulares):\nSe \\(L_1\\) e \\(L_2\\) são Linguagens Regulares sobre um alfabeto \\(\\Sigma\\), então:\n\n\\(L_1 \\cap L_2\\) é uma Linguagem Regular\n\\(\\overline{L_1}\\) é uma Linguagem Regular\n\nEsta propriedade de fechamento é uma das características que tornam as Linguagens Regulares tão robustas e úteis na prática. Ela garante que operações complexas entre Linguagens Regulares sempre produzem linguagens que permanecem dentro da mesma classe de complexidade.\n\n\n\n3.3.4 As Leis de De Morgan em Linguagens Regulares\nAs Leis de De Morgan são um par de regras fundamentais da teoria dos conjuntos que se aplicam diretamente às linguagens formais, uma vez que linguagens são conjuntos de strings. Elas estabelecem uma relação poderosa entre as operações de união, interseção e complemento.\nAs leis são definidas da seguinte forma:\n\nO complemento da união é a interseção dos complementos: \\[ \\overline{L_1 \\cup L_2} = \\overline{L_1} \\cap \\overline{L_2} \\]\nO complemento da interseção é a união dos complementos: \\[ \\overline{L_1 \\cap L_2} = \\overline{L_1} \\cup \\overline{L_2} \\]\n\nA principal importância dessas leis reside na sua capacidade de simplificar e transformar especificações de linguagens, especialmente aquelas que envolvem condições negativas (negações). As Leis de De Morgan nos permitem reescrever uma condição complexa de uma forma diferente e, muitas vezes, mais fácil de entender ou construir. A utilidade dessas leis se manifesta em:\n\nExpressar Negações Complexas: as Leis de De Morgan permitem converter uma negação sobre uma operação complexa (como não ser (A ou B)) em operações mais simples sobre negações individuais (não ser A E não ser B).\nManipulação Algébrica: Assim como na álgebra tradicional, as leis permitem manipular formalmente as descrições de linguagens para otimizá-las ou provar equivalências.\n\nComo exemplo, se quisermos uma linguagem que descreva strings que não terminam em $01$ nem em $10$, estamos descrevendo uma condição $ \\neg (A \\lor B) $. Usando a primeira lei de De Morgan, podemos transformar isso em $ (\\neg A) \\land (\\neg B) $:\n\\[ \\overline{L_{01} \\cup L_{10}} = \\overline{L_{01}} \\cap \\overline{L_{10}} \\]\nIsso significa que podemos construir a linguagem procurando por strings que satisfaçam duas condições simultaneamente: não terminar em $01$ E não terminar em $10$.\nNo contexto das Linguagens Regulares, as Leis de De Morgan são mais uma ferramenta teórica do que uma sintaxe prática dentro das expressões regulares básicas. As operações de interseção (\\(`\\cap`\\)) e complemento (\\(`\\overline{L}`\\)) não possuem operadores diretos nas expressões regulares tradicionais (como $*$ para Kleene ou $ \\cup $ para união).\nO seu uso prático e teórico ocorre principalmente na teoria dos autômatos finitos:\n\nConstrução de Autômatos: Sabemos que as Linguagens Regulares são fechadas sob interseção e complemento. Isso significa que se temos autômatos para $L_1$ e $L_2$, podemos construir autômatos para $L_1 \\cap L_2$ e $ \\overline{L_1} $. As Leis de De Morgan garantem que podemos, por exemplo, construir um autômato para $ \\overline{L_1 \\cup L_2} $ construindo autômatos para $ \\overline{L_1} $ e $ \\overline{L_2} $ e depois aplicando o algoritmo de interseção sobre eles.\nProvas de Equivalência: As leis são usadas para provar que duas descrições de linguagens diferentes são, na verdade, equivalentes, o que é fundamental para a otimização de analisadores léxicos.\n\nEm resumo, embora você não escreva $ \\overline{L_1} \\cap \\overline{L_2} $ diretamente em uma expressão regular comum, as Leis de De Morgan são o fundamento matemático que garante que podemos construir uma máquina (um autômato finito) para reconhecer essa linguagem complexa, validando o poder e a robustez da classe das Linguagens Regulares.\n\n3.3.4.1 Limitações das Expressões Regulares Básicas\nÉ importante observar que, embora as Linguagens Regulares sejam fechadas sob interseção e complemento, essas operações não são diretamente expressáveis usando apenas as três operações fundamentais das expressões regulares (união \\(\\cup\\), concatenação \\(\\cdot\\), e fechamento de Kleene \\(*\\)).\nPara expressar interseções e complementos, frequentemente precisamos:\n\nConstruir autômatos finitos correspondentes às expressões regulares;\nAplicar algoritmos específicos para interseção e complemento de autômatos;\nConverter o resultado de volta para uma expressão regular sempre que possível.\n\nEste é um exemplo de como a teoria matemática subjacente (autômatos finitos) pode ser mais expressiva que a notação conveniente (expressões regulares) para certas operações, embora ambas representem exatamente a mesma classe de linguagens.\n\n\n\n3.3.5 Exercícios 3\n\nSejam \\(L_1 = \\{a, ab, b\\}\\) e \\(L_2 = \\{b, ba, \\epsilon\\}\\). Calcule:\n\n\\(L_1 \\cup L_2\\);\n\\(L_1 \\cap L_2\\);\n\\(L_1 - L_2\\) (diferença);\n\\(|L_1 \\cup L_2|\\) e \\(|L_1 \\cap L_2|\\).\n\nPara as linguagens \\(L_1 = \\{a, bb\\}\\) e \\(L_2 = \\{c, dd\\}\\):\n\nCalcule \\(L_1 \\cdot L_2\\) e \\(L_2 \\cdot L_1\\);\nDetermine \\(|L_1 \\cdot L_2|\\);\nVerifique se \\(L_1 \\cdot L_2 = L_2 \\cdot L_1\\).\n\nSeja \\(L = \\{a, b\\}\\). Determine:\n\n\\(L^0\\), \\(L^1\\), \\(L^2\\);\n\\(|L^n|\\) em função de \\(n\\);\nAs três primeiras strings em ordem lexicográfica de \\(L^3\\).\n\nPara \\(L = \\{ab\\}\\):\n\nListe os elementos de \\(L^*\\) até strings de comprimento 6;\nDetermine \\(L^+\\);\nVerifique se \\(\\epsilon \\in L^*\\) e se \\(\\epsilon \\in L^+\\).\n\nSeja \\(L = \\{a\\}\\). Prove ou refute:\n\n\\(L^* = L^+\\);\n\\(L^* \\cup L^+ = L^*\\);\n\\((L^*)^* = L^*\\);\nSe \\(\\epsilon \\in L\\), então \\(L^+ = L^*\\).",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Alfabetos, Linguagens e Strings: Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "01a-lexico.html#expressões-regulares-uma-notação-concisa",
    "href": "01a-lexico.html#expressões-regulares-uma-notação-concisa",
    "title": "3  Alfabetos, Linguagens e Strings: Fundamentos Matemáticos",
    "section": "3.4 Expressões Regulares: Uma Notação Concisa",
    "text": "3.4 Expressões Regulares: Uma Notação Concisa\nAs expressões regulares fornecem uma notação algébrica concisa e poderosa para descrever linguagens. Esta notação, introduzida por Stephen Kleene, como vimos na Section 2.1.2, permite especificar conjuntos potencialmente infinitos de strings por meio de padrões finitos.\n\n3.4.1 Definição Indutiva\nA definição de uma expressão regular sobre um alfabeto \\(\\Sigma\\) possui uma natureza indutiva, o que significa que ela é construída progressivamente, de elementos simples para estruturas complexas. Primeiramente, definimos as expressões regulares mais elementares, que servem como nossos casos base. Em seguida, estabelecemos as regras de construção, ou casos indutivos, que nos permitem combinar expressões existentes para formar novas:\nCasos base:\n\n\\(\\emptyset\\) é uma expressão regular que denota a linguagem vazia;\n\\(\\epsilon\\) é uma expressão regular que denota a linguagem \\(\\{\\epsilon\\}\\);\nPara cada \\(a \\in \\Sigma\\), \\(a\\) é uma expressão regular que denota a linguagem \\(\\{a\\}\\).\n\nCasos indutivos: Se \\(r\\) e \\(s\\) são expressões regulares, então:\n\n\\((r \\cup s)\\) é uma expressão regular (união);\n\\((r \\cdot s)\\) ou simplesmente \\((rs)\\) é uma expressão regular (concatenação);\n\\((r^*)\\) é uma expressão regular (fechamento de Kleene).\n\n\n\n3.4.2 Precedência dos Operadores\nDe forma análoga à precedência de operadores na aritmética, onde a multiplicação (como em \\(5 \\cdot 2\\)) é executada antes da adição (como em \\(3 + 5\\)), as operações em expressões regulares também seguem uma hierarquia de prioridade. Esta convenção é adotada para simplificar a escrita e reduzir o uso excessivo de parênteses, tornando as expressões mais limpas e legíveis. A ordem de precedência, da maior para a menor, é a seguinte:\n\nFechamento de Kleene (\\(*\\)) - maior precedência\nConcatenação (\\(\\cdot\\)) - precedência média\nUnião (\\(\\cup\\)) - menor precedência\n\nTomemos como exemplo a expressão \\(ab^* \\cup c\\). Para interpretá-la, aplicamos a hierarquia de precedência em etapas:\n\nPrimeiro, o operador de maior prioridade, o Fechamento de Kleene (\\(*\\)), é aplicado ao seu argumento imediato à esquerda, \\(b\\), resultando em (b*).\nEm seguida, a concatenação (que tem precedência sobre a união) liga o a ao resultado anterior, formando (a(b*)).\nPor fim, o operador de menor prioridade, a união (\\(\\cup\\)), combina os termos adjacentes, levando à interpretação final e inequívoca: \\(((a(b^*)) \\cup c)\\).\n\n\n\n3.4.3 Linguagem Denotada por uma Expressão Regular\nAté agora, definimos uma expressão regular \\(r\\) como uma sequência de símbolos, ou seja, uma construção puramente sintática. Agora, precisamos lhe atribuir um significado (semântica), definindo precisamente qual linguagem ela descreve. Para isso, introduzimos a notação \\(L(r)\\) para representar a linguagem denotada por \\(r\\). A definição de \\(L(r)\\) é indutiva, espelhando perfeitamente a estrutura usada para definir a sintaxe de uma expressão regular:\nCasos base:\n\n\\(L(\\emptyset) = \\emptyset\\)\n\\(L(\\epsilon) = \\{\\epsilon\\}\\)\n\\(L(a) = \\{a\\}\\) para \\(a \\in \\Sigma\\)\n\nCasos indutivos:\n\n\\(L(r \\cup s) = L(r) \\cup L(s)\\)\n\\(L(rs) = L(r) \\cdot L(s)\\)\n\\(L(r^*) = (L(r))^*\\)\n\nEste processo de definição nos permite computar a linguagem de qualquer expressão. Por exemplo, para decodificar \\(r = (a \\cup b)c\\), aplicamos as regras indutivamente: partindo dos casos base \\(L(a)=\\{a\\}\\), \\(L(b)=\\{b\\}\\) e \\(L(c)=\\{c\\}\\), usamos a regra da união para obter \\(L(a \\cup b) = \\{a, b\\}\\). Por fim, a regra da concatenação nos dá o resultado: \\(L(r) = L(a \\cup b) \\cdot L(c) = \\{a, b\\} \\cdot \\{c\\} = \\{ac, bc\\}\\).\n\n3.4.3.1 Exercícios 4\n\nDetermine a linguagem denotada pelas seguintes expressões regulares:\n\n\\(r_1 = a \\cup b\\);\n\\(r_2 = (a \\cup b)(a \\cup b)\\);\n\\(r_3 = a^*b\\);\n\\(r_4 = (ab)^*\\).\n\nReescreva as seguintes expressões com parênteses explícitos, respeitando a precedência:\n\n\\(ab^* \\cup c\\);\n\\(a \\cup bc^*\\);\n\\(ab \\cup cd^*e\\);\n\\(a^*b^* \\cup c^*\\).\n\nPara cada expressão regular, determine se as strings dadas pertencem à linguagem:\n\n\\(r = a^*ba^*\\): strings \\(\\{ab, ba, aba, baa, bb\\}\\);\n\\(r = (a \\cup b)^*b\\): strings \\(\\{b, ab, ba, abb, bbb\\}\\).\n\nUse a definição indutiva de \\(L(r)\\) para calcular \\(L((a \\cup b)c)\\):\n\nIdentifique os casos base aplicáveis;\nAplique as regras indutivas passo a passo;\nApresente o resultado.\n\nConstrua expressões regulares que denotem as seguintes linguagens:\n\n\\(L_1 = \\{a, b, aa, bb\\}\\);\n\\(L_2 = \\{\\epsilon, a, aa, aaa\\}\\);\n\\(L_3 = \\{w \\in \\{a,b\\}^* \\mid w \\text{ termina com } a\\}\\).\n\n\n\n\n\n3.4.4 Exemplos de Expressões Regulares\nCom os fundamentos teóricos e a semântica de \\(L(r)\\) estabelecidos, a melhor maneira de permitir que a esforçada leitora crie uma intuição sobre o poder expressivo das expressões regulares é por meio da análise de exemplos práticos. Os casos a seguir ilustram como padrões textuais concisos podem descrever com precisão linguagens complexas e, frequentemente, infinitas. Cada exemplo serve para solidificar a conexão entre a notação abstrata e os conjuntos de strings concretos que ela representa.\n\nExemplo 1: strings binárias terminando em \\(01\\) \\[r_1 = (0 \\cup 1)^*01\\] \\[L(r_1) = \\{01, 001, 101, 0001, 0101, 1001, 1101, \\ldots\\}\\]\nAnálise: Esta expressão é dividida em duas partes. A primeira, $(0 \\cup 1)^*$, gera qualquer sequência possível de ’0’s e ’1’s de qualquer comprimento, incluindo a string vazia \\(\\epsilon\\). A segunda parte, 01, é uma string literal. Ao concatenar as duas, forçamos que qualquer string gerada pela primeira parte seja seguida por 01, garantindo assim a terminação desejada.\n\\(L(r_1)\\) contém: \\(\\{01, 101, 001, 11101, \\ldots\\}\\). \\(L(r_1)\\) não contém: \\(\\{\\epsilon, 0, 1, 10, 010, \\ldots\\}\\).\nExemplo 2: strings sobre \\(\\{a, b\\}\\) com número par de \\(a\\)’s \\[r_2 = b^*(ab^*ab^*)^*\\]\nAnálise: esta é uma construção elegante. A parte central é $(ab^*ab^*)$. Observe que dentro dos parênteses existem exatamente dois \\(a\\)’s. Os \\(b^*\\)’s permitem que qualquer número de \\(b\\)’s apareçam antes, entre e depois desses \\(a\\)’s. O Fechamento de Kleene externo, $(...)^*, permite que este bloco contendo dois \\(a\\)’s se repita zero ou mais vezes. Se o bloco se repete \\(k\\) vezes, o número total de \\(a\\)’s será \\(2k\\), que é sempre um número par (0, 2, 4, …). O \\(b^*\\) no início permite que a string comece com \\(b\\)’s ou, caso o bloco não se repita nenhuma vez, gere strings compostas apenas por \\(b\\)’s (que possuem zero \\(a\\)’s, e zero é par).\n\\(L(r_2)\\) contém: \\(\\{b, bb, abab, aab, baab, bbaabb, \\ldots\\}\\). \\(L(r_2)\\) não contém: \\(\\{a, bbbab, aaabb, \\ldots\\}\\).\nExemplo 3: Identificadores que começam com letra \\[r_3 = (a \\cup b \\cup \\ldots \\cup z)(a \\cup b \\cup \\ldots \\cup z \\cup 0 \\cup 1 \\cup \\ldots \\cup 9)^*\\]\nAnálise: Esta expressão regular define uma regra clássica para identificadores (nomes de variáveis, funções, etc.) em muitas linguagens de programação. Ela é composta por duas partes concatenadas que impõem uma estrutura rígida:\n\nA primeira parte, $(a \\cup b \\cup \\ldots \\cup z)$, estabelece a condição para o caractere inicial. Ela exige que a string comece com exatamente uma letra minúscula.\nA segunda parte, $(a \\cup b \\cup \\ldots \\cup z \\cup 0 \\cup 1 \\cup \\ldots \\cup 9)^*$, define os caracteres subsequentes. O Fechamento de Kleene ($*$) permite que o caractere inicial seja seguido por uma sequência de zero ou mais caracteres, que podem ser letras minúsculas ou dígitos.\n\n\\(L(r_3)\\) contém: {x, nome, var1, contador, a1b2c3, ...}. \\(L(r_3)\\) não contém: {1var, _nome, temp-1, $var, ...}.\nExemplo 4: Números inteiros com sinal opcional \\[r_4 = (\\epsilon \\cup + \\cup -)(0 \\cup 1 \\cup \\ldots \\cup 9)(0 \\cup 1 \\cup \\ldots \\cup 9)^*\\]\nAnálise: Esta expressão é construída em três partes lógicas para capturar a estrutura de um número inteiro:\n\n$(\\epsilon \\cup + \\cup -)$: A primeira parte define o prefixo do número. A união com a string vazia ($\\epsilon$) torna o caractere de sinal (+ ou -) opcional. Isso permite que o número comece diretamente com um dígito.\n$(0 \\cup 1 \\cup \\ldots \\cup 9)$: Esta parte garante que, após o sinal (ou a ausência dele), exista pelo menos um dígito. Isso é fundamental para invalidar strings que contenham apenas um sinal, como + ou -.\n$(0 \\cup 1 \\cup \\ldots \\cup 9)^*: O Fechamento de Kleene na parte final permite que este primeiro dígito seja seguido por uma sequência de zero ou mais dígitos adicionais, formando números de qualquer comprimento.\n\n\\(L(r_4)\\) contém: {42, -199, +7, 0, 9, 007, ...}. \\(L(r_4)\\) não contém: {\\epsilon, +, -, --5, 1+1, 9A, ...}.\nNota sobre a limitação: é importante que a atenta leitora note que esta expressão regular, embora funcional, aceita números com zeros à esquerda (como 007), o que pode não ser desejável em todos os contextos de programação ou validação. Expressões mais complexas podem ser criadas para proibir essa característica, por exemplo, tratando o 0 como um caso especial separado de números que começam com $[1-9]$.\n\nEsses exemplos demonstram a notável versatilidade das expressões regulares. Com apenas três operações fundamentais — união, concatenação e Fechamento de Kleene — somos capazes de descrever uma vasta gama de padrões, desde sequências simples e finitas até conjuntos infinitos com regras estruturais complexas. Fica evidente como essa ferramenta se torna indispensável na análise léxica, validação de dados e em inúmeras outras tarefas da computação. Contudo, apesar de seu poder, veremos adiante que existem linguagens, até mesmo algumas com descrições aparentemente simples, que transcendem a capacidade expressiva das expressões regulares.\n\n3.4.4.1 Exercícios 5\n\nBaseando-se no exemplo de strings terminando em \\(01\\):\n\nConstrua uma expressão para strings terminando em \\(10\\);\nConstrua uma expressão para strings começando com \\(01\\);\nConstrua uma expressão para strings que contêm \\(01\\) como substring;\nConstrua uma expressão para strings que não contêm \\(01\\).\n\nInspirando-se no exemplo de número par de \\(a\\)’s:\n\nConstrua uma expressão para strings com número ímpar de \\(a\\)’s sobre \\(\\{a,b\\}\\);\nConstrua uma expressão para strings com número múltiplo de 3 de \\(a\\)’s;\nConstrua uma expressão para strings com pelo menos dois \\(a\\)’s.\n\nBaseando-se no padrão de identificadores:\n\nModifique para permitir underscores em qualquer posição;\nModifique para proibir dígitos na primeira e última posições;\nCrie um padrão para identificadores que devem ter entre 3 e 8 caracteres.\n\nEstendendo o exemplo de números inteiros:\n\nConstrua uma expressão para números decimais (com ponto decimal);\nConstrua uma expressão para números em notação científica simples (\\(1e5\\), \\(2e-3\\));\nConstrua uma expressão para números hexadecimais com prefixo \\(0x\\).\n\nPara as expressões construídas nos exercícios anteriores:\n\nVerifique se as strings \\(\\{101, 1010, 0101\\}\\) pertencem ao padrão termina em 10;\nVerifique se \\(\\{aab, baba, ababa\\}\\) têm número ímpar de \\(a\\)’s;\nTeste se \\(\\{var_1, _temp, item2_\\}\\) são identificadores válidos com underscores.\n\n\n\n\n\n3.4.5 Equivalência de Expressões Regulares e suas Leis Algébricas\nAo explorarmos o poder das expressões regulares, a atenta leitora descobrirá que diferentes expressões podem, na verdade, descrever exatamente a mesma linguagem. De forma análoga à álgebra tradicional, onde \\(x(y+z)\\) e \\(xy + xz\\) são formulações distintas para o mesmo resultado, na teoria das linguagens podemos ter duas expressões sintaticamente diferentes que são semanticamente idênticas. Essa noção de equivalência não é apenas uma curiosidade teórica; ela é fundamental para a otimização e simplificação de padrões, permitindo-nos encontrar a representação mais concisa ou computacionalmente mais eficiente para uma dada linguagem.\nA perspicaz leitora perceberá que a capacidade de manipular expressões e transformá-las em formas equivalentes, porém mais simples, é uma habilidade poderosa, especialmente na construção de analisadores léxicos e ferramentas de processamento de texto.\nFormalmente, duas expressões regulares \\(r\\) e \\(s\\) são ditas equivalentes, o que denotamos por \\(r \\equiv s\\), se, e somente se, elas denotam a mesma linguagem. Matematicamente, isso é expresso como:\n\\[r \\equiv s \\iff L(r) = L(s)\\]\nPara manipular e simplificar expressões regulares, contamos com um conjunto de leis algébricas que governam as operações de união, concatenação e fechamento de Kleene. Estas leis são a base para a otimização de padrões.\n\n3.4.5.1 Principais Leis Algébricas\nA seguir, apresentamos as identidades mais importantes que as expressões regulares satisfazem. Para expressões regulares \\(r\\), \\(s\\) e \\(t\\):\n\n3.4.5.1.1 Leis Associativas e Comutativas\nEstas leis nos permitem reagrupar e reordenar os termos em operações de união e concatenação.\n\nComutatividade da União: a ordem na união não importa. \\[r \\cup s \\equiv s \\cup r\\]\nAssociatividade da União: podemos agrupar uniões de qualquer forma. \\[(r \\cup s) \\cup t \\equiv r \\cup (s \\cup t)\\]\nAssociatividade da Concatenação: o agrupamento na concatenação também é flexível. Lembre-se, no entanto, que a concatenação não é comutativa (\\(rs \\not\\equiv sr\\) em geral). \\[(rs)t \\equiv r(st)\\]\n\n\n\n3.4.5.1.2 Leis de Identidade e Anulação\nEstas leis definem o papel dos elementos especiais \\(\\epsilon\\) (a string vazia) e \\(\\emptyset\\) (a linguagem vazia).\n\nElemento Neutro da Concatenação: A string vazia é o elemento neutro da concatenação. \\[r\\epsilon \\equiv \\epsilon r \\equiv r\\]\nElemento Anulador da Concatenação: Concatenar com a linguagem vazia resulta na linguagem vazia. \\[r\\emptyset \\equiv \\emptyset r \\equiv \\emptyset\\]\nElemento Neutro da União: A linguagem vazia é o elemento neutro da união. \\[r \\cup \\emptyset \\equiv \\emptyset \\cup r \\equiv r\\]\n\n\n\n3.4.5.1.3 Lei Distributiva\nEsta lei conecta as operações de concatenação e união, de forma muito semelhante à álgebra numérica.\n\nDistributividade da Concatenação sobre a União: \\[r(s \\cup t) \\equiv rs \\cup rt\\] \\[(s \\cup t)r \\equiv sr \\cup tr\\]\n\n\n\n3.4.5.1.4 Lei da Idempotência\nEsta lei estabelece que a união de uma expressão com ela mesma não adiciona nada novo.\n\nIdempotência da União: \\[r \\cup r \\equiv r\\]\n\n\n\n3.4.5.1.5 Leis do Fechamento de Kleene\nEstas propriedades definem a natureza do operador de fechamento.\n\nDefinição Recursiva: o fechamento de \\(r\\) é a string vazia ou um \\(r\\) seguido por mais r’s. \\[r^* \\equiv \\epsilon \\cup rr^*\\]\n\n2.Fechamento do Fechamento: aplicar o fechamento duas vezes é redundante.\n$$(r^*)^* \\equiv r^*$$\n\nFechamento da Linguagem Vazia: A única string que pode ser formada por zero ou mais escolhas da linguagem vazia é a string vazia.\n\\[\\emptyset^* \\equiv \\epsilon\\]\nFechamento da string Vazia: o mesmo se aplica à linguagem contendo apenas a string vazia.\n\\[\\epsilon^* \\equiv \\epsilon\\]\n\n\n\n\n3.4.5.2 Exemplo Prático de Simplificação\nPara que a atenta leitora possa ver a utilidade dessas leis em ação, vamos simplificar a expressão regular \\(r = a(b \\cup c) \\cup ab\\). Nosso objetivo é encontrar uma expressão equivalente que seja mais curta.\n\nExpressão Inicial: \\(r = a(b \\cup c) \\cup ab\\)\nAplicar a Distributividade: usamos a lei distributiva à esquerda no termo \\(a(b \\cup c)\\).\n\\[r \\equiv (ab \\cup ac) \\cup ab\\]\nAplicar a Comutatividade: reordenamos os termos da união para agrupar os termos idênticos.\n\\[r \\equiv ab \\cup ab \\cup ac\\]\nAplicar a Idempotência: a união de \\(ab\\) com \\(ab\\) é simplesmente \\(ab\\).\n\\[r \\equiv ab \\cup ac\\]\nChegamos à expressão: \\(s = ab \\cup ac\\). Como \\(L(r) = L(s)\\).\n\nAs expressões são equivalentes (\\(a(b \\cup c) \\cup ab \\equiv ab \\cup ac\\)), mas a segunda é visivelmente mais simples. Essa capacidade de simplificação tem valor prático no projeto de compiladores e em sistemas de busca de texto.\n\n\n3.4.5.3 Exercícios 6\n\nUse as leis algébricas para simplificar:\n\n\\((a \\cup \\emptyset)b\\);\n\\(a(\\epsilon \\cup b)\\);\n\\((a \\cup a)^*\\);\n\\(a \\cup ab^*a\\).\n\nSimplifique a expressão \\(((a \\cup b)a) \\cup (aa)\\) usando as leis passo a passo:\n\nIdentifique que leis podem ser aplicadas;\nMostre cada passo da simplificação;\nVerifique o resultado testando strings específicas.\n\nProve que as seguintes expressões são equivalentes:\n\n\\(a^*a\\) e \\(aa^*\\);\n\\((a \\cup b)^*\\) e \\(\\epsilon \\cup (a \\cup b)(a \\cup b)^*\\);\n\\(a^*b^*\\) e \\((a \\cup b)^*\\) (esta é falsa - encontre um contraexemplo).\n\nSimplifique usando as leis do fechamento:\n\n\\((a^*)^*\\);\n\\(\\epsilon^* \\cup a^*\\);\n\\(\\emptyset^* \\cup a\\);\n\\((a \\cup \\epsilon)^*\\).\n\nPara a expressão \\(ab^* \\cup abb^* \\cup abbb^*\\):\n\nIdentifique o padrão comum;\nUse a distributividade para fatorar;\nSimplifique usando propriedades do fechamento de Kleene;\nVerifique que as linguagens são idênticas.\n\n\n\n\n\n3.4.6 Notações Convencionais Adicionais\nEmbora as três operações fundamentais, união (\\(\\cup\\)), concatenação (\\(\\cdot\\)) e fechamento de Kleene (\\(*\\)), sejam teoricamente suficientes para descrever qualquer Linguagem Regular, na prática, elas podem gerar expressões longas, repetitivas e de difícil leitura. Para contornar essa complexidade, foram introduzidas diversas notações adicionais que funcionam como abreviações ou macros.\nA criativa leitora pode enxergar estas notações como funções ou módulos predefinidos: elas não adicionam um novo poder teórico ao formalismo, mas aumentam a expressividade e a conveniência da escrita, permitindo construir padrões complexos de forma mais limpa e intuitiva. Dominar estas abreviações é um passo importante para escrever expressões regulares eficazes no mundo real.\nA seguir, detalhamos as notações mais comuns.\n\nFechamento Positivo (\\(r^+\\))\n\nDefinição: É uma abreviação para uma ou mais ocorrências de \\(r\\). Formalmente, \\(r^+ \\equiv rr^*\\).\nAnálise: Enquanto \\(r^*\\) corresponde a zero ou mais repetições, \\(r^+\\) exige que o padrão ocorra pelo menos uma vez. É uma das abreviações mais utilizadas.\n\n\nExemplo: Para descrever números inteiros positivos, podemos usar a expressão \\([1-9][0-9]^*\\). Usando o fechamento positivo, a expressão para um ou mais dígitos, \\([0-9]^+\\), pode ser mais intuitiva em certos contextos, embora a primeira seja mais precisa para evitar zeros à esquerda. Um exemplo mais direto é \\(a^+\\), que denota a linguagem \\(\\{a, aa, aaa, \\ldots\\}\\), sendo mais concisa que \\(aa^*\\).\n\nOpcionalidade (\\(r?\\))\nDefinição: Indica que a expressão \\(r\\) é opcional, podendo aparecer uma ou nenhuma vez. É um atalho para \\((\\epsilon \\cup r)\\). Análise: Esta notação é perfeita para partes de um padrão que podem ou não estar presentes.\nExemplo: Para validar URLs que podem ser http ou https (com ‘s’ opcional), usamos a expressão https?. Ela corresponde a http ou https. Outro exemplo seria modelar um número com sinal opcional: \\((+ \\cup -)? [0-9]^+\\).\nClasses de Caracteres (\\([\\ldots]\\))\nDefinição: Funcionam como uma abreviação para uma união de múltiplos caracteres. Por exemplo, \\([abc] \\equiv (a \\cup b \\cup c)\\). Análise: Tornam a expressão muito mais compacta quando precisamos permitir um de vários caracteres possíveis em uma determinada posição.\nExemplo: Para encontrar qualquer vogal minúscula, em vez de escrever \\((a \\cup e \\cup i \\cup o \\cup u)\\), podemos simplesmente usar \\([aeiou]\\).\nIntervalos em Classes de Caracteres (\\([a-z]\\))\nDefinição: Dentro de uma classe de caracteres, o hífen - pode ser usado para denotar um intervalo de símbolos com base em uma ordem convencional (como a da tabela ASCII). Análise: Esta é uma generalização poderosa das classes de caracteres, evitando a necessidade de listar todos os símbolos individualmente.\nExemplo: Para descrever uma letra minúscula qualquer, usamos \\([a-z]\\). Para um dígito hexadecimal, podemos combinar intervalos: \\([0-9a-fA-F]\\).\nNegação de Classes de Caracteres (\\([^\\ldots]\\))\nDefinição: O acento circunflexo ^, quando é o primeiro símbolo dentro de uma classe, nega o conjunto. A classe passa a corresponder a qualquer caractere do alfabeto \\(\\Sigma\\) exceto os que estão listados. Análise: É útil para especificar proibições, ou seja, tudo, exceto um pequeno conjunto de caracteres.\nExemplo: Uma expressão para encontrar uma string que não contenha vogais poderia usar \\([^aeiou]\\). Para encontrar um caractere que não é um dígito, usamos \\([^0-9]\\).\nQuantificadores de Repetição (\\(\\{n, m\\}\\))\nDefinição: Oferecem um controle preciso sobre o número de repetições de uma expressão \\(r\\). Análise: Generalizam as operações \\(?\\), \\(*\\) e \\(+\\), permitindo especificar limites exatos, mínimos ou intervalos de ocorrências.\n\n\\(r\\{n\\}\\): \\(r\\) repetido exatamente \\(n\\) vezes.\n\\(r\\{n,m\\}\\): \\(r\\) repetido no mínimo \\(n\\) e no máximo \\(m\\) vezes.\n\\(r\\{n,\\}\\): \\(r\\) repetido pelo menos \\(n\\) vezes.\n\nExemplos:\n\nCEP Brasileiro: Um CEP no formato XXXXX-XXX pode ser descrito por \\([0-9]\\{5\\}-[0-9]\\{3\\}\\).\nValidade de Senha: Uma regra de senha que exige de \\(8\\) a \\(16\\) caracteres alfanuméricos pode ser modelada por \\([a-zA-Z0-9]\\{8,16\\}\\).\nIdentificador Mínimo: Um nome de variável que precisa ter pelo menos \\(3\\) caracteres, começando com uma letra e seguido por letras ou números, pode ser escrito como \\([a-zA-Z][a-zA-Z0-9]\\{2,\\}\\).\n\n\n\n3.4.6.1 Exercícios 7\n\nReescreva usando apenas união (\\(\\cup\\)), concatenação e fechamento de Kleene (\\(*\\)):\n\n\\(a^+\\);\n\\(b?\\);\n\\([abc]\\);\n\\(a\\{3\\}\\);\n\\(b\\{2,4\\}\\).\n\nConstrua expressões usando classes de caracteres para:\n\nQualquer dígito: \\([0-9]\\);\nQualquer letra minúscula: \\([a-z]\\);\nQualquer caractere que não seja espaço: \\([^ ]\\);\nQualquer caractere alfanumérico: \\([a-zA-Z0-9]\\).\n\nUse quantificadores para construir padrões para:\n\nCEP brasileiro no formato \\(99999-999\\);\nPlaca de carro brasileira antiga \\(AAA-9999\\);\nSenha com exatamente 8 caracteres alfanuméricos;\nCódigo de área de telefone com 2 ou 3 dígitos.\n\nConstrua expressões regulares para validar:\n\nURL simples começando com \\(http\\) ou \\(https\\);\nData no formato \\(dd/mm/aaaa\\) (versão simples);\nHorário no formato \\(hh:mm\\) (24 horas);\nNúmero de CPF no formato \\(999.999.999-99\\).\n\nReescreva as seguintes expressões de forma mais concisa:\n\n\\((a \\cup b \\cup c \\cup d)(a \\cup b \\cup c \\cup d)^*\\);\n\\(a(\\epsilon \\cup b)\\);\n\\((0 \\cup 1 \\cup 2 \\cup 3 \\cup 4 \\cup 5 \\cup 6 \\cup 7 \\cup 8 \\cup 9)(0 \\cup 1 \\cup 2 \\cup 3 \\cup 4 \\cup 5 \\cup 6 \\cup 7 \\cup 8 \\cup 9)^*\\);\n\n\n\n\n\n3.4.7 Aplicações Práticas\nApós a jornada pelos fundamentos matemáticos, a curiosa leitora pode estar se perguntando: onde essa teoria se manifesta no dia a dia da computação? A resposta curta é: em praticamente todos os lugares. As expressões regulares representam um dos casos mais bem-sucedidos de uma teoria matemática que transcendeu a academia e se tornou uma ferramenta indispensável, utilizada diariamente por desenvolvedores de software, administradores de sistemas e cientistas de dados.\nPara a resposta longa precisamos abandonar o formalismo puro para ilustrar como as expressões regulares, com suas notações concisas, resolvem problemas concretos.\n\n\n\n\n\n\nDo Formalismo à Prática: Expressões Regulares no Mundo Real\n\n\n\nA leitora atenta deve notar que as expressões regulares definidas formalmente neste capítulo, baseadas apenas em união, concatenação e Fechamento de Kleene, são o alicerce matemático da teoria. No entanto, as ferramentas que usamos no dia a dia, conhecidas como regex engines, são equipadas com funções que vão além dessa definição.\nEsses engines, como o PCRE (Perl Compatible Regular Expressions), que influencia as implementações em linguagens como Python, PHP e JavaScript, estendem a notação formal com recursos para maior conveniência e poder expressivo. Os mais notáveis são:\n\nLookarounds (lookaheads e lookbehinds): Permitem verificar a existência de um padrão antes (lookbehind) ou depois (lookahead) da posição atual, sem consumir os caracteres verificados (ou seja, sem incluí-los no resultado da captura). Um exemplo clássico é a expressão q(?=u), que encontra a letra q apenas se ela for imediatamente seguida pela letra u, mas sem incluir o u no resultado.\nBackreferences (Retrovisores): Permitem referenciar um grupo que foi previamente capturado dentro da mesma expressão. Por exemplo, a expressão (\\w)\\1 encontra qualquer caractere alfanumérico (\\w) que é seguido imediatamente por ele mesmo. É com esta capacidade que um motor de regex pode reconhecer a linguagem não-regular \\(L = \\{ww \\mid w \\in \\{a,b\\}^*\\}\\).\n\nA consequência mais importante é que esses sistemas de regex modernos podem reconhecer linguagens que não são regulares no sentido estrito da teoria. Enquanto uma expressão regular formal jamais poderia reconhecer a linguagem \\(L = \\{a^n b^n \\mid n \\geq 1\\}\\), as extensões práticas oferecem um poder que transcende os limites dos Autômatos Finitos.\nPortanto, é valioso distinguir entre a classe teórica das Linguagens Regulares e o conjunto de padrões que as ferramentas de software podem processar. O formalismo discutido neste livro é a base para entender o poder, as limitações e a complexidade computacional por trás dessas operações, enquanto as extensões práticas oferecem conveniência e expressividade para resolver problemas do mundo real.\n\n\n\n3.4.7.1 Análise Léxica em Compiladores\nA análise léxica é a primeira fase da compilação de um programa. O analisador, chamado de scanner ou lexer, lê o código-fonte como uma sequência de caracteres e a converte em uma sequência de tokens, unidades lexicais como identificadores, palavras-chave, números e operadores. Cada tipo de token é definido precisamente por um padrão, que é, em sua essência, uma expressão regular.\nExemplo 1: identificadores: nomes de variáveis, funções, etc..\nPadrão: [a-zA-Z_][a-zA-Z0-9_]*. Análise: Este padrão decreta que um identificador deve começar com uma letra (maiúscula ou minúscula) ou um underscore ([a-zA-Z_]), seguido por zero ou mais caracteres que podem ser letras, números ou underscores ([a-zA-Z0-9_]*).\nExemplo 2: números inteiros:\nPadrão: -?[1-9][0-9]*|0. Análise: Este padrão elegante lida com vários casos. O -? torna o sinal de negativo opcional. O trecho [1-9][0-9]* garante que números com múltiplos dígitos não comecem com zero (como 042). Por fim, o |0 trata o número zero como um caso especial.\n\n\n3.4.7.2 Validação de Dados de Entrada\nEm qualquer aplicação que receba dados de um usuário, é vital garantir que esses dados estejam no formato correto antes de serem processados. As expressões regulares são a ferramenta padrão para essa tarefa de validação.\nExemplo 1: endereço de e-mail (Muito Simplificado):\nPadrão: [a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,} Análise: Embora um padrão de e-mail 100% compatível com as RFCs seja extremamente complexo, esta versão simplificada cobre a maioria dos casos. Ela busca: uma sequência de caracteres de usuário ([...]+), o símbolo @, uma sequência de caracteres de domínio ([...]+), um ponto literal (\\.) e, por fim, o domínio de topo (TLD) com pelo menos duas letras ([a-zA-Z]{2,}).\n\n\n3.4.7.3 Telefone (Formato Brasileiro)\nPadrão: \\([0-9]{2}\\) ?[0-9]{4,5}-[0-9]{4} Análise: Este padrão modela um número de telefone comum no Brasil, incluindo o DDD entre parênteses e um espaço opcional e pode ser dividido nas seguintes partes:\n\n\\( e \\): Correspondem aos parênteses literais. Como ( e ) são metacaracteres em expressões regulares, eles precisam ser escapados com uma barra invertida para serem tratados como caracteres literais;\n[0-9]{2}: Exige exatamente dois dígitos numéricos para o código de área (DDD);\n?: Permite a existência opcional de um único caractere de espaço após os parênteses do DDD;\n[0-9]{4,5}: Captura a primeira parte do número, que pode ter 4 ou 5 dígitos, acomodando tanto números fixos quanto números móveis que já adotaram o nono dígito;\n-[0-9]{4}: Corresponde ao hífen literal e aos 4 dígitos finais do número.\n\n\n\n3.4.7.4 Busca, Extração e Substituição de Texto\nEsta é talvez a aplicação mais visível das expressões regulares, formando o coração de ferramentas como grep e sed em sistemas Unix, e as funcionalidades de Localizar e Substituir em editores de texto e ambientes integrados de edição (IDEs).\nExemplo 1: busca por Palavras Exatas:\nPadrão: \\b[Pp]alavra\\b Análise: O \\b é uma âncora que corresponde a uma fronteira de palavra (word boundary). Isso garante que a busca encontre palavra como uma palavra inteira, e não como parte de subpalavra. O [Pp] torna a busca insensível a maiúsculas para a primeira letra.\n\n\n3.4.7.5 Hora (Formato 24h: HH:MM ou HH:MM:SS)\nPadrão: ([01][0-9]|2[0-3]):[0-5][0-9](:[0-5][0-9])? Análise: Este é um excelente exemplo de como expressões regulares podem validar formatos com regras numéricas. Ele valida horas no formato 24h, com segundos opcionais e pode ser dividido em partes:\n\n([01][0-9]|2[0-3]): Esta parte valida as horas. A união | cria duas possibilidades: ou um dígito de 0 ou 1 seguido por qualquer dígito (00-19), ou o dígito 2 seguido por um dígito de 0 a 3 (20-23).\n:: Corresponde ao separador literal.\n[0-5][0-9]: Valida os minutos, garantindo que estejam no intervalo de 00 a 59.\n(:[0-5][0-9])?: Esta parte torna os segundos opcionais. Os parênteses agrupam o separador : e os dígitos dos segundos. O quantificador ? aplicado a este grupo faz com que toda a seção de segundos (:SS) possa aparecer uma ou nenhuma vez.\n\n\\(L(r)\\) contém: {14:30, 09:15, 23:59:59, 00:00}. \\(L(r)\\) não contém: {25:00, 12:61, 9:30, 14:30:, 15:10:99}.\nEsses exemplos arranham apenas a superfície, mas demonstram a imensa versatilidade das expressões regulares. Elas formam uma ponte poderosa entre a teoria formal das linguagens e a resolução de problemas práticos e onipresentes no desenvolvimento de software.\n\n\n\n3.4.8 Exercícios 8\n\nProjete expressões regulares para tokens de uma linguagem de programação simples:\n\nPalavras-chave: \\(\\{\\text{if}, \\text{then}, \\text{else}, \\text{while}, \\text{do}\\}\\);\nNúmeros inteiros (incluindo negativos);\nComentários de linha iniciados por \\(//\\);\nOperadores relacionais: \\(\\{&lt;, &gt;, &lt;=, &gt;=, ==, !=\\}\\).\n\nConstrua expressões regulares para validar:\n\nTelefone celular: \\((11) 99999-9999\\);\nRG: \\(99.999.999-9\\);\nCNPJ: \\(99.999.999/9999-99\\);\nCEP: \\(99999-999\\) ou \\(99.999-999\\).\n\nProjete expressões para encontrar:\n\nEndereços de email em um texto;\nValores monetários no formato \\(R\\$ 99,99\\);\nDatas em formatos variados: \\(dd/mm/aaaa\\), \\(dd-mm-aaaa\\), \\(dd.mm.aaaa\\);\nNúmeros de cartão de crédito (formato \\(9999-9999-9999-9999\\)).\n\nIdentifique e corrija os erros nas seguintes expressões:\n\nPara validar email: \\([a-z]+@[a-z]+.[a-z]+\\) (problema: ponto literal);\nPara números decimais: \\([0-9]*.[0-9]*\\) (problema: pontos opcionais);\nPara identificadores: \\([a-zA-Z][a-zA-Z0-9]?\\) (problema: comprimento mínimo).\n\nPara cada expressão, proponha uma versão otimizada:\n\n\\((abc|abd|abe)\\) → \\(ab(c|d|e)\\);\n\\([0-9][0-9][0-9][0-9]\\) → \\([0-9]\\{4\\}\\);\n\\((a^*b^*|b^*a^*)\\) → \\((a|b)^*\\) (verifique se são realmente equivalentes);\nAnalise qual versão seria mais eficiente em uma implementação real.\n\nDefina o alfabeto, e as expressões regulares que sejam capazes de validar expressões aritméticas entre números inteiros considerando apenas as operações de soma, subtração, divisão e multiplicação.\nConstrua expressões regulares para descrever as seguintes linguagens sobre o alfabeto \\(\\Sigma=\\{a,b,c\\}\\), usando apenas as operações de união (∪), concatenação e fechamento de Kleene (∗).\n\n\nTodas as cadeias que começam com a, terminam com \\(c\\) e contêm pelo menos um \\(b\\) no meio.\n\nTodas as cadeias em que cada \\(a\\) é imediatamente seguido por um \\(b\\) ou por outro \\(a\\).\n\nTodas as cadeias que não contêm a substring \\(ac\\).\n\n\n\n3.4.9 Limitações das Expressões Regulares\n\nParênteses aninhados arbitrariamente não são reconhecíveis\nRequer gramáticas livres de contexto para estruturas recursivas\nExemplo problemático: ((3 + 5) * (7 - 2))",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Alfabetos, Linguagens e Strings: Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "01a-lexico.html#o-lugar-das-linguagens-regulares-poder-e-limitações",
    "href": "01a-lexico.html#o-lugar-das-linguagens-regulares-poder-e-limitações",
    "title": "3  Alfabetos, Linguagens e Strings: Fundamentos Matemáticos",
    "section": "3.5 O Lugar das Linguagens Regulares: Poder e Limitações",
    "text": "3.5 O Lugar das Linguagens Regulares: Poder e Limitações\nApós testemunharmos a versatilidade das expressões regulares em aplicações práticas, uma questão natural emerge: será que elas são capazes de descrever qualquer padrão ou linguagem que possamos imaginar? A resposta, embora surpreendente para alguns, é não. As expressões regulares ocupam um lugar específico e bem definido no universo das linguagens formais.\nPara compreender essa especificidade, recorremos à Hierarquia de Chomsky, um sistema de classificação proposto pelo linguista Noam Chomsky que organiza as linguagens formais em níveis de complexidade crescente. Neste framework, as expressões regulares definem o primeiro e mais fundamental degrau: a classe das Linguagens Regulares.\nA aventureira leitora depois de entender as expressões regulares pode explorar as propriedades que definem o poder desta classe de linguagens e, igualmente importante, as fronteiras que revelam suas limitações.\n\n3.5.1 A Essência do Poder: Simplicidade e Previsibilidade\nO poder das Linguagens Regulares reside em sua simplicidade e previsibilidade. Esse poder é o resultado de um conjunto de propriedades matemáticas e uma correspondência fundamental com um tipo específico de máquina teórica: o Autômato Finito. Vamos explorar essas propriedades que definem o que torna as Linguagens Regulares tão úteis e amplamente aplicáveis.\nA curiosa leitora deve saber que existe uma associação clássica entre as Linguagens Regulares e os Autômatos Finitos. De fato, a propriedade mais importante de uma Linguagem Regular é que ela pode ser reconhecida por um Autômato Finito. Um Autômato Finito, como vimos antes no Capítulo Chapter 2, é uma máquina teórica com uma quantidade finita de memória, representada por seus estados. A máquina lê uma string de entrada, um símbolo por vez, e sem poder voltar atrás, decide se a string pertence ou não à linguagem. A incapacidade de armazenar uma quantidade ilimitada de informações é a característica que define, limita e, ao mesmo tempo, determina o poder das Linguagens Regulares. Veremos Autômatos Finitos mais detalhadamente no Capítulo Chapter 4.\n\n\n3.5.2 Propriedades Fundamentais das Linguagens Regulares\nA utilidade prática das linguagens regulares não se limita à sua capacidade de descrever padrões, mas também se baseia em um conjunto de garantias matemáticas. Duas propriedades centrais neste contexto são:\n\nFechamento sob Operações: a classe das Linguagens Regulares é fechada sob todas as operações que vimos: união, concatenação, fechamento de Kleene, e também sob interseção e complemento. Isso significa que se combinarmos Linguagens Regulares usando essas operações, o resultado será sempre outra Linguagem Regular. Essa propriedade é de grande valor prático, ela garante que podemos construir sistemas de reconhecimento complexos a partir de componentes simples com a certeza de que o todo permanecerá computacionalmente tratável.\nDecidibilidade: Para as Linguagens Regulares, questões fundamentais são sempre decidíveis, ou seja, existe um algoritmo que pode respondê-las em um tempo finito. Podemos sempre construir um programa para determinar, por exemplo:\n\nSe uma string \\(w\\) pertence à linguagem \\(L(r)\\);\nSe a linguagem \\(L(r)\\) é vazia;\nSe duas expressões, \\(r\\) e \\(s\\), são equivalentes (\\(L(r) = L(s)\\)).\n\n\nA previsibilidade e a eficiência dos algoritmos associados às Linguagens Regulares são características que as destacam em comparação com outras classes de linguagens mais complexas, como as Linguagens Livres de Contexto ou as Linguagens Sensíveis ao Contexto.\n\n\n3.5.3 As Fronteiras do Mundo Regular: O Limite da Memória\nA principal limitação das Linguagens Regulares está diretamente ligada à memória finita dos autômatos. Tarefas que exigem contar ou lembrar uma quantidade arbitrária e ilimitada de informações estão além de seu alcance. Para reconhecer linguagens que envolvem contagem ou correspondência simétrica, a máquina precisaria de uma memória que pudesse crescer conforme o tamanho da entrada. Como um autômato finito tem um número fixo de estados, ele não consegue realizar essa tarefa.\nExemplos Clássicos de Linguagens Não-Regulares:\n\n\\(L = \\{a^n b^n \\mid n \\geq 0\\}\\): Para verificar se uma string pertence a esta linguagem (por exemplo, aaabbb), uma máquina precisaria contar todos os \\(a\\)’s e depois garantir que o número de \\(b\\)’s é exatamente o mesmo. Como \\(n\\) pode ser qualquer número, a contagem exige uma memória potencialmente infinita.\nPalíndromos (\\(L = \\{w \\mid w = w^R\\}\\)): Para reconhecer um palíndromo como abccba, a máquina teria que memorizar a primeira metade da string (abc) para compará-la com a segunda metade (cba). Novamente, o comprimento da string é ilimitado, exigindo memória ilimitada.\nParênteses Balanceados: A linguagem das expressões com parênteses corretamente aninhados, como ((())), exige uma estrutura de pilha para lembrar quantos parênteses foram abertos e ainda não foram fechados. Esta é uma forma de memória mais poderosa do que a disponível para um autômato finito.\n\n\n\n3.5.4 O Lema do Bombeamento: Uma Ferramenta de Prova\nPara provar formalmente que uma linguagem não é regular, utilizamos uma ferramenta poderosa chamada Lema do Bombeamento (Pumping Lemma). Em vez de ser uma fórmula, é melhor compreendido como uma propriedade que toda Linguagem Regular deve satisfazer. Se conseguirmos mostrar que uma linguagem viola essa propriedade, então provamos que ela não pode ser regular.\nA sua intuição é a seguinte:\nSe uma linguagem \\(L\\) é regular, então ela é aceita por um autômato finito com um número fixo de estados, digamos, \\(p\\). Agora, considere uma string \\(w\\) em \\(L\\) que seja suficientemente longa (especificamente, com comprimento \\(|w| \\geq p\\)). Como a string tem mais símbolos do que o autômato tem estados, o Princípio da Casa dos Pombos nos garante que o autômato necessariamente revisitará pelo menos um de seus estados enquanto lê a string. Isso significa que o caminho do autômato ao ler \\(w\\) contém um ciclo. Podemos, então, dividir a string \\(w\\) em três partes, \\(w = xyz\\):\n\n$x$: a parte da string lida antes do ciclo começar;\n$y$: a parte da string lida durante o ciclo. É uma condição fundamental do lema que esta parte não pode ser vazia (\\(|y| &gt; 0\\));\n$z$: o restante da string, lida após o ciclo.\n\nO lema afirma que, como \\(y\\) corresponde a um ciclo, essa porção pode ser bombeada: podemos percorrê-la zero vezes (removendo \\(y\\)), uma vez (a string original), ou múltiplas vezes. Todas as strings resultantes, da forma \\(xy^iz\\) para qualquer \\(i \\geq 0\\), ainda devem ser aceitas pela máquina e, portanto, devem pertencer à linguagem \\(L\\).\nPara provar que uma linguagem não é regular, usamos esta propriedade para criar uma contradição: encontramos pelo menos uma string longa na linguagem onde o bombeamento de sua parte cíclica a quebra, gerando uma string que não pertence à linguagem. Se isso for possível, a linguagem viola o lema e, portanto, não é regular.\n\n\n3.5.5 Exemplo do Lema do Bombeamento\nVamos usar o Lema do Bombeamento para provar formalmente que a linguagem \\(L = \\{a^n b^n \\mid n \\geq 0\\}\\), uma sequência de \\(a\\)’s seguida pelo mesmo número de \\(b\\)’s, não é regular. A prova segue um roteiro de contradição cujo passo a passo a atenta leitora pode ver a seguir:\n\nAssunção Inicial (para Contradição): assumimos que \\(L\\) é uma linguagem regular.\nAplicação do Lema: se \\(L\\) é regular, então o Lema do Bombeamento deve se aplicar. Isso significa que existe uma constante, o comprimento de bombeamento \\(p\\), para a linguagem \\(L\\).\nEscolha da String: escolhemos uma string \\(w\\) que pertence a \\(L\\) e que seja longa o suficiente, ou seja, \\(|w| \\geq p\\). A escolha mais estratégica é \\(w = a^p b^p\\). Esta string claramente pertence a \\(L\\) (o número de \\(a\\)’s é igual ao de \\(b\\)’s) e seu comprimento é \\(2p\\), que é maior ou igual a \\(p\\).\nDivisão da String: de acordo com o lema, \\(w\\) pode ser dividida em três partes, \\(w=xyz\\), que devem satisfazer as seguintes condições:\n\n\\(|y| &gt; 0\\);\n\\(|xy| \\leq p\\);\n\\(xy^iz \\in L\\) para todo \\(i \\geq 0\\).\n\nAnálise da Parte Bombeável (\\(y\\)): a condição \\(|xy| \\leq p\\) é a chave. Como nossa string é \\(w = \\underbrace{a a \\ldots a}_{p \\text{ vezes}} \\underbrace{b b \\ldots b}_{p \\text{ vezes}}\\), a parte \\(xy\\) (com no máximo \\(p\\) caracteres) deve estar inteiramente contida no bloco inicial de \\(a\\)’s. Além disso, como \\(|y| &gt; 0\\), a string \\(y\\) deve conter pelo menos um \\(a\\). Portanto, \\(y\\) é da forma \\(y=a^k\\) para algum \\(k\\) onde \\(1 \\leq k \\leq p\\).\nO Bombeamento (A Contradição): o lema afirma que qualquer string bombeada \\(xy^iz\\) também deve pertencer a \\(L\\). Vamos testar para \\(i=2\\):\n\nA nova string é \\(w' = xy^2z\\).\nComo \\(x\\), \\(y\\) e \\(z\\) juntos formavam \\(a^p b^p\\), e \\(y\\) era \\(a^k\\), a nova string \\(w'\\) terá \\(k\\) \\(a\\)’s a mais que a original.\nPortanto, \\(w' = a^{p+k} b^p\\).\nComo sabemos que \\(k \\geq 1\\), o número de \\(a\\)’s (\\(p+k\\)) não é mais igual ao número de \\(b\\)’s (\\(p\\)). Logo, a string \\(w' = a^{p+k} b^p\\) não pertence a \\(L\\).\n\nConclusão: chegamos a uma contradição. Nossa suposição inicial de que \\(L\\) era regular nos levou, por meio do Lema do Bombeamento, à conclusão de que uma string gerada pelo bombeamento (\\(w'\\)) deveria estar em \\(L\\), mas mostramos que ela viola a definição de \\(L\\). Portanto, a suposição inicial estava errada. A linguagem \\(L = \\{a^n b^n \\mid n \\geq 0\\}\\) não é regular.\n\n\n3.5.5.1 Por que o Lema do Bombeamento é Importante?\nO Lema do Bombeamento é uma ferramenta diagvérticestica fundamental na teoria da computação. Sua importância não está em descrever o que as linguagens regulares são, mas sim em fornecer um método rigoroso para provar o que elas não são. [cite_start]Ele formaliza a noção de que autômatos finitos possuem memória finita. A existência de um ciclo bombeável é a consequência direta dessa memória finita; qualquer tarefa que exija contagem ou memória ilimitada, como garantir que o número de \\(a\\)’s é igual ao de \\(b\\)’s, irá quebrar sob o bombeamento. Na prática, isso permite aos cientistas da computação e engenheiros de software classificar problemas, entendendo quando uma ferramenta simples (como uma expressão regular) é insuficiente e quando uma abordagem mais poderosa (como um analisador sintático para gramáticas livres de contexto) é necessária.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Alfabetos, Linguagens e Strings: Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "01a-lexico.html#exercícios-desafiadores",
    "href": "01a-lexico.html#exercícios-desafiadores",
    "title": "3  Alfabetos, Linguagens e Strings: Fundamentos Matemáticos",
    "section": "3.6 Exercícios Desafiadores",
    "text": "3.6 Exercícios Desafiadores\n\n3.6.1 Exercício 1:\nSeja \\(\\Sigma = \\{a, b, c\\}\\) e considere a linguagem \\(L = \\{w \\in \\Sigma^* \\mid |w|_a + 2|w|_b = |w|_c\\}\\), onde \\(|w|_x\\) denota o número de ocorrências do símbolo \\(x\\) na string \\(w\\).\n\nDetermine se \\(L\\) é uma linguagem regular. Justifique sua resposta.\nSe \\(L\\) não for regular, use o Lema do Bombeamento para prová-lo formalmente.\nConstrua uma expressão regular para a linguagem \\(L' = \\{w \\in \\{a,b\\}^* \\mid |w|_a \\leq 3\\}\\).\n\nSolução:\n\nA linguagem \\(L\\) não é regular. A condição \\(|w|_a + 2|w|_b = |w|_c\\) estabelece uma relação aritmética que requer contagem precisa dos símbolos, algo que excede a capacidade de memória finita dos autômatos finitos.\nProva usando o Lema do Bombeamento:\n\nAssumimos por contradição que \\(L\\) é regular. Então existe uma constante \\(p\\) tal que qualquer string \\(w \\in L\\) com \\(|w| \\geq p\\) pode ser dividida como \\(w = xyz\\) satisfazendo: a. \\(|y| &gt; 0\\); b. \\(|xy| \\leq p\\); c. \\(xy^iz \\in L\\) para todo \\(i \\geq 0\\).\nEscolhemos \\(w = a^p c^p \\in L\\) (neste caso, \\(p + 2 \\cdot 0 = p\\)). Como \\(|xy| \\leq p\\) e \\(w\\) começa com \\(p\\) símbolos \\(a\\), temos que \\(y = a^k\\) para algum \\(1 \\leq k \\leq p\\).\nPara \\(i = 2\\): \\(w' = xy^2z = a^{p+k}c^p\\)\nNa string \\(w'\\): \\(|w'|_a = p + k\\), \\(|w'|_b = 0\\), \\(|w'|_c = p\\)\nA condição requer: \\((p + k) + 2 \\cdot 0 = p\\), ou seja, \\(p + k = p\\), logo \\(k = 0\\).\nMas isso contradiz \\(|y| &gt; 0\\), que implica \\(k \\geq 1\\).\nPortanto, \\(L\\) não é regular.\n\nPara \\(L' = \\{w \\in \\{a,b\\}^* \\mid |w|_a \\leq 3\\}\\):\n\n\\[L' = b^* \\cup b^*ab^* \\cup b^*ab^*ab^* \\cup b^*ab^*ab^*ab^*\\]\nForma mais concisa: \\(L' = b^*(\\epsilon \\cup ab^* \\cup ab^*ab^* \\cup ab^*ab^*ab^*)\\)",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Alfabetos, Linguagens e Strings: Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "01a-lexico.html#exercício-2",
    "href": "01a-lexico.html#exercício-2",
    "title": "3  Alfabetos, Linguagens e Strings: Fundamentos Matemáticos",
    "section": "3.7 Exercício 2:",
    "text": "3.7 Exercício 2:\nConsidere as expressões regulares sobre \\(\\Sigma = \\{0, 1\\}\\):\n\n\\(r_1 = (01)^*0(10)^*\\)\n\\(r_2 = 0(10)^* \\cup (01)^*0\\)\n\n\nDetermine se \\(r_1 \\equiv r_2\\) construindo strings específicas que testem a equivalência.\nConstrua uma expressão regular equivalente mais simples.\nDetermine \\(|L(r_1) \\cap \\{w \\in \\{0,1\\}^* \\mid |w| = 5\\}|\\).\n\nSolução Letra a:\n\nTeste de equivalência por exemplos:\n\\(L(r_1) = L((01)^*0(10)^*)\\) gera strings que:\n\nComeçam com zero ou mais pares \\(01\\);\nTêm um \\(0\\) central obrigatório;\nTerminam com zero ou mais pares \\(10\\).\n\n\\(L(r_2) = L(0(10)^* \\cup (01)^*0)\\) gera strings que:\n\nOu começam com \\(0\\) seguido de pares \\(10\\);\nOu começam com pares \\(01\\) e terminam com \\(0\\).\n\nTeste com strings específicas:\n\nstring \\(010\\): Em \\(r_1\\): \\((01)^1 \\cdot 0 \\cdot (10)^0 = 010\\). Ok!\nstring \\(010\\): Em \\(r_2\\): \\((01)^1 \\cdot 0 = 010\\). Ok!\nstring \\(0\\): Em ambas. Ok!\nstring \\(01010\\): Em \\(r_1\\): \\((01)^1 \\cdot 0 \\cdot (10)^1 = 01010\\). Ok!\nstring \\(01010\\): Em \\(r_2\\): \\((01)^2 \\cdot 0 = 01010\\). Ok!\n\nPor análise estrutural, \\(r_1 \\equiv r_2\\).\n\nSolução Letra b:\nA expressão \\(r_2 = 0(10)^* \\cup (01)^*0\\) é, de fato, a forma simplificada de \\(r_1\\). A afirmação pode ser mais detalhada da seguinte forma:\n\nSimplificação Estrutural:\n\nA expressão original, \\(r_1 = (01)^*0(10)^*\\), descreve a linguagem por meio de uma concatenação de três partes. Isso exige que qualquer string da linguagem seja mentalmente dividida em um prefixo de \\((01)^*\\), um \\(0\\) central e um sufixo de \\((10)^*\\).\nA expressão \\(r_2\\), já comprovada como equivalente na letra (a), descreve a mesma linguagem como a união de dois padrões mais diretos. Essa forma é frequentemente considerada mais simples por representar a linguagem como strings que pertencem ao padrão A OU ao padrão B, o que pode ser mais fácil de analisar e implementar.\n\nAnálise de Minimalidade:\n\nA expressão \\(r_2\\) é considerada minimal na prática. A linguagem consiste em strings que começam e terminam com \\(0\\) e têm \\(0\\)s e \\(1\\)s alternados no interior. Existem duas maneiras naturais de gerar tais strings:\n\nComeçando com um \\(0\\) e adicionando pares de \\(10\\) à direita (padrão \\(0(10)^*\\)).\nTerminando com um \\(0\\) e adicionando pares de \\(01\\) à esquerda (padrão \\((01)^*0\\)).\n\nA união em \\(r_2\\) captura perfeitamente essas duas perspectivas geradoras. Tentar unir os dois padrões em uma única expressão sem o operador de união (\\(\\cup\\)) muito provavelmente resultaria em uma expressão mais longa, complexa e menos intuitiva.\n\nConclusão:pPortanto, embora a prova formal de minimalidade de uma expressão regular seja um problema complexo (geralmente envolvendo autômatos finitos), a expressão \\(r_2 = 0(10)^* \\cup (01)^*0\\) é a representação equivalente mais simples e canônica de \\(r_1\\).\n\nSolução Letra c:\nContagem para \\(|w| = 5\\):\nStrings de \\(L(r_1)\\) têm comprimento ímpar (todas começam e terminam com \\(0\\), com pares no meio).\nPara comprimento 5: a. Caso \\(0(10)^*\\): \\(0\\) seguido de \\((10)^k\\) onde \\(1 + 2k = 5\\), logo \\(k = 2\\): \\(01010\\). Ok! b. Caso \\((01)^*0\\): \\((01)^k\\) seguido de \\(0\\) onde \\(2k + 1 = 5\\), logo \\(k = 2\\): \\(01010\\). Ok!\nPortanto: \\(|L(r_1) \\cap \\{w \\in \\{0,1\\}^* \\mid |w| = 5\\}| = 1\\)",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Alfabetos, Linguagens e Strings: Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "01a-lexico.html#exercício-3",
    "href": "01a-lexico.html#exercício-3",
    "title": "3  Alfabetos, Linguagens e Strings: Fundamentos Matemáticos",
    "section": "3.8 Exercício 3:",
    "text": "3.8 Exercício 3:\nSeja \\(\\Sigma\\) o alfabeto contendo dígitos de 0 a 9 e os símbolos ‘(’, ‘)’, ‘-’, e o espaço ’ ’. Defina as seguintes expressões para conjuntos de dígitos: 1. \\(D = (0 \\cup 1 \\cup 2 \\cup 3 \\cup 4 \\cup 5 \\cup 6 \\cup 7 \\cup 8 \\cup 9)\\); 2. \\(D_{nz} = (1 \\cup 2 \\cup 3 \\cup 4 \\cup 5 \\cup 6 \\cup 7 \\cup 8 \\cup 9)\\); 3. \\(D_{fixo} = (2 \\cup 3 \\cup 4 \\cup 5 \\cup 6 \\cup 7 \\cup 8)\\).\nCom base nessas definições, considere a linguagem \\(L_{tel}\\) de números de telefone brasileiros.\n\nConstrua expressões regulares separadas para os componentes da linguagem: o DDD e o Número do telefone.\nCombine as expressões da letra (a) para construir a expressão regular final para a linguagem \\(L_{tel}\\) completa.\nDemonstre que a string \\((11) 99999-9999\\) pertence a \\(L_{tel}\\) mostrando sua derivação a partir da expressão final.\n\nSolução Letra a:\n\nComponente DDD (\\(r_{ddd}\\)):\n\nO DDD consiste em dois dígitos, o primeiro não podendo ser zero. A base é \\(D_{nz}D\\);\nEle pode ter dois formatos: com parênteses ou sem, ambos seguidos por um espaço opcional;\nExpressão para o formato com parênteses: \\(\\( D_{nz} D \\) (\\epsilon \\cup \\text{ })\\);\nExpressão para o formato sem parênteses: \\(D_{nz} D (\\epsilon \\cup \\text{ })\\);\nA expressão completa para o DDD é a união dos dois formatos:\n\n\\[r_{ddd} = (\\( D_{nz} D \\) (\\epsilon \\cup \\text{ })) \\cup (D_{nz} D (\\epsilon \\cup \\text{ }))\\]\nComponente Número (\\(r_{em um}\\)):\n\nO número pode ser de celular (9 dígitos começando com 9) ou fixo (8 dígitos começando com 2-8). Ambos podem ter um hífen opcional;\nExpressão para celular: \\(9 D^4 (\\epsilon \\cup -) D^4\\);\nExpressão para fixo: \\(D_{fixo} D^3 (\\epsilon \\cup -) D^4\\);\nA expressão completa para o número é a união dos dois tipos.\n\n\\[r_{em um} = (9 D^4 (\\epsilon \\cup -) D^4) \\cup (D_{fixo} D^3 (\\epsilon \\cup -) D^4)\\]\n\nSolução Letra b:\nA expressão regular final \\(r_{tel}\\) é a concatenação do componente DDD com o componente Número.\n\nExpressão Final: \\[r_{tel} = r_{ddd} \\cdot r_{em um}\\]\nSubstituindo as definições da letra (a):\n\\[r_{tel} = [(\\( D_{nz} D \\) (\\epsilon \\cup \\text{ })) \\cup (D_{nz} D (\\epsilon \\cup \\text{ }))] \\cdot [(9 D^4 (\\epsilon \\cup -) D^4) \\cup (D_{fixo} D^3 (\\epsilon \\cup -) D^4)]\\]\n\nSolução Letra c:\nVamos demonstrar que a string \\(w = (11) 99999-9999\\) pertence a \\(L(r_{tel})\\).\n\nDivisão da String: Podemos dividir \\(w\\) em duas partes, \\(w_{ddd}\\) e \\(w_{em um}\\), onde:\n\n\\(w_{ddd} = (11)\\text{ }\\) (inclui o espaço);\n\\(w_{em um} = 99999-9999\\);\n\nVerificação do Componente DDD: A substring \\(w_{ddd}\\) deve pertencer a \\(L(r_{ddd})\\). Ela corresponde ao primeiro termo da união em \\(r_{ddd}\\):\n\n\\(\\( D_{nz} D \\) (\\epsilon \\cup \\text{ })\\);\n\\(\\(\\) corresponde ao símbolo literal \\((\\);\n\\(1\\) pertence a \\(L(D_{nz})\\) neste caso, \\(1 \\in \\{1,2,3,4,5,6,7,8,9\\}\\);\n\\(1\\) pertence a \\(L(D)\\) neste caso, \\(1 \\in \\{0,1,2,3,4,5,6,7,8,9\\}\\);\n\\(\\)\\) corresponde ao símbolo literal \\()\\);\nO espaço pertence a \\(L(\\epsilon \\cup \\text{ })\\);\nPortanto, \\(w_{ddd} \\in L(r_{ddd})\\). Ok!\n\nVerificação do Componente Número: A substring \\(w_{em um}\\) deve pertencer a \\(L(r_{em um})\\). Ela corresponde ao primeiro termo da união (celular) em \\(r_{em um}\\):\n\n\\(9 D^4 (\\epsilon \\cup -) D^4\\);\nO primeiro \\(9\\) corresponde ao símbolo literal \\(9\\);\n\\(9999\\) pertence a \\(L(D^4) = L(D \\cdot D \\cdot D \\cdot D)\\);\n\\(-\\) pertence a \\(L(\\epsilon \\cup -)\\);\n\\(9999\\) pertence a \\(L(D^4)\\);\nPortanto, \\(w_{em um} \\in L(r_{em um})\\). Ok!\n\nConclusão: como \\(w = w_{ddd} \\cdot w_{em um}\\) e ambas as partes são geradas pelas respectivas subexpressões de \\(r_{tel}\\), pela definição de concatenação de linguagens, temos que \\(w \\in L(r_{tel})\\).\n\n\n3.8.1 Exercício 4:\nSeja \\(\\Sigma = \\{a, b\\}\\) e considere as seguintes linguagens: - \\(L_1 = \\{a^nb^m \\mid n \\geq m \\geq 0\\}\\) - \\(L_2 = \\{a^mb^n \\mid n \\geq m \\geq 0\\}\\)\n\nAnalise se \\(L_1\\) e \\(L_2\\) são linguagens regulares usando o Lema do Bombeamento.\nDetermine as linguagens resultantes das operações \\(L_1 \\cap L_2\\), \\(L_1 \\cup L_2\\), e \\(L_1 \\cdot L_2\\), e classifique cada uma quanto à regularidade.\nConstrua uma expressão regular para \\(L_3 = \\{a^nb^n \\mid n \\geq 0\\} \\cup \\{a^mb^m \\mid m \\geq 0\\}\\) ou prove que não existe.\n\nSolução Letra a:\n\nAnálise de \\(L_1 = \\{a^nb^m \\mid n \\geq m \\geq 0\\}\\)\nCaracterização da linguagem: \\(L_1\\) contém strings onde o número de \\(a\\)’s é maior ou igual ao número de \\(b\\)’s. Exemplos: \\(\\{\\epsilon, a, aa, ab, aaa, aab, aaab, \\ldots\\}\\).\nAplicação do Lema do Bombeamento:\nPasso 1 - Hipótese: Assumimos por contradição que \\(L_1\\) é regular.\nPasso 2 - Constante de bombeamento: Se \\(L_1\\) é regular, então existe uma constante \\(p &gt; 0\\) tal que qualquer string \\(w \\in L_1\\) com \\(|w| \\geq p\\) pode ser dividida como \\(w = xyz\\) satisfazendo:\n\n\\(|y| &gt; 0\\);\n\\(|xy| \\leq p\\);\n\\(xy^iz \\in L_1\\) para todo \\(i \\geq 0\\).\n\nPasso 3 - Escolha da string: Escolhemos \\(w = a^pb^p \\in L_1\\) (válida porque \\(p \\geq p \\geq 0\\)).\nPasso 4 - Análise da divisão: Como \\(|xy| \\leq p\\) e \\(w\\) inicia com \\(p\\) símbolos \\(a\\) seguidos de \\(p\\) símbolos \\(b\\), a substring \\(xy\\) está inteiramente contida no bloco inicial de \\(a\\)’s. Portanto, \\(y = a^k\\) para algum \\(k\\) com \\(1 \\leq k \\leq p\\).\nPasso 5 - Teste do bombeamento: Consideramos \\(i = 2\\): \\[xy^2z = a^{p+k}b^p\\]\nPasso 6 - Verificação da condição: Para que \\(xy^2z \\in L_1\\), devemos ter: \\[\\text{número de } a\\text{'s} \\geq \\text{número de } b\\text{'s}\\] \\[p + k \\geq p\\] \\[k \\geq 0\\]\nComo \\(k \\geq 1\\) (porque \\(|y| &gt; 0\\)), a condição \\(k \\geq 0\\) é sempre satisfeita.\nPasso 7 - Teste com \\(i = 0\\): Consideramos \\(i = 0\\): \\[xy^0z = xz = a^{p-k}b^p\\]\nPara que \\(xz \\in L_1\\), devemos ter: \\[p - k \\geq p\\] \\[-k \\geq 0\\] \\[k \\leq 0\\]\nPasso 8 - Contradição: Temos \\(k \\geq 1\\) (de \\(|y| &gt; 0\\)) e \\(k \\leq 0\\) (da condição de bombeamento), o que é uma contradição.\nConclusão: \\(L_1\\) não é regular.\nAnálise de \\(L_2 = \\{a^mb^n \\mid n \\geq m \\geq 0\\}\\)\nCaracterização da linguagem: \\(L_2\\) contém strings onde o número de \\(b\\)’s é maior ou igual ao número de \\(a\\)’s. Exemplos: \\(\\{\\epsilon, b, a, bb, ab, bbb, abb, abbb, \\ldots\\}\\).\nAplicação do Lema do Bombeamento:\nPassos 1-4: Idênticos à análise de \\(L_1\\), escolhendo \\(w = a^pb^p \\in L_2\\).\nPasso 5 - Teste do bombeamento com \\(i = 2\\): \\[xy^2z = a^{p+k}b^p\\]\nPasso 6 - Verificação da condição: Para que \\(xy^2z \\in L_2\\), devemos ter: \\[\\text{número de } b\\text{'s} \\geq \\text{número de } a\\text{'s}\\] \\[p \\geq p + k\\] \\[0 \\geq k\\] \\[k \\leq 0\\]\nPasso 7 - Contradição: Temos \\(k \\geq 1\\) (de \\(|y| &gt; 0\\)) e \\(k \\leq 0\\) (da condição), o que é uma contradição.\nConclusão: \\(L_2\\) não é regular.\n\nSolução Letra b:\n\nAnálise de \\(L_1 \\cap L_2\\)\nDeterminação da interseção: \\[L_1 \\cap L_2 = \\{a^nb^m \\mid n \\geq m \\geq 0\\} \\cap \\{a^mb^n \\mid n \\geq m \\geq 0\\}\\]\nPara uma string \\(a^rb^s\\) pertencer à interseção, deve satisfazer simultaneamente:\n\n\\(r \\geq s\\) (condição de \\(L_1\\))\n\\(s \\geq r\\) (condição de \\(L_2\\))\n\nIsso implica \\(r = s\\), portanto: \\[L_1 \\cap L_2 = \\{a^nb^n \\mid n \\geq 0\\}\\]\nAnálise de regularidade: Esta é a linguagem clássica de balanceamento, que não é regular. Pode ser provada usando o Lema do Bombeamento com a string \\(a^pb^p\\).\nAnálise de \\(L_1 \\cup L_2\\)\nDeterminação da união: \\[L_1 \\cup L_2 = \\{a^nb^m \\mid n \\geq m \\geq 0\\} \\cup \\{a^mb^n \\mid n \\geq m \\geq 0\\}\\]\nAnálise de regularidade: Como \\(L_1 \\subseteq L_1 \\cup L_2\\) e \\(L_1\\) não é regular, se \\(L_1 \\cup L_2\\) fosse regular, então pela propriedade de fechamento das linguagens regulares sob interseção com linguagens regulares, poderíamos construir \\(L_1\\) como uma interseção envolvendo \\(L_1 \\cup L_2\\). Isso criaria uma contradição.\nConclusão: \\(L_1 \\cup L_2\\) não é regular.\nAnálise de \\(L_1 \\cdot L_2\\)\nCaracterização da concatenação: \\[L_1 \\cdot L_2 = \\{uv \\mid u \\in L_1 \\text{ e } v \\in L_2\\}\\]\nAnálise de regularidade: A concatenação de duas linguagens não-regulares não é necessariamente não-regular. No entanto, neste caso específico, podemos mostrar que \\(L_1 \\cdot L_2\\) contém como subset a linguagem \\(\\{a^na^nb^nb^n \\mid n \\geq 0\\} = \\{a^{2n}b^{2n} \\mid n \\geq 0\\}\\), que é uma variação da linguagem de balanceamento e não é regular.\nConclusão: \\(L_1 \\cdot L_2\\) não é regular.\n\nSolução Letra c:\n\nAnálise de \\(L_3 = \\{a^nb^n \\mid n \\geq 0\\} \\cup \\{a^mb^m \\mid m \\geq 0\\}\\)\nSimplificação da linguagem: \\[L_3 = \\{a^nb^n \\mid n \\geq 0\\} \\cup \\{a^mb^m \\mid m \\geq 0\\} = \\{a^kb^k \\mid k \\geq 0\\}\\]\n\nAmbos os conjuntos na união representam a mesma linguagem (apenas com variáveis diferentes).\nAnálise de regularidade: A linguagem \\(L_3 = \\{a^kb^k \\mid k \\geq 0\\}\\) é a linguagem clássica de strings balanceadas, que sabemos não ser regular.\nAplicação do Lema do Bombeamento para \\(L_3\\):\nAssumindo que \\(L_3\\) é regular, seja \\(p\\) a constante de bombeamento. Escolhemos \\(w = a^pb^p \\in L_3\\).\nPara qualquer divisão \\(w = xyz\\) com \\(|xy| \\leq p\\) e \\(|y| &gt; 0\\), temos \\(y = a^k\\) com \\(k \\geq 1\\).\nPara \\(i = 2\\): \\(xy^2z = a^{p+k}b^p\\). Como \\(p + k \\neq p\\), esta string não pertence a \\(L_3\\).\nIsso contradiz o Lema do Bombeamento.\nConclusão: \\(L_3\\) não é regular, portanto não existe expressão regular para \\(L_3\\).\n\n\n3.8.2 Exercício 5\nSeja \\(\\Sigma\\) o alfabeto ASCII estendido. Defina os seguintes conjuntos de símbolos: a. \\(L = (a \\cup b \\cup c \\cup \\ldots \\cup z \\cup A \\cup B \\cup C \\cup \\ldots \\cup Z)\\) (letras); b. \\(D = (0 \\cup 1 \\cup 2 \\cup 3 \\cup 4 \\cup 5 \\cup 6 \\cup 7 \\cup 8 \\cup 9)\\) (dígitos); c. \\(D_{nz} = (1 \\cup 2 \\cup 3 \\cup 4 \\cup 5 \\cup 6 \\cup 7 \\cup 8 \\cup 9)\\) (dígitos não-zero); d. \\(S = (+ \\cup - \\cup * \\cup / \\cup = \\cup &lt; \\cup &gt; \\cup !)\\) (símbolos de operadores).\nProjete um analisador léxico para uma linguagem que suporte identificadores, números (inteiros, decimais, científicos), operadores aritméticos e relacionais, e comentários.\n\nConstrua expressões regulares sistemáticas para cada categoria de token.\nAnalise os conflitos de ambiguidade entre as categorias.\nEstabeleça uma hierarquia de precedência para resolver os conflitos.\n\nSolução Letra a:\n\n3.8.2.1 1. Categoria: Identificadores (\\(r_{id}\\))\nEspecificação: Identificadores começam com letra ou underscore, seguidos de letras, dígitos ou underscores.\nConstrução sistemática: 1. Símbolos iniciais válidos: \\(S_{ini} = L \\cup \\{\\_\\}\\); 2. Símbolos de continuação: \\(S_{cont} = L \\cup D \\cup \\{\\_\\}\\); 3. Expressão para identificadores:\n\\[r_{id} = S_{ini} \\cdot S_{cont}^*\\]\nExpandindo as definições: \\[r_{id} = (L \\cup \\{\\_\\}) \\cdot (L \\cup D \\cup \\{\\_\\})^*\\]\n\n\n3.8.2.2 2. Categoria: Números (\\(r_{em um}\\))\nSubcategoria 2.1 - Números Inteiros (\\(r_{int}\\)): 1. Zero isolado: \\(0\\); 2. Números positivos: \\(D_{nz} \\cdot D^*\\); 3. Números com sinal: \\((+ \\cup - \\cup \\epsilon) \\cdot (0 \\cup (D_{nz} \\cdot D^*))\\).\n\\[r_{int} = (+ \\cup - \\cup \\epsilon) \\cdot (0 \\cup (D_{nz} \\cdot D^*))\\]\nSubcategoria 2.2 - Números Decimais (\\(r_{dec}\\)): - Parte inteira obrigatória, ponto, parte fracionária obrigatória:\n\\[r_{dec} = r_{int} \\cdot \\{.\\} \\cdot D \\cdot D^*\\]\nSubcategoria 2.3 - Notação Científica (\\(r_{sci}\\)): - Base (inteiro ou decimal), indicador científico, expoente:\n\\[r_{sci} = (r_{int} \\cup r_{dec}) \\cdot (e \\cup E) \\cdot (+ \\cup - \\cup \\epsilon) \\cdot D \\cdot D^*\\]\nExpressão completa para números: \\[r_{em um} = r_{sci} \\cup r_{dec} \\cup r_{int}\\]\n\n\n3.8.2.3 3. Categoria: Operadores (\\(r_{op}\\))\nSubcategoria 3.1 - Operadores Simples (\\(r_{op\\_simples}\\)): \\[r_{op\\_simples} = + \\cup - \\cup * \\cup / \\cup = \\cup &lt; \\cup &gt;\\]\nSubcategoria 3.2 - Operadores Compostos (\\(r_{op\\_comp}\\)):\n\nIncremento/Decremento: \\((+ \\cdot +) \\cup (- \\cdot -)\\);\nIgualdade/Desigualdade: \\((= \\cdot =) \\cup (! \\cdot =)\\);\nRelacionais compostos: \\((&lt; \\cdot =) \\cup (&gt; \\cdot =)\\).\n\n\\[r_{op\\_comp} = (+ \\cdot +) \\cup (- \\cdot -) \\cup (= \\cdot =) \\cup (! \\cdot =) \\cup (&lt; \\cdot =) \\cup (&gt; \\cdot =)\\]\nExpressão completa para operadores: \\[r_{op} = r_{op\\_comp} \\cup r_{op\\_simples}\\]\n\n\n3.8.2.4 4. Categoria: Comentários (\\(r_{com}\\))\nSubcategoria 4.1 - Comentários de Linha (\\(r_{com\\_linha}\\)): 1. Sequência // seguida de qualquer caractere até quebra de linha:\n\\[r_{com\\_linha} = / \\cdot / \\cdot \\Sigma_{texto}^* \\cdot \\text{EOL}\\]\nonde \\(\\Sigma_{texto}\\) representa todos os caracteres exceto quebra de linha.\nSubcategoria 4.2 - Comentários de Bloco (\\(r_{com\\_bloco}\\)): 1. Sequência /* seguida de qualquer texto até */:\n\\[r_{com\\_bloco} = / \\cdot * \\cdot \\Sigma_{qualquer}^* \\cdot * \\cdot /\\]\nonde \\(\\Sigma_{qualquer}\\) representa qualquer caractere, mas com restrição de não formar */ prematuramente.\nExpressão completa para comentários: \\[r_{com} = r_{com\\_linha} \\cup r_{com\\_bloco}\\]\nSolução Letra b:\n\n\n3.8.2.5 Análise de Conflitos de Ambiguidade\n\nConflito 1 - Identificadores vs Palavras-chave: palavras reservadas como if, while, for são sintaticamente válidas como identificadores.\n\nExemplo: A string if satisfaz \\(r_{id} = (L \\cup \\{\\_\\}) \\cdot (L \\cup D \\cup \\{\\_\\})^*\\)\nImpacto: O analisador não pode distinguir entre palavra-chave e identificador apenas pela forma\n\nConflito 2 - Operadores Compostos vs Sequência de Operadores Simples: sequências como ++ podem ser interpretadas como operador composto ou dois operadores +.\nExemplo: A string ++ satisfaz tanto \\(r_{op\\_comp}\\) quanto \\(r_{op\\_simples} \\cdot r_{op\\_simples}\\) Implicação: Ambiguidade na tokenização que afeta a análise sintática\nConflito 3 - Números Decimais vs Operador Ponto: se existir operador ponto (.), pode conflitar com números decimais.\n\nExemplo: 1.5 pode ser interpretado como número decimal ou 1 seguido de . seguido de 5 Contexto: Especialmente problemático em linguagens com notação de acesso a membros\n\nConflito 4 - Comentários vs Operadores de Divisão: o símbolo / inicia tanto comentários quanto operação de divisão\n\nExemplo: A sequência / pode ser início de //, /* ou operador de divisão Complexidade: Requer lookahead para decidir a interpretação\n\nConflito 5 - Sinais vs Operadores vs Números com Sinal: os símbolos + e - podem ser operadores binários, unários, ou parte de números.\n\nExemplo: -5 pode ser número negativo ou operador - seguido de número 5 Dependência: Resolução depende do contexto sintático\nSolução Letra c:\n\n\n3.8.2.6 Hierarquia de Precedência para Resolução de Conflitos\n\nNível 1 (Maior Precedência): Comentários\n\n\nRegra: Reconhecer // e /* antes de qualquer outra análise\nJustificativa: Comentários alteram fundamentalmente o processamento (podem “comentar” outros tokens)\nImplementação: Scanner deve verificar comentários primeiro em cada posição\n\n\nNível 2: Palavras-chave Reservadas\n\n\nRegra: Lista finita de palavras reservadas tem precedência sobre \\(r_{id}\\)\nMétodo: Lookup table de palavras-chave após reconhecer padrão de identificador\nAlgoritmo:\n\nAplicar \\(r_{id}\\) para reconhecer padrão\nConsultar tabela de palavras reservadas\nSe encontrada, classificar como palavra-chave; senão, como identificador\n\n\n\nNível 3: Números (Regra do Maior Match)\n\n\nRegra: Sempre consumir a maior sequência válida como número\nOrdem de tentativa: \\(r_{sci} \\succ r_{dec} \\succ r_{int}\\)\nExemplo: 1.5e-3 deve ser reconhecido como um token científico, não como três tokens separados\n\n\nNível 4: Operadores Compostos (Regra do Maior Match)\n\n\nRegra: Operadores compostos têm precedência sobre sequências de operadores simples\nExemplo: ++ é reconhecido como incremento, não como dois operadores +\nImplementação: Verificar padrões compostos antes dos simples\n\n\nNível 5: Operadores Simples\n\n\nRegra: Reconhecer operadores individuais\nResolução de ambiguidade: Dependente do contexto sintático (análise posterior)\n\nAlgoritmo de Tokenização:\nA seguir a esforçada leitora pode ver um pseudocódigo de um dos possíveis algoritmos de tokenização que implementa a hierarquia de precedência conforme definido neste exercício:\nPara cada posição no texto fonte:\n   1. Ignorar espaços em branco\n   2. Se match(r_com): retornar TOKEN_COMENTARIO\n   3. Se match(r_num): retornar TOKEN_NUMERO\n   4. Se match(r_id):\n      4.1. Se é palavra_reservada: retornar TOKEN_PALAVRA_CHAVE\n      4.2. Senão: retornar TOKEN_IDENTIFICADOR\n   5. Se match(r_op_comp): retornar TOKEN_OPERADOR_COMPOSTO\n   6. Se match(r_op_simples): retornar TOKEN_OPERADOR_SIMPLES\n   7. Senão: erro léxico\nConsiderações Especiais:\n\nLookahead Necessário:\n\nComentários: Verificar / seguido de / ou *;\nOperadores: Verificar + seguido de +, = seguido de =, etc.;\nNúmeros científicos: Verificar e/E em contexto numérico.\n\nTratamento de Contexto:\n\nSinais unários vs binários: Resolver na fase de análise sintática;\nNúmeros negativos: Tratar como operador unário aplicado a número positivo.\n\n\n\n\n3.8.2.7 Em C++ 23\nO código a seguir contém uma implementação do analisador léxico definido neste problema em C++23, usando as características desta linguagem para simplificar a implementação:\n#include &lt;iostream&gt;\n#include &lt;string&gt;\n#include &lt;string_view&gt;\n#include &lt;vector&gt;\n#include &lt;unordered_set&gt;\n#include &lt;expected&gt;\n#include &lt;print&gt;\n#include &lt;ranges&gt;\n#include &lt;algorithm&gt;\n#include &lt;concepts&gt;\n#include &lt;optional&gt;\n\n// Definição dos tipos de token seguindo nossa hierarquia\nenum class TokenType {\n    COMENTARIO,\n    PALAVRA_CHAVE, \n    NUMERO,\n    IDENTIFICADOR,\n    OPERADOR_COMPOSTO,\n    OPERADOR_SIMPLES,\n    UNKNOWN,\n    END_OF_INPUT\n};\n\n// Estrutura do token\nstruct Token {\n    TokenType type;\n    std::string value;\n    \n    void print() const {\n        std::println(\"Token: {} | Valor: '{}'\", to_string(type), value);\n    }\n};\n\n// Conversão de TokenType para string\nstd::string to_string(TokenType type) {\n    switch (type) {\n        case TokenType::COMENTARIO: return \"COMENTARIO\";\n        case TokenType::PALAVRA_CHAVE: return \"PALAVRA_CHAVE\";\n        case TokenType::NUMERO: return \"NUMERO\";\n        case TokenType::IDENTIFICADOR: return \"IDENTIFICADOR\";\n        case TokenType::OPERADOR_COMPOSTO: return \"OPERADOR_COMPOSTO\";\n        case TokenType::OPERADOR_SIMPLES: return \"OPERADOR_SIMPLES\";\n        case TokenType::UNKNOWN: return \"UNKNOWN\";\n        case TokenType::END_OF_INPUT: return \"END_OF_INPUT\";\n    }\n    return \"INVALID\";\n}\n\n// Conceitos para classificação de caracteres (conjuntos L, D, etc.)\ntemplate&lt;typename T&gt;\nconcept Character = std::same_as&lt;T, char&gt;;\n\nclass CharacterClassifier {\npublic:\n    // Conjunto L = (a ∪ b ∪ ... ∪ z ∪ A ∪ B ∪ ... ∪ Z)\n    static constexpr bool isLetter(char c) {\n        return (c &gt;= 'a' && c &lt;= 'z') || (c &gt;= 'A' && c &lt;= 'Z');\n    }\n    \n    // Conjunto D = (0 ∪ 1 ∪ 2 ∪ ... ∪ 9)\n    static constexpr bool isDigit(char c) {\n        return c &gt;= '0' && c &lt;= '9';\n    }\n    \n    // Conjunto D_nz = (1 ∪ 2 ∪ ... ∪ 9)\n    static constexpr bool isNonZeroDigit(char c) {\n        return c &gt;= '1' && c &lt;= '9';\n    }\n    \n    // Conjunto S_ini = L ∪ {_}\n    static constexpr bool isIdentifierStart(char c) {\n        return isLetter(c) || c == '_';\n    }\n    \n    // Conjunto S_cont = L ∪ D ∪ {_}\n    static constexpr bool isIdentifierContinuation(char c) {\n        return isLetter(c) || isDigit(c) || c == '_';\n    }\n    \n    static constexpr bool isWhitespace(char c) {\n        return c == ' ' || c == '\\t' || c == '\\n' || c == '\\r';\n    }\n    \n    static constexpr bool isOperatorSymbol(char c) {\n        return c == '+' || c == '-' || c == '*' || c == '/' || \n               c == '=' || c == '&lt;' || c == '&gt;' || c == '!';\n    }\n};\n\nclass LexicalAnalyzer {\nprivate:\n    std::string_view input;\n    size_t position = 0;\n    \n    // Palavras-chave reservadas (Nível 2 da hierarquia)\n    std::unordered_set&lt;std::string&gt; keywords = {\n        \"if\", \"else\", \"while\", \"for\", \"int\", \"float\", \"double\",\n        \"char\", \"void\", \"return\", \"break\", \"continue\", \"true\", \"false\"\n    };\n    \npublic:\n    explicit LexicalAnalyzer(std::string_view text) : input(text) {}\n    \n    // Método principal de tokenização seguindo nossa hierarquia\n    std::expected&lt;Token, std::string&gt; nextToken() {\n        skipWhitespace();\n        \n        if (position &gt;= input.length()) {\n            return Token{TokenType::END_OF_INPUT, \"\"};\n        }\n        \n        // Nível 1: Comentários (maior precedência)\n        if (auto comment = tryParseComment()) {\n            return *comment;\n        }\n        \n        // Nível 3: Números (regra do maior match)\n        if (auto number = tryParseNumber()) {\n            return *number;\n        }\n        \n        // Nível 2: Identificadores (depois verificamos se é palavra-chave)\n        if (auto identifier = tryParseIdentifier()) {\n            return *identifier;\n        }\n        \n        // Nível 4: Operadores compostos (regra do maior match)\n        if (auto compoundOp = tryParseCompoundOperator()) {\n            return *compoundOp;\n        }\n        \n        // Nível 5: Operadores simples\n        if (auto simpleOp = tryParseSimpleOperator()) {\n            return *simpleOp;\n        }\n        \n        // Erro léxico\n        return std::unexpected(std::format(\"Caractere inválido '{}' na posição {}\", \n                                         input[position], position));\n    }\n    \nprivate:\n    void skipWhitespace() {\n        while (position &lt; input.length() && CharacterClassifier::isWhitespace(input[position])) {\n            position++;\n        }\n    }\n    \n    char peek(size_t offset = 0) const {\n        size_t pos = position + offset;\n        return pos &lt; input.length() ? input[pos] : '\\0';\n    }\n    \n    // Nível 1: r_com = r_com_linha ∪ r_com_bloco\n    std::optional&lt;Token&gt; tryParseComment() {\n        if (peek() != '/') return std::nullopt;\n        \n        // r_com_linha = / · / · Σ_texto* · EOL\n        if (peek(1) == '/') {\n            size_t start = position;\n            position += 2; // Consome \"//\"\n            \n            // Consome até o final da linha\n            while (position &lt; input.length() && input[position] != '\\n') {\n                position++;\n            }\n            if (position &lt; input.length()) position++; // Consome '\\n'\n            \n            return Token{TokenType::COMENTARIO, \n                        std::string(input.substr(start, position - start))};\n        }\n        \n        // r_com_bloco = / · * · Σ_qualquer* · * · /\n        if (peek(1) == '*') {\n            size_t start = position;\n            position += 2; // Consome \"/*\"\n            \n            // Procura por \"*/\"\n            while (position &lt; input.length() - 1) {\n                if (input[position] == '*' && input[position + 1] == '/') {\n                    position += 2; // Consome \"*/\"\n                    return Token{TokenType::COMENTARIO,\n                                std::string(input.substr(start, position - start))};\n                }\n                position++;\n            }\n            \n            // Comentário não fechado - erro\n            return std::nullopt;\n        }\n        \n        return std::nullopt;\n    }\n    \n    // Nível 3: r_num = r_sci ∪ r_dec ∪ r_int\n    std::optional&lt;Token&gt; tryParseNumber() {\n        if (!CharacterClassifier::isDigit(peek()) && \n            peek() != '+' && peek() != '-') {\n            return std::nullopt;\n        }\n        \n        size_t start = position;\n        \n        // Consome sinal opcional\n        if (peek() == '+' || peek() == '-') {\n            position++;\n        }\n        \n        // r_int = (+ ∪ - ∪ ε) · (0 ∪ (D_nz · D*))\n        if (!parseIntegerPart()) {\n            position = start;\n            return std::nullopt;\n        }\n        \n        // Tenta parte decimal: r_dec = r_int · {.} · D · D*\n        bool hasDecimal = false;\n        if (peek() == '.') {\n            size_t dotPos = position;\n            position++; // Consome '.'\n            \n            if (CharacterClassifier::isDigit(peek())) {\n                hasDecimal = true;\n                // Consome dígitos após o ponto\n                while (CharacterClassifier::isDigit(peek())) {\n                    position++;\n                }\n            } else {\n                // Não é número decimal, volta atrás\n                position = dotPos;\n            }\n        }\n        \n        // Tenta notação científica: r_sci = (r_int ∪ r_dec) · (e ∪ E) · (+ ∪ - ∪ ε) · D · D*\n        if (peek() == 'e' || peek() == 'E') {\n            size_t expPos = position;\n            position++; // Consome 'e' ou 'E'\n            \n            // Sinal opcional no expoente\n            if (peek() == '+' || peek() == '-') {\n                position++;\n            }\n            \n            // Deve ter pelo menos um dígito no expoente\n            if (CharacterClassifier::isDigit(peek())) {\n                while (CharacterClassifier::isDigit(peek())) {\n                    position++;\n                }\n            } else {\n                // Notação científica inválida\n                position = expPos;\n            }\n        }\n        \n        return Token{TokenType::NUMERO, \n                    std::string(input.substr(start, position - start))};\n    }\n    \n    bool parseIntegerPart() {\n        if (peek() == '0') {\n            position++;\n            return true;\n        }\n        \n        if (CharacterClassifier::isNonZeroDigit(peek())) {\n            position++;\n            while (CharacterClassifier::isDigit(peek())) {\n                position++;\n            }\n            return true;\n        }\n        \n        return false;\n    }\n    \n    // Nível 2: r_id = (L ∪ {_}) · (L ∪ D ∪ {_})*\n    std::optional&lt;Token&gt; tryParseIdentifier() {\n        if (!CharacterClassifier::isIdentifierStart(peek())) {\n            return std::nullopt;\n        }\n        \n        size_t start = position;\n        position++; // Consome primeiro caractere\n        \n        // Consome caracteres de continuação\n        while (CharacterClassifier::isIdentifierContinuation(peek())) {\n            position++;\n        }\n        \n        std::string value(input.substr(start, position - start));\n        \n        // Verifica se é palavra-chave (Nível 2 da hierarquia)\n        TokenType type = keywords.contains(value) ? \n                        TokenType::PALAVRA_CHAVE : TokenType::IDENTIFICADOR;\n        \n        return Token{type, value};\n    }\n    \n    // Nível 4: r_op_comp = (+ · +) ∪ (- · -) ∪ (= · =) ∪ (! · =) ∪ (&lt; · =) ∪ (&gt; · =)\n    std::optional&lt;Token&gt; tryParseCompoundOperator() {\n        char first = peek();\n        char second = peek(1);\n        \n        if ((first == '+' && second == '+') ||\n            (first == '-' && second == '-') ||\n            (first == '=' && second == '=') ||\n            (first == '!' && second == '=') ||\n            (first == '&lt;' && second == '=') ||\n            (first == '&gt;' && second == '=')) {\n            \n            std::string value{first, second};\n            position += 2;\n            return Token{TokenType::OPERADOR_COMPOSTO, value};\n        }\n        \n        return std::nullopt;\n    }\n    \n    // Nível 5: r_op_simples = + ∪ - ∪ * ∪ / ∪ = ∪ &lt; ∪ &gt;\n    std::optional&lt;Token&gt; tryParseSimpleOperator() {\n        char c = peek();\n        \n        if (CharacterClassifier::isOperatorSymbol(c)) {\n            position++;\n            return Token{TokenType::OPERADOR_SIMPLES, std::string(1, c)};\n        }\n        \n        return std::nullopt;\n    }\n};\n\n// Função para processar uma linha de entrada\nvoid processLine(std::string_view line) {\n    std::println(\"\\n=== Analisando: '{}' ===\", line);\n    \n    LexicalAnalyzer analyzer(line);\n    \n    while (true) {\n        auto result = analyzer.nextToken();\n        \n        if (!result) {\n            std::println(\"ERRO: {}\", result.error());\n            break;\n        }\n        \n        Token token = *result;\n        \n        if (token.type == TokenType::END_OF_INPUT) {\n            std::println(\"=== Fim da análise ===\");\n            break;\n        }\n        \n        token.print();\n    }\n}\n\nint main() {\n    std::println(\"=== Analisador Léxico **C++**23 ===\");\n    std::println(\"Digite expressões linha por linha (Ctrl+C para sair):\");\n    std::println();\n    \n    std::string line;\n    while (std::getline(std::cin, line)) {\n        if (!line.empty()) {\n            processLine(line);\n        }\n        std::println(\"\\nDigite a próxima expressão:\");\n    }\n    \n    return 0;\n}\nPara usar:\nDigite: int x = 42;\nNeste caso a saída deve ser:\nToken: PALAVRA_CHAVE | Valor: 'int'\nToken: IDENTIFICADOR | Valor: 'x'\nToken: OPERADOR_SIMPLES | Valor: '='\nToken: NUMERO | Valor: '42'",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Alfabetos, Linguagens e Strings: Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "01a-lexico.html#exercício-6-classes-de-equivalência",
    "href": "01a-lexico.html#exercício-6-classes-de-equivalência",
    "title": "3  Alfabetos, Linguagens e Strings: Fundamentos Matemáticos",
    "section": "3.9 Exercício 6: Classes de Equivalência",
    "text": "3.9 Exercício 6: Classes de Equivalência\nConsidere a linguagem \\(L = \\{w \\in \\{0,1\\}^* \\mid w \\text{ contém um número par de } 0\\text{'s e um número ímpar de } 1\\text{'s}\\}\\).\n\nDetermine quantas classes de equivalência existem na relação de Myhill-Nerode para \\(L\\) (ver Chapter 20).\nConstrua o autômato finito correspondente.\n\nSolução Letra a\n\nClasses de equivalência: precisamos rastrear paridades de 0’s e 1’s:\n\n\n\\(q_{00}\\): par de 0’s, par de 1’s;\n\\(q_{01}\\): par de 0’s, ímpar de 1’s ← estado de aceitação;\n\\(q_{10}\\): ímpar de 0’s, par de 1’s;\n\\(q_{11}\\): ímpar de 0’s, ímpar de 1’s.\n\nTotal: 4 classes de equivalência\nSolução Letra b: Autômato Finito:\nEstados: {q00, q01, q10, q11} Estado inicial: q00 Estado final: {q01}\n\n\n\nEstado\n0\n1\n\n\n\n\n→q00\nq10\nq01\n\n\n*q01\nq11\nq00\n\n\nq10\nq00\nq11\n\n\nq11\nq01\nq10\n\n\n\nLegenda: → Estado inicial; * Estado final.\n\n3.9.1 Exercício 7: Lema do Bombeamento\nConsidere a linguagem \\(L = \\{0^i1^j0^k \\mid i, j, k \\geq 1 \\text{ e } i + k = j\\}\\).\n\nProve que \\(L\\) não é regular usando o Lema do Bombeamento.\nIdentifique qual propriedade específica de \\(L\\) viola a capacidade dos autômatos finitos.\nConstrua uma linguagem regular \\(L'\\) que seja o “mais próxima possível” de \\(L\\).\n\nSolução letra a:\n\nProva usando Lema do Bombeamento:\n\nAssumimos que \\(L\\) é regular. Seja \\(p\\) a constante do bombeamento.\nEscolhemos \\(w = 0^p1^{2p}0^p \\in L\\) (porque \\(p + p = 2p\\)).\nComo \\(|w| = 4p \\geq p\\), podemos dividir \\(w = xyz\\) onde: - \\(|y| &gt; 0\\) - \\(|xy| \\leq p\\)\n- \\(xy^iz \\in L\\) para todo \\(i \\geq 0\\)\nComo \\(|xy| \\leq p\\) e \\(w\\) começa com \\(p\\) zeros, temos \\(y = 0^m\\) para algum \\(1 \\leq m \\leq p\\).\nPara \\(i = 2\\): \\(w' = xy^2z = 0^{p+m}1^{2p}0^p\\)\nNa string \\(w'\\): - Número de 0’s iniciais: \\(p + m\\) - Número de 1’s: \\(2p\\)\n- Número de 0’s finais: \\(p\\)\nA condição \\(i + k = j\\) requer: \\((p + m) + p = 2p\\), ou seja, \\(2p + m = 2p\\), logo \\(m = 0\\).\nMas isso contradiz \\(|y| &gt; 0\\), que implica \\(m \\geq 1\\).\nPortanto, \\(L\\) não é regular.\nSolução Letra b:\nA linguagem \\(L\\) requer uma relação aritmética entre partes não-adjacentes da string (\\(i + k = j\\)). Um autômato finito não pode “lembrar” o valor de \\(i\\) para comparar com \\(k\\) após processar toda a sequência de 1’s.\nSolução Letra c:\n\\[L' = 0^+1^+0^+\\]\nEsta linguagem captura a estrutura (0’s, depois 1’s, depois 0’s) sem a restrição aritmética.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Alfabetos, Linguagens e Strings: Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "02-lexico.html",
    "href": "02-lexico.html",
    "title": "4  Autômatos Finitos Determinísticos",
    "section": "",
    "text": "4.1 Definição Formal e Componentes\nUm Autômato Finito Determinístico é formalmente definido como uma 5-tupla matemática:\n\\[M = (Q, \\Sigma, \\delta, q_0, F)\\]\nna qual cada componente possui um papel específico e bem definido:\nÉ importante que a atenta leitora observe que esta definição impõe restrições matemáticas precisas:\n\\[q_0 \\in Q \\quad \\text{e} \\quad F \\subseteq Q\\]\nEstas restrições garantem a consistência do modelo: o estado inicial deve necessariamente pertencer ao conjunto de estados, e todos os estados de aceitação devem ser estados válidos da máquina. Mais formalmente, um Autômato Finito Determinístico representa um sistema que, em qualquer momento, encontra-se em exatamente um estado do conjunto \\(Q\\), e que, ao receber um símbolo de entrada do alfabeto \\(\\Sigma\\), transita deterministicamente para um novo estado, que pode, inclusive, ser o mesmo estado atual.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autômatos Finitos Determinísticos</span>"
    ]
  },
  {
    "objectID": "02-lexico.html#sec-definicao-formal",
    "href": "02-lexico.html#sec-definicao-formal",
    "title": "4  Autômatos Finitos Determinísticos",
    "section": "",
    "text": "\\(Q\\) é o conjunto finito de estados da máquina;\n\\(\\Sigma\\) é o alfabeto de entrada, um conjunto finito de símbolos;\n\\(\\delta\\) é a função de transição determinística;\n\\(q_0\\) é o estado inicial único;\n\\(F\\) é o conjunto de estados de aceitação (ou estados finais).\n\n\n\n\n\n4.1.1 Exercícios 1\n1. Dado o Autômato Finito Determinístico \\(M =(\\{s_0,s_1,s_2,s_3\\},\\{a,b\\},\\delta,s_0,\\{s_0,s_3\\})\\), identifique cada um dos cinco componentes da tupla com sua respectiva descrição.\n2. Um modelo foi definido como \\(M =(Q,\\Sigma,\\delta,q_0,F)\\) onde \\(Q=\\{q_0,q_1\\}\\), \\(\\Sigma=\\{0,1\\}\\), \\(q_0=q_0\\), e \\(F=\\{q_2\\}\\). Este modelo representa um Autômato Finito Determinístico válido? Justifique sua resposta com base nas restrições formais.\n3. Se um Autômato Finito Determinístico possui \\(\\mid Q \\mid=3\\) estados e um alfabeto com \\(\\mid \\Sigma \\mid=4\\) símbolos, qual é o número exato de pares no domínio da função de transição \\(\\delta\\)?\n4. É possível que o conjunto de estados de aceitação \\(F\\) seja igual ao conjunto de todos os estados \\(Q\\)? Se sim, o que isso significaria sobre a linguagem reconhecida pelo autômato?\n5. É possível que o estado inicial \\(q_0\\) também seja um estado de aceitação, ou seja, q_0inF? Se sim, o que isso implica sobre a aceitação da string vazia, epsilon?\n\n\n4.1.2 A Função de Transição Determinística\nO coração de um Autômato Finito Determinístico reside na sua função de transição \\(\\delta\\), que captura completamente o comportamento da máquina. Esta função é definida matematicamente como:\n\\[\\delta : Q \\times \\Sigma \\rightarrow Q\\]\nA função de transição \\(\\delta\\) possui uma propriedade fundamental que a distingue de modelos não-determinísticos: para cada par \\((q, a)\\) no qual \\(q \\in Q\\) e \\(a \\in \\Sigma\\), existe exatamente um estado de destino. Esta unicidade elimina qualquer ambiguidade no processo de computação e permite que a máquina seja simulada de forma eficiente em tempo linear.\nA definição matemática da função de transição \\(\\delta : Q \\times \\Sigma \\rightarrow Q\\) implica que ela deve ser uma função total, ou seja, deve haver exatamente uma transição definida para cada par de estado e símbolo de entrada. Um Autômato Finito Determinístico que cumpre essa exigência é chamado de completo.\nNa prática, muitos autômatos possuem combinações de estado e símbolo que quebram a lógica do padrão que está sendo reconhecido. Por exemplo, em um autômato que reconhece a palavra “treco”, o que acontece se ele estiver no estado inicial e ler a letra ‘z’?\nPara tratar esses casos e manter a função de transição total, introduz-se um estado de erro, também conhecido como estado poço ou sumidouro. Este é um estado especial não-final do qual não há escapatória: toda transição a partir do estado de erro aponta para ele mesmo.\n\\[\n\\forall a \\in \\Sigma, \\quad \\delta(q_{\\text{erro}}, a) = q_{\\text{erro}}\n\\]\nDessa forma, qualquer sequência de entrada que desvie do padrão desejado é permanentemente capturada pelo estado de erro, garantindo que a string seja rejeitada. Por uma questão de clareza visual, muitos diagramas de transição omitem o estado de erro e as setas que levam a ele, mas é importante saber que, para um autômato ser formalmente completo, essas transições implícitas devem existir.\nA característica determinística da função de transição tem implicações profundas para a implementação prática. Em um Autômato Finito Determinístico, não há escolhas a serem feitas durante a execução: dado o estado atual e o símbolo de entrada, o próximo estado é inequivocamente determinado. Esta propriedade permite implementações extremamente eficientes, nas quais cada símbolo de entrada requer apenas uma consulta à tabela de transições, tipicamente implementada como um array bidimensional, ou uma operação equivalente.\nO domínio da função \\(\\delta\\) é o produto cartesiano \\(Q \\times \\Sigma\\), que representa o conjunto de todas as combinações possíveis de estado atual e símbolo de entrada. Para uma máquina com cardinalidade \\(\\mid Q \\mid = n\\) estados e um alfabeto com cardinalidade \\(\\mid \\Sigma \\mid = k\\) símbolos, existem exatamente \\(n \\times k\\) transições possíveis, e cada uma deve estar definida para que o autômato seja completo e funcional.\n\n4.1.2.1 Exercícios 2\n1. Considere um Autômato Finito Determinístico que reconhece identificadores que começam com ‘l’ (letra) e são seguidos por ‘d’ (dígito). O alfabeto é \\(\\Sigma=l,d\\). As transições definidas são \\(\\delta(q_0,l)=q_1\\) e \\(\\delta(q_1,d)=q_1\\). O estado de aceitação é \\(F=q_1\\). Este autômato é completo? Se não, adicione um estado de erro \\(q_e\\) e liste todas as transições que faltam para completá-lo.\n2. Explique com suas próprias palavras por que a propriedade determinística (para cada par \\((q,a)\\), existe exatamente um estado de destino) é importante para a eficiência da implementação de um analisador léxico.\n3. Um estado de erro (ou poço) pode ser um estado de aceitação? Justifique sua resposta com base na função de um estado de erro.\n4. Se um Autômato Finito Determinístico completo não possui um estado de erro explicitamente desenhado em seu diagrama, o que se assume sobre as transições não mostradas?\n5. Dada a definição \\(\\delta:Q \\times \\Sigma \\rightarrow Q\\), se a máquina está no estado \\(q_i\\) e lê um símbolo \\(a \\in \\Sigma\\), é possível que ela permaneça no mesmo estado, ou seja, \\(\\delta(q_i,a)=q_i\\)? Dê um exemplo prático.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autômatos Finitos Determinísticos</span>"
    ]
  },
  {
    "objectID": "02-lexico.html#representações-de-autômatos-finitos-determinísticos",
    "href": "02-lexico.html#representações-de-autômatos-finitos-determinísticos",
    "title": "4  Autômatos Finitos Determinísticos",
    "section": "4.2 Representações de Autômatos Finitos Determinísticos",
    "text": "4.2 Representações de Autômatos Finitos Determinísticos\nOs Autômatos Finitos Determinísticos podem ser representados de diversas formas, cada uma adequada a diferentes contextos e propósitos. As três representações mais comuns são a algébrica, a tabular e a gráfica. Cada representação oferece vantagens específicas: a forma algébrica é precisa e compacta para definições formais, a tabela de transições é eficiente para implementação computacional, e o diagrama de transições oferece intuição visual sobre o comportamento da máquina.\n\n4.2.1 Representação Tabular\nA tabela de transições constitui uma das formas mais práticas e amplamente utilizadas para representar Autômatos Finitos Determinísticos, especialmente em implementações computacionais. Esta representação organiza a função de transição \\(\\delta\\) em uma matriz bidimensional, na qual as linhas correspondem aos estados e as colunas aos símbolos do alfabeto.\nA Table 4.1 ilustra um exemplo de tabela de transições para um Autômato Finito Determinístico:\n\n\n\nTable 4.1: Exemplo de tabela de transições para um Autômato Finito Determinístico que aceita strings que contenham pelo menos um ‘0’ seguido, em algum momento, por pelo menos um ‘1’.\n\n\n\n\n\nEstado            \n0    \n1    \n\n\n\n\n\\(\\rightarrow q_0\\)\n\\(q_2\\)\n\\(q_0\\)\n\n\n\\(*q_1\\)            \n\\(q_1\\)\n\\(q_1\\)\n\n\n\\(q_2\\)            \n\\(q_2\\)\n\\(q_1\\)\n\n\n\n\n\n\nNa Table 4.1, as convenções notacionais seguem o padrão estabelecido na literatura de teoria da computação:\n\nOs estados estão listados na primeira coluna, cada um representando um elemento do conjunto \\(Q\\);\nAs colunas subsequentes representam os símbolos do alfabeto \\(\\Sigma = \\{0, 1\\}\\);\nAs células da tabela contêm os estados de destino, definindo completamente a função \\(\\delta\\);\nA seta \\(\\rightarrow\\) identifica o estado inicial \\(q_0\\);\nO asterisco \\(*\\) marca os estados de aceitação, neste caso, o conjunto \\(F = \\{q_1\\}\\).\n\nEsta representação tabular possui vantagens computacionais significativas. A implementação de um Autômato Finito Determinístico baseada em tabela permite acesso em tempo constante \\(O(1)\\) para cada transição, resultando em uma complexidade total de \\(O(n)\\) para processar uma string de entrada de comprimento \\(n\\). Além disso, a estrutura tabular mapeia-se naturalmente para arrays bidimensionais na maioria das linguagens de programação.\n\n\n4.2.2 Representação Gráfica\nA representação gráfica, conhecida como diagrama de transições, oferece uma visualização intuitiva do comportamento dinâmico do Autômato Finito Determinístico. Nesta representação, a máquina é modelada como um grafo direcionado, no qual os vértices representam estados e as arestas rotuladas representam transições.\nA Figure 4.1 apresenta o diagrama de transições correspondente à tabela Table 4.1:\n\n\n\n\n\n\nFigure 4.1: Diagrama de transições de um Autômato Finito Determinístico que aceita strings que contenham pelo menos um ‘0’ seguido, em algum momento, por pelo menos um ‘1’.\n\n\n\nNo diagrama da Figure 4.1, as convenções visuais estabelecem uma linguagem gráfica precisa:\n\nOs círculos representam os estados do conjunto \\(Q\\);\nAs setas direcionadas representam as transições, codificando a função \\(\\delta\\);\nOs rótulos nas setas indicam os símbolos de entrada que causam as transições;\nO estado inicial é identificado por uma seta sem origem, vinda do nada;\nOs estados de aceitação são representados por círculos duplos, destacando visualmente sua função especial.\n\nA representação gráfica é particularmente valiosa para compreender o comportamento global da máquina e para visualizar caminhos de computação. Para a atenta leitora, o diagrama torna evidente que qualquer quantidade de ‘1’s iniciais será ignorada no estado \\(q_0\\). A máquina só avança ao ler um ’0’, transicionando para o estado \\(q_2\\). A partir daí, qualquer ‘1’ lido leva ao estado de aceitação \\(q_1\\), enquanto mais ’0’s mantêm a máquina em \\(q_2\\). Esta visualização facilita a compreensão do fluxo de estados e das condições de aceitação.\n\n\n4.2.3 Especificação Formal do Exemplo\nPara o Autômato Finito Determinístico ilustrado na Table 4.1 e na Figure 4.1, um autômato que aceita strings contendo um ‘0’ seguido em algum momento por um ‘1’ será dada pela 5-tupla \\(M = (Q, \\Sigma, \\delta, q_0, F)\\) definida como:\n\nConjunto de Estados (\\(Q\\)): \\(Q = \\{q_0, q_1, q_2\\}\\);\nAlfabeto (\\(\\Sigma\\)): \\(\\Sigma = \\{0, 1\\}\\);\nEstado Inicial (\\(q_0\\)): o estado inicial é \\(q_0\\);\nConjunto de Estados de Aceitação (\\(F\\)): \\(F = \\{q_1\\}\\);\nFunção de Transição (\\(\\delta\\)):\n\n\\(\\delta(q_0, 0) = q_2\\)\n\\(\\delta(q_0, 1) = q_0\\)\n\\(\\delta(q_1, 0) = q_1\\)\n\\(\\delta(q_1, 1) = q_1\\)\n\\(\\delta(q_2, 0) = q_2\\)\n\\(\\delta(q_2, 1) = q_1\\)\n\n\nEsta especificação formal define completamente o comportamento do autômato, incluindo seus estados, alfabeto, estado inicial, estados de aceitação e a função de transição. A atenta leitora notará que esta definição é suficiente para implementar ou simular o autômato em qualquer sistema computacional.\n\n\n4.2.4 Exercícios 3\n1. Desenhe o diagrama de transições para o Autômato Finito Determinístico descrito pela seguinte tabela. Use as convenções gráficas para o estado inicial e os estados de aceitação.\n\n\n\nEstado\n0\n1\n\n\n\n\nq_A\nq_B\nq_A\n\n\nq_B\nq_B\nq_C\n\n\n*q_C\nq_B\nq_A\n\n\n\n2. Para o diagrama de transições abaixo, que reconhece strings em a,b com um número par de ’a’s e um número par de ’b’s, forneça a especificação formal completa (a 5-tupla).\n3. Converta a especificação formal do Exemplo 2 da seção “Exemplos Práticos” (reconhecimento da senha abre) em uma tabela de transições completa, incluindo o estado de erro e todas as transições para ele. Considere o alfabeto =a,b,r,e,x, onde ‘x’ representa qualquer outro caractere.\n4. Qual representação (tabular, gráfica ou formal) você considera mais útil para depurar o comportamento de um autômato? E para implementá-lo em um programa? Justifique.\n5. Considere a tabela de transições da questão 3.1. Modifique-a para que o autômato passe a aceitar strings que terminam com “010” em vez de “01”.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autômatos Finitos Determinísticos</span>"
    ]
  },
  {
    "objectID": "02-lexico.html#computação-e-aceitação-de-strings",
    "href": "02-lexico.html#computação-e-aceitação-de-strings",
    "title": "4  Autômatos Finitos Determinísticos",
    "section": "4.3 Computação e Aceitação de strings",
    "text": "4.3 Computação e Aceitação de strings\nO processo de computação em um Autômato Finito Determinístico segue um algoritmo simples e elegante. Dada uma string de entrada \\(w = a_1a_2...a_k\\) onde cada \\(a_i \\in \\Sigma\\), a máquina executa os seguintes passos:\n\nInicialização: a máquina posiciona-se no estado inicial \\(q_0\\);\nProcessamento sequencial: para cada símbolo \\(a_i\\) da string de entrada, a máquina transita do estado atual \\(q\\) para o estado \\(\\delta(q, a_i)\\);\nDecisão de aceitação: após processar todos os símbolos, a string é aceita se, e somente se, o estado final atingido pertence ao conjunto \\(F\\) de estados de aceitação.\n\nFormalmente, uma string \\(w\\) é aceita pelo Autômato Finito Determinístico \\(M\\) se existe uma sequência de estados \\(r_0, r_1, ..., r_k\\) tal que:\n\n\\(r_0 = q_0\\) (inicia no estado inicial);\n\\(\\delta(r_i, a_{i+1}) = r_{i+1}\\) para \\(i = 0, 1, ..., k-1\\) (cada transição é válida);\n\\(r_k \\in F\\) (termina em um estado de aceitação).\n\nEsta formalização matemática da computação em Autômatos Finitos Determinísticos fundamenta toda a teoria de Linguagens Regulares e serve como base para os algoritmos de análise léxica utilizados em compiladores modernos.\n\n4.3.1 Exercícios 4\n1. Usando o autômato da questão 3.1, trace a computação para a string 01101 e determine se ela é aceita ou rejeitada.\n2. Para o autômato do Exemplo 3 (operadores relacionais), trace a computação para a string &lt;= e determine o resultado.\n3. Para o mesmo autômato de operadores relacionais, o que acontece ao processar a string !=? E a string =!?\n4. Uma string é aceita se a computação termina em um estado de aceitação. Se uma string passa por um estado de aceitação no meio de sua computação, mas não termina em um, ela é aceita? Justifique com base na definição formal de aceitação.\n5. Descreva uma string de comprimento 5 que é aceita pelo autômato da questão 3.2 e uma que é rejeitada. Mostre o caminho para ambas.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autômatos Finitos Determinísticos</span>"
    ]
  },
  {
    "objectID": "02-lexico.html#exemplos-práticos-de-autômatos-finitos-determinísticos",
    "href": "02-lexico.html#exemplos-práticos-de-autômatos-finitos-determinísticos",
    "title": "4  Autômatos Finitos Determinísticos",
    "section": "4.4 Exemplos Práticos de Autômatos Finitos Determinísticos",
    "text": "4.4 Exemplos Práticos de Autômatos Finitos Determinísticos\nPara consolidar a compreensão dos conceitos apresentados, a dedicada leitora encontrará a seguir três exemplos práticos que demonstram a versatilidade e aplicabilidade dos Autômatos Finitos Determinísticos em diferentes contextos. Estes exemplos ilustram desde problemas matemáticos básicos, como verificação de paridade, até aplicações práticas em análise léxica, como reconhecimento de senhas e operadores relacionais.\n\n4.4.1 Exemplo 1: Reconhecimento de Números Binários com Paridade Par\nO primeiro exemplo aborda um problema clássico na teoria da computação: o reconhecimento de números binários que possuem uma quantidade par de bits ‘1’. Este problema ilustra elegantemente como um Autômato Finito Determinístico pode manter informações de estado sobre propriedades matemáticas da entrada processada.\nDefinição do Problema: construir um Autômato Finito Determinístico que aceite todas as strings binárias (sobre o alfabeto \\(\\Sigma = \\{0, 1\\}\\)) que contenham um número par de símbolos ‘1’, incluindo zero ocorrências.\nAnálise: O autômato deve contar o número de bits ‘1’ processados, distinguindo entre quantidades pares e ímpares. Como só precisamos da paridade (par ou ímpar), dois estados são suficientes para capturar toda a informação necessária.\nUma forma de construir este autômato é definir dois estados: um para quando o número de ‘1’s é par e outro para quando é ímpar. A transição entre esses estados ocorre sempre que um símbolo ’1’ é lido, enquanto os símbolos ‘0’ não afetam a paridade. Em geral, tentar desenhar o diagrama de transições melhora a compreensão do comportamento do autômato. Neste caso, o Figure 4.2 ilustra o diagrama de transições correspondente:\n\n\n\n\n\n\nFigure 4.2: Diagrama de transições de um Autômato Finito Determinístico que aceita strings binárias com número par de símbolos ‘1’.\n\n\n\nEspecificação Formal:\n\nEstados: \\(Q = \\{q_{\\text{par}}, q_{\\text{ímpar}}\\}\\);\nAlfabeto: \\(\\Sigma = \\{0, 1\\}\\);\nEstado inicial: \\(q_0 = q_{\\text{par}}\\), zero, nenhum ‘1’, é par;\nEstados de aceitação: \\(F = \\{q_{\\text{par}}\\}\\);\nFunção de transição: \\(\\delta\\) definida por:\n\n\\(\\delta(q_{\\text{par}}, 0) = q_{\\text{par}}\\), zeros não alteram a paridade;\n\\(\\delta(q_{\\text{par}}, 1) = q_{\\text{ímpar}}\\), primeiro ‘1’ torna ímpar;\n\\(\\delta(q_{\\text{ímpar}}, 0) = q_{\\text{ímpar}}\\), zeros não alteram a paridade;\n\\(\\delta(q_{\\text{ímpar}}, 1) = q_{\\text{par}}\\), segundo ‘1’ retorna ao par.\n\n\nA Table 4.2 apresenta a tabela de transições correspondente:\n\n\n\nTable 4.2: Tabela de transições para reconhecimento de números binários com paridade par.\n\n\n\n\n\n\n\n\n\n\nEstado\n0\n1\n\n\n\n\n\\(\\rightarrow *q_{\\text{par}}\\)\n\\(q_{\\text{par}}\\)\n\\(q_{\\text{ímpar}}\\)\n\n\n\\(q_{\\text{ímpar}}\\)\n\\(q_{\\text{ímpar}}\\)\n\\(q_{\\text{par}}\\)\n\n\n\n\n\n\nEste autômato funciona como um contador módulo 2. O estado \\(q_{\\text{par}}\\) representa que um número par de ‘1’s foi processado, enquanto \\(q_{\\text{ímpar}}\\) indica um número ímpar. Os bits ’0’ são irrelevantes para a paridade e não causam mudanças de estado. Este é um exemplo perfeito de como um Autômato Finito Determinístico pode manter informações agregadas sobre a entrada sem necessidade de memória ilimitada. A definição zero é par pode ser vista como uma convenção prática. Quando temos apenas um zero não há ’1’s para contar. Por isso, o estado inicial (que representa a leitura de zero ’1’s) é também um estado de aceitação.\n\n\n4.4.2 Exemplo 2: Reconhecimento da Senha abre\nO segundo exemplo demonstra como Autômatos Finitos Determinísticos podem ser utilizados para reconhecer sequências específicas de caracteres, como senhas ou palavras-chave em linguagens de programação. Este tipo de reconhecimento é fundamental em analisadores léxicos.\nDefinição do Problema: construir um Autômato Finito Determinístico que aceite exatamente a string abre sobre o alfabeto das letras minúsculas.\nAnálise: o autômato deve reconhecer a sequência exata de caracteres ‘a’, ‘b’, ‘r’, ‘e’. Qualquer desvio desta sequência deve levar a um estado de rejeição. Como a entrada deve ser exatamente abre, necessitamos de cinco estados: um inicial, três intermediários correspondentes aos prefixos a, ab, abr, e um final de aceitação que corresponda a abre.\nNovamente, o diagrama de transições ajuda a visualizar o comportamento do autômato. A Figure 4.3 ilustra o diagrama de transições correspondente:\n\n\n\n\n\n\nFigure 4.3: Diagrama de transições de um Autômato Finito Determinístico que aceita a string abre.\n\n\n\nNo diagrama da Figure 4.3, cada estado representa um prefixo da string abre. A transição entre os estados ocorre conforme os caracteres são lidos, e o autômato rejeita qualquer entrada que não siga a sequência exata representada no diagrama por \\(x\\). O estado final \\(q_4\\) é o único estado de aceitação, indicando que a string completa foi reconhecida com sucesso.\nEspecificação Formal:\n\nEstados: \\(Q = \\{q_0, q_1, q_2, q_3, q_4, q_{\\text{erro}}\\}\\).\nAlfabeto: \\(\\Sigma = \\{a, b, c, ..., z\\}\\) (letras minúsculas).\nEstado inicial: \\(q_0\\).\nEstados de aceitação: \\(F = \\{q_4\\}\\).\nFunção de transição: \\(\\delta\\) definida por:\n\n\\(\\delta(q_0, a) = q_1\\), \\(\\delta(q_0, x) = q_{\\text{erro}}\\) para \\(x \\neq a\\);\n\\(\\delta(q_1, b) = q_2\\), \\(\\delta(q_1, x) = q_{\\text{erro}}\\) para \\(x \\neq b\\);\n\\(\\delta(q_2, r) = q_3\\), \\(\\delta(q_2, x) = q_{\\text{erro}}\\) para \\(x \\neq r\\);\n\\(\\delta(q_3, e) = q_4\\), \\(\\delta(q_3, x) = q_{\\text{erro}}\\) para \\(x \\neq e\\);\n\\(\\delta(q_4, x) = q_{\\text{erro}}\\) para qualquer \\(x \\in \\Sigma\\);\n\\(\\delta(q_{\\text{erro}}, x) = q_{\\text{erro}}\\) para qualquer \\(x \\in \\Sigma\\).\n\n\nA Table 4.3 apresenta uma versão simplificada da tabela de transições:\n\n\n\nTable 4.3: Tabela de transições para reconhecimento da senha abre.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstado\na\nb\nr\ne\noutros\n\n\n\n\n\\(\\rightarrow q_0\\)\n\\(q_1\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(q_1\\)\n\\(q_{\\text{erro}}\\)\n\\(q_2\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(q_2\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_3\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(q_3\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_4\\)\n\\(q_{\\text{erro}}\\)\n\n\n*\\(q_4\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\n\n\n\n\n\n4.4.3 Exemplo 3: Reconhecimento de Operadores Relacionais\nO terceiro exemplo aborda um problema típico da análise léxica: o reconhecimento de operadores relacionais compostos. Este exemplo ilustra como tratar ambiguidades que surgem quando alguns tokens são prefixos de outros (1).\nDefinição do Problema: Construir um Autômato Finito Determinístico que reconheça os operadores relacionais: &gt;, &lt;, &gt;=, &lt;=, ==, e !=.\nAnálise: Este problema apresenta desafios interessantes. Os operadores &gt; e &lt; são prefixos dos operadores &gt;= e &lt;=, respectivamente. Similarmente, = seria prefixo de ==. O autômato deve implementar a regra da correspondência mais longa (maximal munch), continuando a ler enquanto uma correspondência mais longa for possível.\nvértices discutimos a regra da correspondência mais longa na seção Section 2.2 do capítulo anterior. Esta regra é fundamental para resolver ambiguidades de prefixos. O autômato deve ser capaz de distinguir entre os operadores simples e compostos, aceitando os mais longos quando possível.\nVamos ver como construir o autômato passo a passo, definindo estados que correspondem a cada prefixo dos operadores. O estado inicial \\(q_0\\) inicia o reconhecimento, e os estados subsequentes são alcançados conforme os símbolos de entrada são lidos. A Figure 4.4 ilustra o diagrama de transições correspondente:\n\n\n\n\n\n\nFigure 4.4: Diagrama de transições de um Autômato Finito Determinístico capaz de identificar os operadores relacionais ‘&lt;’, ‘&gt;’, ‘&lt;=’, ‘&gt;=’, ‘==’, ‘!=’.\n\n\n\nEspecificação Formal:\n\nEstados: \\(Q = \\{q_0, q_&gt;, q_&lt;, q_=, q_!, q_{\\geq}, q_{\\leq}, q_{==}, q_{\\neq}, q_{\\text{erro}}\\}\\) ;\nAlfabeto: \\(\\Sigma = \\{&gt;, &lt;, =, !, \\text{outros}\\}\\) ;\nEstado inicial: \\(q_0\\) ;\nEstados de aceitação: \\(F = \\{q_&gt;, q_&lt;, q_{\\geq}, q_{\\leq}, q_{==}, q_{\\neq}\\}\\).\n\nA Table 4.4 apresenta a tabela de transições:\n\n\n\nTable 4.4: Tabela de transições para reconhecimento de operadores relacionais.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstado\n&gt;\n&lt;\n=\n!\noutros\n\n\n\n\n\\(\\rightarrow q_0\\)\n\\(*q_&gt;\\)\n\\(*q_&lt;\\)\n\\(q_=\\)\n\\(q_!\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(*q_&gt;\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\geq}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(*q_&lt;\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\leq}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(*q_=\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{==}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(*q_!\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\neq}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(*q_{\\geq}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(*q_{\\leq}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(*q_{==}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(*q_{\\neq}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\n\n\n\nNote que os estados \\(q_&gt;\\) e \\(q_&lt;\\) são marcados como estados de aceitação, permitindo que os operadores simples &gt; e &lt; sejam reconhecidos. Contudo, se a entrada continuar com =, o autômato transita para os estados de aceitação \\(q_{\\geq}\\) ou \\(q_{\\leq}\\), implementando assim a regra da correspondência mais longa. Os estados \\(q_=\\) e \\(q_!\\) não são de aceitação porque = e ! isolados não são operadores relacionais válidos neste contexto - apenas == e != são aceitos.\n\n\n4.4.4 Exercícios 5\n1. Projete o diagrama de transições de um Autômato Finito Determinístico sobre \\(\\Sigma=\\{a,b\\}\\) que aceite todas e somente as strings que contêm a substring aba.\n2. Projete o diagrama de transições de um Autômato Finito Determinístico sobre \\(\\Sigma=\\{0,1\\}\\) que aceite strings que representam números binários cujo valor é múltiplo de \\(3\\). (Dica: Mantenha o resto da divisão por \\(3\\) como estado. \\(v(w0)=2 \\cdot v(w)\\) e \\(v(w1)=2 \\cdot v(w)+1\\)).\n3. Projete a tabela de transições de um Autômato Finito Determinístico que reconhece comentários de uma linha em uma linguagem de programação hipotética. Um comentário começa com // e vai até o final da linha (não nos preocuparemos com o final da linha). O alfabeto é \\(\\Sigma=\\{/,c\\}\\), onde ‘c’ representa qualquer outro caractere.\n4. Desenhe o diagrama de um Autômato Finito Determinístico que aceite strings em \\(\\Sigma=\\{a,b\\}\\) que tenham comprimento ímpar e terminem com ‘a’.\n5. Projete um Autômato Finito Determinístico que aceite apenas as strings cat e car sobre o alfabeto \\(\\Sigma=\\{c,a,t,r\\}\\).",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autômatos Finitos Determinísticos</span>"
    ]
  },
  {
    "objectID": "02-lexico.html#considerações-sobre-implementação-prática",
    "href": "02-lexico.html#considerações-sobre-implementação-prática",
    "title": "4  Autômatos Finitos Determinísticos",
    "section": "4.5 Considerações sobre Implementação Prática",
    "text": "4.5 Considerações sobre Implementação Prática\nEstes três exemplos demonstram alguns aspectos importantes dos Autômatos Finitos Determinísticos na análise léxica:\n\nManutenção de Estado: O exemplo da paridade mostra como informações agregadas podem ser mantidas em estados finitos.\nReconhecimento de Sequências: O exemplo da senha ilustra o reconhecimento determinístico de strings específicas, fundamental para palavras-chave.\nTratamento de Ambiguidades: O exemplo dos operadores demonstra como resolver conflitos de prefixos por meio da regra da correspondência mais longa.\n\nA atenta leitora observará que estes padrões são ubíquos na construção de analisadores léxicos em aplicações práticas, formando os blocos que definem o reconhecimento de identificadores, números, palavras-chave e operadores em linguagens de programação reais.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autômatos Finitos Determinísticos</span>"
    ]
  },
  {
    "objectID": "02-lexico.html#propriedades-matemáticas-dos-autômatos-finitos-determinísticos",
    "href": "02-lexico.html#propriedades-matemáticas-dos-autômatos-finitos-determinísticos",
    "title": "4  Autômatos Finitos Determinísticos",
    "section": "4.6 Propriedades Matemáticas dos Autômatos Finitos Determinísticos",
    "text": "4.6 Propriedades Matemáticas dos Autômatos Finitos Determinísticos\nOs Autômatos Finitos Determinísticos não são apenas modelos computacionais elegantes; eles formam uma estrutura algébrica rica, dotada de propriedades matemáticas que fundamentam sua aplicabilidade prática. A perspicaz leitora descobrirá que estas propriedades não apenas garantem a robustez teórica do modelo, mas também possibilitam otimizações e transformações essenciais para implementações eficientes.\n\n4.6.1 Propriedades de Fechamento\nUma das características notáveis da classe dos Autômatos Finitos Determinísticos é que ela é fechada sob as operações fundamentais da teoria dos conjuntos. Isto significa que, ao combinarmos Autômatos Finitos Determinísticos usando estas operações, o resultado é sempre outro Autômato Finito Determinístico. Esta propriedade tem implicações profundas tanto teóricas quanto práticas, especialmente na construção de analisadores léxicos que precisam reconhecer múltiplos padrões simultaneamente.\nConsidere o seguinte cenário prático: um analisador léxico precisa reconhecer tanto identificadores válidos quanto palavras-chave reservadas. Em vez de construir um único autômato complexo que trate ambos os casos, seria elegante construir dois autômatos simples e depois combiná-los. As propriedades de fechamento garantem que tal composição sempre resulta em outro Autômato Finito Determinístico válido, permitindo uma abordagem modular e sistemática para problemas complexos de reconhecimento.\n\n4.6.1.1 Fechamento sob União\nTeorema (Fechamento sob União): Sejam \\(M_1\\) e \\(M_2\\) dois Autômatos Finitos Determinísticos que reconhecem as linguagens \\(L_1\\) e \\(L_2\\), respectivamente. Então existe um Autômato Finito Determinístico \\(M\\) que reconhece \\(L_1 \\cup L_2\\).\nA intuição por trás da construção da união é elegante: executamos ambos os autômatos simultaneamente sobre a mesma entrada, mantendo o estado atual de cada um em um par ordenado. A atenta leitora pode visualizar isto como duas máquinas trabalhando em paralelo, processando o mesmo fluxo de símbolos. Se ao menos uma delas aceitar a entrada, o autômato combinado também aceita.\nConstrução Formal do Produto: Dados:\n\n\\(M_1 = (Q_1, \\Sigma, \\delta_1, q_{01}, F_1)\\);\n\\(M_2 = (Q_2, \\Sigma, \\delta_2, q_{02}, F_2)\\).\n\nConstruímos \\(M = (Q, \\Sigma, \\delta, q_0, F)\\) no qual:\n\n\\(Q = Q_1 \\times Q_2\\) (produto cartesiano dos conjuntos de estados);\n\\(q_0 = (q_{01}, q_{02})\\) (par dos estados iniciais);\n\\(F = (F_1 \\times Q_2) \\cup (Q_1 \\times F_2)\\) (aceita se pelo menos um dos autômatos aceita);\n\\(\\delta((q_1, q_2), a) = (\\delta_1(q_1, a), \\delta_2(q_2, a))\\) para todo \\((q_1, q_2) \\in Q\\) e \\(a \\in \\Sigma\\).\n\nPara entender esse processo, considere dois Autômatos Finitos Determinísticos sobre o alfabeto \\(\\Sigma = \\{a, b\\}\\):\n\nAutômato \\(M_1\\): Aceita strings que começam com ‘a’\n\n\nEstados: \\(Q_1 = \\{s_0, s_{\\text{aceita}}, s_{\\text{rejeita}}\\}\\)\nEstado inicial: \\(s_0\\)\nEstados finais: \\(F_1 = \\{s_{\\text{aceita}}\\}\\)\nTransições:\n\n\\(\\delta_1(s_0, a) = s_{\\text{aceita}}\\)\n\\(\\delta_1(s_0, b) = s_{\\text{rejeita}}\\)\n\\(\\delta_1(s_{\\text{aceita}}, x) = s_{\\text{aceita}}\\) para \\(x \\in \\{a,b\\}\\)\n\\(\\delta_1(s_{\\text{rejeita}}, x) = s_{\\text{rejeita}}\\) para \\(x \\in \\{a,b\\}\\)\n\n\n\nAutômato \\(M_2\\): Aceita strings com número par de ’b’s\n\n\nEstados: \\(Q_2 = \\{t_{\\text{par}}, t_{\\text{ímpar}}\\}\\)\nEstado inicial: \\(t_{\\text{par}}\\)\nEstados finais: \\(F_2 = \\{t_{\\text{par}}\\}\\)\nTransições:\n\n\\(\\delta_2(t_{\\text{par}}, a) = t_{\\text{par}}\\), \\(\\delta_2(t_{\\text{par}}, b) = t_{\\text{ímpar}}\\)\n\\(\\delta_2(t_{\\text{ímpar}}, a) = t_{\\text{ímpar}}\\), \\(\\delta_2(t_{\\text{ímpar}}, b) = t_{\\text{par}}\\)\n\n\nA construção do autômato união \\(M = M_1 \\cup M_2\\) resulta em:\nEstados: \\(Q = Q_1 \\times Q_2\\) possui \\(\\mid Q_1 \\mid \\times \\mid Q_2 \\mid = 3 \\times 2 = 6\\) estados\nEstados de Aceitação: um estado \\((s_i, t_j)\\) é final se \\(s_i \\in F_1\\) ou \\(t_j \\in F_2\\). Analisando cada par, teremos:\n\n\\((s_0, t_{\\text{par}})\\): aceita (porque \\(t_{\\text{par}} \\in F_2\\));\n\\((s_0, t_{\\text{ímpar}})\\): rejeita;\n\\((s_{\\text{aceita}}, t_{\\text{par}})\\): aceita (ambos aceitam);\n\\((s_{\\text{aceita}}, t_{\\text{ímpar}})\\): aceita (porque \\(s_{\\text{aceita}} \\in F_1\\));\n\\((s_{\\text{rejeita}}, t_{\\text{par}})\\): aceita (porque \\(t_{\\text{par}} \\in F_2\\));\n\\((s_{\\text{rejeita}}, t_{\\text{ímpar}})\\): rejeita.\n\nA Table 4.5 apresenta a tabela de transições completa:\n\n\n\nTable 4.5: Tabela de transições para o autômato união \\(M_1 \\cup M_2\\)\n\n\n\n\n\n\n\n\n\n\nEstado\na\nb\n\n\n\n\n\\(\\rightarrow *(s_0, t_{\\text{par}})\\)\n\\((s_{\\text{aceita}}, t_{\\text{par}})\\)\n\\((s_{\\text{rejeita}}, t_{\\text{ímpar}})\\)\n\n\n\\((s_0, t_{\\text{ímpar}})\\)\n\\((s_{\\text{aceita}}, t_{\\text{ímpar}})\\)\n\\((s_{\\text{rejeita}}, t_{\\text{par}})\\)\n\n\n\\(*(s_{\\text{aceita}}, t_{\\text{par}})\\)\n\\((s_{\\text{aceita}}, t_{\\text{par}})\\)\n\\((s_{\\text{aceita}}, t_{\\text{ímpar}})\\)\n\n\n\\(*(s_{\\text{aceita}}, t_{\\text{ímpar}})\\)\n\\((s_{\\text{aceita}}, t_{\\text{ímpar}})\\)\n\\((s_{\\text{aceita}}, t_{\\text{par}})\\)\n\n\n\\(*(s_{\\text{rejeita}}, t_{\\text{par}})\\)\n\\((s_{\\text{rejeita}}, t_{\\text{par}})\\)\n\\((s_{\\text{rejeita}}, t_{\\text{ímpar}})\\)\n\n\n\\((s_{\\text{rejeita}}, t_{\\text{ímpar}})\\)\n\\((s_{\\text{rejeita}}, t_{\\text{ímpar}})\\)\n\\((s_{\\text{rejeita}}, t_{\\text{par}})\\)\n\n\n\n\n\n\nVerificação: Testemos a string “ba”:\n\n\\((s_0, t_{\\text{par}}) \\xrightarrow{b} (s_{\\text{rejeita}}, t_{\\text{ímpar}}) \\xrightarrow{a} (s_{\\text{rejeita}}, t_{\\text{ímpar}})\\);\nEstado final: \\((s_{\\text{rejeita}}, t_{\\text{ímpar}})\\) não é de aceitação;\n\\(M_1\\) rejeita (não começa com ‘a’), \\(M_2\\) rejeita (número ímpar de ’b’s);\nUnião rejeita corretamente.\n\nA matemática é ótima, contudo, a construção da união pode requerer um pouco de prática para ser compreendida completamente. Para treinar, vamos construir a união de dois autômatos simples, ainda sobre o alfabeto \\(\\Sigma = \\{a, b\\}\\), com uma pequena alteração do exemplo anterior, para solidificar o processo algébrico. Considere os autômatos a seguir:\nAutômato \\(M_1\\): Aceita strings que começam com ‘a’: um autômato que reconhece a linguagem \\(L_1 = \\{a, aa, ab, aaa, aab, aba, aaaa, ...\\}\\). Algebricamente, teremos:\n\nEstados: \\(Q_1 = \\{s_0, s_1, s_2\\}\\) nos quais:\n\n\\(s_0\\): estado inicial (ainda não leu nada);\n\\(s_1\\): leu ‘a’ como primeiro símbolo (aceita);\n\\(s_2\\): leu ‘b’ como primeiro símbolo (rejeita).\n\nEstado inicial: \\(s_0\\).\nEstados finais: \\(F_1 = \\{s_1\\}\\).\nTransições:\n\n\\(\\delta_1(s_0, a) = s_1\\) (primeiro símbolo é ‘a’ → aceita);\n\\(\\delta_1(s_0, b) = s_2\\) (primeiro símbolo é ‘b’ → rejeita);\n\\(\\delta_1(s_1, a) = s_1\\) (já aceitou, continua aceitando);\n\\(\\delta_1(s_1, b) = s_1\\) (já aceitou, continua aceitando);\n\\(\\delta_1(s_2, a) = s_2\\) (já rejeitou, continua rejeitando);\n\\(\\delta_1(s_2, b) = s_2\\) (já rejeitou, continua rejeitando);\n\n\nAutômato \\(M_2\\): Aceita strings com número ímpar de ’b’s: um autômato que reconhece a linguagem \\(L_2 = \\{b, ab, aab, aba, aaab, aababb, ...\\}\\). Que pode ser especificado como:\n\nEstados: \\(Q_2 = \\{t_0, t_1\\}\\) onde:\n\n\\(t_0\\): número par de ’b’s (incluindo zero);\n\\(t_1\\): número ímpar de ’b’s.\n\nEstado inicial: \\(t_0\\).\nEstados finais: \\(F_2 = \\{t_1\\}\\).\nTransições:\n\n\\(\\delta_2(t_0, a) = t_0\\) (‘a’ não afeta a contagem de ’b’s);\n\\(\\delta_2(t_0, b) = t_1\\) (de par para ímpar);\n\\(\\delta_2(t_1, a) = t_1\\) (‘a’ não afeta a contagem);\n\\(\\delta_2(t_1, b) = t_0\\) (de ímpar para par).\n\n\nOs autômatos \\(M_1\\) e \\(M_2\\) podem ser vistos, representados como diagramas de transição na Figure 4.5.\n\n\n\n\n\n\nDiagrama de transição do autômato \\(M_1\\) e \\(M_2\\)\n\n\n\n\nFigure 4.5\n\n\n\nPara construir o autômato união \\(M = M_1 \\cup M_2\\), a esforçada leitora deve seguir os seguintes passos:\n1. Passo 1: Criar o conjunto de estados: o conjunto de estados do autômato união será dado pelo o produto cartesiano \\(Q = Q_1 \\times Q_2\\). Enumerando todos os pares possíveis teremos:\n\n\\((s_0, t_0)\\): \\(M_1\\) no estado inicial e \\(M_2\\) com número par de ’b’s;\n\\((s_0, t_1)\\): \\(M_1\\) no estado inicial e \\(M_2\\) com número ímpar de ’b’s;\n\\((s_1, t_0)\\): \\(M_1\\) aceitando (começou com ‘a’) e \\(M_2\\) com número par de ’b’s;\n\\((s_1, t_1)\\): \\(M_1\\) aceitando e \\(M_2\\) com número ímpar de ’b’s;\n\\((s_2, t_0)\\): \\(M_1\\) rejeitando (começou com ‘b’) e \\(M_2\\) com número par de ’b’s;\n\\((s_2, t_1)\\): \\(M_1\\) rejeitando e \\(M_2\\) com número ímpar de ’b’s.\n\nTotal: \\(6\\) estados \\((3 \\times 2 = 6)\\).\n2. Passo 2: Determinar o estado inicial: o estado inicial é simplesmente o par dos estados iniciais:\n\n\\(q_0 = (s_0, t_0)\\)\n\n3. Passo 3: Determinar os estados finais: para a união, um estado é final se pelo menos um dos autômatos originais estaria em estado final. Para determinar os estados finais, verificamos quais pares \\((s_i, t_j)\\) satisfazem a condição de aceitação. Para cada estado perguntamos:“\\(M_1\\) aceita OU \\(M_2\\) aceita?”\n\n\\((s_0, t_0)\\): \\(s_0 \\notin F_1\\) e \\(t_0 \\notin F_2\\) → NÃO é final;\n\\((s_0, t_1)\\): \\(s_0 \\notin F_1\\) mas \\(t_1 \\in F_2\\) → É FINAL;\n\\((s_1, t_0)\\): \\(s_1 \\in F_1\\) mas \\(t_0 \\notin F_2\\) → É FINAL;\n\\((s_1, t_1)\\): \\(s_1 \\in F_1\\) e \\(t_1 \\in F_2\\) → É FINAL;\n\\((s_2, t_0)\\): \\(s_2 \\notin F_1\\) e \\(t_0 \\notin F_2\\) → NÃO é final;\n\\((s_2, t_1)\\): \\(s_2 \\notin F_1\\) mas \\(t_1 \\in F_2\\) → É FINAL.\n\nAssim, os Estados finais são: \\(F = \\{(s_0, t_1), (s_1, t_0), (s_1, t_1), (s_2, t_1)\\}\\)\n4. Passo 4: Construir a função de transição: para cada estado \\((s_i, t_j)\\) e cada símbolo \\(x \\in \\{a, b\\}\\), calculamos: \\[\\delta((s_i, t_j), x) = (\\delta_1(s_i, x), \\delta_2(t_j, x))\\]\nTransições com ‘a’:\n\n\\(\\delta((s_0, t_0), a) = (\\delta_1(s_0, a), \\delta_2(t_0, a)) = (s_1, t_0)\\);\n\\(\\delta((s_0, t_1), a) = (\\delta_1(s_0, a), \\delta_2(t_1, a)) = (s_1, t_1)\\);\n\\(\\delta((s_1, t_0), a) = (\\delta_1(s_1, a), \\delta_2(t_0, a)) = (s_1, t_0)\\);\n\\(\\delta((s_1, t_1), a) = (\\delta_1(s_1, a), \\delta_2(t_1, a)) = (s_1, t_1)\\);\n\\(\\delta((s_2, t_0), a) = (\\delta_1(s_2, a), \\delta_2(t_0, a)) = (s_2, t_0)\\);\n\\(\\delta((s_2, t_1), a) = (\\delta_1(s_2, a), \\delta_2(t_1, a)) = (s_2, t_1)\\).\n\nTransições com ‘b’:\n\n\\(\\delta((s_0, t_0), b) = (\\delta_1(s_0, b), \\delta_2(t_0, b)) = (s_2, t_1)\\);\n\\(\\delta((s_0, t_1), b) = (\\delta_1(s_0, b), \\delta_2(t_1, b)) = (s_2, t_0)\\);\n\\(\\delta((s_1, t_0), b) = (\\delta_1(s_1, b), \\delta_2(t_0, b)) = (s_1, t_1)\\);\n\\(\\delta((s_1, t_1), b) = (\\delta_1(s_1, b), \\delta_2(t_1, b)) = (s_1, t_0)\\);\n\\(\\delta((s_2, t_0), b) = (\\delta_1(s_2, b), \\delta_2(t_0, b)) = (s_2, t_1)\\);\n\\(\\delta((s_2, t_1), b) = (\\delta_1(s_2, b), \\delta_2(t_1, b)) = (s_2, t_0)\\).\n\n5. Passo 5: Construir a Tabela de Transições da União:\n\n\n\nEstado\na\nb\n\n\n\n\n→(s₀,t₀)\n(s₁,t₀)\n(s₂,t₁)\n\n\n*(s₀,t₁)\n(s₁,t₁)\n(s₂,t₀)\n\n\n*(s₁,t₀)\n(s₁,t₀)\n(s₁,t₁)\n\n\n*(s₁,t₁)\n(s₁,t₁)\n(s₁,t₀)\n\n\n(s₂,t₀)\n(s₂,t₀)\n(s₂,t₁)\n\n\n*(s₂,t₁)\n(s₂,t₁)\n(s₂,t₀)\n\n\n\n6. Passo 6: Depuração: vamos testar algumas strings para verificar que o autômato funciona corretamente:\n\nString ab (começa com ‘a’ e tem um ‘b’, número ímpar de ’b’s):\n\n\n\\((s_0,t_0) \\xrightarrow{a} (s_1,t_0) \\xrightarrow{b} (s_1,t_1)\\) ACEITA;\n\\(M_1\\) aceita (começa com ‘a’);\n\\(M_2\\) aceita (um ‘b’ = ímpar);\nUnião aceita.\n\n\nString b (não começa com ‘a’ mas tem número ímpar de ’b’s):\n\n\n\\((s_0,t_0) \\xrightarrow{b} (s_2,t_1)\\) ACEITA;\n\\(M_1\\) rejeita (começa com ‘b’);\n\\(M_2\\) aceita (um ‘b’ = ímpar);\nUnião aceita (pelo menos um aceita).\n\n\nString bb (não começa com ‘a’ e tem número par de ’b’s):\n\n\n\\((s_0,t_0) \\xrightarrow{b} (s_2,t_1) \\xrightarrow{b} (s_2,t_0)\\) REJEITA;\n\\(M_1\\) rejeita (começa com ‘b’);\n\\(M_2\\) rejeita (dois ’b’s = par);\nUnião rejeita (nenhum aceita).\n\n\nString aaa (começa com ‘a’ mas zero ’b’s):\n\n\n\\((s_0,t_0) \\xrightarrow{a} (s_1,t_0) \\xrightarrow{a} (s_1,t_0) \\xrightarrow{a} (s_1,t_0)\\) ACEITA;\n\\(M_1\\) aceita (começa com ‘a’);\n\\(M_2\\) rejeita (zero ’b’s = par);\nUnião aceita (pelo menos um aceita).\n\nO autômato resultante da união entre \\(M_1\\) e \\(M_2\\) terá estados que representam todas as combinações possíveis de estados de \\(M_1\\) e \\(M_2\\) e pode ser visto na Figure 4.6.\n\n\n\n\n\n\nFigure 4.6\n\n\n\nA atenta leitora deve notar que a construção por produto cartesiano funciona como se estivéssemos executando ambos os autômatos simultaneamente em duas trilhas paralelas. A cada símbolo lido:\n\nAtualizamos ambas as trilhas: cada componente do par de estados evolui independentemente segundo sua própria função de transição;\nMantemos a história completa: o estado \\((s_i, t_j)\\) nos diz exatamente onde cada autômato original estaria após processar a entrada até aquele ponto;\nDecidimos aceitação com lógica OR: Para a união, basta que uma das trilhas aceite para o autômato combinado aceitar.\n\nEste algoritmo de construção em seis passos, com um passo específico para depuração, sempre funciona, servindo como evidência de que a classe das linguagens regulares é fechada sob união.\n\n4.6.1.1.1 Exercícios 6\n1. Considere dois Autômatos Finitos Determinísticos sobre \\(\\Sigma=\\{a,b\\}\\): \\(M_1\\) aceita strings que contêm aa e \\(M_2\\) aceita strings de comprimento ímpar. Quantos estados terá o autômato produto \\(M = M_1 \\cup M_2\\) se \\(\\mid Q_1 \\mid = 3\\) e \\(\\mid Q_2 \\mid = 2\\)?\n2. No autômato união, um estado \\((p, q)\\) é de aceitação quando \\(p \\in F_1\\) ou \\(q \\in F_2\\). Se modificássemos a condição para \\(p \\in F_1\\) e \\(q \\in F_2\\), que operação estaríamos realizando?\n3. Construa a tabela de transições completa para a união de: \\(M_1\\) que aceita apenas a e \\(M_2\\) que aceita apenas b, sobre \\(\\Sigma = \\{a, b\\}\\).\n4. Considere dois autômatos sobre \\(\\Sigma = \\{0, 1\\}\\): \\(M_1\\) aceita strings que terminam em ‘01’ e \\(M_2\\) aceita strings com número par de ’1’s. Construa a tabela de transições para o autômato união \\(M_1 \\cup M_2\\).\n5. Dados dois autômatos binários \\(M_1\\) e \\(M_2\\) onde \\(M_1\\) aceita strings que começam com ‘1’ (2 estados) e \\(M_2\\) aceita strings de comprimento par (2 estados). Construa o passo a passo completo para determinar o autômato união \\(M_1 \\cup M_2\\) sobre \\(\\Sigma = \\{0, 1\\}\\).\n\n\n\n4.6.1.2 Fechamento sob Interseção\nTeorema (Fechamento sob Interseção): Sejam \\(M_1\\) e \\(M_2\\) dois Autômatos Finitos Determinísticos que reconhecem as linguagens \\(L_1\\) e \\(L_2\\), respectivamente. Então existe um Autômato Finito Determinístico \\(M\\) que reconhece \\(L_1 \\cap L_2\\).\nA elegante leitora notará que a construção da interseção é notavelmente similar à da união. A diferença reside apenas na definição dos estados finais: enquanto a união implementa uma lógica OU, a interseção implementa uma lógica E.\nConstrução Formal: Utilizamos a mesma estrutura de produto cartesiano, mas com:\n\\[F = F_1 \\times F_2\\]\nOu seja, um estado \\((q_1, q_2)\\) é final se, e somente se, ambos \\(q_1 \\in F_1\\) e \\(q_2 \\in F_2\\).\nVamos construir um exemplo mais elaborado para solidificar o entendimento da construção de interseção. Considere os autômatos \\(M_1\\) e \\(M_2\\) descritos a seguir:\nAutômato \\(M_1\\): aceita strings que começam com ‘a’. A linguagem formal é:\n\\[ L_1 = \\{ w \\in \\{a, b\\}^* \\mid w = a \\cdot x \\text{ para algum } x \\in \\{a, b\\}^* \\} \\]\n\nEstados: \\(Q_1 = \\{s_0, s_1, s_2\\}\\) onde:\n\n\\(s_0\\): estado inicial (ainda não leu nada)\n\\(s_1\\): leu ‘a’ como primeiro símbolo (aceita)\n\\(s_2\\): leu ‘b’ como primeiro símbolo (rejeita)\n\nEstado inicial: \\(s_0\\)\nEstados finais: \\(F_1 = \\{s_1\\}\\)\nTransições:\n\n\\(\\delta_1(s_0, a) = s_1\\), \\(\\delta_1(s_0, b) = s_2\\)\n\\(\\delta_1(s_1, a) = s_1\\), \\(\\delta_1(s_1, b) = s_1\\)\n\\(\\delta_1(s_2, a) = s_2\\), \\(\\delta_1(s_2, b) = s_2\\)\n\n\nAutômato \\(M_2\\): aceita strings com número par de ’b’s. A linguagem formal é:\n\\[ L_2 = \\{ w \\in \\{a, b\\}^* \\mid \\#_b(w) \\equiv 0 \\pmod{2} \\} \\]\nno qual \\(\\#_b(w)\\) denota o número de ’b’s na string \\(w\\).\n\nEstados: \\(Q_2 = \\{t_{par}, t_{ímpar}\\}\\) onde:\n\n\\(t_{par}\\): número par de ’b’s (incluindo zero)\n\\(t_{ímpar}\\): número ímpar de ’b’s\n\nEstado inicial: \\(t_{par}\\)\nEstados finais: \\(F_2 = \\{t_{par}\\}\\)\nTransições:\n\n\\(\\delta_2(t_{par}, a) = t_{par}\\), \\(\\delta_2(t_{par}, b) = t_{ímpar}\\)\n\\(\\delta_2(t_{ímpar}, a) = t_{ímpar}\\), \\(\\delta_2(t_{ímpar}, b) = t_{par}\\)\n\n\nPasso a Passo para a Construção do Autômato Interseção:\nPasso 1: Estados: \\(Q = Q_1 \\times Q_2 = \\{(s_0, t_{par}), (s_0, t_{ímpar}), (s_1, t_{par}), (s_1, t_{ímpar}), (s_2, t_{par}), (s_2, t_{ímpar})\\}\\)\nPasso 2: Estado inicial: \\(q_0 = (s_0, t_{par})\\)\nPasso 3: Estados finais: Para a interseção, um estado é final apenas se ambos os componentes estão em estados finais: - \\(F = F_1 \\times F_2 = \\{s_1\\} \\times \\{t_{par}\\} = \\{(s_1, t_{par})\\}\\)\nPasso 4: Tabela de Transições:\n\n\n\nEstado\na\nb\n\n\n\n\n\\(\\rightarrow (s_0, t_{par})\\)\n\\((s_1, t_{par})\\)\n\\((s_2, t_{ímpar})\\)\n\n\n\\((s_0, t_{ímpar})\\)\n\\((s_1, t_{ímpar})\\)\n\\((s_2, t_{par})\\)\n\n\n\\(*(s_1, t_{par})\\)\n\\((s_1, t_{par})\\)\n\\((s_1, t_{ímpar})\\)\n\n\n\\((s_1, t_{ímpar})\\)\n\\((s_1, t_{ímpar})\\)\n\\((s_1, t_{par})\\)\n\n\n\\((s_2, t_{par})\\)\n\\((s_2, t_{par})\\)\n\\((s_2, t_{ímpar})\\)\n\n\n\\((s_2, t_{ímpar})\\)\n\\((s_2, t_{ímpar})\\)\n\\((s_2, t_{par})\\)\n\n\n\nPasso 5: Verificação: Vamos testar algumas strings:\nString “aa” (começa com ‘a’ e tem zero ‘b’s - par): - \\((s_0, t_{par}) \\xrightarrow{a} (s_1, t_{par}) \\xrightarrow{a} (s_1, t_{par})\\) ACEITA; - \\(M_1\\) aceita (começa com ’a’); - \\(M_2\\) aceita (zero ’b’s = par); - Interseção aceita (ambos aceitam).\nString “ab” (começa com ‘a’ mas tem um ‘b’ - ímpar): - \\((s_0, t_{par}) \\xrightarrow{a} (s_1, t_{par}) \\xrightarrow{b} (s_1, t_{ímpar})\\) REJEITA; - \\(M_1\\) aceita (começa com ‘a’); - \\(M_2\\) rejeita (um ‘b’ = ímpar); - Interseção rejeita (nem todos aceitam).\nString “abb” (começa com ‘a’ e tem dois ’b’s - par): - \\((s_0, t_{par}) \\xrightarrow{a} (s_1, t_{par}) \\xrightarrow{b} (s_1, t_{ímpar}) \\xrightarrow{b} (s_1, t_{par})\\) ACEITA\nO autômato da interseção aceita apenas strings que começam com ‘a’ E têm número par de ’b’s.\n\n4.6.1.2.1 Exercícios 7\n1. Considere dois autômatos sobre \\(\\Sigma = \\{0, 1\\}\\): - \\(M_1\\) aceita strings que terminam em ‘10’ - \\(M_2\\) aceita strings com número ímpar de ’0’s\nConstrua o conjunto de estados finais para \\(M_1 \\cap M_2\\) e determine a linguagem aceita pela interseção.\n2. É possível que \\(L(M_1 \\cup M_2) = L(M_1 \\cap M_2)\\)? Em que condição isso ocorreria?\n3. Se \\(F_1 = Q_1\\) (todos os estados de \\(M_1\\) são finais), o que podemos afirmar sobre \\(L(M_1 \\cap M_2)\\) em relação a \\(L(M_2)\\)?\n4. Considere \\(M_1\\) que aceita strings terminadas em ‘ab’ e \\(M_2\\) que aceita strings com número par de ’a’s. Construa o autômato interseção \\(M_1 \\cap M_2\\) e determine sua linguagem.\n5. Dados três autômatos \\(M_1\\), \\(M_2\\) e \\(M_3\\), tal que:\n\n\\(M_1\\) aceita strings que começam com ‘a’ (3 estados)\n\\(M_2\\) aceita strings de comprimento múltiplo de 3 (3 estados)\n\n\\(M_3\\) aceita strings que contêm ‘bb’ (3 estados)\n\nDetermine quantos estados teria \\((M_1 \\cup M_2) \\cap M_3\\) e construa o passo a passo completo desta operação composta.\n\n\n\n4.6.1.3 Fechamento sob Complemento\nTeorema (Fechamento sob Complemento): Seja \\(M\\) um Autômato Finito Determinístico que reconhece a linguagem \\(L\\). Então existe um Autômato Finito Determinístico \\(M'\\) que reconhece \\(\\overline{L} = \\Sigma^* - L\\).\nA construção do complemento é surpreendentemente elegante em sua simplicidade. Para a atenta leitora que acompanhou as construções anteriores, esta parecerá quase trivial: simplesmente invertemos quais estados são finais.\nConstrução Formal: Dado \\(M = (Q, \\Sigma, \\delta, q_0, F)\\), construímos:\n\\[M' = (Q, \\Sigma, \\delta, q_0, Q - F)\\]\nCondição Crítica: Esta construção requer que o autômato seja completo. Se não for, devemos primeiro adicionar um estado de erro \\(q_{\\text{erro}}\\) com:\n\\[\\forall a \\in \\Sigma: \\delta(q_{\\text{erro}}, a) = q_{\\text{erro}}\\]\nExemplo Ilustrativo: Complemento de “strings com número par de ’a’s”\nAutômato original \\(M\\):\n\n\n\nTable 4.6: Autômato que aceita número par de ’a’s\n\n\n\n\n\n\n\n\n\n\nEstado\na\nb\n\n\n\n\n\\(\\rightarrow *q_{\\text{par}}\\)\n\\(q_{\\text{ímpar}}\\)\n\\(q_{\\text{par}}\\)\n\n\n\\(q_{\\text{ímpar}}\\)\n\\(q_{\\text{par}}\\)\n\\(q_{\\text{ímpar}}\\)\n\n\n\n\n\n\nAutômato complemento \\(\\overline{M}\\):\n\n\n\nTable 4.7: Autômato que aceita número ímpar de ’a’s\n\n\n\n\n\nEstado\na\nb\n\n\n\n\n\\(\\rightarrow q_{\\text{par}}\\)\n\\(q_{\\text{ímpar}}\\)\n\\(q_{\\text{par}}\\)\n\n\n\\(*q_{\\text{ímpar}}\\)\n\\(q_{\\text{par}}\\)\n\\(q_{\\text{ímpar}}\\)\n\n\n\n\n\n\nA perspicaz leitora observará que apenas trocamos os estados finais: \\(F' = \\{q_{\\text{ímpar}}\\}\\).\nPropriedades Algébricas: O fechamento sob complemento, combinado com união e interseção, estabelece que as linguagens regulares formam uma álgebra booleana:\n\nLeis de De Morgan: \\(\\overline{L_1 \\cup L_2} = \\overline{L_1} \\cap \\overline{L_2}\\)\nDupla Negação: \\(\\overline{\\overline{L}} = L\\)\nDiferença: \\(L_1 - L_2 = L_1 \\cap \\overline{L_2}\\)\n\n\n4.6.1.3.1 Exercícios 8\n1. Considere o Autômato Finito Determinístico \\(M\\) com \\(Q = \\{q_0, q_1\\}\\), \\(F = \\{q_0\\}\\), \\(\\delta(q_0, a) = q_1\\), \\(\\delta(q_0, b) = q_0\\), \\(\\delta(q_1, a) = q_0\\), \\(\\delta(q_1, b) = q_1\\). Construa a tabela de transições de \\(\\overline{M}\\).\n2. Por que a completude é essencial para a construção do complemento? Dê um exemplo no qual a construção falharia sem completude.\n3. Prove usando as propriedades de fechamento que se \\(L_1\\) e \\(L_2\\) são regulares, então \\(L_1 - L_2\\) também é regular.\n4. Se \\(L\\) é regular e \\(\\overline{L} = \\emptyset\\), o que podemos concluir sobre \\(L\\)?\n5. Usando as leis de De Morgan, expresse \\(\\overline{L_1 \\cap L_2 \\cap L_3}\\) em termos de uniões e complementos individuais.\n\n\n\n\n4.6.2 Implicações Práticas das Propriedades de Fechamento\nAs propriedades de fechamento que a dedicada leitora acaba de estudar não são meras curiosidades matemáticas. Elas fundamentam técnicas essenciais na construção de compiladores e processadores de texto:\n\nComposição Modular: Permite construir reconhecedores complexos a partir de componentes simples e bem testados.\nOtimização de Consultas: Em sistemas de busca, a interseção permite refinar resultados, enquanto a união expande o escopo.\nAnálise Léxica Incremental: Novos padrões podem ser adicionados a um analisador existente por meio de união, sem reconstruir todo o autômato.\nVerificação de Propriedades: Para verificar se dois autômatos são equivalentes, basta testar se \\((L_1 - L_2) \\cup (L_2 - L_1) = \\emptyset\\).\n\nA compreensão profunda destas propriedades capacita a engenheira a fazer escolhas arquiteturais informadas, sabendo que a modularidade não compromete a eficiência ou a correção do sistema final.### Minimização de Estados\nUm dos resultados mais importantes e práticos da teoria dos Autômatos Finitos Determinísticos é que toda linguagem regular possui um único Autômato Finito Determinístico mínimo (a menos de renomeação de estados). Este resultado não apenas tem valor teórico, mas permite otimizações significativas em implementações práticas.\n\n4.6.2.1 O Conceito de Estados Equivalentes\nDois estados \\(p\\) e \\(q\\) são equivalentes (denotado \\(p \\equiv q\\)) se, para toda string \\(w \\in \\Sigma^*\\):\n\\[\\delta^*(p, w) \\in F \\iff \\delta^*(q, w) \\in F\\]\nIntuitivamente, dois estados são equivalentes se, a partir deles, o autômato aceita exatamente as mesmas continuações. Esta relação de equivalência particiona o conjunto de estados em classes de equivalência.\n\n\n4.6.2.2 O Algoritmo de Minimização por Refinamento de Partições\nO algoritmo clássico para minimização de Autômatos Finitos Determinísticos opera refinando iterativamente uma partição dos estados até alcançar o ponto fixo:\nAlgoritmo de Minimização:\n1. Inicialização: Particione Q em dois conjuntos:\n   - P₀ = F (estados de aceitação)\n   - P₁ = Q - F (estados de não-aceitação)\n\n2. Refinamento: Enquanto a partição mudar:\n   Para cada classe de equivalência P na partição atual:\n     Para cada símbolo a ∈ Σ:\n       Divida P baseado em δ(q, a) para q ∈ P\n       \n3. Construção: Cada classe de equivalência torna-se\n   um estado do Autômato Finito Determinístico mínimo\nExemplo de Minimização: Considere um Autômato Finito Determinístico com estados redundantes que reconhece \\((a|b)*abb\\):\nEstados originais: \\(\\{q_0, q_1, q_2, q_3, q_4, q_5\\}\\) nos quais alguns estados são equivalentes.\nApós aplicar o algoritmo: 1. Partição inicial: \\(\\{\\{q_3\\}, \\{q_0, q_1, q_2, q_4, q_5\\}\\}\\) 2. Após refinamentos: \\(\\{\\{q_3\\}, \\{q_2\\}, \\{q_1, q_4\\}, \\{q_0, q_5\\}\\}\\) 3. Autômato Finito Determinístico mínimo tem apenas 4 estados ao invés de 6\n\n\n\n4.6.3 Exercícios 9\n1. Considere o alfabeto \\(\\Sigma = \\{0, 1, +, -, *, /, (, )\\}\\) representando tokens simplificados de expressões aritméticas binárias. Sejam os autômatos:\n\n\\(M_1\\): aceita strings que começam com ‘(’ e terminam com ‘)’;\n\\(M_2\\): aceita strings que contêm pelo menos um operador aritmético (+, -, * ou /);\n\\(M_3\\): aceita strings de comprimento par.\n\nConstrua o autômato que reconhece \\((M_1 \\cap M_2) \\cup \\overline{M_3}\\) e determine três strings aceitas e três rejeitadas.\n2. Sobre o alfabeto \\(\\Sigma = \\{a, b, \\_, 0, 1\\}\\) representando caracteres válidos em identificadores, considere:\n\n\\(M_{id}\\): aceita strings que começam com letra (a ou b) ou underscore, seguidos de qualquer combinação de letras, underscore ou dígitos;\n\\(M_{kw}\\): aceita exatamente as strings {“a”, “ab”, “b0”, “b1”} (palavras-chave).\n\nConstrua o autômato \\(M_{id} - M_{kw} = M_{id} \\cap \\overline{M_{kw}}\\) que aceita identificadores válidos que não são palavras-chave. Quantos estados tem o autômato mínimo resultante?\n3. Considere \\(\\Sigma = \\{/, *, \", \\text{outro}\\}\\) onde ‘outro’ representa qualquer caractere diferente dos anteriores. Sejam:\n\n\\(M_{comment}\\): aceita strings da forma /* … */ (comentários de bloco);\n\\(M_{string}\\): aceita strings da forma ” … ” (strings literais);\n\\(M_{nested}\\): aceita strings que contêm pelo menos um ’/*’ dentro de aspas.\n\nProve que \\(L(M_{comment}) \\cap L(M_{string}) = \\emptyset\\) e construa \\(M = (M_{comment} \\cup M_{string}) \\cap \\overline{M_{nested}}\\).\n4. Sobre \\(\\Sigma = \\{0, 1, \\&, |, !, (, )\\}\\) representando expressões booleanas:\n\n\\(M_{balanced}\\): aceita strings com parênteses balanceados (mesmo número de ‘(’ e ‘)’);\n\\(M_{op}\\): aceita strings onde todo operador binário (&, |) é precedido e seguido por 0, 1, ou );\n\\(M_{not}\\): aceita strings onde todo ‘!’ é seguido por 0, 1, ou (.\n\nConstrua passo a passo o autômato \\((M_{balanced} \\cap M_{op} \\cap M_{not}) \\cup \\overline{(M_{op} \\cup M_{not})}\\). Esta construção resulta em um autômato que aceita que tipo de strings?\n5. Dados três autômatos sobre \\(\\Sigma = \\{&lt;, &gt;, =, !, 0, 1\\}\\):\n\n\\(M_1\\) com 4 estados, reconhece operadores relacionais válidos (&lt;, &gt;, &lt;=, &gt;=, ==, !=);\n\\(M_2\\) com 3 estados, reconhece strings que começam com operador;\n\\(M_3\\) com 2 estados, reconhece strings de comprimento ímpar.\n\n\nDetermine o número máximo de estados do autômato \\((M_1 \\cup M_2) \\cap \\overline{M_3}\\) antes da minimização.\nSe \\(L(M_1) \\subseteq L(M_2)\\), simplifique a expressão \\((M_1 \\cap M_2) \\cup (\\overline{M_1} \\cap M_3)\\) e justifique algebricamente.\nProve que \\(\\overline{M_1 \\cup M_2 \\cup M_3} = \\overline{M_1} \\cap \\overline{M_2} \\cap \\overline{M_3}\\) usando as leis de De Morgan e construa o autômato resultante.\n\n\n\n4.6.4 A Relação de Myhill-Nerode\nA relação de Myhill-Nerode fornece uma caracterização fundamental das linguagens regulares e estabelece uma conexão profunda entre a estrutura algébrica de uma linguagem e o autômato mínimo que a reconhece.\n\n4.6.4.1 Definição da Relação\nDada uma linguagem \\(L \\subseteq \\Sigma^*\\), definimos a relação de equivalência de Myhill-Nerode sobre \\(\\Sigma^*\\):\n\\[x \\equiv_L y \\iff \\forall z \\in \\Sigma^* : xz \\in L \\leftrightarrow yz \\in L\\]\nDuas strings \\(x\\) e \\(y\\) são equivalentes se elas são indistinguíveis com respeito a \\(L\\): qualquer sufixo \\(z\\) que, concatenado com \\(x\\), produza uma string em \\(L\\), também produzirá uma string em \\(L\\) quando concatenado com \\(y\\), e vice-versa.\n\n\n4.6.4.2 O Teorema de Myhill-Nerode\nTeorema (Myhill-Nerode): Uma linguagem \\(L\\) é regular se, e somente se, a relação \\(\\equiv_L\\) tem um número finito de classes de equivalência. Além disso, o número de estados do Autômato Finito Determinístico mínimo para \\(L\\) é exatamente igual ao número de classes de equivalência de \\(\\equiv_L\\).\nEste teorema tem implicações profundas:\n\nCaracterização Algébrica: Fornece uma condição necessária e suficiente para regularidade sem mencionar autômatos ou expressões regulares.\nLimite Inferior: Estabelece o número mínimo de estados necessários para reconhecer uma linguagem.\nTécnica de Prova: Oferece um método alternativo para provar que uma linguagem não é regular.\n\nExemplo de Aplicação: Para a linguagem \\(L = \\{a^nb^n | n \\geq 0\\}\\):\nAs strings \\(\\epsilon, a, aa, aaa, ...\\) estão todas em classes de equivalência distintas porque: - \\(a^i \\cdot b^i \\in L\\) mas \\(a^j \\cdot b^i \\notin L\\) para \\(i \\neq j\\)\nComo existem infinitas classes de equivalência, \\(L\\) não é regular. Esta é uma prova alternativa ao Lema do Bombeamento!\n\n\n\n4.6.5 Equivalência entre Modelos\nUm dos resultados fundamentais da teoria das linguagens formais é o Teorema de Kleene, que estabelece a equivalência entre três formalismos aparentemente distintos.\n\n4.6.5.1 O Teorema de Kleene\nTeorema (Kleene): As seguintes classes de linguagens são idênticas: 1. Linguagens reconhecidas por Autômatos Finitos Determinísticos 2. Linguagens reconhecidas por Autômatos Finitos Não-Determinísticos 3. Linguagens denotadas por Expressões Regulares\nA demonstração deste teorema envolve três construções algorítmicas:\n1. Expressão Regular → Autômato Finito Determinístico (Construção de Thompson): Para cada operador da expressão regular, existe uma construção sistemática: - Base: \\(a \\in \\Sigma\\) produz um Autômato Finito Determinístico de dois estados; - União: \\(r|s\\) combina Autômatos Finitos Determinísticos com transições-\\(\\epsilon\\); - Concatenação: \\(rs\\) conecta Autômatos Finitos Determinísticos em sequência; - Fechamento: \\(r^*\\) adiciona ciclo com transições-\\(\\epsilon\\).\n2. Autômato Finito Determinístico (Construção de Subconjuntos): O algoritmo de subconjuntos constrói um Autômato Finito Determinístico no qual cada estado representa um conjunto de estados do Autômato Finito Determinístico original. Embora potencialmente exponencial, garante determinismo.\n3. Autômato Finito Determinístico → Expressão Regular (Eliminação de Estados): Remove sistematicamente estados do Autômato Finito Determinístico, substituindo-os por expressões regulares nas transições, até restar apenas uma expressão entre o estado inicial e os finais.\n\n\n\n4.6.6 Teste de Equivalência entre Autômatos Finitos Determinísticos\nDeterminar se dois Autômatos Finitos Determinísticos reconhecem a mesma linguagem é um problema fundamental com aplicações práticas importantes, especialmente na otimização de compiladores.\n\n4.6.6.1 Algoritmo de Teste de Equivalência\nMétodo 1: Via Minimização: 1. Minimize ambos os Autômatos Finitos Determinísticos 2. Verifique isomorfismo entre os Autômatos Finitos Determinísticos mínimos\nMétodo 2: Via Construção do Produto: 1. Construa Autômato Finito Determinístico para \\((L_1 - L_2) \\cup (L_2 - L_1)\\) 2. Verifique se a linguagem resultante é vazia\nComplexidade: a complexidade do teste de equivalência entre dois Autômatos Finitos Determinísticos (\\(M_1\\) e \\(M_2\\)) depende do método utilizado.\n\nO método via minimização tem sua complexidade dominada pelo algoritmo de minimização. Utilizando o algoritmo de Hopcroft, a complexidade é de \\(O(k \\cdot n \\log n)\\), na qual \\(n\\) é o número de estados e \\(k\\) é o tamanho do alfabeto. Se o alfabeto é tratado como uma constante, a complexidade é frequentemente citada como \\(O(n \\log n)\\).\nO método via construção do produto, que verifica se a linguagem \\((L_1 \\setminus L_2) \\cup (L_2 \\setminus L_1)\\) é vazia, possui uma complexidade de \\(O(n_1 \\cdot n_2)\\), na qual \\(n_1\\) é o número de estados de \\(M_1\\) e \\(n_2\\) é o número de estados de \\(M_2\\).\n\n\n\n\n4.6.7 Implicações Práticas das Propriedades\nA diligente leitora observará que estas propriedades matemáticas têm consequências diretas na engenharia de compiladores:\n\nComposição Modular: As propriedades de fechamento permitem construir analisadores léxicos complexos a partir de componentes simples, sabendo que o resultado permanecerá tratável.\nOtimização Garantida: A minimização garante o menor autômato possível, otimizando tanto memória quanto tempo de execução.\nVerificação Formal: O teste de equivalência permite verificar se otimizações preservam a semântica.\nGeração Automática: A equivalência entre formalismos permite escolher a representação mais conveniente para cada fase do desenvolvimento.\n\nEstas propriedades fundamentais estabelecem os Autômatos Finitos Determinísticos não apenas como um modelo teórico elegante, mas como uma ferramenta prática poderosa para o processamento eficiente de linguagens. A compreensão profunda destas propriedades capacita a engenheira de software a fazer escolhas informadas sobre representações, otimizações e implementações em sistemas reais de análise léxica.\n\n\n4.6.8 Exercícios 10\n1.: Considere a linguagem \\(L = \\{w \\in \\{0,1\\}^* \\mid w \\text{ contém } 00 \\text{ como substring}\\}\\).\n\nDetermine todas as classes de equivalência da relação \\(\\equiv_L\\).\nPara cada classe, forneça um representante e explique por que strings dessa classe são equivalentes.\nConstrua o Autômato Finito Determinístico mínimo para \\(L\\) usando as classes de equivalência encontradas.\nVerifique que o número de estados do Autômato Finito Determinístico mínimo corresponde ao número de classes.\n\n2.: Use a relação de Myhill-Nerode para provar que a linguagem \\(L = \\{0^i1^j \\mid i &gt; j \\geq 0\\}\\) não é regular.\nDica: Considere as strings \\(0^n\\) para diferentes valores de \\(n\\) e mostre que estão em classes distintas.\n3.:Considere o Autômato Finito Determinístico \\(M = (\\{q_0, q_1, q_2\\}, \\{a, b\\}, \\delta, q_0, \\{q_2\\})\\) com transições:\n\n\\(\\delta(q_0, a) = q_1\\), \\(\\delta(q_0, b) = q_0\\);\n\\(\\delta(q_1, a) = q_1\\), \\(\\delta(q_1, b) = q_2\\);\n\\(\\delta(q_2, a) = q_2\\), \\(\\delta(q_2, b) = q_2\\).\n\nUse o método de eliminação de estados para encontrar a expressão regular equivalente. Mostre cada passo da eliminação.\n4.: Dados dois Autômatos Finitos Determinísticos sobre \\(\\Sigma = \\{a, b\\}\\):\nAutômato Finito Determinístico \\(M_1\\): Aceita strings com número par de \\(a\\)’s - Estados: \\(\\{p_0, p_1\\}\\) - Estado inicial: \\(p_0\\) - Estados finais: \\(\\{p_0\\}\\) - \\(\\delta_1(p_0, a) = p_1\\), \\(\\delta_1(p_0, b) = p_0\\) - \\(\\delta_1(p_1, a) = p_0\\), \\(\\delta_1(p_1, b) = p_1\\)\nAutômato Finito Determinístico \\(M_2\\): Aceita strings onde a posição de cada \\(a\\) é ímpar (primeira posição = 1) - Estados: \\(\\{q_{par}, q_{impar}, q_{aceita}, q_{rejeita}\\}\\) - Estado inicial: \\(q_{par}\\) - Estados finais: \\(\\{q_{par}, q_{aceita}\\}\\)\n\nDetermine se \\(L(M_1) = L(M_2)\\) usando o método do produto.\nVerifique sua resposta testando as strings: \\(\\epsilon\\), \\(a\\), \\(b\\), \\(aa\\), \\(ab\\), \\(ba\\), \\(aba\\).\n\n5. Considere um Autômato Finito Determinístico \\(M\\) com 8 estados que reconhece strings sobre \\(\\{0, 1\\}\\) terminadas em \\(01\\).\n\nAplique o algoritmo de minimização de Hopcroft para encontrar o Autômato Finito Determinístico mínimo.\nCalcule a complexidade temporal da minimização considerando \\(|\\Sigma| = 2\\) e \\(n = 8\\).\nSe tivéssemos dois Autômatos Finitos Determinísticos com 8 e 10 estados respectivamente, compare a complexidade de testar equivalência via minimização versus via construção do produto.\n\n6. Você está projetando um analisador léxico que deve reconhecer três tipos de tokens:\n\nInteiros: sequências de dígitos (0-9);\nIdentificadores: começam com letra, seguidos de letras ou dígitos;\nOperadores: ++, --, +=, -=.\n\n\nConstrua Autômatos Finitos Determinísticos individuais para cada tipo.\nUse as propriedades de fechamento para criar um único Autômato Finito Determinístico que reconheça qualquer um dos três tipos.\nDiscuta como você implementaria a distinção entre os tipos de tokens no Autômato Finito Determinístico combinado.\n\n\n\n4.6.9 Limitações dos Autômatos Finitos\nApesar de sua grande aplicabilidade na análise léxica e no reconhecimento de padrões, os Autômatos Finitos Determinísticos possuem uma limitação fundamental que reside na própria natureza de sua memória: ela é finita. Um Autômato Finito Determinístico só consegue se lembrar de informações por meio do estado em que se encontra. Como o conjunto de estados \\(Q\\) é finito, a capacidade de memorização da máquina é limitada.\nEsta limitação impede que os Autômatos Finitos Determinísticos reconheçam linguagens que exigem uma memória ilimitada ou a capacidade de contar sem um teto predefinido. O exemplo mais clássico de uma linguagem que um Autômato Finito Determinístico não pode reconhecer é a linguagem de parênteses balanceados ou, de forma análoga, a linguagem \\(L\\):\n\\[\nL = \\{0^n1^n \\mid n \\ge 0\\}\n\\]\nEsta linguagem consiste em qualquer número \\(n\\) de símbolos ‘0’, seguido pela mesma quantidade \\(n\\) de símbolos ‘1’. Alguns exemplos de strings em \\(L\\) são \\(\\epsilon\\) (a string vazia, com \\(n=0\\)), \\(01\\), \\(0011\\), \\(000111\\), e assim por diante.\nPara que um autômato pudesse reconhecer esta linguagem, ele precisaria ler todos os ’0’s, contar quantos foram lidos, e então verificar se a quantidade de ’1’s subsequentes é exatamente a mesma. Se \\(n\\) pode ser um número arbitrariamente grande, a máquina precisaria de um número infinito de estados para memorizar cada contagem possível de ’0’s, o que viola a definição de um autômato finito.\nEssa incapacidade de lidar com estruturas de aninhamento ou contagens ilimitadas é a razão pela qual modelos computacionais mais poderosos, como os Autômatos de Pilha, são necessários para as fases seguintes da compilação, como a análise sintática.\n\n\n\n\n[1] AHO, A. V. et al. Compilers: Principles, techniques, and tools. 2nd. ed. [s.l.] Pearson; Addison-Wesley, 2007.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autômatos Finitos Determinísticos</span>"
    ]
  },
  {
    "objectID": "04-Gramaticas.html",
    "href": "04-Gramaticas.html",
    "title": "5  Gramáticas e Linguagens Livres de Contexto",
    "section": "",
    "text": "5.1 Hierarquia de Chomsky: História e Contexto\nA Hierarquia de Chomsky representa um dos marcos mais elegantes da ciência do século XX, estabelecendo pontes entre linguística, matemática e ciência da computação. Desenvolvida por Noam Chomsky entre 1956 e 1959, esta classificação não apenas transformou os estudos linguísticos de uma disciplina descritiva em uma ciência formal rigorosa, mas também forneceu as bases teóricas para a construção de compiladores e o processamento computacional de linguagens. A hierarquia emergiu de uma convergência única de fatores históricos: a revolução cognitiva dos anos 1950, o desenvolvimento da teoria da computação, e a genialidade de um jovem linguista que soube sintetizar influências matemáticas diversas em uma estrutura unificada que perdura até hoje.\nA história da Hierarquia de Chomsky começa com um encontro transformador que ocorreu em 1947, quando Chomsky, então com 19 anos e considerando abandonar a universidade, conheceu Zellig S. Harris na Universidade da Pensilvânia. Harris era um dos fundadores da linguística estrutural americana e tinha estabelecido o primeiro departamento moderno de linguística dos Estados Unidos em 1946. Sob a orientação de Harris, Chomsky também estudou matemática com Nathan Fine em Harvard. Essas influências foram cruciais para moldar sua abordagem aos sistemas formais e metodologia científica. Sua tese de mestrado de 1951, A Morfofonêmica do Hebraico Moderno, e especialmente seu trabalho em A Estrutura Lógica da Teoria Linguística (LSLT), escrito enquanto era fellow júnior em Harvard (1951-55), começaram a transformar a abordagem estrutural de Harris em algo inteiramente novo.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Gramáticas e Linguagens Livres de Contexto</span>"
    ]
  },
  {
    "objectID": "04-Gramaticas.html#hierarquia-de-chomsky-história-e-contexto",
    "href": "04-Gramaticas.html#hierarquia-de-chomsky-história-e-contexto",
    "title": "5  Gramáticas e Linguagens Livres de Contexto",
    "section": "",
    "text": "5.1.1 A revolução cognitiva e o ambiente do MIT\nOs anos 1950 representavam um período de fermentação intelectual que se tornaria conhecido como a Revolução Cognitiva. George Miller, uma das figuras-chave, datou o início dessa revolução em 11 de setembro de 1956, quando pesquisadores de psicologia experimental, ciência da computação e linguística teórica apresentaram trabalhos sobre temas relacionados à ciência cognitiva em uma reunião do Special Interest Group in Information Theory no MIT.\nDurante esse período, vários paradigmas dominantes estavam sendo desafiados. O behaviorismo era a doutrina dominante em psicologia e linguística, enfatizando organização de dados e classificação taxonômica enquanto rejeitava o estudo de estados mentais internos. A linguística estrutural, liderada por figuras como Leonard Bloomfield, tratava a linguagem como um fenômeno social estudado por meio de análise de corpus.\nA Hierarquia de Chomsky emergiu de uma convergência sofisticada de lógica matemática, teoria dos autômatos e teoria das funções recursivas desenvolvidas entre as décadas de 1930 e 1950. Chomsky construiu sistematicamente sobre trabalhos matemáticos anteriores, criando uma síntese notável.\nO trabalho de Emil Post (1936, 1944) foi fundamental. Post desenvolveu sistemas canônicos de Post usando técnicas de reescrita de strings que se tornaram fundamentais para a abordagem de Chomsky. Seu trabalho de 1936 Processos combinatórios finitos – Formulação 1 criou modelos computacionais essencialmente equivalentes às máquinas de Turing. Post queria derivar mecanicamente inferências de uma sentença axiomática inicial, e Chomsky aplicou diretamente essa estrutura lógica para descrever conjuntos de strings na linguagem humana.\nO modelo de McCulloch-Pitts de 1943 forneceu a ponte computacional. Seu trabalho Um Cálculo Lógico das Ideias Imanentes na Atividade Nervosa ofereceu o primeiro modelo matemático conectando redes neurais à computação, levando diretamente à noção de autômatos finitos que se tornaram o Tipo-3 na Hierarquia de Chomsky. Eles demonstraram que neurônios poderiam funcionar como portas lógicas, estabelecendo a conexão entre sistemas biológicos e modelos computacionais formais.\nO trabalho de Stephen Cole Kleene estabeleceu a equivalência fundamental entre expressões regulares e máquinas de estado finito (Teorema de Kleene), fornecendo a base matemática para as gramáticas do Tipo-3. Suas propriedades de fechamento para linguagens regulares sob união, concatenação e operação estrela de Kleene criaram o framework teórico necessário.\nA estrutura teórica se baseou explicitamente na teoria das funções recursivas. Chomsky declarou que a gramática gerativa se desenvolveu dentro de uma teoria matemática particular, a saber, a teoria das funções recursivas. Isso foi importante porque a teoria das funções recursivas nas décadas de 1930-1940 formalizou a noção de computação, construindo sobre a tese de Church-Turing e as provas de equivalência entre diferentes modelos de computação.\n\n\n5.1.2 A elegância matemática dos quatro tipos\nO desenvolvimento dos quatro tipos específicos de gramáticas emergiu da aplicação de restrições matemáticas crescentes. A classificação não foi arbitrária, mas resultou de uma lógica matemática rigorosa que Chomsky articulou em seus trabalhos seminais de 1956 (Três Modelos para a Descrição da Linguagem) e 1959 (Sobre Certas Propriedades Formais das Gramáticas).\nA estrutura hierárquica aninhada representa uma beleza matemática notável. Cada classe é um subconjunto próprio da próxima classe menos restritiva: \\(\\text{Tipo 3} ⊂ \\text{Tipo 2} ⊂ \\text{Tipo 1} ⊂ \\text{Tipo 0}\\). Cada tipo de gramática corresponde exatamente a uma classe de autômatos com poder computacional correspondente, criando uma correspondência perfeita entre restrições gramaticais e modelos computacionais.\n\nO Tipo 0 (Gramáticas Irrestritas) não possui restrições nas regras de produção (\\(\\alpha \\rightarrow \\beta\\)), sendo equivalente às máquinas de Turing e capaz de gerar linguagens recursivamente enumeráveis.\nO Tipo 1 (Gramáticas Sensíveis ao Contexto) adiciona a propriedade não-contrativa (\\(\\mid \\alpha \\mid ≤ \\mid \\beta \\mid\\)), correspondendo aos autômatos linearmente limitados.\nO Tipo 2 (Gramáticas Livres de Contexto) restringe o lado esquerdo a um único não-terminal (\\(A \\rightarrow γ\\)), equivalendo aos autômatos de pilha.\nO Tipo 3 (Gramáticas Regulares) impõe as restrições mais específicas em padrões terminal/não-terminal, correspondendo aos autômatos finitos.\n\nA elegância conceitual reside na relação inversa entre liberdade gramatical e complexidade computacional: gramáticas mais restritas (Tipo 3) requerem menor poder computacional (autômatos finitos), enquanto gramáticas menos restritas (Tipo 0) necessitam do maior poder computacional (máquinas de Turing).\nNa ciência da computação, a hierarquia estabeleceu as bases matemáticas para a teoria das linguagens formais, criando uma classificação sistemática que permanece fundamental para a ciência da computação teórica. A realização de Chomsky de que a descrição de um tipo de autômato e de um tipo de linguagem estão relacionadas conectou campos anteriormente desconectados da linguística e computação.\n\n\n5.1.3 Aplicações práticas em compiladores e processamento de linguagem\nNa análise léxica, gramáticas do Tipo-3 (regulares) e autômatos finitos tornaram-se fundamentais para tokenizar código fonte, identificando palavras-chave, strings e identificadores. Na análise sintática, gramáticas do Tipo-2 (livres de contexto) formam a base teórica para analisar a maioria das linguagens de programação, lidando com estruturas aninhadas como parênteses, chamadas de função e estruturas de controle.\nGeradores de parser modernos usam especificações de Gramáticas Livres de Contextos para gerar automaticamente parsers, com ferramentas implementando diretamente conceitos da Hierarquia de Chomsky. Classes de Gramáticas Livres de Contexto (LL, LR) tornaram-se padrão para parsing eficiente, tornando a compilação tratável mantendo poder expressivo suficiente.\nA Forma de Backus-Naur (\\(BNF\\)) desenvolvida por John Backus e Peter Naur para ALGOL 60 foi criada com conhecimento explícito do trabalho anterior de Chomsky sobre Gramáticas Livres de Contexto, tornando-se o padrão da indústria para definir sintaxe de linguagens de programação e implementando diretamente conceitos de gramática formal de Chomsky.\nNão bastasse o que a atenta leitora viu até o momento, cada nível da hierarquia possui propriedades de complexidade computacional bem definidas. Linguagens regulares (Tipo 3) podem ser reconhecidas em tempo linear \\(O(n)\\) com espaço constante. Linguagens livres de contexto (Tipo 2) são decidíveis em tempo polinomial \\(O(n^3)\\) usando o algoritmo CYK, formando a base para a maioria dos compiladores de linguagens de programação. Linguagens sensíveis ao contexto (Tipo 1) são PSPACE-completas, com complexidade de tempo exponencial no pior caso, mas complexidade de espaço linear. Linguagens recursivamente enumeráveis (Tipo 0) são indecidíveis em geral.\n\nAlgoritmo CYK (Cocke-Younger-Kasami)\nO algoritmo CYK é um método de programação dinâmica para determinar se uma string pertence a uma linguagem livre de contexto. Desenvolvido independentemente por Cocke, Younger e Kasami, o algoritmo requer que a gramática esteja na Forma Normal de Chomsky, na qual todas as produções têm a forma \\(A \\rightarrow BC\\) ou \\(A \\rightarrow a\\).\nO algoritmo constrói uma tabela triangular de tamanho \\(n \\times n\\), na qual \\(n\\) é o comprimento da string de entrada. Cada entrada \\((i,j)\\) da tabela indica quais não-terminais podem derivar a sub_string_ que inicia na posição \\(i\\) com comprimento \\(j\\). O preenchimento ocorre bottom-up: primeiro as substrings de comprimento 1, depois 2, e assim sucessivamente até \\(n\\).\nA complexidade \\(O(n^3)\\) resulta de três loops aninhados: dois para percorrer a tabela (\\(O(n^2)\\) entradas) e um terceiro para testar todas as possíveis divisões de cada sub_string_ (\\(O(n)\\) divisões por entrada). Esta complexidade polinomial torna o CYK prático para parsing de linguagens de programação, sendo fundamental na construção de parsers bottom-up.\n\n\nPSPACE-Completas\nPSPACE é a classe de complexidade que contém todos os problemas de decisão solucionáveis por uma máquina de Turing determinística usando espaço polinomial. Um problema é PSPACE-completo se pertence ao PSPACE e qualquer problema em PSPACE pode ser reduzido a ele em tempo polinomial.\nAs linguagens sensíveis ao contexto (Tipo 1) são PSPACE-completas porque seu reconhecimento requer espaço proporcional ao comprimento da entrada, mas pode demandar tempo exponencial. Isso ocorre porque um autômato linearmente limitado (que reconhece essas linguagens) possui número finito de configurações possíveis, mas esse número cresce exponencialmente com o tamanho da entrada.\nA aparente contradição entre “espaço linear” e “PSPACE-completo” se resolve considerando que PSPACE inclui problemas que usam espaço polinomial, e espaço linear é um caso particular de espaço polinomial (\\(O(n) \\subset O(n^k)\\)). O tempo exponencial surge porque, mesmo com espaço limitado, o número de estados alcançáveis pode crescer exponencialmente, exigindo exploração exaustiva no pior caso.\n\nAlém disso, os lemas de bombeamento para cada classe fornecem ferramentas matemáticas para provar que linguagens NÃO pertencem às classes inferiores. As propriedades de fechamento variam sistematicamente: linguagens regulares são fechadas sob todas as operações padrão, linguagens livres de contexto são fechadas sob união, concatenação e estrela de Kleene mas NÃO sob interseção e complemento.\n\n\n5.1.4 O Poder e as Limitações das Linguagens Regulares (Tipo 3)\nAs linguagens regulares representam a classe mais fundamental e restrita da hierarquia. Elas são formalmente definidas como o conjunto de linguagens que podem ser descritas por expressões regulares ou, equivalentemente, geradas por gramáticas regulares.\nA estrutura de uma gramática regular é estritamente limitada. Suas regras de produção, que ditam como os símbolos podem ser reescritos, devem aderir a um formato rígido. Em uma gramática linear à direita, por exemplo, todas as regras devem ser da forma \\(A \\rightarrow tN\\) ou \\(A \\rightarrow t\\), na qual \\(A\\) e \\(N\\) são símbolos não terminais e \\(t\\) é uma string de símbolos terminais que pode ser vazia. Uma restrição similar se aplica às gramáticas lineares à esquerda. Esta limitação estrutural não é meramente uma convenção; ela é a fonte da principal limitação computacional das linguagens regulares: a incapacidade de modelar dependências aninhadas ou recursivas.\nO autômato finito, o reconhecedor para a classe das linguagens regulares, opera sem uma memória externa; seu único histórico está contido no estado atual em que se encontra. Consequentemente, ele não pode lembrar ou contar ocorrências de símbolos para garantir correspondências em uma string. O exemplo canônico das limitações de um autômato finito são os parênteses aninhados. Um autômato finito não pode verificar se uma string contém um número igual de parênteses abertos e fechados se estes parênteses puderem ser aninhados.\n\n\n5.1.5 A Ascensão às Linguagens Livres de Contexto (Tipo 2)\nA necessidade de modelar estruturas sintáticas mais complexas, como expressões aritméticas com parênteses aninhados, blocos de código delimitados (´begin´…´end´) ou estruturas de dados recursivas, revela a insuficiência das linguagens regulares. Para superar essas limitações, ascendemos na hierarquia para as linguagens livres de contexto.\nAs Linguagens Livres de Contexto formam um superconjunto estrito das linguagens regulares; toda linguagem regular é, por definição, livre de contexto, mas o inverso não é verdadeiro. O poder expressivo adicional das Linguagens Livres de Contexto emana diretamente de uma flexibilização nas regras de produção de suas gramáticas. Em uma Gramática Livre de Contexto, uma regra de produção tem a forma \\(A \\rightarrow \\beta\\), na qual \\(A\\) é um único símbolo não-terminal e \\(\\beta\\) é uma string qualquer de símbolos terminais e não terminais. A ausência de restrições sobre a posição dos não terminais em \\(\\beta\\) permite a definição de recursão central, como na regra \\(S \\rightarrow aSb\\), que é a chave para modelar estruturas aninhadas.\n\n\n5.1.6 Exemplos Práticos que Distinguem as Duas Classes\nA distinção teórica entre essas duas classes é melhor ilustrada pelo exemplo canônico da linguagem \\(L={a^n b^n \\mid n \\geq 0}\\), que consiste em strings com um número de as seguido pelo mesmo número de bs.\nEsta linguagem não é regular. A tentativa de construir um autômato finito para reconhecê-la falha porque a máquina precisaria de uma quantidade infinita de estados para lembrar o número exato de as lidos e garantir que o número de bs corresponda. Uma prova formal de que \\(L\\) não é uma linguagem regular pode ser rigorosamente construída usando o Lema do Bombeamento para linguagens regulares. Este lema afirma que, para qualquer string suficientemente longa em uma linguagem regular, existe uma sub_string_ que pode ser bombeada (repetida um número arbitrário de vezes) e a nova string resultante ainda pertencerá à linguagem. Ao aplicar o lema à string com \\(p\\) ’a’s seguidos de \\(p\\) ’b’s é a \\(a^pb^p\\), no qual \\(p\\) é o comprimento de bombeamento, a sub_string_ bombeada consistirá inteiramente de as, quebrando o equilíbrio entre as e bs e provando que a linguagem não pode ser regular. No entanto, \\(L\\) é uma linguagem livre de contexto por excelência. Ela pode ser gerada pela gramática extremamente simples e elegante:\n\\[S \\rightarrow aSb \\mid \\epsilon\\]\nNesta gramática o símbolo \\(\\epsilon\\) representa a string vazia. A regra recursiva \\(S \\rightarrow aSb\\) encapsula perfeitamente a capacidade de aninhamento que define as Linguagens Livres de Contexto. Outros exemplos práticos que exigem o poder das Linguagens Livres de Contexto incluem a linguagem dos palíndromos (ex.: radar) e a validação estrutural de documentos XML, no qual as tags de abertura e fechamento devem ser corretamente aninhadas, uma tarefa impossível para expressões regulares.\nA atenta leitora deve observar que essa progressão não é apenas um exercício teórico. A necessidade prática de analisar a sintaxe de linguagens de programação, que são repletas de construções aninhadas como laços, condicionais e chamadas de função, é a força motriz que torna as linguagens regulares insuficientes e as linguagens livres de contexto absolutamente essenciais para a ciência da computação. A Tabela #tbl-resumo1 condensa as diferenças entre linguagens regulares e linguagens livres de contexto.\n\nResumo das características e diferenças das linguagens regulares e livres de contexto.{#tbl-resumo1}\n\n\n\n\n\n\n\nCaracterística\nLinguagens Regulares (Tipo 3)\nLinguagens Livres de Contexto (Tipo 2)\n\n\n\n\nTipo de Gramática\nGramática Regular\nGramática Livre de Contexto\n\n\nFormato das Regras\nRestrito (ex.: \\(A \\rightarrow aB\\) ou \\(A\\rightarrow a\\))\nIrrestrito (ex.: \\(A \\rightarrow \\beta\\), na qual \\(\\beta\\) é qualquer string)\n\n\nAutômato Reconhecedor\nAutômato Finito\nAutômato com Pilha\n\n\nCapacidade de Memória\nNenhuma (limitada a estados finitos)\nIlimitada (via pilha LIFO)\n\n\nExemplo Característico\n\\(a(ba)^∗\\)\n\\(a^nb^n\\) para \\(n \\geq 0\\)\n\n\nExemplo Não-Pertencente\n\\(a^nb^n\\) para \\(n \\geq 0\\)\n\\(a^nb^nc^n\\) para \\(n \\geq 0\\)",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Gramáticas e Linguagens Livres de Contexto</span>"
    ]
  },
  {
    "objectID": "04-Gramaticas.html#sec-anatomia-gramaticas-livres-de-contexto",
    "href": "04-Gramaticas.html#sec-anatomia-gramaticas-livres-de-contexto",
    "title": "5  Gramáticas e Linguagens Livres de Contexto",
    "section": "5.2 A Anatomia das Gramáticas Livres de Contexto",
    "text": "5.2 A Anatomia das Gramáticas Livres de Contexto\nA atenta leitora verá que para analisar e processar linguagens com estruturas aninhadas, é imprescindível o uso de um formalismo matemático preciso. Uma Gramática Livre de Contexto fornece a base formal necessária.\nO termo livre de contexto é fundamental para a compreensão da natureza das gramáticas que a atenta leitora estudará neste capítulo. Esse termo significa que a aplicação de uma regra de produção \\(A\\rightarrow \\beta\\) a um símbolo não-terminal \\(A\\) é incondicional; ela pode ocorrer independentemente dos símbolos que cercam \\(A\\), seu contexto, em uma forma sentencial intermediária. Esta propriedade simplifica a análise sintática em comparação com gramáticas mais complexas, como as sensíveis ao contexto (Tipo 1).\nFormalmente, uma Gramática Livre de Contexto é definida como uma quádrupla \\(G=(N,\\Sigma,P,S)\\), na qual cada componente é um conjunto e tem um papel específico na geração das strings da linguagem. Os quatro componentes da tupla que define uma Gramática Livre de Contexto são:\n\nN: O Conjunto de Símbolos Não Terminais. Este é um conjunto finito de variáveis que representam as diferentes construções sintáticas ou categorias gramaticais da linguagem. Por exemplo, em uma gramática para uma linguagem de programação, os não terminais podem incluir EXPRESSÃO, COMANDO e DECLARAÇÃO. Eles são os elementos que podem ser substituídos ou expandidos durante o processo de derivação e que aqui, neste livro, serão representados por letras maiúsculas do alfabeto latino.\n\\(\\Sigma\\): O Conjunto de Símbolos Terminais. Este é um conjunto finito, disjunto de N, que constitui o alfabeto da linguagem. Os terminais são os símbolos literais, os átomos que compõem as strings finais da linguagem e não podem ser mais decompostos. Exemplos incluem palavras-chave (if, while, etc.), operadores (+, \\, *, etc.) e identificadores (contador, salario, etc.).\nP: O Conjunto de Regras de Produção. Este é um conjunto finito de regras que definem como os não terminais podem ser substituídos. Cada regra tem a forma \\(A \\rightarrow β\\), na qual \\(A\\in N\\) é um único não-terminal (a cabeça da produção) e \\(\\beta \\in (N∪Σ)^∗\\) é uma string, possivelmente vazia, de símbolos terminais e/ou não terminais que chamaremos de corpo da produção. Essas regras são o motor do sistema gerativo.\nS: O Símbolo Inicial. Um não-terminal especial, \\(S \\in N\\), que serve como ponto de partida para todas as derivações. Ele geralmente representa a construção sintática mais abrangente da linguagem, como um programa ou uma sentença.\n\nA linguagem gerada por uma gramática \\(G\\), denotada por \\(L(G)\\), é o conjunto de todas as strings de símbolos terminais que podem ser derivadas a partir do símbolo inicial \\(S\\) por meio da aplicação sucessiva das regras de produção em \\(P\\).\nEmbora a definição formal de uma Gramática Livre de Contexto seja inerentemente gerativa, descrevendo como construir sentenças válidas a partir de \\(S\\), sua aplicação primária em compiladores é reconhecedora. O objetivo de um analisador sintático não é gerar programas aleatórios, mas sim verificar se uma dada sequência de tokens, produzida pelo programador, pode ser gerada pela gramática. A gramática, portanto, atua como a especificação formal contra a qual o processo de reconhecimento é executado. O parser, na prática, tenta reverter o processo de derivação para validar a estrutura do código fonte.\n\n5.2.1 Exemplo Canônico: A Linguagem dos Palíndromos\nPara solidificar esses conceitos abstratos, vamos construir uma Gramática Livre de Contexto para a linguagem dos palíndromos sobre o alfabeto \\(\\Sigma = \\{0,1\\}\\). Só para refrescar a memória: um palíndromo é uma string que se lê da mesma forma da esquerda para a direita e da direita para a esquerda.\nA estrutura recursiva dos palíndromos pode ser definida da seguinte forma:\n\nCasos Base: A string vazia (\\(\\epsilon\\)), 0 e 1 são palíndromos.\nPasso Indutivo: Se \\(w\\) é um palíndromo, então \\(0w0\\) e \\(1w1\\) também são palíndromos.\n\nEsta definição recursiva pode ser traduzida diretamente em um conjunto de regras de produção para uma Gramática Livre de Contexto. Seja \\(K\\) o nosso símbolo não-terminal para palíndromo:\n\n\\(K \\rightarrow \\epsilon\\)\n\n\\(K \\rightarrow 0\\)\n\n\\(K \\rightarrow 1\\)\n\n\\(K \\rightarrow 0K0\\)\n\n\\(K \\rightarrow 1K1\\)\n\nNeste exemplo, a gramática completa \\(G_{pal}\\) é definida pela quádrupla:\n\n\\(N = \\{K\\}\\)\n\\(\\Sigma = \\{0,1\\}\\)\n\\(P\\) é o conjunto das cinco regras listadas acima.\n\\(S = K\\)\n\nEsta gramática pode gerar qualquer palíndromo sobre \\(\\{0,1\\}\\). Por exemplo, a string 0110 pode ser derivada da seguinte forma:\nA derivação da string 0110 a partir do símbolo inicial \\(K\\) é feita aplicando-se as regras da gramática sequencialmente.\n\nPasso 1: Iniciar com o símbolo inicial.\n\nComeçamos com o símbolo inicial da gramática, que é \\(K\\).\n\\(K\\)\n\nPasso 2: Aplicar a Regra 4 (\\(K \\rightarrow 0K0\\)).\n\nPara gerar uma string que começa e termina com 0, aplicamos a regra 4.\n\\(K \\Rightarrow 0K0\\)\n\nPasso 3: Aplicar a Regra 5 (\\(K \\rightarrow 1K1\\)).\n\nAgora, precisamos gerar a parte interna do palíndromo. Substituímos o \\(K\\) restante pela regra 5 para obter os 1s internos.\n\\(0K0 \\Rightarrow 0(1K1)0 = 01K10\\)\n\nPasso 4: Aplicar a Regra 1 (\\(K \\rightarrow \\epsilon\\)).\n\nO centro do palíndromo 0110 é vazio. Para finalizar a derivação, substituímos o último \\(K\\) pela cadeia vazia, \\(\\epsilon\\) (epsilon), usando a regra 1.\n\\(01K10 \\Rightarrow 01(\\epsilon)10 = 0110\\)\n\n\nA sequência completa da derivação será:\n\\[K \\Rightarrow 0K0 \\Rightarrow 01K10 \\Rightarrow 01\\epsilon10 \\Rightarrow 0110\\]\n\n\n5.2.2 Exercícios de Derivação\n\n5.2.2.1 Exercício 1: Palíndromo Ímpar\nDada a gramática de palíndromos \\(G_{pal}\\):\n\n\\(N = \\{K\\}\\)\n\\(\\Sigma = \\{0,1\\}\\)\n\\(P = \\{ K \\rightarrow \\epsilon, K \\rightarrow 0, K \\rightarrow 1, K \\rightarrow 0K0, K \\rightarrow 1K1 \\}\\)\n\\(S = K\\)\n\nFaça a derivação da string 101.\n\n\n5.2.2.2 Exercício 2: Expressão Aritmética Simples\nConsidere uma gramática simplificada para expressões aritméticas, \\(G_{exp}\\) dada por:\n\n\\(N = \\{E\\}\\)\n\\(\\Sigma = \\{id, +, *, (, )\\}\\)\n\\(P = \\{ E \\rightarrow E + E, E \\rightarrow E * E, E \\rightarrow (E), E \\rightarrow id \\}\\)\n\\(S = E\\)\n\nFaça a derivação da string id * id + id.\n\n\n5.2.2.3 Exercício 3: Palíndromo de Comprimento Par e Aninhado\nUsando a gramática de palíndromos \\(G_{pal}\\) dada a seguir, faça a derivação da string 011110.\n\n\\(N = \\{K\\}\\)\n\\(\\Sigma = \\{0,1\\}\\)\n\\(P = \\{ K \\rightarrow \\epsilon, K \\rightarrow 0, K \\rightarrow 1, K \\rightarrow 0K0, K \\rightarrow 1K1 \\}\\)\n\\(S = K\\)\n\n\n\n5.2.2.4 Exercício 4: Linguagem \\(a^nb^n\\)\nConsidere a gramática \\(G_{ab}\\) que gera strings com um número de a``s seguido pelo mesmo número deb``s:\n\n\\(N = \\{S\\}\\)\n\\(\\Sigma = \\{a, b\\}\\)\n\\(P = \\{ S \\rightarrow aSb, S \\rightarrow \\epsilon \\}\\)\n\\(S = S\\)\n\nFaça a derivação da string aaabbb.\n\n\n5.2.2.5 Exercício 5: Comando Condicional if-else\nSeja uma gramática para um comando if-else simplificado, \\(G_{if}\\):\n\n\\(N = \\{C, A\\}\\)\n\\(\\Sigma = \\{ \\text{if}, \\text{then}, \\text{else}, id, :=, 0 \\}\\)\n\\(P = \\{ C \\rightarrow \\text{if } id \\text{ then } A \\text{ else } A, A \\rightarrow id := 0 \\}\\)\n\\(S = C\\)\n\nFaça a derivação da string if id then id := 0 else id := 0.\n\n\n5.2.2.6 Exercício 6: Parênteses Balanceados\nConsidere a gramática \\(G_{par}\\) para gerar sequências de parênteses balanceados:\n\n\\(N = \\{B\\}\\)\n\\(\\Sigma = \\{ (, ) \\}\\)\n\\(P = \\{ B \\rightarrow (B), B \\rightarrow BB, B \\rightarrow \\epsilon \\}\\)\n\\(S = B\\)\n\nFaça a derivação da string ()(()).\n\n\n5.2.2.7 Exercício 7: Palíndromo Vazio\nUsando a gramática de palíndromos \\(G_{pal}\\) do primeiro exercício, faça a derivação da string vazia, \\(\\epsilon\\).",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Gramáticas e Linguagens Livres de Contexto</span>"
    ]
  },
  {
    "objectID": "04-Gramaticas.html#sec-notacao-BNF",
    "href": "04-Gramaticas.html#sec-notacao-BNF",
    "title": "5  Gramáticas e Linguagens Livres de Contexto",
    "section": "5.3 A Notação \\(BNF\\)",
    "text": "5.3 A Notação \\(BNF\\)\nA Forma de Backus-Naur, universalmente conhecida pela sigla \\(BNF\\), representa um dos sistemas de notação mais elegantes e influentes da ciência da computação teórica. Desenvolvida no final da década de 1950 para descrever formalmente a sintaxe da linguagem de programação ALGOL 60, a \\(BNF\\) tornou-se o padrão de facto para especificar a sintaxe de linguagens de programação e protocolos de comunicação.\n\n5.3.1 Contexto Histórico e Desenvolvimento\nA criação da \\(BNF\\) emergiu de uma necessidade prática urgente durante o desenvolvimento de ALGOL 60. John Backus, líder do projeto, enfrentava o desafio de descrever a sintaxe da linguagem de forma precisa e não ambígua para uma audiência internacional de implementadores. As descrições informais em linguagem natural mostravam-se inadequadas, gerando interpretações divergentes e implementações incompatíveis.\nA solução veio por meio da colaboração entre Backus e Peter Naur, que refinou e popularizou a notação. Naur, como editor do relatório ALGOL 60, foi responsável por sistematizar e formalizar o que inicialmente eram esboços conceituais de Backus. A notação resultante combinava a precisão matemática das gramáticas formais com uma legibilidade que a tornava acessível tanto para teóricos quanto para implementadores práticos.\nA \\(BNF\\) foi desenvolvida com conhecimento explícito dos trabalhos de Chomsky sobre Gramáticas Livres de Contexto. Backus e Naur compreenderam que estavam essencialmente criando uma notação prática para expressar gramáticas do Tipo 2 da Hierarquia de Chomsky, estabelecendo assim uma ponte fundamental entre a teoria formal e a aplicação industrial.\n\n\n5.3.2 Estrutura e Componentes da Notação \\(BNF\\)\nA \\(BNF\\) define uma gramática por meio de um conjunto de regras de produção (ou regras de reescrita), na qual cada regra especifica como um símbolo não-terminal pode ser expandido em uma sequência de símbolos terminais e não-terminais. A estrutura básica de uma regra \\(BNF\\) segue o padrão:\n&lt;símbolo-não-terminal&gt; ::= expressão-alternativa\nOs elementos fundamentais da notação \\(BNF\\) são:\n\nSímbolos Não-terminais: representados entre colchetes angulares (&lt; &gt;), estes são os elementos que podem ser definidos por outras regras. Exemplos: &lt;expressão&gt;, &lt;comando&gt;, &lt;identificador&gt;.\nSímbolos Terminais: representam os elementos atômicos da linguagem - palavras-chave, operadores, delimitadores. São escritos literalmente, frequentemente entre aspas. Exemplos: \"if\", \"+\", \";\".\nOperador de Definição (::=): lê-se “é definido como” ou “pode ser substituído por”. Estabelece a relação entre o lado esquerdo (não-terminal) e o lado direito (expressão de definição).\nOperador de Alternativa (|): indica escolhas mutuamente exclusivas. Lê-se “ou”. Permite múltiplas definições para um mesmo não-terminal.\nAgrupamento: parênteses são utilizados para agrupar elementos e controlar a precedência das operações.\n\n\n\n5.3.3 Exemplo Fundamental: Expressões Aritméticas Simples\nPara ilustrar a aplicação prática da \\(BNF\\), consideremos uma gramática para expressões aritméticas básicas que inclui adição, multiplicação, parênteses e identificadores:\n&lt;expressão&gt;     ::= &lt;termo&gt; \"+\" &lt;expressão&gt; | &lt;termo&gt;\n&lt;termo&gt;         ::= &lt;fator&gt; \"*\" &lt;termo&gt; | &lt;fator&gt;\n&lt;fator&gt;         ::= \"(\" &lt;expressão&gt; \")\" | &lt;identificador&gt;\n&lt;identificador&gt; ::= &lt;letra&gt; | &lt;letra&gt; &lt;identificador&gt;\n&lt;letra&gt;         ::= \"a\" | \"b\" | \"c\" | \"d\" | \"e\" | \"f\" | \"g\" | \"h\" | \n                    \"i\" | \"j\" | \"k\" | \"l\" | \"m\" | \"n\" | \"o\" | \"p\" | \n                    \"q\" | \"r\" | \"s\" | \"t\" | \"u\" | \"v\" | \"w\" | \"x\" | \n                    \"y\" | \"z\"\nEsta gramática demonstra características essenciais da \\(BNF\\):\n\nHierarquia de Precedência: a estrutura &lt;expressão&gt; → &lt;termo&gt; → &lt;fator&gt; estabelece que multiplicação tem precedência sobre adição.\nRecursão: &lt;expressão&gt; referencia a si mesma, permitindo expressões de comprimento arbitrário.\nAssociatividade: a recursão à direita em &lt;termo&gt; \"+\" &lt;expressão&gt; define associatividade à esquerda para adição.\n\nUtilizando a gramática acima, podemos derivar a expressão a + b * c:\n&lt;expressão&gt;\n→ &lt;termo&gt; \"+\" &lt;expressão&gt;\n→ &lt;fator&gt; \"+\" &lt;expressão&gt;\n→ &lt;identificador&gt; \"+\" &lt;expressão&gt;\n→ &lt;letra&gt; \"+\" &lt;expressão&gt;\n→ \"a\" \"+\" &lt;expressão&gt;\n→ \"a\" \"+\" &lt;termo&gt;\n→ \"a\" \"+\" &lt;fator&gt; \"*\" &lt;termo&gt;\n→ \"a\" \"+\" &lt;identificador&gt; \"*\" &lt;termo&gt;\n→ \"a\" \"+\" &lt;letra&gt; \"*\" &lt;termo&gt;\n→ \"a\" \"+\" \"b\" \"*\" &lt;termo&gt;\n→ \"a\" \"+\" \"b\" \"*\" &lt;fator&gt;\n→ \"a\" \"+\" \"b\" \"*\" &lt;identificador&gt;\n→ \"a\" \"+\" \"b\" \"*\" &lt;letra&gt;\n→ \"a\" \"+\" \"b\" \"*\" \"c\"\nEsta derivação demonstra como a gramática força a interpretação a + (b * c), respeitando a precedência de operadores.\n\n\n\n\n\n\nTip\n\n\n\nExtensões da \\(BNF\\)\nA \\(BNF\\) original foi posteriormente estendida para aumentar sua expressividade e conveniência:\n\n\\(EBNF\\) (Extended \\(BNF\\)): introduz operadores adicionais como {} para repetição (zero ou mais), [] para elementos opcionais, e () para agrupamento explícito.\n\\(ABNF\\) (Augmented \\(BNF\\)): usado em RFCs da Internet, adiciona suporte para especificação de repetições numéricas e valores em diferentes bases.\n\nExemplo em EBNF:\nidentificador = letra, { letra | dígito };\nletra = \"a\" | \"b\" | ... | \"z\";\ndígito = \"0\" | \"1\" | ... | \"9\";\nEsta notação é equivalente à \\(BNF\\) original, mas significativamente mais concisa.\n\n\n\n\n5.3.4 Comparação com Gramáticas Livres de Contexto\nA \\(BNF\\) é essencialmente uma notação alternativa para Gramáticas Livres de Contexto. A correspondência é direta, como pode ser visto na Table 5.1:\n\n\n\nTable 5.1: Correspondência entre elementos de Gramáticas Livres de Contexto e notação \\(BNF\\).\n\n\n\n\n\nElemento GLC\nNotação \\(BNF\\)\nExemplo\n\n\n\n\nNão-terminal\n&lt;símbolo&gt;\n&lt;expressão&gt;\n\n\nTerminal\nLiteral ou entre aspas\n\"if\", +\n\n\nRegra de produção\n::=\n&lt;E&gt; ::= &lt;E&gt; + &lt;T&gt;\n\n\nAlternativas\n\\|\n&lt;E&gt; ::= &lt;T&gt; \\| &lt;E&gt; + &lt;T&gt;\n\n\n\n\n\n\n\n\n5.3.5 Aplicações e Influência\nA \\(BNF\\) estabeleceu um precedente que influenciou profundamente o desenvolvimento de linguagens de programação e ferramentas de compilação:\nEspecificação de Linguagens: praticamente todas as linguagens modernas utilizam variações da \\(BNF\\) para definir sintaxe formal. Exemplos incluem C, Pascal, Ada, e mais recentemente, linguagens como Python e Rust.\nGeradores de Parsers: ferramentas como YACC, Bison, ANTLR e muitas outras utilizam sintaxe baseada em \\(BNF\\) como entrada, traduzindo especificações declarativas em código de análise sintática.\nProtocolos de Rede: muitos RFCs da Internet utilizam \\(ABNF\\) para especificar protocolos como HTTP, SMTP, e URI.\nDocumentação Técnica: A \\(BNF\\) tornou-se linguagem franca para comunicar especificações sintáticas de forma precisa e não ambígua.\nA elegância da \\(BNF\\) reside em sua capacidade de tornar formal acessível. Ela democratizou a especificação rigorosa de linguagens, permitindo que desenvolvedores sem formação profunda em teoria formal pudessem criar e comunicar especificações precisas. Esta contribuição para a prática da engenharia de software não pode ser subestimada: a \\(BNF\\) facilitou o desenvolvimento sistemático e a interoperabilidade de linguagens de programação, estabelecendo as fundações sobre as quais repousa muito do ecossistema computacional contemporâneo.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Gramáticas e Linguagens Livres de Contexto</span>"
    ]
  },
  {
    "objectID": "04-Gramaticas.html#geração-de-sentenças-derivações-árvores-e-ambiguidade",
    "href": "04-Gramaticas.html#geração-de-sentenças-derivações-árvores-e-ambiguidade",
    "title": "5  Gramáticas e Linguagens Livres de Contexto",
    "section": "5.4 Geração de Sentenças: Derivações, Árvores e Ambiguidade",
    "text": "5.4 Geração de Sentenças: Derivações, Árvores e Ambiguidade\nUma derivação é a sequência de passos que transforma o símbolo inicial em uma string final de terminais por meio da aplicação das regras de produção. Exatamente o processo que a esforçada leitora aprendeu na seção Section 5.2. Em cada passo, um não-terminal é escolhido e substituído pelo corpo de uma da regras de produção deste não-terminal. Como uma forma sentencial intermediária pode conter múltiplos não terminais, precisaremos de uma convenção para determinar qual deles expandir. Isso leva a duas estratégias de derivação:\n\nDerivação Mais à Esquerda (Leftmost Derivation): em cada passo, o símbolo não-terminal que aparece mais à esquerda na forma sentencial é sempre o escolhido para ser substituído.\n\nDerivação Mais à Direita (Rightmost Derivation): em cada passo, o símbolo não-terminal que aparece mais à direita é o escolhido para ser substituído.\n\nPara uma gramática não ambígua, embora as sequências de passos sejam diferentes, tanto a derivação mais à esquerda quanto a mais à direita para uma dada sentença resultarão na mesma estrutura sintática. Como exemplo, a atenta leitora pode estudar o exemplo a seguir:\nExemplo 1: Gramática Não Ambígua para Expressões Aritméticas\nConsidere a seguinte gramática:\n\nSímbolos não-terminais (N): \\(\\{E, T, F\\}\\)\nSímbolos terminais (): \\(\\{id, +, *, (, )\\}\\)\nSímbolo inicial (S): \\(E\\)\nRegras de produção (P):\n\n\\(E \\rightarrow E + T\\)\n\\(E \\rightarrow T\\)\n\\(T \\rightarrow T * F\\)\n\\(T \\rightarrow F\\)\n\\(F \\rightarrow (E)\\)\n\\(F \\rightarrow id\\)\n\n\nVamos fazer a Derivação da string “id + id * id”, primeiro usando a derivação Mais à Esquerda (Leftmost):\n\\[\n\\begin{align}\nE &\\Rightarrow E + T                &&\\text{[regra 1: expandir E]}\\\n  &\\Rightarrow T + T                &&\\text{[regra 2: expandir E mais à esquerda]}\\\n  &\\Rightarrow F + T                &&\\text{[regra 4: expandir T mais à esquerda]}\\\n  &\\Rightarrow id + T               &&\\text{[regra 6: expandir F mais à esquerda]}\\\n  &\\Rightarrow id + T * F           &&\\text{[regra 3: expandir T]}\\\n  &\\Rightarrow id + F * F           &&\\text{[regra 4: expandir T mais à esquerda]}\\\n  &\\Rightarrow id + id * F          &&\\text{[regra 6: expandir F mais à esquerda]}\\\n  &\\Rightarrow id + id * id         &&\\text{[regra 6: expandir F]}\n\\end{align}\n\\]\nAgora vamos derivar a mesma string usando a derivação Mais à Direita (Rightmost):\n\\[\n\\begin{align}\nE &\\Rightarrow E + T                &&\\text{[regra 1: expandir E]}\\\n  &\\Rightarrow E + T * F            &&\\text{[regra 3: expandir T mais à direita]}\\\n  &\\Rightarrow E + T * id           &&\\text{[regra 6: expandir F mais à direita]}\\\n  &\\Rightarrow E + F * id           &&\\text{[regra 4: expandir T mais à direita]}\\\n  &\\Rightarrow E + id * id          &&\\text{[regra 6: expandir F mais à direita]}\\\n  &\\Rightarrow T + id * id          &&\\text{[regra 2: expandir E]}\\\n  &\\Rightarrow F + id * id          &&\\text{[regra 4: expandir T]}\\\n  &\\Rightarrow id + id * id         &&\\text{[regra 6: expandir F]}\n\\end{align}\n\\]\nObservando as duas derivações é possível perceber que a gramática usada neste exemplo, não é ambígua. Observe que:\n\nPrecedência clara: A multiplicação \\((*)\\) tem precedência maior que a adição \\((+)\\)\n\n\\(T\\) (termo) gera multiplicações\n\\(E\\) (expressão) gera adições de termos\n\nAssociatividade definida: Operadores associam à esquerda\n\n\\(E \\rightarrow E + T\\) (não \\(E \\rightarrow T + E\\))\n\\(T \\rightarrow T * F\\) (não \\(T \\rightarrow F * T\\))\n\nEstrutura única: Para qualquer string válida, existe exatamente uma árvore de derivação, independentemente da ordem de derivação (leftmost ou rightmost).\n\nA atenta leitora deve notar que embora a sequência de passos seja diferente nas duas derivações, ambas produzem a mesma estrutura sintática: \\(id + (id * id)\\), na qual a multiplicação tem precedência sobre a adição.\n\n5.4.1 A Construção de Árvores de Derivação\nO processo mais intuitivo de visualizar a estrutura hierárquica imposta por uma gramática a uma sentença é por meio de uma árvore de derivação, ou árvore sintática. Uma árvore de derivação é uma representação gráfica de uma derivação que abstrai a ordem em que as produções foram aplicadas. Suas propriedades são definidas por:\n\nA raiz da árvore é rotulada com o símbolo inicial \\(S\\).\nCada vértice interno é rotulado com um símbolo não-terminal.\nCada folha é rotulada com um símbolo terminal ou com \\(\\epsilon\\).\nSe um vértice interno é rotulado com \\(A\\) e seus filhos, da esquerda para a direita, são rotulados com \\(X_1\\), \\(X_2\\), …, \\(X_n\\), então deve existir uma regra de produção \\(A \\rightarrow X_1 X_2 ... X_n\\) na gramática.\n\nA concatenação das folhas da árvore, lidas da esquerda para a direita, forma a sentença gerada, também conhecida como yield da árvore. A árvore de derivação captura a estrutura sintática essencial da sentença, tornando explícitas as relações entre suas subpartes.\nExemplo 2: Gramática Não Ambígua para Listas\nConsidere a gramática definida como:\n\nSímbolos não-terminais (N): \\(\\{L, E\\}\\)\nSímbolos terminais (Σ): \\(\\{a, b, [, ], ,\\}\\)\nSímbolo inicial (S): \\(L\\)\nRegras de produção (P):\n\n\\(L \\rightarrow [E]\\)\n\\(L \\rightarrow [\\,]\\)\n\\(E \\rightarrow E, a\\)\n\\(E \\rightarrow E, b\\)\n\\(E \\rightarrow a\\)\n\\(E \\rightarrow b\\)\n\n\nVamos derivar a string “[a, b, a]”. Primeiro com a Derivação Mais à Esquerda (Leftmost):\n\\[\n\\begin{align}\nL &\\Rightarrow [E]                  &&\\text{[regra 1: expandir L]}\\\\\n  &\\Rightarrow [E, a]               &&\\text{[regra 3: expandir E]}\\\\\n  &\\Rightarrow [E, b, a]            &&\\text{[regra 4: expandir E mais à esquerda]}\\\\\n  &\\Rightarrow [a, b, a]            &&\\text{[regra 5: expandir E mais à esquerda]}\n\\end{align}\n\\]\nAgora com a Derivação Mais à Direita (Rightmost):\n\\[\n\\begin{align}\nL &\\Rightarrow [E]                  &&\\text{[regra 1: expandir L]}\\\\\n  &\\Rightarrow [E, a]               &&\\text{[regra 3: expandir E]}\\\\\n  &\\Rightarrow [E, b, a]            &&\\text{[regra 4: expandir E]}\\\\\n  &\\Rightarrow [a, b, a]            &&\\text{[regra 5: expandir E]}\n\\end{align}\n\\]\nA Figure 5.1 apresenta duas árvores geradas por dois processos de derivação, um à esquerda e outro à direita, para a string “[a, b, a]”. A atenta leitora deve observar que as árvores são iguais.\n\n\n\n\n\n\nFigure 5.1: Apresentação das árvores de derivação à esquerda e à direita da string “[a, b, a]” de forma gráfica.\n\n\n\nExemplo 3: Considerando a gramática do Exemplo 2, faça a derivação da string “[b]”\nNovamente, começando com a Derivação Mais à Esquerda (Leftmost):\n\\[\n\\begin{align}\nL &\\Rightarrow [E]                  &&\\text{[regra 1: expandir L]}\\\\\n  &\\Rightarrow [b]                  &&\\text{[regra 6: expandir E]}\n\\end{align}\n\\]\nFinalmente, a Derivação Mais à Direita (Rightmost):\n\\[\n\\begin{align}\nL &\\Rightarrow [E]                  &&\\text{[regra 1: expandir L]}\\\\\n  &\\Rightarrow [b]                  &&\\text{[regra 6: expandir E]}\n\\end{align}\n\\]\nA atenta leitora deve perceber que esta gramática não é ambígua. Considerando que:\n\nEstrutura hierárquica clara:\n\n\\(L\\) gera apenas a estrutura de lista com colchetes;\n\\(E\\) gera apenas a sequência de elementos separados por vírgula.\n\nAssociatividade única:\n\nAs regras \\(E \\rightarrow E, a\\) e \\(E \\rightarrow E, b\\) forçam associatividade à esquerda;\nNão há regras como \\(E \\rightarrow a, E\\) que criariam ambiguidade.\n\nSem sobreposição de produções:\n\nCada não-terminal tem um papel específico e não conflitante;\nLista vazia \\([\\,]\\) é tratada separadamente, evitando ambiguidade.\n\n\nNota: A gramática usada nos exemplos 2 e 3 garante que elementos são adicionados sempre à direita da lista, construindo-a da esquerda para a direita. A estrutura \\([a, b, a]\\) só pode ser interpretada de uma forma: uma lista contendo três elementos na ordem especificada.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Gramáticas e Linguagens Livres de Contexto</span>"
    ]
  },
  {
    "objectID": "04-Gramaticas.html#o-problema-da-ambiguidade-em-gramáticas-formais",
    "href": "04-Gramaticas.html#o-problema-da-ambiguidade-em-gramáticas-formais",
    "title": "5  Gramáticas e Linguagens Livres de Contexto",
    "section": "5.5 O Problema da Ambiguidade em Gramáticas Formais",
    "text": "5.5 O Problema da Ambiguidade em Gramáticas Formais\nUma gramática G = (V, T, P, S) é ambígua se e somente se existe pelo menos uma cadeia \\(w \\in L(G)\\) que possui duas ou mais árvores de derivação distintas. Em termos práticos, isto quer dizer que a mesma sentença pode ser construída por meio de diferentes sequências de aplicação de regras de produção. A existência de diferentes estruturas sintáticas implica na existência de interpretações semânticas diferentes. Neste caso, o parser não consegue determinar univocamente qual estrutura sintática representa a intenção do programador.\nExemplo 1: Considerando a gramática \\(G_1\\), mostre que a sentença id + id * id é ambígua.\n\\[G_1 = (\\{E\\}, \\{+, *, (, ), id\\}, P, E)\\]\nNa qual \\(P\\) consiste das produções:\n\\[\n\\begin{align}\nE &\\Rightarrow E + E \\\\\nE &\\Rightarrow E * E \\\\\nE &\\Rightarrow (E) \\\\\nE &\\Rightarrow id\n\\end{align}\n\\]\nAnalisando a sequência id + id * id, podemos observar que esta string pode ser interpretada de duas maneiras diferentes, dependendo da ordem em que as operações são realizadas:\n\nDerivação 1 (Multiplicação com maior precedência):\n\n\\[\n  \\begin{align}\n  E &\\Rightarrow E + E \\\\\n  &\\Rightarrow id + E \\\\\n  &\\Rightarrow id + E * E \\\\\n  &\\Rightarrow id + id * E \\\\\n  &\\Rightarrow id + id * id\n  \\end{align}\n  \\]\nO resultado da primeira derivação permite que a string id + id * id seja interpretada como id + (id * id).\n\nDerivação 2 (Adição com maior precedência):\n\n\\[\n  \\begin{align}\n  E &\\Rightarrow E * E \\\\\n    &\\Rightarrow E + E * E \\\\\n    &\\Rightarrow id + E * E \\\\\n    &\\Rightarrow id + id * E \\\\\n    &\\Rightarrow id + id * id\n  \\end{align}\n  \\]\nO resultado da segunda derivação permite que a string id + id * id seja interpretada como (id + id) * id.\n\n5.5.1 Características e Taxonomia da Ambiguidade\nA ambiguidade em gramáticas formais manifesta-se de diferentes formas e com distintos níveis de penetração na estrutura da linguagem:\nA ambiguidade local representa conflitos que afetam construções sintáticas específicas e bem delimitadas dentro da gramática. O exemplo mais emblemático desta categoria são os problemas de precedência de operadores, nos quais a gramática permite múltiplas interpretações para expressões como id + id * id. Neste caso, a ambiguidade está circunscrita às regras que governam operadores aritméticos e pode ser resolvida por meio da reescrita da gramática ou da especificação explícita de precedência e associatividade.\nA ambiguidade global, por outro lado, permeia toda a estrutura da gramática e afeta a arquitetura fundamental da linguagem. O problema clássico do dangling else exemplifica perfeitamente esta categoria. Quando uma construção como if (c1) if (c2) s1 else s2 pode ser interpretada de duas formas distintas, a ambiguidade não está limitada a uma regra específica, mas emerge da interação complexa entre múltiplas produções que definem comandos condicionais aninhados.\nAs manifestações da ambiguidade podem ser classificadas em três categorias fundamentais que refletem diferentes aspectos da análise sintática.\nA ambiguidade de precedência surge quando operadores de diferentes níveis hierárquicos não possuem uma ordem de avaliação claramente definida pela gramática. Esta forma de ambiguidade é particularmente comum em linguagens que incluem operadores aritméticos, lógicos e relacionais. A ausência de uma hierarquia bem estabelecida força o parser a tomar decisões arbitrárias sobre qual operação deve ser executada primeiro, podendo resultar em interpretações semânticas drasticamente diferentes.\nA ambiguidade de associatividade manifesta-se quando operadores do mesmo nível de precedência podem ser agrupados tanto à esquerda quanto à direita, alterando o resultado da computação. Por exemplo, em uma expressão como a - b - c, a associatividade à esquerda produz (a - b) - c, enquanto a associatividade à direita resulta em a - (b - c). Embora matematicamente equivalentes para operações comutativas como adição, essa distinção torna-se crítica para operações não-comutativas.\nA ambiguidade estrutural representa a forma mais complexa e desafiadora de ambiguidade, na qual diferentes agrupamentos de construções sintáticas produzem estruturas hierárquicas distintas. Esta categoria transcende questões operacionais e atinge o núcleo da organização sintática da linguagem, afetando como blocos de código, estruturas de controle e definições de escopo são interpretados pelo compilador.\nExemplo 2: o Problema do Dangling Else\nConsidere a gramática \\(G_2\\) (Ambígua para if-then-else) definida por:\n\\[G_2 = (\\{S, E\\}, \\{if, then, else, (, ), c, s\\}, P, S)\\]\nNa qual \\(P\\) consiste das produções:\n\\[\n\\begin{align}\nS &\\Rightarrow if (E) S \\\\\n&\\Rightarrow if (E) S else S \\\\\n&\\Rightarrow s \\\\\nE &\\Rightarrow c\n\\end{align}\n\\]\nPara entender a ambiguidade vamos analisar a sentença if (c1) if (c2) s1 else s2.\n\nNesta interpretação, a estrutura if (c2) s1 else s2 é tratada como um único comando \\(S\\), que por sua vez está aninhado dentro do primeiro if. A derivação ocorre da seguinte forma:\n\n\\[\n\\begin{align}\nS &\\Rightarrow \\text{if } (E) S \\\\\n&\\Rightarrow \\text{if } (c_1) S \\\\\n&\\Rightarrow \\text{if } (c_1) \\text{ if } (E) S \\text{ else } S \\\\\n&\\Rightarrow \\text{if } (c_1) \\text{ if } (c_2) S \\text{ else } S \\\\\n&\\Rightarrow \\text{if } (c_1) \\text{ if } (c_2) s_1 \\text{ else } S \\\\\n&\\Rightarrow \\text{if } (c_1) \\text{ if } (c_2) s_1 \\text{ else } s_2\n\\end{align}\n\\]\nResultando em:\nif (c1) {\n    if (c2)\n        s1\n    else\n        s2\n}\n\nNesta interpretação, a estrutura if (c2) s1 é o corpo do if principal, e o else s2 está associado a este if mais externo.\n\n\\[\n\\begin{align}\nS &\\Rightarrow \\text{if } (E) S \\text{ else } S \\\\\n&\\Rightarrow \\text{if } (c_1) S \\text{ else } S \\\\\n&\\Rightarrow \\text{if } (c_1) \\text{ if } (E) S \\text{ else } S \\\\\n&\\Rightarrow \\text{if } (c_1) \\text{ if } (c_2) S \\text{ else } S \\\\\n&\\Rightarrow \\text{if } (c_1) \\text{ if } (c_2) s_1 \\text{ else } S \\\\\n&\\Rightarrow \\text{if } (c_1) \\text{ if } (c_2) s_1 \\text{ else } s_2\n\\end{align}\n\\]\nif (c1) {\n    if (c2)\n        s1\n}\nelse\n    s2\nComo existem duas derivações à esquerda distintas para a mesma sentença, a gramática \\(G_2\\) é formalmente ambígua.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Gramáticas e Linguagens Livres de Contexto</span>"
    ]
  },
  {
    "objectID": "04-Gramaticas.html#técnicas-de-eliminação-de-ambiguidade",
    "href": "04-Gramaticas.html#técnicas-de-eliminação-de-ambiguidade",
    "title": "5  Gramáticas e Linguagens Livres de Contexto",
    "section": "5.6 Técnicas de Eliminação de Ambiguidade",
    "text": "5.6 Técnicas de Eliminação de Ambiguidade\nA atenta leitora deve compreender que a ambiguidade em uma gramática não é uma fatalidade irreversível. Ao longo do desenvolvimento da teoria das linguagens formais, foram desenvolvidas técnicas sistemáticas para transformar gramáticas ambíguas em equivalentes não-ambíguas, preservando a linguagem gerada. Estas técnicas representam não apenas exercícios teóricos, mas ferramentas práticas essenciais no projeto de compiladores modernos.\n\n5.6.1 Reescrita Sistemática da Gramática\nA técnica mais fundamental para eliminar ambiguidade consiste na reescrita estruturada da gramática. Esta abordagem requer uma análise cuidadosa das fontes de ambiguidade e sua eliminação por meio da introdução de níveis hierárquicos explícitos na estrutura gramatical.\nConsidere novamente a gramática ambígua \\(G_1\\) para expressões aritméticas:\n\\[G_1 = (\\{E\\}, \\{+, *, (, ), id\\}, P_1, E)\\]\nNa qual \\(P_1\\) consiste das produções:\n\n\\(E \\rightarrow E + E\\)\n\\(E \\rightarrow E * E\\)\n\\(E \\rightarrow (E)\\)\n\\(E \\rightarrow id\\)\n\nA transformação desta gramática em sua versão não-ambígua \\(G_3\\) segue um princípio arquitetural elegante: a estratificação sintática. Cada nível de precedência é codificado como um não-terminal distinto:\n\\[G_3 = (\\{E, T, F\\}, \\{+, *, (, ), id\\}, P_3, E)\\]\nNa qual \\(P_3\\) consiste das produções:\n\n\\(E \\rightarrow E + T \\mid T\\)\n\\(T \\rightarrow T * F \\mid F\\)\n\\(F \\rightarrow (E) \\mid id\\)\n\nEsta hierarquia de não-terminais estabelece inequivocamente que:\n\nPrecedência operacional: A multiplicação, governada por \\(T\\), tem precedência sobre a adição, governada por \\(E\\). Uma expressão como id + id * id só pode ser derivada como id + (id * id).\nAssociatividade uniforme: Ambos operadores associam à esquerda devido à recursão à esquerda nas produções \\(E \\rightarrow E + T\\) e \\(T \\rightarrow T * F\\). A expressão a + b + c é interpretada como (a + b) + c.\nPreservação da linguagem: \\(L(G_1) = L(G_3)\\), ou seja, ambas gramáticas geram exatamente o mesmo conjunto de strings, diferindo apenas na estrutura de derivação.\n\n\n\n5.6.2 Declarações Explícitas de Precedência e Associatividade\nO desenvolvimento de geradores de parsers como YACC (Yet Another Compiler-Compiler) e seu sucessor GNU Bison introduziu uma abordagem pragmática: permitir que o desenvolvedor especifique diretamente as regras de desambiguação sem reescrever a gramática.\n%left '+'     /* menor precedência, associa à esquerda */\n%left '*'     /* precedência intermediária */\n%right '^'    /* maior precedência, associa à direita */\n%%\nexpr: expr '+' expr\n| expr '*' expr\n| expr '^' expr\n| '(' expr ')'\n| ID;\nEstas declarações instruem o gerador de parser a resolver conflitos shift/reduce de forma determinística. Quando confrontado com a + b * c, o parser saberá que deve realizar o shift do * (maior precedência) antes de reduzir a adição. Assim, os eventuais problemas de ambiguidade decorrentes da gramática original podem ser resolvidos.\n\n5.6.2.1 Resolução do Problema do Dangling Else\nPara a ambiguidade estrutural do dangling else, a solução canônica adotada pela maioria das linguagens modernas é a convenção do else mais próximo. Esta pode ser formalizada por meio de uma gramática cuidadosamente estruturada:\n\\[G_{if} = (\\{S, M, U\\}, \\{if, else, (, ), c, s\\}, P_{if}, S)\\]\nna qual \\(P_{if}\\) consiste das seguintes produções:\n\n\\(S \\rightarrow M \\mid U\\);\n\\(M \\rightarrow \\text{if } (E) \\text{ } M \\text{ else } M \\mid s\\);\n\\(U \\rightarrow \\text{if } (E) \\text{ } S \\mid \\text{if } (E) \\text{ } M \\text{ else } U\\).\n\nNesta gramática, \\(M\\) (matched) representa comandos completos nos quais todo if tem seu else correspondente, enquanto \\(U\\) (unmatched) representa comandos com if sem else. A estrutura garante que um else sempre se associa ao if mais interno ainda não pareado.\n\n\n\n5.6.3 Detecção Sistemática de Ambiguidade\nA identificação de ambiguidade em uma gramática arbitrária é, em geral, um problema indecidível. Ou seja, não existe um algoritmo que sempre termine e determine se qualquer gramática é ambígua. Entretanto, a atenta leitora pode empregar uma das seguintes estratégias práticas:\n\nAbordagens Algorítmicas\nVerificação exaustiva limitada: Para gramáticas pequenas, pode-se enumerar sistematicamente todas as derivações para strings até um comprimento \\(n\\) fixo. Se duas árvores de derivação distintas forem encontradas para a mesma string, a ambiguidade está provada. Esta técnica, embora computacionalmente intensiva, é conclusiva quando encontra ambiguidade.\nAnálise de tabelas de parsing: Ferramentas como YACC e Bison constroem tabelas de ação/transição para parsers \\(LR\\). A presença de conflitos shift/reduce ou reduce/reduce nestas tabelas indica potencial ambiguidade. Um conflito shift/reduce ocorre quando o parser não consegue decidir entre consumir mais entrada ou aplicar uma redução; um conflito reduce/reduce surge quando múltiplas produções podem ser aplicadas.\nVerificadores especializados: Ferramentas como o CFG checker empregam técnicas sofisticadas de análise estática para detectar ambiguidade, incluindo aproximações conservativas que podem provar não-ambiguidade para classes específicas de gramáticas.\nHeurísticas e Padrões Problemáticos\n\nA experiência acumulada na construção de compiladores identificou padrões gramaticais que frequentemente indicam ambiguidade:\nProduções recursivas bilaterais: Regras da forma \\(A \\rightarrow A \\alpha A\\) são quase sempre ambíguas, permitem múltiplas formas de agrupar três ou mais ocorrências de \\(A\\).\nPrefixos comuns extensos: Quando múltiplas produções para o mesmo não-terminal compartilham longos prefixos, como \\(A \\rightarrow \\alpha\\beta\\gamma\\) e \\(A \\rightarrow \\alpha\\beta\\delta\\), a decisão sobre qual produção aplicar pode requerer _Lookahead_arbitrariamente longo.\nAninhamento sem delimitadores explícitos: Construções que permitem aninhamento sem marcadores claros de início e fim, como o problema clássico do dangling else, invariavelmente levam a ambiguidade estrutural.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Gramáticas e Linguagens Livres de Contexto</span>"
    ]
  },
  {
    "objectID": "04-Gramaticas.html#autômatos-com-pilha-a-máquina-por-trás-das-linguagens-livres-de-contexto",
    "href": "04-Gramaticas.html#autômatos-com-pilha-a-máquina-por-trás-das-linguagens-livres-de-contexto",
    "title": "5  Gramáticas e Linguagens Livres de Contexto",
    "section": "5.7 Autômatos com Pilha: A Máquina por Trás das Linguagens Livres de Contexto",
    "text": "5.7 Autômatos com Pilha: A Máquina por Trás das Linguagens Livres de Contexto\nComo estabelecido anteriormente, os autômatos finitos, os reconhecedores para linguagens regulares, são fundamentalmente limitados por sua falta de memória. A capacidade de lembrar está restrita ao conjunto finito de estados da máquina. Essa limitação os impede de reconhecer linguagens que exigem a correspondência de símbolos ou contagem, como a linguagem \\(L=\\{a^n b^n \\mid n \\geq 0\\}\\).\nPara reconhecer a classe mais ampla das linguagens livres de contexto, é necessário um modelo de computação mais poderoso. O autômato de pilha é esse modelo. Um Autômato de Pilha pode ser concebido como um autômato finito não determinístico ao qual foi adicionada uma memória auxiliar: uma pilha, do inglês stack.\nA pilha é uma estrutura de dados com acesso restrito, operando no modo LIFO (Last-In, First-Out), o que significa que o último elemento inserido é o primeiro a ser removido. As transições de um Autômato de Pilha são mais complexas que as de um Autômato Finito. A decisão de qual transição tomar depende de três fatores: o estado atual, o próximo símbolo na string de entrada e o símbolo que está no topo da pilha.\nEm cada transição, além de mudar de estado e, opcionalmente, consumir um símbolo de entrada, o Autômato de Pilha pode realizar uma de três operações na pilha:\n\nEmpilhar (Push): Adicionar um ou mais símbolos ao topo da pilha;\nDesempilhar (Pop): Remover o símbolo do topo da pilha;\nManter: Não alterar o conteúdo da pilha.\n\nEsta capacidade de armazenar e recuperar informações de forma estruturada confere ao Autômato de Pilha seu poder computacional superior.\n\n5.7.1 A Equivalência Fundamental\nO resultado mais importante da teoria das Linguagens Livres de Contexto é a equivalência formal entre Gramáticas Livres de Contexto e autômatos com pilha. Uma linguagem é livre de contexto se, e somente se, existe um autômato com pilha que a reconhece.\nEsta equivalência é a espinha dorsal da análise sintática. Ela garante que para qualquer sintaxe de linguagem de programação que possa ser descrita por uma Gramática Livre de Contexto, podemos construir um mecanismo computacional o Autômato de Pilha para reconhecer programas escritos nessa linguagem. É importante notar que a classe de linguagens reconhecidas por Autômatos de Pilha não determinísticos é estritamente maior que a classe reconhecida por Autômatos de Pilha Determinísticos. São os Autômatos de Pilha não Determinísticos que são equivalentes em poder às Gramáticas Livres de Contexto em geral.\nA escolha de uma pilha como o mecanismo de memória para reconhecer Linguagens Livres de Contexto não é acidental. A estrutura LIFO de uma pilha espelha perfeitamente a natureza recursiva e aninhada das derivações em uma Gramática Livre de Contexto. Considere novamente a gramática \\(S \\rightarrow aSb\\) para a linguagem \\(a^nb^n\\). A derivação de aabb será dada por \\(S\\Rightarrow aSb \\Rightarrow aaSbb \\Rightarrow aabb\\). Observe como a estrutura se expande simetricamente de dentro para fora. Um Autômato de Pilha para esta linguagem implementa essa simetria de forma operacional: ao ler um a, ele empilha um símbolo de marcador (ex.: \\(X\\)); ao ler o próximo a, empilha outro \\(X\\). Quando começa a ler os bs, ele desempilha um \\(X\\) para cada b lido. Se a entrada terminar exatamente quando a pilha se esvaziar, a string é aceita. O processo de empilhar na primeira metade e desempilhar na ordem inversa na segunda metade é a encarnação mecânica da recursão gramatical. A pilha lembra as obrigações sintáticas (gerar um b correspondente para cada a) e as descarrega na ordem correta, tornando-a a estrutura de dados canônica para processar estruturas livres de contexto.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Gramáticas e Linguagens Livres de Contexto</span>"
    ]
  },
  {
    "objectID": "04-Gramaticas.html#as-fronteiras-do-contexto-livre-o-lema-do-bombeamento",
    "href": "04-Gramaticas.html#as-fronteiras-do-contexto-livre-o-lema-do-bombeamento",
    "title": "5  Gramáticas e Linguagens Livres de Contexto",
    "section": "5.8 As Fronteiras do Contexto Livre: O Lema do Bombeamento",
    "text": "5.8 As Fronteiras do Contexto Livre: O Lema do Bombeamento\nAssim como existe uma ferramenta para provar que uma linguagem não é regular, existe um análogo para as linguagens livres de contexto: o Lema do Bombeamento para Linguagens Livres de Contexto, também conhecido como Lema de Bar-Hillel. Sua principal aplicação é demonstrar, por contradição, que uma determinada linguagem não é livre de contexto.\nA intuição por trás do lema está enraizada na estrutura finita das gramáticas e na natureza das árvores de derivação. Para uma Gramática Livre de Contexto com um número finito de não terminais, qualquer string suficientemente longa gerada por ela deve ter uma árvore de derivação alta. Pelo princípio da casa dos pombos, um caminho longo da raiz a uma folha nessa árvore deve necessariamente conter pelo menos um não-terminal repetido. Essa repetição cria uma sub-árvore que pode ser podada ou duplicada, bombeando a string de uma maneira específica.\nO lema afirma formalmente que para qualquer linguagem livre de contexto \\(L\\), existe um inteiro \\(p \\geq 1\\), o comprimento de bombeamento, tal que qualquer string \\(s \\in L\\) com comprimento \\(\\mid s \\mid  \\geq p\\) pode ser decomposta em cinco substrings, \\(s = uvxyz\\), que devem satisfazer as seguintes três condições:\n\n\\(\\mid vxy \\mid \\leq p\\): a substring que contém as partes bombeáveis não é excessivamente longa;\n\\(\\mid vy \\mid \\geq 1\\): pelo menos uma das duas substrings bombeáveis (\\(v\\) ou \\(y\\)) não é vazia. Isso garante que o bombeamento realmente altera a string;\n\\(uv^nxy^nz \\in L\\) para todo inteiro \\(n \\geq 0\\): as duas substrings, \\(v\\) e \\(y\\), podem ser bombeadas (repetidas) em conjunto um número arbitrário de vezes, incluindo zero, o que corresponde a removê-las), e a string resultante permanecerá na linguagem \\(L\\).\n\n\n5.8.1 Aplicação Prática: Prova de que \\(L = \\{a^nb^nc^n \\mid n \\geq 0\\}\\) não é Livre de Contexto\nA linguagem \\(L=\\{a^nb^nc^n \\mid n \\geq 0\\}\\) é o exemplo clássico de uma linguagem que está além do alcance das Gramáticas Livres de Contexto. Podemos provar isso rigorosamente usando o lema do bombeamento.\nA prova segue por contradição:\n\nSuposição: Suponha que \\(L\\) é livre de contexto.\nInvocação do Lema: Pelo lema, deve existir um comprimento de bombeamento \\(p\\).\nEscolha da string: Selecionamos a string \\(s=a^pb^pc^p\\). Claramente, \\(s \\in L\\) e seu comprimento, \\(3p\\), é maior ou igual a \\(p\\).\nAnálise da Decomposição: O lema garante que \\(s\\) pode ser decomposta como \\(s=uvxyz\\), sujeita às condições do lema. A condição \\(\\mid vxy \\mid \\leq p\\) é a chave. Dada a estrutura de \\(s\\) (um bloco de as, seguido por um bloco de bs, seguido por um bloco de cs), esta condição implica que a sub_string_ \\(vxy\\) não pode conter ocorrências de todos os três símbolos (a, b e c). Ela pode estar inteiramente dentro do bloco de as, inteiramente dentro do de bs, ou abranger a fronteira entre as e bs, ou entre bs e cs.\nContradição: A condição \\(\\mid vy \\mid \\geq 1\\) garante que o bombeamento adicionará (ou removerá) pelo menos um símbolo. Vamos considerar o bombeamento para cima, com \\(n=2\\), resultando na string \\(s′=uv^2xy^2z\\).\n\nSe \\(vxy\\) contivesse apenas as, então \\(v\\) e \\(y\\) conteriam apenas as. A string \\(s′\\) teria mais as do que bs e cs, violando a condição da linguagem.\nSe \\(vxy\\) contivesse uma mistura de as e bs, então \\(v\\) e \\(y\\) poderiam conter as e bs, mas nenhum c. A string \\(s′\\) teria um número aumentado de as e/ou bs, mas o número de cs permaneceria \\(p\\). Novamente, a igualdade \\(n=n=n\\) seria quebrada.\nO mesmo raciocínio se aplica a todas as outras localizações possíveis de \\(vxy\\). Em nenhum caso, o bombeamento pode aumentar o número de as, bs e cs na mesma proporção.\n\nConclusão: A string bombeada \\(s′\\) não pertence a \\(L\\). Isso contradiz a terceira condição do lema. Portanto, a suposição inicial de que \\(L\\) é livre de contexto deve ser falsa.\n\nA estrutura de bombeamento duplo (\\(v\\) e \\(y\\)) do lema não é arbitrária. Ela revela a limitação fundamental das Linguagens Livres de Contexto a dependências de, no máximo, dois pontos. Uma Gramática Livre de Contexto, por meio de recursão como \\(S→aSb\\), pode correlacionar duas partes de uma string (os as no início e os bs no fim), que correspondem às partes \\(v\\) e \\(y\\) que são bombeadas em conjunto. A linguagem \\(a^nb^nc^n\\) exige uma dependência de três pontos. Um autômato com pilha gasta sua memória para verificar a correspondência entre as e bs, não restando capacidade para verificar os cs contra a contagem original. O lema do bombeamento formaliza essa limitação, mostrando que o bombeamento inevitavelmente quebra essa dependência tripla.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Gramáticas e Linguagens Livres de Contexto</span>"
    ]
  },
  {
    "objectID": "04-Gramaticas.html#os-analisadores-sintáticos",
    "href": "04-Gramaticas.html#os-analisadores-sintáticos",
    "title": "5  Gramáticas e Linguagens Livres de Contexto",
    "section": "5.9 Os Analisadores Sintáticos",
    "text": "5.9 Os Analisadores Sintáticos\nA análise sintática, ou parsing, é a segunda fase do processo de compilação, posicionada entre a análise léxica e a análise semântica. O analisador sintático atua como o guardião da gramática da linguagem. Enquanto o analisador léxico verifica a ortografia, se as palavras, ou tokens, são válidas, o analisador sintático verifica a gramática, se a sequência de tokens forma sentenças estruturalmente corretas. O objetivo principal do analisador sintático é determinar se o fluxo de tokens de entrada pode ser gerado pela gramática livre de contexto que define a linguagem e, em caso afirmativo, construir uma representação explícita dessa estrutura.\nA entrada para o analisador sintático é o fluxo de tokens produzido pelo analisador léxico. A saída, para um programa sintaticamente correto, é uma estrutura de dados que representa a estrutura hierárquica do código. Embora a árvore de derivação seja a representação teórica direta, na prática, os compiladores constroem uma Árvore Sintática Abstrata. A Árvore Sintática Abstrata é uma versão condensada e mais abstrata da árvore de derivação, que omite detalhes sintáticos intermediários, como parênteses para agrupamento ou não terminais que apenas passam a derivação adiante, e captura a estrutura lógica e semântica essencial do programa, tornando-a mais adequada para as fases subsequentes de análise e geração de código.\nA maneira como a árvore de derivação é construída em relação à entrada define as duas principais estratégias de parsing, cada uma com suas próprias características, pontos fortes e limitações:\n\nAnálise Descendente (Top-Down Parsing): A construção da árvore de derivação começa no topo, a raiz, que é o símbolo inicial da gramática, e avança para baixo, em direção às folhas, a string de tokens de entrada. Este método tenta encontrar a derivação mais à esquerda para a entrada.\n\nAnálise Ascendente (Bottom-Up Parsing): A construção da árvore de derivação começa na base, as folhas, que são a string de tokens de entrada, e avança para cima, em direção à raiz, o símbolo inicial. Este método efetivamente reverte uma derivação mais à direita.\n\nUm erro sintático detectado pelo parser é um erro fatal que interrompe o processo de compilação. Sem uma estrutura sintática válida e inequívoca representada pela Árvore Sintática Abstrata, as fases subsequentes, que dependem dessa estrutura para realizar a verificação de tipos e a geração de código, não podem prosseguir.\n\n5.9.1 Estratégias de Análise Descendente (Top-Down)\nA análise descendente tenta construir uma árvore de derivação para a string de entrada começando pela raiz (símbolo inicial) e criando os vértices da árvore em pré-ordem. Isso equivale a encontrar uma derivação mais à esquerda para a string de entrada.\n\n5.9.1.1 Analisadores de Descida Recursiva (Recursive-Descent)\nUma das implementações mais diretas e intuitivas de um parser descendente é o analisador de descida recursiva. Nesta abordagem, um conjunto de procedimentos mutuamente recursivos é escrito, geralmente um para cada não-terminal na gramática. O procedimento associado a um não-terminal A é responsável por reconhecer na entrada uma sub_string_ que pode ser derivada de A. A simplicidade e a facilidade de implementação manual tornam esta técnica atraente. No entanto, analisadores de descida recursiva ingênuos podem ser ineficientes. Estes analisadores podem exigir retrocesso, backtracking, se a escolha de uma produção se revelar incorreta. Além disso, eles não conseguem lidar com gramáticas que contêm recursão à esquerda, regras da forma \\(A\\rightarrow A \\beta\\). Isso levaria a uma recursão infinita.\n\n\n5.9.1.2 Analisadores Preditivos (\\(LL\\))\nPara superar as desvantagens do retrocesso, foi desenvolvida uma classe de parsers descendentes chamada de analisadores preditivos. Estes são parsers que podem prever qual produção aplicar a um não-terminal olhando para a frente na string de entrada, sem precisar adivinhar e retroceder. O tipo mais comum é o\nparser \\(LL(1)\\). A notação \\(LL(1)\\) significa:\n\nL (primeiro): A entrada é lida da esquerda, Left para a direita;\nL (segundo): O parser constrói uma derivação mais à esquerda, Leftmost;\n(1): Ele usa 1 símbolo de Lookahead (antecipação) para tomar suas decisões.\n\nUm parser \\(LL(1)\\) opera com três componentes: uma pilha de análise, um ponteiro de entrada e uma tabela de análise. A tabela de análise é uma matriz na qual as linhas correspondem aos não terminais e as colunas aos terminais. Cada célula \\(M[A,a]\\) contém a regra de produção que deve ser usada se o não-terminal A estiver no topo da pilha e o terminal a for o próximo símbolo de entrada, o lookahead. O algoritmo é determinístico: para cada par, não-terminal no topo da pilha, símbolo de Lookahead, há no máximo uma ação a ser tomada. Se a célula estiver vazia, um erro sintático é detectado.\nOs parsers \\(LL\\) são ansiosos. No momento em que um não-terminal \\(A\\) está no topo da pilha, eles devem se comprometer imediatamente com uma única regra \\(A\\rightarrow \\beta\\), baseando sua decisão exclusivamente no próximo token de entrada. Essa necessidade de uma decisão precoce e inequívoca é a razão pela qual as gramáticas \\(LL(1)\\) não podem ter ambiguidades, recursão à esquerda ou prefixos comuns, duas produções para o mesmo não-terminal que começam com o mesmo símbolo. Tais características tornam impossível para o parser fazer uma escolha determinística com apenas um símbolo de Lookahead.\n\n\n\n5.9.2 Estratégias de Análise Ascendente (Bottom-Up)\nEm contraste com a abordagem descendente, a análise ascendente constrói a árvore de derivação a partir das folhas, a string de entrada, em direção à raiz, o símbolo inicial da gramática. O processo pode ser visto como uma tentativa de reduzir a string de entrada de volta ao símbolo inicial, essencialmente traçando uma derivação mais à direita ao contrário.\n\n5.9.2.1 A Abordagem Shift-Reduce\nO mecanismo fundamental por trás da maioria dos parsers ascendentes é o shift-reduce (deslocar-reduzir). O parser utiliza uma pilha para armazenar símbolos da gramática e toma uma de quatro ações possíveis em cada passo:\n\nShift (Deslocar): o próximo símbolo de entrada é movido (deslocado) para o topo da pilha;\nReduce (Reduzir): o parser reconhece que uma sequência de símbolos \\(\\beta\\) no topo da pilha corresponde ao lado direito de uma regra de produção \\(A\\rightarrow \\beta\\). Ele então substitui (reduz) \\(\\beta\\) na pilha pelo não-terminal \\(A\\);\nAccept (Aceitar): a análise é concluída com sucesso. Isso ocorre quando a entrada foi totalmente consumida e a pilha contém apenas o símbolo inicial.\nError (Erro): um erro sintático é encontrado, e o parser não pode continuar.\n\nA principal dificuldade em um parser shift-reduce é decidir quando deslocar e quando reduzir (um conflito shift/reduce) ou, ao decidir reduzir, qual regra usar se múltiplas corresponderem (um conflito reduce/reduce).\n\n\n5.9.2.2 A Família de Analisadores \\(LR\\)\nA classe mais poderosa e amplamente utilizada de parsers ascendentes é a família \\(LR\\). Eles são capazes de analisar uma classe de gramáticas significativamente maior do que os parsers \\(LL\\). A notação \\(LR\\) significa:\n\nL: A entrada é lida da esquerda, Left para a direita;\nR: O parser constrói uma derivação mais à direita, Rightmost, ao contrário.\n\nExistem várias variantes de parsers \\(LR\\), que diferem principalmente na forma como suas tabelas de análise são construídas e na quantidade de informação de Lookahead que utilizam para resolver conflitos:\n\nLR(0): o mais simples, não usa Lookahead;\nSLR (Simple LR): usa os conjuntos FOLLOW do não-terminal para decidir sobre as reduções;\nLALR(1) (Look-Ahead LR): uma versão otimizada do \\(LR(1)\\) com tabelas menores, mas poder de reconhecimento ligeiramente reduzido. É a base para ferramentas como YACC e Bison;\nLR(1) Canônico: o mais poderoso da família, mas que gera tabelas de análise muito grandes.\n\nAo contrário dos parsers \\(LL\\) ansiosos, os parsers \\(LR\\) são pacientes. Eles não precisam decidir qual regra de produção usar no momento em que veem o primeiro símbolo de seu lado direito. Em vez disso, eles continuam a deslocar símbolos para a pilha até que o lado direito completo de uma produção, conhecido como handle, esteja no topo da pilha. Somente então eles realizam a redução. Essa capacidade de adiar a decisão até que mais contexto esteja disponível é a fonte de seu maior poder e de sua capacidade de lidar com uma gama mais ampla e natural de gramáticas sem a necessidade de reescritas extensivas.\nO parser LALR(1) (Look-Ahead LR) merece destaque. Esta família de parsers representa uma solução engenhosa para um problema prático fundamental: os parsers LR(1), embora teoricamente mais poderosos, geram tabelas de análise de tamanho proibitivo para gramáticas reais de linguagens de programação. O LALR(1) resolve essa limitação por meio de uma técnica de fusão de estados (state merging). Durante a construção do autômato LR(1), estados que possuem o mesmo core — isto é, a mesma coleção de itens LR(0), mas diferentes conjuntos de lookahead, são consolidados em um único estado. O conjunto de lookahead do estado resultante torna-se a união dos conjuntos originais. Esta fusão reduz drasticamente o número de estados, frequentemente de milhares para centenas, tornando as tabelas de análise praticamente viáveis sem sacrificar significativamente o poder de reconhecimento.\nA diferença fundamental entre LALR(1) e LR(1) manifesta-se tanto em termos de eficiência quanto de capacidade de reconhecimento. Em termos de eficiência, o LR(1) pode gerar tabelas com dezenas de milhares de estados para gramáticas complexas, consumindo megabytes de memória e tornando o parser lento, enquanto o LALR(1) produz tabelas compactas com centenas de estados, adequadas para uso industrial. Em termos de poder de reconhecimento, o LR(1) aceita rigorosamente todas as gramáticas LR(1), enquanto o LALR(1) constitui um subconjunto próprio dessa classe, algumas gramáticas LR(1) podem se tornar ambíguas durante o processo de fusão de estados, gerando conflitos reduce/reduce. Contudo, esta limitação raramente impacta gramáticas práticas de linguagens de programação, razão pela qual ferramentas como YACC e Bison adotaram LALR(1) como padrão, estabelecendo um equilíbrio ótimo entre expressividade teórica e viabilidade implementacional.\n\n\n\n5.9.3 Impacto da Ambiguidade no Processo de Compilação\nA atenta leitora deve compreender que a ambiguidade em uma gramática não é uma mera curiosidade teórica; ela representa uma falha fundamental na especificação de uma linguagem, com consequências diretas e severas em todas as principais fases do processo de compilação. Um compilador é uma ferramenta determinística que deve traduzir um único código-fonte em um único programa executável. A ambiguidade quebra essa premissa, introduzindo incerteza que se propaga desde a análise sintática até a geração do código final.\n\n5.9.3.1 Fase de Análise Sintática\nA análise sintática é a primeira e mais direta vítima da ambiguidade. O objetivo do parser é construir uma única árvore de derivação que represente a estrutura do código-fonte. Quando a gramática é ambígua, essa tarefa torna-se impossível sem regras externas de desambiguação.\n\nGeração de Múltiplas Árvores de Derivação: Para uma sentença ambígua, o analisador sintático é inerentemente não-determinístico. Ele pode construir duas ou mais árvores de derivação distintas, cada uma representando uma interpretação estrutural válida. Sem um critério para escolher entre elas, o parser não pode prosseguir de forma unívoca.\nConflitos em Parsers LR: Em analisadores sintáticos bottom-up, como os da família LR (LR, SLR, LALR), a ambiguidade manifesta-se diretamente como conflitos shift/reduce ou reduce/reduce. Por exemplo, na gramática ambígua de expressões, ao processar id + id * id, o parser chegará a um estado no qual encontrou E + E. Neste ponto, ele não sabe se deve reduzir a expressão pela regra \\(E \\rightarrow E+E\\) (um reduce) ou aguardar o próximo símbolo * para processar a multiplicação primeiro (um shift). Ferramentas geradoras de parsers, como o YACC ou Bison, relatarão esses conflitos como erros fatais durante a geração do analisador.\nIneficiência em Parsers Top-Down: Para analisadores sintáticos top-down, como os de descida recursiva, a ambiguidade pode levar a um backtracking excessivo. O parser pode ser forçado a explorar múltiplos caminhos de derivação, apenas para descobrir que um deles falha mais tarde. Em casos mal projetados, especialmente com recursão à esquerda, isso pode levar a loops infinitos, paralisando a compilação.\n\n\n\n5.9.3.2 Fase de Análise Semântica\nSupondo que a fase sintática tenha conseguido, de alguma forma, produzir uma árvore, talvez escolhendo arbitrariamente uma das opções, os problemas de ambiguidade se transformam em problemas semânticos. A árvore sintática é a base para toda a análise de significado, e diferentes árvores levam a semânticas drasticamente diferentes.\n\nAtribuição de Tipos Inconsistente: A estrutura da árvore de derivação dita como os tipos são verificados e inferidos. Na expressão (id_float + id_int) * id_int, a adição ocorreria primeiro, possivelmente promovendo o id_int a float. Na interpretação id_float + (id_int * id_int), a multiplicação de inteiros ocorreria primeiro. As duas árvores podem resultar em tipos finais diferentes ou até mesmo em erros de tipo distintos.\nResolução de Escopo Ambígua: A ambiguidade estrutural, como a do dangling else, afeta diretamente a determinação do escopo. Uma variável declarada dentro de um bloco if terá seu escopo e tempo de vida definidos pela árvore sintática. Se não está claro a qual if um bloco else pertence, também não estará claro o escopo das construções dentro daquele bloco.\nOrdem de Avaliação Indefinida: A semântica de uma linguagem define a ordem de avaliação de subexpressões. Isso é fundamental quando há efeitos colaterais (por exemplo, f() + g(), no quais f e g modificam uma variável global). Uma gramática ambígua que permite duas árvores de derivação para esta expressão deixa indefinido qual função será chamada primeiro, tornando o resultado da expressão imprevisível.\n\n\n\n5.9.3.3 Fase de Geração de Código\nFinalmente, as inconsistências estruturais e semânticas culminam em um processo de geração de código que é, na melhor das hipóteses, não-determinístico e, na pior, incorreto.\n\nGeração de Código Não-Determinístico: Cada Árvore Sintática Abstrata distinta mapeia para uma sequência diferente de código intermediário ou de máquina. A expressão (a+b)*c gera uma instrução de ADD seguida por uma de MUL. Já a+(b*c) gera uma MUL seguida por uma ADD. Se o compilador escolher uma árvore arbitrariamente, o código gerado será igualmente arbitrário.\nImpossibilidade de Otimização: As otimizações de código dependem de uma análise rigorosa e inequívoca do fluxo de controle e de dados, que é derivada da Árvore Sintática Abstrata. Se o compilador não pode garantir a estrutura correta do programa (por exemplo, a precedência de operadores), ele não pode aplicar otimizações de forma segura. Otimizar uma estrutura que não representa a intenção do programador pode alterar drasticamente a lógica do programa.\nComportamento Imprevisível do Programa: A consequência final é a mais grave. Um programa compilado a partir de uma gramática ambígua pode exibir comportamentos diferentes dependendo da versão do compilador ou até mesmo de fatores aparentemente não relacionados. Isso viola o princípio fundamental de que a compilação deve ser um processo reprodutível e confiável, tornando a linguagem inadequada para qualquer aplicação séria.\n\n\n\n\n5.9.4 Automatizando a Construção: Ferramentas Geradoras de Parsers\nA construção manual de um analisador sintático, especialmente um parser \\(LR\\), é uma tarefa complexa, tediosa e propensa a erros. Para mitigar essa complexidade, foram desenvolvidas ferramentas especializadas conhecidas como geradores de parsers. Essas ferramentas automatizam o processo de criação de um parser a partir de uma especificação de alto nível da gramática da linguagem.\n\n5.9.4.1 YACC e GNU Bison**\nAs ferramentas mais conhecidas e influentes nesta categoria são YACC e seu sucessor, Bison.\n\nYACC (Yet Another Compiler-Compiler): desenvolvido na Bell Labs, YACC é a ferramenta canônica para gerar parsers \\(LALR(1)\\). Ele se tornou um padrão de fato em sistemas Unix.\nGNU Bison: É a implementação do projeto GNU de um gerador de parsers. É amplamente compatível com YACC, mas oferece recursos adicionais, como a geração de parsers \\(GLR\\), Generalized LR para lidar com gramáticas ambíguas e a geração de código em múltiplas linguagens (C, C++, Java).\n\nO fluxo de trabalho com YACC ou Bison pode ser resumido da seguinte forma:\n\nEntrada: o desenvolvedor cria um arquivo de especificação, geralmente com a extensão .y. Este arquivo contém três seções: declarações, a gramática da linguagem escrita em uma notação semelhante à \\(BNF\\), e uma seção de código auxiliar. O desenvolvedor pode associar ações de código (em C ou C++) a cada regra da gramática.\nProcessamento: A ferramenta (YACC ou Bison) lê o arquivo de especificação. Ela analisa a gramática, constrói o autômato \\(LR\\) e a tabela de análise correspondente, e verifica a existência de conflitos (shift/reduce ou reduce/reduce).\nSaída: Se a gramática for adequada (ex.: \\(LALR(1)\\)), a ferramenta gera um arquivo de código fonte (ex.: y.tab.c) que implementa a função do parser (tipicamente chamada yyparse). Esta função implementa o algoritmo shift-reduce dirigido pela tabela gerada. As ações de código fornecidas pelo desenvolvedor são incorporadas à função e são executadas sempre que a regra correspondente é reduzida. Essas ações são tipicamente usadas para construir a Árvore Sintática Abstrata.\n\n\n\n5.9.4.2 Sinergia com Lex/Flex\nYACC/Bison são projetados para lidar com a análise sintática e quase sempre são usados em conjunto com um gerador de analisador léxico, como Lex ou seu sucessor Flex. O Flex recebe uma especificação de padrões de tokens, usando expressões regulares, e gera um scanner (a função yylex). O parser gerado pelo Bison chama yylex para obter o próximo token da entrada, formando um pipeline coeso que transforma o texto fonte em uma Árvore Sintática Abstrata.\nO uso de ferramentas como YACC e Bison representa uma mudança de paradigma fundamental na construção de compiladores: da programação imperativa para a declarativa. Em vez de implementar manualmente o complexo algoritmo de um parser shift-reduce, a abordagem imperativa, o desenvolvedor fornece uma especificação de alto nível da gramática da linguagem, a abordagem declarativa. A ferramenta se encarrega de gerar a implementação de baixo nível. Essa separação de interesses, o quê, a sintaxe da linguagem, do como, o algoritmo de parsing, aumenta drasticamente a produtividade, a robustez e a manutenibilidade do compilador.\nAlém disso, a utilização de geradores de parsers permite que os desenvolvedores se concentrem na lógica da linguagem que estão implementando, em vez de se perderem nos detalhes de implementação do parser. Isso resulta em um desenvolvimento mais rápido e menos propenso a erros, uma vez que a ferramenta cuida de muitos dos aspectos técnicos e repetitivos da construção do parser.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Gramáticas e Linguagens Livres de Contexto</span>"
    ]
  },
  {
    "objectID": "05-parsersLL1.html",
    "href": "05-parsersLL1.html",
    "title": "6  Parsers LL(1): Começando a Análise Sintática",
    "section": "",
    "text": "6.1 Definição e Características dos Parsers LL(1)\nOs parsers preditivos são analisadores sintáticos descendentes (top-down) que utilizam um único símbolo de lookahead (antecipação) para determinar a regra de produção correta a ser aplicada em cada etapa da análise. Eles predizem qual regra usar com base no próximo símbolo da entrada e no não-terminal atualmente sendo analisado. O termo \\(LL(1)\\) significa:\nUm parser \\(LL(1)\\), requer uma gramática \\(LL(1)\\). Nesta gramática não pode existir qualquer ambiguidade na escolha da regra de produção que será aplicada a cada símbolo de lookahead. Além disso, a gramática não pode ter recursão à esquerda, seja esta recursão direta ou indireta. A recursão à esquerda é um desafio considerável. Existem duas formas de recursão à esquerda:\nAlém do perigo do laço de repetição infinito que faz o pobre Turing se revolver no túmulo, a recursão à esquerda impede o desenvolvimento de uma Tabela de Derivação, graças a criação de regras em conflito. Duas ou mais regras, para a mesma combinação de símbolo terminal e símbolo não-terminal em um determinado momento do processo de parser.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parsers LL(1): Começando a Análise Sintática</span>"
    ]
  },
  {
    "objectID": "05-parsersLL1.html#definição-e-características-dos-parsers-ll1",
    "href": "05-parsersLL1.html#definição-e-características-dos-parsers-ll1",
    "title": "6  Parsers LL(1): Começando a Análise Sintática",
    "section": "",
    "text": "L: Left-to-right scan (varredura da esquerda para a direita) da entrada.\nL: Leftmost derivation (derivação mais à esquerda) da gramática.\n1: Um símbolo de lookahead (antecipação) para tomada de decisão.\n\n\n\nRecursão à Esquerda Direta: ocorre quando um símbolo não-terminal pode ser derivado em uma sequência que começa com ele mesmo. Por exemplo, na regra \\(A \\rightarrow Aa \\mid b\\), o símbolo \\(A\\) pode ser substituído no processo de derivação em \\(Aa\\), no qual \\(A\\) aparece novamente no início da regra. E aqui está o laço infinito.\nRecursão à Esquerda Indireta: acontece quando um símbolo não-terminal pode ser derivado em uma sequência que começa com outro símbolo não-terminal, que por sua vez pode ser derivado de volta ao símbolo original. Ilustrando, nas regras \\(A \\rightarrow Ba\\) e \\(B \\rightarrow Ab\\), \\(A\\) deriva para \\(Ba\\), \\(B\\) deriva para \\(Ab\\) e \\(Ab\\) pode derivar novamente para \\(A\\), criando outro laço infinito.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parsers LL(1): Começando a Análise Sintática</span>"
    ]
  },
  {
    "objectID": "05-parsersLL1.html#eliminação-da-recursão-à-esquerda",
    "href": "05-parsersLL1.html#eliminação-da-recursão-à-esquerda",
    "title": "6  Parsers LL(1): Começando a Análise Sintática",
    "section": "6.2 Eliminação da Recursão à Esquerda",
    "text": "6.2 Eliminação da Recursão à Esquerda\nFelizmente, existem técnicas para eliminar a recursão à esquerda em gramáticas livres de contexto. Uma técnica comum e eficaz envolve a introdução de novos símbolos não terminais e a substituição de regras recursivas por regras equivalentes que não apresentem recursão. Em alguns casos, a substituição direta dos símbolos não terminais recursivos por suas respectivas regras pode ser suficiente para eliminar a recursão à esquerda direta. Outra técnica, a fatoração à esquerda, pode ser utilizada para eliminar ambiguidades na gramática, mas não resolve diretamente o problema da recursão.\nExemplo 1: eliminando a Recursão à Esquerda Direta, considere a regra \\(A \\rightarrow Aa \\mid b\\). Esta regra pode ser reescrita como:\n\n\\(A \\rightarrow bA'\\)\n\\(A' \\rightarrow aA' \\mid \\varepsilon\\)\n\nAgora, a gramática não contém mais recursão à esquerda direta. Este é um exemplo simples, adequado a este texto cujo objetivo é o parser em si. A nova regra inclui o não-terminal \\(A'\\), que permite zero ou mais repetições do símbolo \\(a\\). O uso de \\(\\varepsilon\\) (a produção vazia) permite terminar a derivação de \\(A'\\). Será?\nPara verificar a recursão à esquerda indireta, você precisa observar se é possível derivar uma recursão por meio de uma cadeia de substituições:\n\nSubstituindo \\(A\\):\n\n\\(A \\rightarrow bA'\\)\n\nSubstituindo \\(A'\\):\n\n\\(A' \\rightarrow aA'\\)\n\\(A' \\rightarrow \\varepsilon\\)\n\n\nObserve que substituindo \\(A\\) por \\(bA'\\) e depois \\(A'\\) por \\(aA'\\) ou \\(\\varepsilon\\) não leva de volta a \\(A\\). E parece não haver recursão. Contudo, é necessário verificar se foi criada alguma recursão à esquerda indireta, focando em \\(A'\\):\n\nPrimeira substituição:\n\n\\(A \\rightarrow bA'\\)\n\nSubstituindo \\(A'\\) por \\(aA'\\):\n\n\\(bA' \\rightarrow b(aA')\\)\n\\(bA' \\rightarrow baA'\\)\n\nSubstituindo novamente \\(A'\\) por \\(aA'\\):\n\n\\(baA' \\rightarrow baaA'\\)\n\nE assim por diante… Aqui, \\(A'\\) substitui a si próprio com um prefixo \\(a\\), mas isto não cria recursão indireta ao \\(A'\\) inicial de forma a levar a uma cadeia circular que retorne ao símbolo inicial \\(A\\). A gramática transformada não apresenta recursão à esquerda indireta para \\(A'\\).\n\\(A\\): Não tem recursão à esquerda direta nem indireta, porque \\(A \\rightarrow bA'\\) começa com um terminal.\n\\(A'\\): A regra \\(A' \\rightarrow aA' \\mid \\varepsilon\\) apenas permite que \\(A'\\) produza cadeias de \\(a\\) seguidos possivelmente por \\(\\varepsilon\\), sem retornar a um estado anterior que causaria recursão indireta.\n\nPortanto, a transformação feita elimina a recursão à esquerda direta sem introduzir recursão à esquerda indireta. A recursão à esquerda indireta é mais complexa e requer um texto específico para o assunto. Mas, em linhas gerais você terá que refazer a gramática em face dos objetivos originais para eliminar este tipo de recursão.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parsers LL(1): Começando a Análise Sintática</span>"
    ]
  },
  {
    "objectID": "05-parsersLL1.html#elementos-fundamentais-do-parser-ll1",
    "href": "05-parsersLL1.html#elementos-fundamentais-do-parser-ll1",
    "title": "6  Parsers LL(1): Começando a Análise Sintática",
    "section": "6.3 Elementos Fundamentais do Parser LL(1)",
    "text": "6.3 Elementos Fundamentais do Parser LL(1)\nComo a classe de gramáticas para um parser \\(LL(1)\\) é limitada (nem todas as gramáticas livres de contexto são \\(LL(1)\\), é muito comum que seja necessário modificar a sua ideia original de gramática para eliminar ambiguidades e recursões à esquerda. Um parser \\(LL(1)\\), para funcionar, precisa dos seguintes elementos:\n\nTabela de Derivação: o parser \\(LL(1)\\) utiliza uma Tabela de Derivação, ou Tabela de Análise (parsing), que mapeia cada combinação de não-terminal e terminal (ou símbolo de fim de entrada) para a regra de produção que deve ser aplicada. Essa tabela é construída a partir da gramática e dos conjuntos \\(FIRST\\) e \\(FOLLOW\\) e será o mapa que guiará todo o processo de análise sintática.\nPilha e Buffer: O parser mantém uma pilha e lê a entrada da esquerda para a direita, carácter por carácter. A pilha inicialmente contém o símbolo inicial da gramática e o símbolo de fim de entrada, um cifrão: $. A entrada frequentemente é mantida em uma estrutura de dados com funcionalidades de buffer, que pode ser a própria string que está sendo analisada.\nAnálise: Em cada passo:\n\nO parser consulta a Tabela de Derivação usando como índices o não-terminal no topo da pilha e o próximo símbolo da entrada.\nA tabela indica a produção a ser aplicada.\nO não-terminal no topo da pilha é substituído pelos símbolos da produção (empilhados em ordem inversa).\nSe o topo da pilha for um terminal que coincide com o próximo símbolo da entrada, ambos são removidos da pilha e da entrada.\n\nSucesso ou Erro: A análise termina com sucesso quando a pilha e a entrada estão vazias. Caso contrário, ocorre um erro sintático. Erros poderão ocorrer durante o processo sempre que a combinação de símbolos na pilha e no buffer apontarem para uma célula vazia da Tabela de Derivação.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parsers LL(1): Começando a Análise Sintática</span>"
    ]
  },
  {
    "objectID": "05-parsersLL1.html#conjuntos-nullable-first-e-follow",
    "href": "05-parsersLL1.html#conjuntos-nullable-first-e-follow",
    "title": "6  Parsers LL(1): Começando a Análise Sintática",
    "section": "6.4 Conjuntos NULLABLE, FIRST e FOLLOW",
    "text": "6.4 Conjuntos NULLABLE, FIRST e FOLLOW\nNão dá nem para começar a pensar em criar um parser \\(LL(1)\\) se não entender os conjuntos \\(NULLABLE\\), \\(FIRST\\) e \\(FOLLOW\\). Imagine que você está aprendendo um novo idioma. Para formar frases corretas, você precisará entender quais palavras podem vir antes ou depois de outras. Ou corre o risco de falar como o Yoda. Se quiser evitar ser confundido com um velho alienígena, precisa aprender, no mínimo, a ordem das palavras, muito antes de entender a classe gramatical destas mesmas palavras. Como uma criança aprendendo a falar.\nEu forcei um pouco a barra na metáfora, mas na análise sintática de linguagens livres de contexto, os conjuntos \\(NULLABLE\\), \\(FIRST\\) e \\(FOLLOW\\) desempenham um papel importante que quase valida minha metáfora. Estes conjuntos ajudam a decifrar a gramática da linguagem de forma determinística, determinando as regras de produção que serão aplicadas aos símbolos da string de entrada para garantir que ela faça parte da linguagem.\nAntes de detalharmos o \\(FIRST\\) e o \\(FOLLOW\\), precisamos de um conceito fundamental: o conjunto de símbolos NULLABLE. Um símbolo não-terminal é considerado NULLABLE se ele pode derivar a cadeia vazia (\\(\\varepsilon\\)), ou seja, se ele pode efetivamente “desaparecer” em uma derivação. Saber se um símbolo pode ou não desaparecer é essencial para os cálculos seguintes.\nO conjunto \\(FIRST\\) de um símbolo não-terminal será composto dos símbolos terminais que podem aparecer como primeiro símbolo de qualquer sequência de símbolos que seja derivada desse não-terminal, incluindo \\(\\varepsilon\\) se o símbolo for NULLABLE. Em outras palavras, o conjunto \\(FIRST\\) indica quais terminais podem iniciar uma declaração válida (frase) dentro da estrutura gramática definida por um não-terminal. Por exemplo, considere uma gramática para definir expressões aritméticas. O não-terminal EXPR pode derivar diversas sequências de símbolos, como 2 + 3, (4 5), x - y. O conjunto \\(FIRST\\) do não-terminal EXPR* seria, neste caso específico, \\({número, '+', '-', '('}\\), porque esses são os símbolos que podem iniciar qualquer expressão aritmética válida nesta gramática até onde podemos saber com as informações passadas neste parágrafo.\nO conjunto \\(FOLLOW\\), por sua vez, determina o conjunto de símbolos terminais que podem aparecer imediatamente após um não-terminal em alguma derivação da gramática. Ou colocando de outra forma, o conjunto \\(FOLLOW\\) indica quais terminais podem seguir (follow) um não-terminal em uma declaração válida da linguagem.\nDiferentemente do \\(FIRST\\), que se concentra no início de uma derivação, o \\(FOLLOW\\) analisa o contexto no qual um não-terminal aparece. Seu cálculo depende de saber quais símbolos são NULLABLE, se um símbolo \\(y\\) que segue um não-terminal \\(X\\) pode desaparecer, então tudo o que pode seguir a regra na qual \\(X\\) e \\(y\\) aparecem também pode seguir \\(X\\). Por exemplo, considere uma gramática que define declarações de variáveis. O não-terminal DECLARACAO_VAR pode ser seguido por diferentes símbolos, dependendo do contexto. Em uma linguagem como a linguagem C, por exemplo, uma declaração de variável pode terminar com um ponto e vírgula, ser seguida por um operador de atribuição e uma expressão, ou até mesmo ser parte de uma estrutura maior. Neste cenário, o conjunto \\(FOLLOW\\) do não-terminal DECLARACAO_VAR incluiria o ponto e vírgula ‘;’, o sinal de igual ‘=’, e todos os outros símbolos que podem iniciar uma expressão ou um comando que a linguagem permita ocorrer na mesma linha da declaração da variável.\nOs conjuntos \\(NULLABLE\\), \\(FIRST\\) e \\(FOLLOW\\) serão utilizados para construir a Tabela de Derivação \\(LL(1)\\). A forma tecnicamente mais correta seria dizer que estes conjuntos formam a Tabela De Análise \\(LL(1)\\). Entretanto, pobre de mim, prefiro chamar de Tabela de Derivação.\nAs Tabelas de Derivação são tabelas que guiam o processo de análise sintática descendente preditiva no parser \\(LL(1)\\) deterministicamente. Cada célula dessas tabelas corresponde a relação que existe em um par não-terminal, terminal. De forma que o valor da célula apontada por este par indica qual regra de produção deve ser aplicada quando o analisador encontrar este par específico durante a análise preditiva \\(LL(1)\\).\n\n6.4.1 O Conjunto NULLABLE\nAntes de nos aprofundarmos nos conjuntos \\(FIRST\\) e \\(FOLLOW\\), é essencial entender um conceito preliminar: o conjunto dos símbolos \\(NULLABLE\\). Um símbolo não-terminal é considerado \\(NULLABLE\\) se ele pode derivar a cadeia vazia (\\(\\varepsilon\\)) por meio de qualquer sequência de derivações possível.\nImportante: um símbolo pode ser \\(NULLABLE\\) de três formas distintas:\n\nNullable Direto: quando existe uma produção explícita \\(A \\rightarrow \\varepsilon\\);\nNullable Indireto Simples: quando \\(A \\rightarrow B\\) e \\(B\\) é \\(NULLABLE\\);\nNullable Indireto Múltiplo: quando \\(A \\rightarrow B_1 B_2 ... B_n\\) e todos os símbolos \\(B_1, B_2, ..., B_n\\) são \\(NULLABLE\\) (seja direta ou indiretamente).\n\nPor exemplo, considere as produções listadas a seguir:\n\n\\(A \\rightarrow B C\\);\n\\(B \\rightarrow D E\\);\n\\(C \\rightarrow \\varepsilon\\);\n\\(D \\rightarrow \\varepsilon\\);\n\\(E \\rightarrow \\varepsilon\\).\n\nNeste caso, teremos:\n\n\\(C\\), \\(D\\) e \\(E\\) são \\(NULLABLE\\) diretos (regra 1);\n\\(B\\) é \\(NULLABLE\\) porque \\(B \\rightarrow D E\\) e tanto \\(D\\) quanto \\(E\\) são \\(NULLABLE\\) (regra 3);\n\\(A\\) é \\(NULLABLE\\) porque \\(A \\rightarrow B C\\) e tanto \\(B\\) quanto \\(C\\) são \\(NULLABLE\\) (regra 3).\n\nNote que \\(A\\) só se torna \\(NULLABLE\\) após várias iterações: primeiro identificamos \\(C\\), \\(D\\) e \\(E\\), depois \\(B\\), e finalmente \\(A\\). Esta propagação em cascata é fundamental para o algoritmo.\nIdentificar quais símbolos são \\(NULLABLE\\) é um passo preparatório fundamental. Essa informação é usada para determinar:\n\nQuando incluir \\(\\varepsilon\\) no conjunto \\(FIRST\\) de um não-terminal;\nQuando o analisador deve “olhar por meio” de um símbolo em uma produção para calcular o \\(FIRST\\) do que vem a seguir;\nQuando o conjunto \\(FOLLOW\\) de um não-terminal deve herdar símbolos do \\(FOLLOW\\) de outro, porque o símbolo que o sucede pode “desaparecer” (derivar em \\(\\varepsilon\\)).\n\n\n6.4.1.1 Regras de Criação do Conjunto NULLABLE\nPara construir o conjunto de todos os símbolos não-terminais NULLABLE de uma gramática, aplicamos as seguintes regras de forma iterativa até que o conjunto não mude mais:\n\nRegra Base (Nulidade Direta): Se existe uma produção da forma \\(A \\rightarrow ε\\), então \\(A\\) é adicionado ao conjunto NULLABLE.\nRegra de Indução (Nulidade Indireta): Se existe uma produção da forma \\(A \\rightarrow Y₁Y₂...Yₙ\\), e todos os símbolos \\(Y₁\\), \\(Y₂\\), …, \\(Yₙ\\) já foram identificados como NULLABLE, então \\(A\\) também é adicionado ao conjunto.\n\n\n\n6.4.1.2 Exemplo de Criação do Conjunto NULLABLE\nConsidere a seguinte gramática:\n\\[\n\\begin{array}{c}\nS \\rightarrow A B C \\\\\n&A \\rightarrow a | ε \\\\\n&B \\rightarrow C D \\\\\n&C \\rightarrow c | ε \\\\\n&D \\rightarrow A\n\\end{array}\n\\]\nO processo de construção do conjunto NULLABLE seria:\n\nIteração 1: Analisando as regras diretas, encontramos A \\rightarrow ε e C \\rightarrow ε. Conjunto Nullable = { A, C }\nIteração 2: Agora, verificamos as outras regras com base no que já sabemos.\n\nNa regra D \\rightarrow A, como A está no conjunto, D também se torna NULLABLE.\nNa regra B \\rightarrow C D, agora sabemos que tanto C quanto D são NULLABLE. Portanto, B também se torna NULLABLE. Conjunto Nullable = { A, C, D, B }\n\nIteração 3: Revisitamos as regras com o conjunto atualizado.\n\nNa regra S \\rightarrow A B C, agora sabemos que A, B e C são todos NULLABLE. Portanto, S também se torna NULLABLE. Conjunto Nullable = { A, C, D, B, S }\n\nIteração 4: Nenhuma nova adição é feita ao conjunto. O algoritmo termina.\n\nO conjunto final de símbolos NULLABLE para esta gramática é { S, A, B, C, D }.\n\n\n6.4.1.3 Algoritmo para Criação do Conjunto NULLABLE\nO algoritmo abaixo assume que a gramática é representada por um dicionário, no qual cada chave é um não-terminal e o valor é uma lista de produções, sendo cada produção uma lista de símbolos.\nfunção calcular_NULLABLE(gramatica):\n    // gramatica é um dicionário: { 'A': [['Y1', 'Y2'], ['ε']], ... }\n    NULLABLE = conjunto vazio\n    \n    // 1. Regra Base: Adiciona não-terminais que derivam ε diretamente.\n    para cada não-terminal A em gramatica:\n        para cada produção P nas alternativas de A:\n            se P é ['EPSILON']:\n                adicionar A em NULLABLE\n\n    // 2. Regra de Indução: Itera até que nenhuma nova adição seja feita.\n    mudou = verdadeiro\n    enquanto mudou:\n        mudou = falso\n        para cada não-terminal A em gramatica:\n            se A não está em NULLABLE:\n                para cada produção P = [Y1, Y2, ..., Yn] nas alternativas de A:\n                    \n                    // Verifica se todos os símbolos na produção P já são NULLABLE.\n                    todos_simbolos_sao_nullable = verdadeiro\n                    para cada simbolo Yk em P:\n                        se Yk não está em NULLABLE:\n                            todos_simbolos_sao_nullable = falso\n                            quebrar // Interrompe a verificação desta produção\n                    \n                    // Se todos eram NULLABLE, então A também é.\n                    se todos_simbolos_sao_nullable:\n                        adicionar A em NULLABLE\n                        mudou = verdadeiro\n                        quebrar // Já sabemos que A é NULLABLE, podemos passar para o próximo não-terminal.\n    \n    retornar NULLABLE\nSobre este pseudocódigo podemos construir um código em Python. Porém, antes da esforçada leitora continuar de observar que ao transpor os conceitos teóricos para os algoritmos em Python, a representação muda de \\(\\varepsilon\\) para a string ‘EPSILON’. É importante esclarecer que essa não é uma contradição, mas sim uma adaptação necessária para a implementação. O símbolo \\(\\varepsilon\\) é universalmente aceito na teoria de linguagens formais e compiladores para representar a cadeia vazia. Mas, é muito chato usar caracteres especiais em Python, C++ ou qualquer outra linguagem de programação.\nNa prática, em um programa de computador que manipula regras de produção como texto, é necessário ter um marcador explícito para a produção vazia. Usar uma string vazia de fato (’‘) poderia levar a ambiguidades no processamento do código. Portanto, vamos adotar a string ’EPSILON’, para representar de forma inequívoca o conceito teórico de \\(\\varepsilon\\).\n# Código 1: Cálculo do conjunto NULLABLE\n\ndef calcular_NULLABLE(gramatica: dict[str, list[list[str]]]) -&gt; set[str]:\n    \"\"\"\n    Calcula o conjunto de não-terminais que podem derivar a cadeia vazia (ε).\n\n    Args:\n        gramatica: Um dicionário representando a gramática.\n                   Ex: {'S': [['A', 'B']], 'A': [['a'], ['EPSILON']]}\n\n    Returns:\n        Um conjunto contendo os não-terminais que são NULLABLE.\n    \"\"\"\n    NULLABLE = set()\n    \n    # 1. Regra Base: Adiciona não-terminais com produções diretas para 'EPSILON'.\n    for nao_terminal, producoes in gramatica.items():\n        for producao in producoes:\n            if producao == ['EPSILON']:\n                NULLABLE.add(nao_terminal)\n\n    # 2. Regra de Indução: Itera até não haver mais mudanças.\n    mudou = True\n    while mudou:\n        mudou = False\n        for nao_terminal, producoes in gramatica.items():\n            if nao_terminal not in NULLABLE:\n                for producao in producoes:\n                    # Se a produção for vazia ou EPSILON, já foi tratada.\n                    if not producao or producao == ['EPSILON']:\n                        continue\n\n                    # Verifica se todos os símbolos da produção são NULLABLE.\n                    todos_sao_nullable = all(simbolo in NULLABLE for simbolo in producao)\n                    \n                    if todos_sao_nullable:\n                        NULLABLE.add(nao_terminal)\n                        mudou = True\n                        # Otimização: Se A se tornou NULLABLE por uma de suas produções,\n                        # podemos parar de verificar as outras produções de A nesta iteração.\n                        break \n    \n    return NULLABLE\n\n# Exemplo de uso com a estrutura de dados robusta\ngramatica_exemplo = {\n    'S': [['A', 'B', 'C']],\n    'A': [['a'], ['EPSILON']],\n    'B': [['C', 'D']],\n    'C': [['c'], ['EPSILON']],\n    'D': [['A']]\n}\n\nnullable_set = calcular_NULLABLE(gramatica_exemplo)\nprint(f\"Conjunto NULLABLE: {sorted(list(nullable_set))}\")\n# Saída esperada: Conjunto NULLABLE: ['A', 'B', 'C', 'D', 'S']\n\n\n\n6.4.2 O Conjunto FIRST\nO conjunto \\(FIRST\\) de um símbolo não-terminal é o conjunto de todos os terminais que podem aparecer no início de qualquer string derivada desse símbolo, incluindo o símbolo vazio (\\(\\varepsilon\\)) se o não-terminal for NULLABLE. Para os símbolos terminais, o elemento do conjunto \\(FIRST\\) será o próprio símbolo terminal.\n\n6.4.2.1 Regras de Criação do Conjunto FIRST\nPara definir o conjunto \\(FIRST(X)\\) para todos os símbolos \\(X\\) de uma gramática, assumindo que o conjunto \\(NULLABLE\\) já foi previamente calculado, podemos seguir os seguintes passos de forma iterativa:\n\nPara símbolos terminais: o conjunto \\(FIRST\\) é o próprio símbolo terminal. Ou seja, se \\(a\\) é um terminal, então \\(FIRST(a) = \\{a\\}\\).\nPara um símbolo não-terminal \\(X\\): olhe para cada regra de produção \\(X \\rightarrow Y_1 Y_2 ... Y_n\\) e siga as seguintes regras:\n\nAdicione a \\(FIRST(X)\\) todos os símbolos de \\(FIRST(Y_1)\\), exceto \\(\\varepsilon\\).\nSe \\(Y_1\\) é NULLABLE, adicione a \\(FIRST(X)\\) todos os símbolos de \\(FIRST(Y_2)\\), exceto \\(\\varepsilon\\).\nContinue este processo: se todos os símbolos de \\(Y_1\\) até \\(Y_{k-1}\\) são NULLABLE, adicione a \\(FIRST(X)\\) todos os símbolos de \\(FIRST(Y_k)\\), exceto \\(\\varepsilon\\). O processo para no primeiro símbolo \\(Y_k\\) que não for NULLABLE.\n\n\nO símbolo vazio \\(\\varepsilon\\) pertence ao conjunto \\(FIRST(X)\\) se, e somente se, \\(X\\) pertence ao conjunto \\(NULLABLE\\).\nRepita esses passos até que os conjuntos \\(FIRST\\) de todos os símbolos não-terminais não possam ser alterados.\n\n\n6.4.2.2 Exemplo 1: Criação de Conjuntos FIRST\nPara ilustrar a aplicação destas regras e a importância do conjunto \\(NULLABLE\\), considere a gramática definida pelo seguinte conjunto de regras de produção:\n\\[\n\\begin{array}{ll}\n1. & S \\rightarrow A B \\\\\n2. & A \\rightarrow a \\mid \\varepsilon \\\\\n3. & B \\rightarrow b \\\\\n\\end{array}\n\\]\nPrimeiro, identificamos o conjunto \\(NULLABLE\\) para esta gramática. Pela regra \\(A \\rightarrow \\varepsilon\\), concluímos que \\(NULLABLE = \\{A\\}\\).\nCom base nisso, o conjunto de regras de produção permite criar a seguinte tabela:\n\n\n\n\n\n\n\n\nSímbolo\nFIRST\nExplicação\n\n\n\n\nA\n\\(\\{a, \\varepsilon\\}\\)\nDa produção \\(A \\rightarrow a\\), adicionamos ‘a’. Como \\(A\\) é NULLABLE, adicionamos \\(\\varepsilon\\).\n\n\nB\n\\(\\{b\\}\\)\nDa produção \\(B \\rightarrow b\\), o único terminal que pode iniciar a derivação é ‘b’.\n\n\nS\n\\(\\{a, b\\}\\)\nPara a produção \\(S \\rightarrow A B\\):  1. Analisamos \\(A\\) e adicionamos \\(FIRST(A) - \\{\\varepsilon\\}\\), que resulta em \\(\\{a\\}\\).  2. Como \\(A\\) é NULLABLE, continuamos a análise para o próximo símbolo, \\(B\\).  3. Adicionamos \\(FIRST(B)\\), que é \\(\\{b\\}\\). Como \\(B\\) não é NULLABLE, o processo para.\n\n\n\nLogo: \\(FIRST =\\{(S,\\{a, b\\}),(A,\\{a, \\varepsilon\\}),(B,\\{b\\})\\}\\), um conjunto de tuplas.\nAgora que entendemos o algoritmo e sua dependência do conjunto \\(NULLABLE\\), podemos formalizá-lo.\n\n\n6.4.2.3 Algoritmo para calcular o conjunto FIRST\n#### Algoritmo para calcular o conjunto FIRST (Revisado)\n\nfunção calcular_FIRST(gramatica, NULLABLE):\n    // gramatica é um dicionário, NULLABLE é um conjunto\n    FIRST = {}\n    para cada não-terminal N em gramatica:\n        FIRST[N] = conjunto vazio\n    \n    // Adiciona ε ao FIRST de todos os não-terminais que são NULLABLE.\n    para cada N em NULLABLE:\n        adicionar 'EPSILON' em FIRST[N]\n\n    mudou = verdadeiro\n    enquanto mudou:\n        mudou = falso\n        para cada não-terminal A em gramatica:\n            para cada produção P = [Y1, Y2, ..., Yn] nas alternativas de A:\n                para cada simbolo Yk em P:\n                    // Se Yk é um terminal\n                    se Yk não é um não-terminal:\n                        se Yk não está em FIRST[A]:\n                            adicionar Yk em FIRST[A]\n                            mudou = verdadeiro\n                        quebrar // Para a análise desta produção\n                    \n                    // Se Yk é um não-terminal\n                    else:\n                        // Adiciona FIRST[Yk] - {ε} ao FIRST[A]\n                        tamanho_anterior = tamanho(FIRST[A])\n                        FIRST[A] = união(FIRST[A], FIRST[Yk] - {'EPSILON'})\n                        se tamanho(FIRST[A]) &gt; tamanho_anterior:\n                            mudou = verdadeiro\n                        \n                        // Se Yk não é NULLABLE, para a análise desta produção\n                        se Yk não está em NULLABLE:\n                            quebrar\n    retornar FIRST\nCódigo em Python para \\(FIRST\\)\n# Código 2: Cálculo do conjunto FIRST   \ndef calcular_FIRST(gramatica: dict, nao_terminais: set, NULLABLE: set) -&gt; dict:\n    \"\"\"\n    Calcula o conjunto FIRST para todos os não-terminais, usando a estrutura de dicionário.\n    \"\"\"\n    FIRST = {nt: set() for nt in nao_terminais}\n    \n    for nt in NULLABLE:\n        FIRST[nt].add('EPSILON')\n    \n    mudou = True\n    while mudou:\n        mudou = False\n        for nt_head, producoes in gramatica.items():\n            for producao in producoes:\n                for simbolo in producao:\n                    if simbolo == 'EPSILON':\n                        continue\n\n                    # Se o símbolo é um terminal\n                    if simbolo not in nao_terminais:\n                        if simbolo not in FIRST[nt_head]:\n                            FIRST[nt_head].add(simbolo)\n                            mudou = True\n                        break  # interrompe a análise aqui\n                    \n                    # Se o símbolo é um não-terminal\n                    else:\n                        tamanho_anterior = len(FIRST[nt_head])\n                        FIRST[nt_head].update(FIRST[simbolo] - {'EPSILON'})\n                        if len(FIRST[nt_head]) &gt; tamanho_anterior:\n                            mudou = True\n                        \n                        if simbolo not in NULLABLE:\n                            break  # interrompe a análise aqui\n    return FIRST\n\n\n\n6.4.3 O Conjunto FOLLOW\nO conjunto \\(FOLLOW\\) de um símbolo não-terminal é o conjunto de terminais que podem aparecer imediatamente à direita (após, follow) desse não-terminal em alguma forma sentencial derivada, ou o símbolo de fim de entrada ($) se o não-terminal puder aparecer no final de uma forma sentencial.\nPara definir o conjunto \\(FOLLOW(A)\\) para cada não-terminal \\(A\\), assumindo que os conjuntos \\(NULLABLE\\) e \\(FIRST\\) já foram calculados, aplicamos as seguintes regras de forma iterativa até que os conjuntos \\(FOLLOW\\) não mudem mais:\n\nRegra do Símbolo Inicial: Coloque o símbolo de fim de entrada ($$$) no conjunto \\(FOLLOW\\) do símbolo inicial da gramática.\nRegra das Produções: Para cada produção da forma \\(A \\rightarrow \\alpha B \\beta\\) na gramática, na qual \\(B\\) é um não-terminal e \\(\\alpha\\) e \\(\\beta\\) são sequências de símbolos quaisquer:\n\nAdicione todos os símbolos do conjunto \\(FIRST(\\beta)\\) ao conjunto \\(FOLLOW(B)\\), exceto por \\(\\varepsilon\\).\nSe a sequência \\(\\beta\\) for NULLABLE (ou seja, todos os símbolos em \\(\\beta\\) pertencem ao conjunto \\(NULLABLE\\)) ou se \\(\\beta\\) for vazia, então adicione todos os símbolos do conjunto \\(FOLLOW(A)\\) ao conjunto \\(FOLLOW(B)\\).\n\n\nA longa explicação para a primeira regra é que, ao colocar o símbolo de fim de entrada ($) no \\(FOLLOW\\) do símbolo inicial da gramática, garantimos que o analisador sintático reconheça a última derivação da gramática como válida. Isso significa que o analisador estará preparado para encontrar o símbolo (\\() ao final da string de entrada, indicando que a análise foi concluída com sucesso. Em outras palavras, o símbolo (\\)) no \\(FOLLOW\\) do símbolo inicial representa a expectativa de que a string de entrada seja completamente processada e que não existam símbolos após a última derivada.\nExemplo: Para ilustrar todas as regras, incluindo a interação com os conjuntos \\(FIRST\\) e \\(NULLABLE\\), vamos usar uma nova gramática:\n\\[\n\\begin{array}{ll}\n1. & S \\rightarrow A B C \\\\\n2. & A \\rightarrow a \\\\\n3. & B \\rightarrow b \\mid \\varepsilon \\\\\n4. & C \\rightarrow c\n\\end{array}\n\\]\nConjuntos Prévios:\n\n\\(NULLABLE\\): Apenas \\(B\\) é NULLABLE devido à produção \\(B \\rightarrow \\varepsilon\\). Logo, \\(NULLABLE = \\{B\\}\\).\n\\(FIRST\\):\n\n\\(FIRST(A) = \\{a\\}\\)\n\\(FIRST(B) = \\{b, \\varepsilon\\}\\)\n\\(FIRST(C) = \\{c\\}\\)\n\\(FIRST(S) = \\{a\\}\\) (\\(FIRST(A)\\) é \\(\\{a\\}\\) e \\(A\\) não é NULLABLE)\n\n\nCálculo do Conjunto FOLLOW:\nVamos aplicar as regras passo a passo:\n\nInicialização: Pela Regra 1, adicionamos $$$ ao \\(FOLLOW\\) do símbolo inicial, \\(S\\).\n\n\\(FOLLOW(S) = \\{\\)}$\n\nAnálise da Produção \\(S \\rightarrow A B C\\):\n\nPara \\(FOLLOW(A)\\): O não-terminal \\(A\\) é seguido pela sequência \\(\\beta = BC\\).\n\n\nAdicionamos \\(FIRST(BC)\\) a \\(FOLLOW(A)\\). Para encontrar \\(FIRST(BC)\\), olhamos \\(FIRST(B)\\), que é \\(\\{b, \\varepsilon\\}\\). Adicionamos ‘b’. Como \\(B\\) é NULLABLE, olhamos para o próximo símbolo, \\(C\\), e adicionamos \\(FIRST(C)\\), que é \\(\\{c\\}\\).\nPortanto, \\(FIRST(BC) = \\{b, c\\}\\). Adicionamos isso a \\(FOLLOW(A)\\).\n\\(FOLLOW(A) = \\{b, c\\}\\)\n\n\nPara \\(FOLLOW(B)\\): O não-terminal \\(B\\) é seguido pela sequência \\(\\beta = C\\).\n\n\nAdicionamos \\(FIRST(C)\\) a \\(FOLLOW(B)\\). \\(FIRST(C)\\) é \\(\\{c\\}\\).\nA sequência \\(C\\) não é NULLABLE, então não aplicamos a segunda parte da regra.\n\\(FOLLOW(B) = \\{c\\}\\)\n\n\nPara \\(FOLLOW(C)\\): O não-terminal \\(C\\) está no final da produção, então a sequência \\(\\beta\\) é vazia.\n\n\nComo \\(\\beta\\) é vazia, adicionamos \\(FOLLOW(S)\\) a \\(FOLLOW(C)\\).\n\\(FOLLOW(C) = \\{\\)}$\n\n\nO conjunto resultante será:\n\n\n\n\n\n\n\n\nSímbolo\nFOLLOW\nExplicação\n\n\n\n\nS\n${ $ }$\nÉ o símbolo inicial (Regra 1).\n\n\nA\n\\(\\{ b, c \\}\\)\nÉ seguido por \\(BC\\) na regra \\(S \\rightarrow ABC\\). \\(FOLLOW(A)\\) recebe \\(FIRST(BC)\\).\n\n\nB\n\\(\\{ c \\}\\)\nÉ seguido por \\(C\\) na regra \\(S \\rightarrow ABC\\). \\(FOLLOW(B)\\) recebe \\(FIRST(C)\\).\n\n\nC\n${ $ }$\nEstá no final da regra \\(S \\rightarrow ABC\\). \\(FOLLOW(C)\\) herda \\(FOLLOW(S)\\).\n\n\n\n\n\n6.4.4 Algoritmo para calcular o conjunto FOLLOW\nCom os conjuntos NULLABLE e FIRST em mãos, podemos finalmente calcular o conjunto FOLLOW. A seguir estão os pseudocódigos e a implementação final em Python, ambos utilizando a estrutura de dados de dicionário para manter a consistência.\n\n6.4.4.1 Pseudocódigo da Função Auxiliar: calcular_first_da_sequencia\nPara calcular FOLLOW, precisamos de uma função auxiliar que calcule o conjunto FIRST de uma sequência arbitrária de símbolos (ex: FIRST(B C D)), o que é um pouco diferente de calcular o FIRST de um único símbolo.\n### Algoritmo para calcular o conjunto FOLLOW \n\nCom os conjuntos `NULLABLE` e `FIRST` em mãos, podemos finalmente calcular o conjunto `FOLLOW`. A seguir estão os pseudocódigos e a implementação final em Python, ambos utilizando a estrutura de dados de dicionário para manter a consistência.\n\n#### Pseudocódigo da Função Auxiliar: `calcular_first_da_sequencia`\n\nPara calcular `FOLLOW`, precisamos de uma função auxiliar que calcule o conjunto `FIRST` de uma sequência arbitrária de símbolos (ex: `FIRST(B C D)`), o que é um pouco diferente de calcular o `FIRST` de um único símbolo.\n\n```pseudo\nfunção calcular_first_da_sequencia(sequencia, FIRST, NULLABLE, nao_terminais):\n    // sequencia é uma lista de símbolos, ex: ['B', 'C']\n    first_da_sequencia = conjunto vazio\n\n    para cada simbolo S na sequencia:\n        // Se S é um terminal, adiciona-o e para.\n        se S não está em nao_terminais:\n            adicionar S em first_da_sequencia\n            retornar first_da_sequencia // Fim da análise para esta sequência\n\n        // Se S é um não-terminal, adiciona seu FIRST (sem EPSILON)\n        adicionar (FIRST[S] - {'EPSILON'}) em first_da_sequencia\n\n        // Se S não pode ser vazio, a análise da sequência para aqui.\n        se S não está em NULLABLE:\n            retornar first_da_sequencia\n\n    // Se o loop terminou, é porque todos os símbolos da sequência são NULLABLE.\n    // Portanto, a própria sequência pode ser vazia.\n    adicionar 'EPSILON' em first_da_sequencia\n    retornar first_da_sequencia\n\nfunção calcular_FOLLOW(gramatica, simbolo_inicial, nao_terminais, FIRST, NULLABLE):\n    FOLLOW = {}\n    para cada N em nao_terminais:\n        FOLLOW[N] = conjunto vazio\n    \n    // Regra 1: Adiciona $ ao FOLLOW do símbolo inicial.\n    adicionar '$' em FOLLOW[simbolo_inicial]\n\n    mudou = verdadeiro\n    enquanto mudou:\n        mudou = falso\n        // Itera sobre cada produção da gramática\n        para cada não-terminal A e suas produções em gramatica:\n            para cada produção P = [Y1, Y2, ..., Yn]:\n                // Itera sobre cada símbolo da produção\n                para i de 0 até tamanho(P) - 1:\n                    B = P[i]\n                    se B é um não-terminal:\n                        beta = o restante da produção após B, ou seja, P[i+1:]\n                        \n                        // Regra 2.a: Adicionar FIRST(beta) a FOLLOW(B)\n                        se beta não é vazio:\n                            first_beta = calcular_first_da_sequencia(beta, ...)\n                            \n                            tamanho_anterior = tamanho(FOLLOW[B])\n                            adicionar (first_beta - {'EPSILON'}) em FOLLOW[B]\n                            se tamanho(FOLLOW[B]) &gt; tamanho_anterior:\n                                mudou = verdadeiro\n\n                            // Regra 2.b: Se beta pode ser vazio, adicionar FOLLOW(A) a FOLLOW(B)\n                            se 'EPSILON' está em first_beta:\n                                tamanho_anterior = tamanho(FOLLOW[B])\n                                adicionar FOLLOW[A] em FOLLOW[B]\n                                se tamanho(FOLLOW[B]) &gt; tamanho_anterior:\n                                    mudou = verdadeiro\n                        \n                        // Regra 2.b (caso alternativo): Se não há nada após B\n                        senão:\n                            tamanho_anterior = tamanho(FOLLOW[B])\n                            adicionar FOLLOW[A] em FOLLOW[B]\n                            se tamanho(FOLLOW[B]) &gt; tamanho_anterior:\n                                mudou = verdadeiro\n    \n    retornar FOLLOW\nEm Python, teremos:\n# Código 3: Cálculo do conjunto FOLLOW \n\ndef calcular_first_da_sequencia(\n    sequencia: list[str],\n    FIRST: dict[str, set],\n    NULLABLE: set[str],\n    nao_terminais: set[str]\n) -&gt; set[str]:\n    \"\"\"\n    Calcula o conjunto FIRST de uma sequência de símbolos.\n    \"\"\"\n    first_seq = set()\n    \n    for simbolo in sequencia:\n        if simbolo not in nao_terminais:  # É terminal\n            first_seq.add(simbolo)\n            return first_seq\n        else:  # É não-terminal\n            first_seq.update(FIRST[simbolo] - {'EPSILON'})\n            if simbolo not in NULLABLE:\n                return first_seq\n    \n    # Se o loop terminou, todos os símbolos da sequência são NULLABLE.\n    first_seq.add('EPSILON')\n    return first_seq\n\n\ndef calcular_FOLLOW(\n    gramatica: dict[str, list[list[str]]],\n    simbolo_inicial: str,\n    nao_terminais: set[str],\n    FIRST: dict[str, set],\n    NULLABLE: set[str]\n) -&gt; dict[str, set]:\n    \"\"\"\n    Calcula o conjunto FOLLOW para todos os não-terminais, usando a estrutura de dicionário.\n    \"\"\"\n    FOLLOW = {nt: set() for nt in nao_terminais}\n    FOLLOW[simbolo_inicial].add('$')\n    \n    mudou = True\n    while mudou:\n        mudou = False\n        for nt_head, producoes in gramatica.items():\n            for producao in producoes:\n                for i, simbolo in enumerate(producao):\n                    if simbolo in nao_terminais:\n                        beta = producao[i+1:]\n                        \n                        # Regra 2.a: Adicionar FIRST(beta) a FOLLOW(simbolo)\n                        if beta:\n                            first_beta = calcular_first_da_sequencia(beta, FIRST, NULLABLE, nao_terminais)\n                            \n                            tamanho_anterior = len(FOLLOW[simbolo])\n                            FOLLOW[simbolo].update(first_beta - {'EPSILON'})\n                            if len(FOLLOW[simbolo]) &gt; tamanho_anterior:\n                                mudou = True\n                            \n                            # Regra 2.b: Se beta é NULLABLE, adicionar FOLLOW(head) a FOLLOW(simbolo)\n                            if 'EPSILON' in first_beta:\n                                tamanho_anterior = len(FOLLOW[simbolo])\n                                FOLLOW[simbolo].update(FOLLOW[nt_head])\n                                if len(FOLLOW[simbolo]) &gt; tamanho_anterior:\n                                    mudou = True\n                        \n                        # Regra 2.b (caso beta seja vazio): Adicionar FOLLOW(head) a FOLLOW(simbolo)\n                        else:\n                            tamanho_anterior = len(FOLLOW[simbolo])\n                            FOLLOW[simbolo].update(FOLLOW[nt_head])\n                            if len(FOLLOW[simbolo]) &gt; tamanho_anterior:\n                                mudou = True\n    return FOLLOW\n\n\n\n6.4.5 Usando os Conjuntos para Decidir Entre Múltiplas Produções\nAgora que conhecemos os conjuntos \\(NULLABLE\\), \\(FIRST\\) e \\(FOLLOW\\), podemos entender como o parser \\(LL(1)\\) utiliza estas informações para decidir qual regra de produção aplicar quando um não-terminal possui múltiplas alternativas.\nPara um não-terminal \\(A\\) com produções \\(A \\rightarrow \\alpha_1 \\mid \\alpha_2 \\mid ... \\mid \\alpha_n\\), o processo de decisão utiliza o lookahead (próximo símbolo no buffer) da seguinte forma:\n\nExamine o próximo símbolo no buffer (sem consumi-lo);\nEscolha a produção \\(\\alpha_i\\) onde o símbolo pertence a \\(FIRST(\\alpha_i)\\);\nSe alguma produção deriva \\(\\varepsilon\\), use \\(FOLLOW(A)\\) para decidir quando aplicá-la.\n\nEm um parser recursive descent, este mecanismo se traduz em estruturas condicionais:\ndef parseA(self):\n    \"\"\"\n    A -&gt; alpha1 | alpha2 | alpha3\n    \"\"\"\n    token = self.lookahead()  # Consulta sem consumir\n    \n    if token in FIRST(alpha1):\n        # Aplica produção A -&gt; alpha1\n        self.parse_alpha1()\n    elif token in FIRST(alpha2):\n        # Aplica produção A -&gt; alpha2\n        self.parse_alpha2()\n    elif token in FIRST(alpha3):\n        # Aplica produção A -&gt; alpha3\n        self.parse_alpha3()\n    else:\n        self.error(f\"Terminal inesperado: {token}\")\nExemplo de ambiguidade: Considere a gramática:\n\\[\n\\begin{aligned}\nE &\\rightarrow T + E \\mid T \\\\\nT &\\rightarrow \\text{num} \\mid (E)\n\\end{aligned}\n\\]\nPara o não-terminal \\(E\\), temos \\(FIRST(T + E) = \\{\\text{num}, (\\}\\) e \\(FIRST(T) = \\{\\text{num}, (\\}\\). Os conjuntos \\(FIRST\\) são idênticos, criando um conflito. Para resolver, refatoramos a gramática:\n\\[\n\\begin{aligned}\nE &\\rightarrow T E' \\\\\nE' &\\rightarrow + E \\mid \\varepsilon\n\\end{aligned}\n\\]\nAgora a decisão é determinística: se lookahead é +, aplica \\(E' \\rightarrow + E\\); se está em \\(FOLLOW(E')\\), aplica \\(E' \\rightarrow \\varepsilon\\).\nCondição LL(1): Uma gramática é \\(LL(1)\\) se, para cada não-terminal \\(A\\) com produções \\(A \\rightarrow \\alpha_1 \\mid \\alpha_2\\):\n\\[FIRST(\\alpha_1) \\cap FIRST(\\alpha_2) = \\emptyset\\]\nE se \\(\\alpha_i \\Rightarrow^* \\varepsilon\\), então:\n\\[FIRST(\\alpha_j) \\cap FOLLOW(A) = \\emptyset \\quad \\text{para todo } j \\neq i\\]\nSe a gramática não satisfaz estas condições, será necessário refatorá-la antes de construir o parser \\(LL(1)\\).\n\n\n6.4.6 Aplicando as Regras: Um Primeiro Exemplo\nPara solidificar o entendimento, vamos aplicar as regras de construção em uma gramática simples e ideal, antes de partirmos para casos mais complexos.\nExemplo 1: Considere a gramática definida pelo seguinte conjunto de regras de produção:\n\\[\n\\begin{array}{cc}\n1. & S \\rightarrow aB \\mid bA \\\\\n2. & A \\rightarrow c \\mid d \\\\\n3. & B \\rightarrow e \\mid f \\\\\n\\end{array}\n\\]\nA partir deste conjunto de regras, como não há produções vazias, o cálculo dos conjuntos é direto.\n1. Conjunto \\(FIRST\\):\n\n\\(FIRST(S) = \\{a, b\\}\\)\n\\(FIRST(A) = \\{c, d\\}\\)\n\\(FIRST(B) = \\{e, f\\}\\)\n\n2. Conjunto \\(FOLLOW\\):\n\n\\(FOLLOW(S) = \\{\\$\\}\\) (Símbolo inicial)\n\\(FOLLOW(A) = \\{\\$\\}\\) (em \\(S \\rightarrow bA\\), A herda o \\(FOLLOW(S)\\))\n\\(FOLLOW(B) = \\{\\$\\}\\) (em \\(S \\rightarrow aB\\), B herda o \\(FOLLOW(S)\\))\n\n3. Construindo a Tabela: Como nenhuma produção deriva em \\(\\varepsilon\\), apenas a Regra 1 (Regra do FIRST) será utilizada:\n\nPara \\(S \\rightarrow aB\\): Como \\(a\\) está em \\(FIRST(aB)\\), adicionamos \\(S \\rightarrow aB\\) em Tabela[S, a].\nPara \\(S \\rightarrow bA\\): Como \\(b\\) está em \\(FIRST(bA)\\), adicionamos \\(S \\rightarrow bA\\) em Tabela[S, b].\nPara \\(A \\rightarrow c\\): Como \\(c\\) está em \\(FIRST(c)\\), adicionamos \\(A \\rightarrow c\\) em Tabela[A, c].\nPara \\(A \\rightarrow d\\): Como \\(d\\) está em \\(FIRST(d)\\), adicionamos \\(A \\rightarrow d\\) em Tabela[A, d].\nPara \\(B \\rightarrow e\\): Como \\(e\\) está em \\(FIRST(e)\\), adicionamos \\(B \\rightarrow e\\) em Tabela[B, e].\nPara \\(B \\rightarrow f\\): Como \\(f\\) está em \\(FIRST(f)\\), adicionamos \\(B \\rightarrow f\\) em Tabela[B, f].\n\nA Tabela de Derivação resultante é:\n\n\n\n\n\n\n\n\n\n\n\n\n\nNão-Terminal\na\nb\nc\nd\ne\nf\n$\n\n\n\n\nS\n\\(S \\rightarrow aB\\)\n\\(S \\rightarrow bA\\)\n\n\n\n\n\n\n\nA\n\n\n\\(A \\rightarrow c\\)\n\\(A \\rightarrow d\\)\n\n\n\n\n\nB\n\n\n\n\n\\(B \\rightarrow e\\)\n\\(B \\rightarrow f\\)\n\n\n\n\nEste exemplo é perfeito: para cada par (não-terminal, terminal) existe no máximo uma regra de produção. Infelizmente, quando estamos construindo linguagens livres de contexto para problemas reais, este não é o cenário mais comum, como veremos a seguir.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parsers LL(1): Começando a Análise Sintática</span>"
    ]
  },
  {
    "objectID": "05-parsersLL1.html#o-exemplo-mais-comum-de-todos",
    "href": "05-parsersLL1.html#o-exemplo-mais-comum-de-todos",
    "title": "6  Parsers LL(1): Começando a Análise Sintática",
    "section": "6.5 O Exemplo Mais Comum de Todos",
    "text": "6.5 O Exemplo Mais Comum de Todos\nO exemplo a seguir está em todos os sites, livros e aulas que eu já vi disponíveis na internet. É tão comum que não me dei ao trabalho de procurar sua origem. Meu instinto me diz que deve ser do livro do Aho, mas não fui conferir. É um exemplo tão bom que deve ser do Aho. Enfim, vamos ao trabalho:\n\n6.5.1 Gramática Original\nConsidere a gramática representada pelo conjunto de regras de produção a seguir:\n\\[\n\\begin{array}{ll}\n1. & S \\rightarrow E \\\\\n2. & E \\rightarrow E + T \\mid T \\\\\n3. & T \\rightarrow T * F \\mid F \\\\\n4. & F \\rightarrow (E) \\mid id\n\\end{array}\n\\]\nAntes de prosseguir, é fundamental que a atenta leitora note que existe um problema grave com a gramática acima: ela possui recursão à esquerda direta nas regras para os não-terminais \\(E\\) e \\(T\\). Como vimos anteriormente, um parser \\(LL(1)\\) não consegue lidar com esse tipo de regra, entraria em um laço infinito.\nPara que possamos construir uma Tabela de Derivação \\(LL(1)\\), precisamos primeiro eliminar essa recursão. Faremos isso aplicando a técnica padrão de transformação.\n1. Eliminando a recursão em \\(E \\rightarrow E + T \\mid T\\):\n\nA regra está no formato \\(A \\rightarrow A\\alpha \\mid \\beta\\), na qual \\(A=E\\), \\(\\alpha = +T\\) e \\(\\beta = T\\).\nA transformamos em \\(A \\rightarrow \\beta A'\\) e \\(A' \\rightarrow \\alpha A' \\mid \\varepsilon\\).\nAs novas regras são:\n\n\\(E \\rightarrow T E'\\)\n\\(E' \\rightarrow + T E' \\mid \\varepsilon\\)\n\n\n2. Eliminando a recursão em \\(T \\rightarrow T * F \\mid F\\):\n\nA regra está no formato \\(A \\rightarrow A\\alpha \\mid \\beta\\), na qual \\(A=T\\), \\(\\alpha = *F\\) e \\(\\beta = F\\).\nA transformamos em \\(A \\rightarrow \\beta A'\\) e \\(A' \\rightarrow \\alpha A' \\mid \\varepsilon\\).\nAs novas regras são:\n\n\\(T \\rightarrow F T'\\)\n\\(T' \\rightarrow * F T' \\mid \\varepsilon\\)\n\n\nApós as transformações, a gramática equivalente e adequada para a análise \\(LL(1)\\) é:\n\\[\n\\begin{array}{ll}\n1. & S \\rightarrow E \\\\\n2. & E \\rightarrow T E' \\\\\n3. & E' \\rightarrow + T E' \\mid \\varepsilon \\\\\n4. & T \\rightarrow F T' \\\\\n5. & T' \\rightarrow * F T' \\mid \\varepsilon \\\\\n6. & F \\rightarrow (E) \\mid id\n\\end{array}\n\\]\nCom esta gramática correta, podemos agora calcular os conjuntos necessários.\n\n\n6.5.2 \\(NULLABLE\\)\nAnalisando a nova gramática, identificamos quais não-terminais podem derivar a cadeia vazia (\\(\\varepsilon\\)):\n\n\\(E' \\rightarrow \\varepsilon\\), portanto \\(E'\\) é NULLABLE;\n\\(T' \\rightarrow \\varepsilon\\), portanto \\(T'\\) é NULLABLE;\n\\(S, E, T, F\\) não podem derivar \\(\\varepsilon\\) diretamente e suas produções sempre contêm símbolos que não são NULLABLE, então eles não são NULLABLE.\n\n\n\n6.5.3 \\(FIRST\\)\nVamos calcular o conjunto \\(FIRST\\) para cada não-terminal da gramática corrigida.\n\n\\(FIRST(F)\\): A partir de \\(F \\rightarrow (E) \\mid id\\), os primeiros terminais possíveis são ( e id.\n\n\\(FIRST(F) = \\{ '(', id \\}\\)\n\n\\(FIRST(T')\\): A partir de \\(T' \\rightarrow * F T' \\mid \\varepsilon\\), os primeiros símbolos são * ou a cadeia vazia.\n\n\\(FIRST(T') = \\{ '*', \\varepsilon \\}\\)\n\n\\(FIRST(T)\\): A regra é \\(T \\rightarrow F T'\\). O \\(FIRST(T)\\) é igual ao \\(FIRST(F)\\).\n\n\\(FIRST(T) = FIRST(F) = \\{ '(', id \\}\\)\n\n\\(FIRST(E')\\): A partir de \\(E' \\rightarrow + T E' \\mid \\varepsilon\\), os primeiros símbolos são + ou a cadeia vazia.\n\n\\(FIRST(E') = \\{ '+', \\varepsilon \\}\\)\n\n\\(FIRST(E)\\): A regra é \\(E \\rightarrow T E'\\). O \\(FIRST(E)\\) é igual ao \\(FIRST(T)\\).\n\n\\(FIRST(E) = FIRST(T) = \\{ '(', id \\}\\)\n\n\\(FIRST(S)\\): A regra é \\(S \\rightarrow E\\). O \\(FIRST(S)\\) é igual ao \\(FIRST(E)\\).\n\n\\(FIRST(S) = FIRST(E) = \\{ '(', id \\}\\)\n\n\n\n\n6.5.4 \\(FOLLOW\\)\nAgora, calculamos o conjunto \\(FOLLOW\\) para cada não-terminal.\n\n\\(FOLLOW(S)\\): \\(S\\) é o símbolo inicial, então iniciamos com o marcador de fim de entrada.\n\n$FOLLOW(S) = { $ }$\n\n\\(FOLLOW(E)\\):\n\nDa regra \\(S \\rightarrow E\\), \\(E\\) está no final, então \\(FOLLOW(E)\\) herda \\(FOLLOW(S)\\). $FOLLOW(E) = { $ }$.\nDa regra \\(F \\rightarrow (E)\\), o símbolo ) segue \\(E\\). Adicionamos ) ao \\(FOLLOW(E)\\).\n$FOLLOW(E) = { \\(, ')' \\}\\)\n\n\\(FOLLOW(E')\\): Da regra \\(E \\rightarrow T E'\\), \\(E'\\) está no final, então \\(FOLLOW(E')\\) herda \\(FOLLOW(E)\\).\n\n$FOLLOW(E’) = FOLLOW(E) = { \\(, ')' \\}\\)\n\n\\(FOLLOW(T)\\):\n\nDa regra \\(E \\rightarrow T E'\\), \\(T\\) é seguido por \\(E'\\). Adicionamos \\(FIRST(E') - \\{\\varepsilon\\}\\) ao \\(FOLLOW(T)\\). Isso adiciona +.\nComo \\(E'\\) é NULLABLE, também adicionamos \\(FOLLOW(E)\\) ao \\(FOLLOW(T)\\).\n$FOLLOW(T) = { ‘+’ } = { ‘+’, \\(, ')' \\}\\)\n\n\\(FOLLOW(T')\\): Da regra \\(T \\rightarrow F T'\\), \\(T'\\) está no final, então \\(FOLLOW(T')\\) herda \\(FOLLOW(T)\\).\n\n$FOLLOW(T’) = FOLLOW(T) = { ‘+’, \\(, ')' \\}\\)\n\n\n\\(FOLLOW(F)\\):\n\nAnalisamos todas as regras em que \\(F\\) aparece no lado direito. A única regra relevante é \\(T \\rightarrow F T'\\).\nPasso 1: O não-terminal \\(F\\) é seguido pela sequência \\(T'\\). Aplicamos a regra que diz para adicionar \\(FIRST(T') - \\{\\varepsilon\\}\\) ao conjunto \\(FOLLOW(F)\\).\n\nSabemos que \\(FIRST(T') = \\{ '*', \\varepsilon \\}\\).\nPortanto, adicionamos \\(\\{ * \\}\\) ao \\(FOLLOW(F)\\).\n\nPasso 2: Como a sequência que segue \\(F\\) (neste caso, \\(T'\\)) é NULLABLE, devemos também adicionar o conjunto \\(FOLLOW\\) do não-terminal que está à esquerda da produção, ou seja, \\(FOLLOW(T)\\), ao conjunto \\(FOLLOW(F)\\).\n\nSabemos que \\(FOLLOW(T) = \\{ '+', \\$, ')' \\}\\).\n\nPasso 3: O conjunto final para \\(FOLLOW(F)\\) é a união dos resultados dos passos anteriores.\n\n\\(FOLLOW(F) = (FIRST(T') - \\{\\varepsilon\\}) \\cup FOLLOW(T)\\)\n\\(FOLLOW(F) = \\{ * \\} \\cup \\{ '+', \\$, ')' \\}\\)\n\\(FOLLOW(F) = \\{ '*', '+', \\$, ')' \\}\\)\n\n\n\n\n\n\n6.5.5 Tabela de Derivação \\(LL(1)\\)\nCom os conjuntos \\(FIRST\\) e \\(FOLLOW\\) corretos, podemos construir a Tabela de Derivação. A construção da tabela segue um algoritmo preciso com duas regras principais, que utilizam os conjuntos que acabamos de calcular para preencher as células Tabela[Não-Terminal, Terminal]:\n\nRegra do \\(FIRST\\): Para cada produção da gramática, na forma \\(A \\rightarrow \\alpha\\): para cada símbolo terminal t que pertence a \\(FIRST(\\alpha)\\), adicione a produção \\(A \\rightarrow \\alpha\\) na célula Tabela[A, t].\nRegra do \\(FOLLOW\\): Se \\(\\varepsilon\\) (a cadeia vazia) pertence a \\(FIRST(\\alpha)\\): para cada símbolo terminal t (incluindo o marcador $) que pertence a \\(FOLLOW(A)\\), adicione a produção \\(A \\rightarrow \\alpha\\) na célula Tabela[A, t].\n\nQualquer célula que permaneça vazia após a aplicação destas regras representará um erro sintático, indicando que a ocorrência daquele terminal é inesperada naquele ponto da análise. Assim, temos:\n\n\n\n\n\n\n\n\n\n\n\n\nNão-Terminal\nid\n+\n*\n(\n)\n$\n\n\n\n\nS\n\\(S \\rightarrow E\\)\n\n\n\\(S \\rightarrow E\\)\n\n\n\n\nE\n\\(E \\rightarrow T E'\\)\n\n\n\\(E \\rightarrow T E'\\)\n\n\n\n\nE’\n\n\\(E' \\rightarrow + T E'\\)\n\n\n\\(E' \\rightarrow \\varepsilon\\)\n\\(E' \\rightarrow \\varepsilon\\)\n\n\nT\n\\(T \\rightarrow F T'\\)\n\n\n\\(T \\rightarrow F T'\\)\n\n\n\n\nT’\n\n\\(T' \\rightarrow \\varepsilon\\)\n\\(T' \\rightarrow * F T'\\)\n\n\\(T' \\rightarrow \\varepsilon\\)\n\\(T' \\rightarrow \\varepsilon\\)\n\n\nF\n\\(F \\rightarrow id\\)\n\n\n\\(F \\rightarrow (E)\\)\n\n\n\n\n\n\n\n6.5.6 Processo de Parser Testando com \\(id + id * id\\)\nVamos agora rastrear a análise da string id + id * id usando a pilha, a entrada e a Tabela de Derivação corrigida em relação ao apontamento feito. A inconsistência estava no passo T(T', +), no qual a tabela indica T' -&gt; ε (o que é o correto, + pertence ao FOLLOW(T')), e o rastreamento anterior poderia gerar dúvida. O rastreamento correto é o seguinte.\nNotação utilizada no rastreamento:\n\nT(X, a) significa “consultar a Tabela de Derivação na linha do não-terminal X e coluna do terminal a”;\nPor exemplo, T(S, id) significa consultar a tabela na linha S, coluna id;\nO resultado da consulta indica qual produção deve ser aplicada;\n“Consumir” significa remover o símbolo tanto do topo da pilha quanto do início da entrada.\n\n\n\n\n\n\n\n\n\nPilha\nEntrada\nAção\n\n\n\n\nS $\nid + id * id $\nT(S, id) \\(\\rightarrow\\) S -&gt; E\n\n\nE $\nid + id * id $\nT(E, id) \\(\\rightarrow\\) E -&gt; T E'\n\n\nT E' $\nid + id * id $\nT(T, id) \\(\\rightarrow\\) T -&gt; F T'\n\n\nF T' E' $\nid + id * id $\nT(F, id) \\(\\rightarrow\\) F -&gt; id\n\n\nid T' E' $\nid + id * id $\nConsumir id\n\n\nT' E' $\n+ id * id $\nT(T', +) \\(\\rightarrow\\) T' -&gt; ε\n\n\nE' $\n+ id * id $\nT(E', +) \\(\\rightarrow\\) E' -&gt; + T E'\n\n\n+ T E' $\n+ id * id $\nConsumir +\n\n\nT E' $\nid * id $\nT(T, id) \\(\\rightarrow\\) T -&gt; F T'\n\n\nF T' E' $\nid * id $\nT(F, id) \\(\\rightarrow\\) F -&gt; id\n\n\nid T' E' $\nid * id $\nConsumir id\n\n\nT' E' $\n* id $\nT(T', *) \\(\\rightarrow\\) T' -&gt; * F T'\n\n\n* F T' E' $\n* id $\nConsumir *\n\n\nF T' E' $\nid $\nT(F, id) \\(\\rightarrow\\) F -&gt; id\n\n\nid T' E' $\nid $\nConsumir id\n\n\nT' E' $\n$\nT(T', $) \\(\\rightarrow\\) T' -&gt; ε\n\n\nE' $\n$\nT(E', $) \\(\\rightarrow\\) E' -&gt; ε\n\n\n$\n$\nAceita\n\n\n\nComo foi possível consumir toda a string e esvaziar a pilha, a string id + id * id é aceita pela gramática.\n\n\n6.5.7 Testando com a string \\(id - id * id\\)\nVamos analisar a string id - id * id. O processo inicial é o mesmo do exemplo anterior.\n\n\n\n\n\n\n\n\nPilha\nEntrada\nAção\n\n\n\n\nS $\nid - id * id $\nT(S, id) \\(\\rightarrow\\) S \\rightarrow E\n\n\nE $\nid - id * id $\nT(E, id) \\(\\rightarrow\\) E \\rightarrow T E'\n\n\nT E' $\nid - id * id $\nT(T, id) \\(\\rightarrow\\) T \\rightarrow F T'\n\n\nF T' E' $\nid - id * id $\nT(F, id) \\(\\rightarrow\\) F \\rightarrow id\n\n\nid T' E' $\nid - id * id $\nConsumir id\n\n\nT' E' $\n- id * id $\nT(T', -) \\(\\rightarrow\\) Erro\n\n\n\nA análise para. A célula da tabela para o não-terminal no topo da pilha (T') e o símbolo de entrada (-) está vazia. Isso indica um erro sintático.\nPortanto, a string id - id * id não faz parte da linguagem definida pela gramática fornecida. O que deveria ser óbvio, já que a gramática não tem regras para lidar com o símbolo -.\nVimos, com a mesma gramática, as duas situações possíveis: ou a string faz parte da linguagem, ou não. Simples assim.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parsers LL(1): Começando a Análise Sintática</span>"
    ]
  },
  {
    "objectID": "05-parsersLL1.html#um-exemplo-nem-tão-comum",
    "href": "05-parsersLL1.html#um-exemplo-nem-tão-comum",
    "title": "6  Parsers LL(1): Começando a Análise Sintática",
    "section": "6.6 Um Exemplo Nem Tão Comum",
    "text": "6.6 Um Exemplo Nem Tão Comum\nEste exemplo saiu das vozes da minha cabeça. Pode ser que exista em algum outro lugar, mas não me dei ao trabalho de verificar. É um exemplo que uso há muitos anos, em aula e já nem me preocupo com ele. Se souber a origem, fico grato.\n\n6.6.1 Gramática Original\nConsidere a seguinte gramática para expressões booleanas (\\(OR, AND, NOT\\)), definida pelo conjunto de regras de produção a seguir:\n\\[\n\\begin{array}{ll}\n1. & S \\rightarrow B \\\\\n2. & B \\rightarrow B \\,\\, OR \\,\\, M \\mid M \\\\\n3. & M \\rightarrow M \\,\\, AND \\,\\, N \\mid N \\\\\n4. & N \\rightarrow NOT \\,\\, N \\mid (B) \\,\\mid id\n\\end{array}\n\\]\n\n\n6.6.2 Corrigindo a Recursão à Esquerda\nAssim como no exemplo anterior, esta gramática não é adequada para um parser \\(LL(1)\\) por possuir recursão à esquerda direta nas regras dos não-terminais \\(B\\) e \\(M\\). Para prosseguir, precisamos primeiro eliminar essa recursão.\n1. Eliminando a recursão em \\(B \\rightarrow B \\,\\, OR \\,\\, M \\mid M\\):\n\nA regra tem o formato \\(A \\rightarrow A\\alpha \\mid \\beta\\), na qual \\(A=B\\), \\(\\alpha = OR \\,\\, M\\) e \\(\\beta = M\\).\nA transformamos nas novas regras:\n\n\\(B \\rightarrow M B'\\);\n\\(B' \\rightarrow OR \\,\\, M B' \\mid \\varepsilon\\).\n\n\n2. Eliminando a recursão em \\(M \\rightarrow M \\,\\, AND \\,\\, N \\mid N\\):\n\nA regra tem o formato \\(A \\rightarrow A\\alpha \\mid \\beta\\), na qual \\(A=M\\), \\(\\alpha = AND \\,\\, N\\) e \\(\\beta = N\\).\nA transformamos nas novas regras:\n\n\\(M \\rightarrow N M'\\);\n\\(M' \\rightarrow AND \\,\\, N M' \\mid \\varepsilon\\).\n\n\n\n\n6.6.3 Gramática Corrigida (Pronta para LL(1))\nA gramática final, equivalente à original mas sem recursão à esquerda, é:\n\\[\n\\begin{array}{ll}\n1. & S \\rightarrow B \\\\\n2. & B \\rightarrow M B' \\\\\n3. & B' \\rightarrow OR \\,\\, M B' \\mid \\varepsilon \\\\\n4. & M \\rightarrow N M' \\\\\n5. & M' \\rightarrow AND \\,\\, N M' \\mid \\varepsilon \\\\\n6. & N \\rightarrow NOT \\,\\, N \\\\\n7. & N \\rightarrow (B) \\\\\n8. & N \\rightarrow id\n\\end{array}\n\\]\nCom esta gramática corrigida, podemos prosseguir com os cálculos.\n\n\n6.6.4 \\(NULLABLE\\)\n\n\\(B' \\rightarrow \\varepsilon\\), portanto \\(B'\\) é NULLABLE.\n\\(M' \\rightarrow \\varepsilon\\), portanto \\(M'\\) é NULLABLE.\nNenhum outro não-terminal pode derivar a cadeia vazia.\n\n\n\n6.6.5 \\(FIRST\\)\n\n\\(FIRST(N)\\): A partir das regras de \\(N\\), os primeiros terminais são NOT, ( e id.\n\n\\(FIRST(N) = \\{ NOT, '(', id \\}\\)\n\n\\(FIRST(M')\\): A partir de \\(M' \\rightarrow AND \\,\\, N M' \\mid \\varepsilon\\).\n\n\\(FIRST(M') = \\{ AND, \\varepsilon \\}\\)\n\n\\(FIRST(M)\\): A regra é \\(M \\rightarrow N M'\\). \\(FIRST(M) = FIRST(N)\\).\n\n\\(FIRST(M) = \\{ NOT, '(', id \\}\\)\n\n\\(FIRST(B')\\): A partir de \\(B' \\rightarrow OR \\,\\, M B' \\mid \\varepsilon\\).\n\n\\(FIRST(B') = \\{ OR, \\varepsilon \\}\\)\n\n\\(FIRST(B)\\): A regra é \\(B \\rightarrow M B'\\). \\(FIRST(B) = FIRST(M)\\).\n\n\\(FIRST(B) = \\{ NOT, '(', id \\}\\)\n\n\\(FIRST(S)\\): A regra é \\(S \\rightarrow B\\). \\(FIRST(S) = FIRST(B)\\).\n\n\\(FIRST(S) = \\{ NOT, '(', id \\}\\)\n\n\n\n\n6.6.6 \\(FOLLOW\\)\n\n\\(FOLLOW(S)\\): Símbolo inicial.\n\n$FOLLOW(S) = { $ }$\n\n\\(FOLLOW(B)\\):\n\nDa regra \\(S \\rightarrow B\\), \\(FOLLOW(B)\\) herda \\(FOLLOW(S)\\), então $FOLLOW(B) = { $ }$.\nDa regra \\(N \\rightarrow (B)\\), o símbolo ) segue \\(B\\).\n$FOLLOW(B) = { \\(, ')' \\}\\)\n\n\\(FOLLOW(B')\\): Da regra \\(B \\rightarrow M B'\\), \\(B'\\) está no final, então \\(FOLLOW(B')\\) herda \\(FOLLOW(B)\\).\n\n$FOLLOW(B’) = { \\(, ')' \\}\\)\n\n\\(FOLLOW(M)\\):\n\nDa regra \\(B \\rightarrow M B'\\), \\(M\\) é seguido por \\(B'\\). Adicionamos \\(FIRST(B') - \\{\\varepsilon\\}\\) ao \\(FOLLOW(M)\\), que é \\(\\{ OR \\}\\).\nComo \\(B'\\) é NULLABLE, também adicionamos \\(FOLLOW(B)\\) ao \\(FOLLOW(M)\\).\n$FOLLOW(M) = { OR } = { OR, \\(, ')' \\}\\)\n\n\\(FOLLOW(M')\\): Da regra \\(M \\rightarrow N M'\\), \\(M'\\) está no final, então \\(FOLLOW(M')\\) herda \\(FOLLOW(M)\\).\n\n$FOLLOW(M’) = { OR, \\(, ')' \\}\\)\n\n\\(FOLLOW(N)\\):\n\nDa regra \\(M \\rightarrow N M'\\), \\(N\\) é seguido por \\(M'\\). Adicionamos \\(FIRST(M') - \\{\\varepsilon\\}\\) ao \\(FOLLOW(N)\\), que é \\(\\{ AND \\}\\).\nComo \\(M'\\) é NULLABLE, também adicionamos \\(FOLLOW(M)\\) ao \\(FOLLOW(N)\\).\n$FOLLOW(N) = { AND } = { AND, OR, \\(, ')' \\}\\)\n\n\n\n\n6.6.7 Tabela de Derivação \\(LL(1)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNão-Terminal\nid\nOR\nAND\nNOT\n(\n)\n$\n\n\n\n\nS\n\\(S \\rightarrow B\\)\n\n\n\\(S \\rightarrow B\\)\n\\(S \\rightarrow B\\)\n\n\n\n\nB\n\\(B \\rightarrow M B'\\)\n\n\n\\(B \\rightarrow M B'\\)\n\\(B \\rightarrow M B'\\)\n\n\n\n\nB’\n\n\\(B' \\rightarrow OR \\,\\, M B'\\)\n\n\n\n\\(B' \\rightarrow \\varepsilon\\)\n\\(B' \\rightarrow \\varepsilon\\)\n\n\nM\n\\(M \\rightarrow N M'\\)\n\n\n\\(M \\rightarrow N M'\\)\n\\(M \\rightarrow N M'\\)\n\n\n\n\nM’\n\n\\(M' \\rightarrow \\varepsilon\\)\n\\(M' \\rightarrow AND \\,\\, N M'\\)\n\n\n\\(M' \\rightarrow \\varepsilon\\)\n\\(M' \\rightarrow \\varepsilon\\)\n\n\nN\n\\(N \\rightarrow id\\)\n\n\n\\(N \\rightarrow NOT \\,\\, N\\)\n\\(N \\rightarrow (B)\\)\n\n\n\n\n\n\n\n6.6.8 Testando com id or not id and (id or id)\nVamos rastrear a análise da string usando a Tabela de Derivação correta.\n\n\n6.6.9 Testando com id or not id and (id or id)\n\n\n\n\n\n\n\n\nPilha\nBuffer\nAção\n\n\n\n\nS $\nid or not id and (id or id) $\nT(S, 'id') \\(\\rightarrow\\) $S \\rightarrow B$\n\n\nB $\nid or not id and (id or id) $\nT(B, 'id') \\(\\rightarrow\\) $B \\rightarrow M B'$\n\n\nM B' $\nid or not id and (id or id) $\nT(M, 'id') \\(\\rightarrow\\) $M \\rightarrow N M'$\n\n\nN M' B' $\nid or not id and (id or id) $\nT(N, 'id') \\(\\rightarrow\\) $N \\rightarrow \\text{id}$\n\n\nid M' B' $\nid or not id and (id or id) $\nConsumir id\n\n\nM' B' $\nor not id and (id or id) $\nT(M', 'or') \\(\\rightarrow\\) $M' \\rightarrow \\varepsilon$\n\n\nB' $\nor not id and (id or id) $\nT(B', 'or') \\(\\rightarrow\\) $B' \\rightarrow \\text{or } M B'$\n\n\nor M B' $\nor not id and (id or id) $\nConsumir or\n\n\nM B' $\nnot id and (id or id) $\nT(M, 'not') \\(\\rightarrow\\) $M \\rightarrow N M'$\n\n\nN M' B' $\nnot id and (id or id) $\nT(N, 'not') \\(\\rightarrow\\) $N \\rightarrow \\text{not } N$\n\n\nnot N M' B' $\nnot id and (id or id) $\nConsumir not\n\n\nN M' B' $\nid and (id or id) $\nT(N, 'id') \\(\\rightarrow\\) $N \\rightarrow \\text{id}$\n\n\nid M' B' $\nid and (id or id) $\nConsumir id\n\n\nM' B' $\nand (id or id) $\nT(M', 'and') \\(\\rightarrow\\) $M' \\rightarrow \\text{and } N M'$\n\n\nand N M' B' $\nand (id or id) $\nConsumir and\n\n\nN M' B' $\n(id or id) $\nT(N, '(') \\(\\rightarrow\\) $N \\rightarrow (B)$\n\n\n( B ) M' B' $\n(id or id) $\nConsumir (\n\n\nB ) M' B' $\nid or id) $\nT(B, 'id') \\(\\rightarrow\\) $B \\rightarrow M B'$\n\n\nM B' ) M' B' $\nid or id) $\nT(M, 'id') \\(\\rightarrow\\) $M \\rightarrow N M'$\n\n\nN M' B' ) M' B' $\nid or id) $\nT(N, 'id') \\(\\rightarrow\\) $N \\rightarrow \\text{id}$\n\n\nid M' B' ) M' B' $\nid or id) $\nConsumir id\n\n\nM' B' ) M' B' $\nor id) $\nT(M', 'or') \\(\\rightarrow\\) $M' \\rightarrow \\varepsilon$\n\n\nB' ) M' B' $\nor id) $\nT(B', 'or') \\(\\rightarrow\\) $B' \\rightarrow \\text{or } M B'$\n\n\nor M B' ) M' B' $\nor id) $\nConsumir or\n\n\nM B' ) M' B' $\nid) $\nT(M, 'id') \\(\\rightarrow\\) $M \\rightarrow N M'$\n\n\nN M' B' ) M' B' $\nid) $\nT(N, 'id') \\(\\rightarrow\\) $N \\rightarrow \\text{id}$\n\n\nid M' B' ) M' B' $\nid) $\nConsumir id\n\n\nM' B' ) M' B' $\n) $\nT(M', ')') \\(\\rightarrow\\) $M' \\rightarrow \\varepsilon$\n\n\nB' ) M' B' $\n) $\nT(B', ')') \\(\\rightarrow\\) $B' \\rightarrow \\varepsilon$\n\n\n) M' B' $\n) $\nConsumir )\n\n\nM' B' $\n$\nT(M', '$') \\(\\rightarrow\\) $M' \\rightarrow \\varepsilon$\n\n\nB' $\n$\nT(B', '$') \\(\\rightarrow\\) $B' \\rightarrow \\varepsilon$\n\n\n$\n$\nAceita\n\n\n\nComo foi possível consumir toda a string de entrada id or not id and (id or id) e esvaziar a pilha sem encontrar nenhum erro, a string id or not id and (id or id) é de fato parte da linguagem definida e pode ser identificada por um parser \\(LL(1)\\).",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parsers LL(1): Começando a Análise Sintática</span>"
    ]
  },
  {
    "objectID": "05-parsersLL1.html#conflitos-na-tabela-de-derivação",
    "href": "05-parsersLL1.html#conflitos-na-tabela-de-derivação",
    "title": "6  Parsers LL(1): Começando a Análise Sintática",
    "section": "6.7 Conflitos na Tabela de Derivação",
    "text": "6.7 Conflitos na Tabela de Derivação\nConflitos na Tabela de Derivação \\(LL(1)\\) ocorrem quando há mais de uma regra de produção associada a um mesmo par indicador (terminal, não-terminal). Esta ambiguidade significa que, ao encontrar esse par, o parser \\(LL(1)\\) não conseguirá determinar, de forma única e inequívoca, qual regra deverá aplicar a um determinado símbolo de entrada, tornando a gramática ambígua e inadequada para uso com parsers \\(LL(1)\\).\nExemplo 1: observe que gramática a seguir foi criada para criar um conflito na Tabela de Derivação. Antes de nos preocuparmos com os tipos de conflito, e como solucioná-los, vamos rever todo o processo de criação de uma Tabela de Derivação para entender o problema.\n\\[\n\\begin{aligned}\n1. &\\ E \\rightarrow T + E \\ \\mid  \\ T \\\\\n2. &\\ T \\rightarrow int \\ \\mid  \\ (E)\n\\end{aligned}\n\\]\n\n6.7.1 Conjunto FIRST\n\nPara o não-terminal E:\n\n\\(E \\rightarrow T + E\\): o primeiro símbolo é \\(T\\). Portanto, incluímos \\(FIRST(T)\\) em \\(FIRST(E)\\).\n\\(E \\rightarrow T\\): o primeiro símbolo é \\(T\\). Portanto, incluímos \\(FIRST(T)\\) em \\(FIRST(E)\\).\n\nPara o não-terminal T:\n\n\\(T \\rightarrow int\\): o primeiro símbolo é \\(int\\). Portanto, \\(FIRST(T)\\) inclui \\(int\\).\n\\(T \\rightarrow (E)\\): o primeiro símbolo é \\((\\). Portanto, \\(FIRST(T)\\) inclui \\((\\).\n\n\nAssim, temos:\n\\[\n\\begin{aligned}\nFIRST(T) &= \\{ int, ( \\} \\\\\nFIRST(E) &= FIRST(T) = \\{ int, ( \\}\n\\end{aligned}\n\\]\n\n\n6.7.2 Conjunto FOLLOW\n\nPara o símbolo inicial E:\n\n\\(FOLLOW(E)\\) inclui $.\n\nPara as produções de E:\n\n\\(E \\rightarrow T + E\\):\n\no símbolo \\(T\\) pode ser seguido por \\(+\\), então \\(+\\) está em \\(FOLLOW(T)\\).\no símbolo \\(E\\) é o último da produção, então \\(FOLLOW(E)\\) inclui \\(FOLLOW(E)\\).\n\n\\(E \\rightarrow T\\): o símbolo \\(T\\) é o último da produção, então \\(FOLLOW(T)\\) inclui \\(FOLLOW(E)\\).\n\nPara as produções de T:\n\n\\(T \\rightarrow int\\): \\(int\\) é um terminal, não influencia \\(FOLLOW\\).\n\\(T \\rightarrow (E)\\): \\(E\\) pode ser seguido por \\()\\), então \\(FOLLOW(E)\\) inclui \\()\\).\n\n\nAssim, teremos:\n\\[\n\\begin{aligned}\nFOLLOW(E) &= \\{ \\$, ) \\} \\\\\nFOLLOW(T) &= \\{ +, \\$, ) \\}\n\\end{aligned}\n\\]\n\n\n6.7.3 Nullable\nPara determinar se algum não-terminal é nullable, verificamos se ele pode derivar a string vazia \\(\\varepsilon\\).\n\nPara E:\n\n\\(E \\rightarrow T + E\\): \\(T\\) não é nullable, portanto, \\(E\\) não é nullable a partir desta produção.\n\\(E \\rightarrow T\\): \\(T\\) não é nullable, portanto, \\(E\\) não é nullable.\n\nPara T:\n\n\\(T \\rightarrow int\\): \\(int\\) não é nullable.\n\\(T \\rightarrow (E)\\): \\(E\\) não é nullable e \\((\\) é um terminal.\n\n\nOu seja, nenhum dos não-terminais é nullable:\n\\[\n\\begin{aligned}\nNullable(E) &= false \\\\\nNullable(T) &= false\n\\end{aligned}\n\\]\n\n\n6.7.4 Resumo dos Conjuntos\n\\[\n\\begin{aligned}\nFIRST(E) &= \\{ int, ( \\} \\\\\nFIRST(T) &= \\{ int, ( \\} \\\\\nFOLLOW(E) &= \\{ \\$ ) \\} \\\\\nFOLLOW(T) &= \\{ +, \\$ ) \\} \\\\\nNullable(E) &= false \\\\\nNullable(T) &= false\n\\end{aligned}\n\\]\nO que permite gerar a seguinte Tabela de Derivação:\n\n\n\n\n\n\n\n\n\n\n\nnão-terminal\nint\n(\n+\n$\n)\n\n\n\n\nE\n\\(E \\rightarrow T + E\\)\\(E \\rightarrow T\\)\n\\(E \\rightarrow T + E\\)\\(E \\rightarrow T\\)\n\n\n\n\n\nT\n\\(T \\rightarrow int\\)\n\\(T \\rightarrow (E)\\)\n\n\n\n\n\n\nNesta tabela podemos ver um conflito explícito. O não-terminal \\(E\\) possui duas produções para os símbolos \\(int\\) e \\((\\).\nSempre que na criação de Tabelas de Derivação existir um conflito estaremos gerando ambiguidades na derivação. A gramática é ambígua, sempre que uma sentença puder ser derivada de duas ou mais formas diferentes, gerando árvores sintáticas diferentes. Por exemplo, na gramática do Exemplo 1, a sentença \\(int + int\\) pode ser derivada tanto como \\(E → T → int\\) seguido de \\(E → T + E → int + int\\) quanto como \\(E → T + E → T + T → int + int\\).\n\n\n6.7.5 Tipos de Conflitos Nas Tabelas de Derivação\nConflitos na Tabela de Derivação, ou tabela de análise, \\(LL(1)\\) podem ser classificados em dois tipos principais:\n\nConflito \\(FIRST\\)/\\(FIRST\\): ocorre quando o conjunto \\(FIRST\\) de duas ou mais produções de um mesmo não-terminal possui um terminal em comum. Na gramática do Exemplo 1, as produções \\(E \\rightarrow T + E\\) e \\(E \\rightarrow T\\) possuem os terminais \\(int\\) e \\((\\) em seus conjuntos \\(FIRST\\). Ao encontrar \\(int\\) ou \\((\\) na entrada, o parser não sabe se deve aplicar a regra que deriva uma expressão com um operador \\(+\\) ou a regra que deriva um termo. Neste momento do processo de parsing determinismo saiu pela janela e o parser \\(LL(1)\\) é inútil.\nConflito \\(FIRST\\)/\\(FOLLOW\\): ocorre quando uma produção tem \\(\\varepsilon\\) (a string vazia) em seu conjunto \\(FIRST\\) e o conjunto \\(FOLLOW\\) do não-terminal da produção possui um terminal em comum com o \\(FIRST\\) de outra produção do mesmo não-terminal. Por exemplo, na gramática do Exemplo 1, se \\(E \\rightarrow \\varepsilon\\) fosse uma produção e o conjunto \\(FOLLOW(E)\\) contivesse \\(+\\), que também está no \\(FIRST\\) da produção \\(E \\rightarrow T + E\\), ao encontrar o fim de uma expressão (representado por um símbolo em \\(FOLLOW(E)\\)), o parser não saberia se deve aplicar a regra que deriva apenas um termo ou a regra que deriva uma expressão com um operador \\(+\\).\n\n\n\n6.7.6 Resolução de Conflitos\nConflitos na tabela \\(LL(1)\\) podem ser resolvidos das seguintes formas:\n\nRefatoração da gramática: A gramática pode ser reescrita para eliminar ambiguidades e recursões à esquerda, evitando assim os conflitos.\nFatoração à esquerda: Produções com prefixos comuns podem ser fatoradas para que a decisão entre elas possa ser tomada com base em um único símbolo de lookahead.\nUso de analisadores mais poderosos: Se os conflitos não puderem ser resolvidos na gramática, pode ser necessário usar um analisador sintático mais poderoso, como um analisador \\(LR(1)\\) ou \\(LALR(1)\\), que conseguem lidar com gramáticas mais complexas.\n\nAs soluções 1 e 2 implicam na modificação da sua gramática, o que ocorre com frequência quando começamos do zero. A solução 3, em linguagens complexas, pode ser a solução adequada, mas implica em mudar de algoritmo de parser\nExemplo 2: resolução de conflito. No exemplo da gramática anterior, o conflito \\(FIRST\\)/\\(FIRST\\) pode ser resolvido fatorando as produções de \\(E\\):\n\\[\n\\begin{aligned}\n1. &\\ E \\rightarrow T E' \\\\\n2. &\\ E' \\rightarrow + E \\ \\mid  \\ \\varepsilon \\\\\n3. &\\ T \\rightarrow int \\ \\mid  \\ (E)\n\\end{aligned}\n\\]\nSe calcularmos os conjuntos \\(FIRST\\) e \\(FOLLOW\\) novamente, teremos:\n\n6.7.6.1 Conjunto FIRST\n\nPara o não-terminal E:\n\n\\(E \\rightarrow T E'\\): o primeiro símbolo é \\(T\\). Portanto, incluímos \\(FIRST(T)\\) em \\(FIRST(E)\\).\n\nPara o não-terminal E’:\n\n\\(E' \\rightarrow + E\\): o primeiro símbolo é \\(+\\). Portanto, \\(FIRST(E')\\) inclui \\(+\\).\n\\(E' \\rightarrow \\varepsilon\\): incluímos \\(\\varepsilon\\) em \\(FIRST(E')\\).\n\nPara o não-terminal T:\n\n\\(T \\rightarrow int\\): o primeiro símbolo é \\(int\\). Portanto, \\(FIRST(T)\\) inclui \\(int\\).\n\\(T \\rightarrow (E)\\): o primeiro símbolo é $( $. Portanto, \\(FIRST(T)\\) inclui $( $.\n\n\nAssim, teremos:\n\\[\n\\begin{aligned}\nFIRST(T) &= \\{ int, ( \\} \\\\\nFIRST(E') &= \\{ +, \\varepsilon \\} \\\\\nFIRST(E) &= FIRST(T) = \\{ int, ( \\}\n\\end{aligned}\n\\]\n\n\n6.7.6.2 Conjunto FOLLOW\n\nPara o símbolo inicial E:\n\n\\(FOLLOW(E)\\) inclui $.\n\nPara as produções de E:\n\n\\(E \\rightarrow T E'\\):\n\nO símbolo \\(E'\\) pode ser seguido por \\(FOLLOW(E)\\).\nEntão, \\(FOLLOW(E')\\) inclui \\(FOLLOW(E)\\).\n\n\nPara as produções de E’:\n\n\\(E' \\rightarrow + E\\):\n\nO símbolo \\(E\\) pode ser seguido por \\(FOLLOW(E')\\).\nEntão, \\(FOLLOW(E)\\) inclui \\(FOLLOW(E')\\).\n\n\\(E' \\rightarrow \\varepsilon\\): não há efeito em \\(FOLLOW\\).\n\nPara as produções de T:\n\n\\(T \\rightarrow int\\): \\(int\\) é um terminal, não influencia \\(FOLLOW\\).\n\\(T \\rightarrow (E)\\): \\(E\\) pode ser seguido por \\()\\), então \\(FOLLOW(E)\\) inclui \\()\\).\n\n\nAssim, teremos:\n\\[\n\\begin{aligned}\nFOLLOW(E) &= \\{ \\$, ) \\} \\\\\nFOLLOW(E') &= \\{ \\$, ) \\} \\\\\nFOLLOW(T) &= \\{ +, \\$, ) \\}\n\\end{aligned}\n\\]\n\n\n6.7.6.3 Nullable\nPara determinar se algum não-terminal é nullable, verificamos se ele pode derivar a string vazia \\(\\varepsilon\\).\n\nPara E:\n\n\\(E \\rightarrow T E'\\): \\(T\\) não é nullable, portanto, \\(E\\) não é nullable.\n\nPara E’:\n\n\\(E' \\rightarrow + E\\): \\(+\\) é um terminal, portanto, não é nullable.\n\\(E' \\rightarrow \\varepsilon\\): \\(E'\\) é nullable.\n\nPara T:\n\n\\(T \\rightarrow int\\): \\(int\\) não é nullable.\n\\(T \\rightarrow (E)\\): $( $ é um terminal, portanto, não é nullable.\n\n\nAssim, teremos:\n\\[\n\\begin{aligned}\nNullable(E) &= false \\\\\nNullable(E') &= true \\\\\nNullable(T) &= false\n\\end{aligned}\n\\]\n\n\n6.7.6.4 Resumo dos Conjuntos\n\\[\n\\begin{aligned}\nFIRST(E) &= \\{ int, ( \\} \\\\\nFIRST(E') &= \\{ +, \\varepsilon \\} \\\\\nFIRST(T) &= \\{ int, ( \\} \\\\\nFOLLOW(E) &= \\{ \\$, ) \\} \\\\\nFOLLOW(E') &= \\{ \\$, ) \\} \\\\\nFOLLOW(T) &= \\{ +, \\$, ) \\} \\\\\nNullable(E) &= false \\\\\nNullable(E') &= true \\\\\nNullable(T) &= false\n\\end{aligned}\n\\]\nO que permite gerar a seguinte Tabela de Derivação:\n\n\n\n\n\n\n\n\n\n\n\nnão-terminal\nint\n(\n+\n$\n)\n\n\n\n\nE\n\\(E \\rightarrow TE'\\)\n\\(E \\rightarrow TE'\\)\n\n\n\n\n\nE’\n\n\n\\(E' \\rightarrow +E\\)\n\\(E' \\rightarrow \\varepsilon\\)\n\\(E' \\rightarrow \\varepsilon\\)\n\n\nT\n\\(T \\rightarrow int\\)\n\\(T \\rightarrow (E)\\)\n\n\n\n\n\n\nAgora, a decisão entre derivar uma expressão com um operador \\(+\\) ou apenas um termo pode ser tomada com base no próximo símbolo da entrada: se for \\(+\\), aplica-se a regra \\(E' \\rightarrow + E\\); caso contrário, aplica-se a regra \\(E' \\rightarrow \\varepsilon\\).\nAssim como fiz nos artigos anteriores, vou sugerir um pseudocódigo, um tanto inocente, para a criação de tabelas de derivação. Acredito que, com um pouco de cuidado, depois que a amável leitora dominar esta técnica possa criar um pseudocódigo mais eficiente. A fé move montanhas.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parsers LL(1): Começando a Análise Sintática</span>"
    ]
  },
  {
    "objectID": "05-parsersLL1.html#pseudocódigo-para-a-criação-da-tabela-de-derivação",
    "href": "05-parsersLL1.html#pseudocódigo-para-a-criação-da-tabela-de-derivação",
    "title": "6  Parsers LL(1): Começando a Análise Sintática",
    "section": "6.8 Pseudocódigo para a Criação da Tabela de Derivação",
    "text": "6.8 Pseudocódigo para a Criação da Tabela de Derivação\nfunção construir_tabela(gramatica, nao_terminais, terminais, FIRST, FOLLOW):\n    // Inicializa a tabela com entradas vazias (erro)\n    Tabela = criar um dicionário aninhado [nao_terminal][terminal]\n\n    // Itera sobre cada produção A -&gt; α\n    para cada não-terminal A e suas produções em gramatica:\n        para cada produção α (lista de símbolos):\n            \n            // Calcula o FIRST da sequência α\n            first_α = calcular_first_da_sequencia(α, ...)\n\n            // Regra 1: Para cada terminal 't' em FIRST(α)\n            para cada terminal t em first_α:\n                se t != 'EPSILON':\n                    se Tabela[A][t] já está preenchida:\n                        Lançar ErroDeConflito(A, t)\n                    senão:\n                        Tabela[A][t] = produção A -&gt; α\n\n            // Regra 2: Se α pode ser vazio (ε)\n            se 'EPSILON' está em first_α:\n                // Para cada terminal 't' em FOLLOW(A)\n                para cada terminal t em FOLLOW[A]:\n                    se Tabela[A][t] já está preenchida:\n                        Lançar ErroDeConflito(A, t)\n                    senão:\n                        Tabela[A][t] = produção A -&gt; ε\n    \n    retornar Tabela\nEm fim, este código pode ser implementado em Python por:\ndef construir_tabela_ll1(\n    gramatica: dict,\n    simbolo_inicial: str) -&gt; dict[str, dict[str, list | None]]:\n    \"\"\"\n    Constrói a Tabela de Análise LL(1) completa a partir de uma gramática.\n    Esta função serve como um integrador para todas as funções anteriores.\n    \"\"\"\n    # 1. Identificar todos os símbolos\n    nao_terminais = set(gramatica.keys())\n    simbolos = set(nao_terminais)\n    for producoes in gramatica.values():\n        for producao in producoes:\n            simbolos.update(producao)\n    terminais = sorted(list(simbolos - nao_terminais - {'EPSILON'})) + ['$']\n    \n    # 2. Calcular os conjuntos\n    nullable = calcular_NULLABLE(gramatica)\n    first = calcular_FIRST(gramatica, nao_terminais, nullable)\n    follow = calcular_FOLLOW(gramatica, simbolo_inicial, nao_terminais, first, nullable)\n    \n    # 3. Inicializar e preencher a tabela\n    tabela = {nt: {t: None for t in terminais} for nt in nao_terminais}\n    \n    for nt_head, producoes in gramatica.items():\n        for producao in producoes:\n            first_producao = calcular_first_da_sequencia(producao, first, nullable, nao_terminais)\n            \n            # Regra 1: FIRST\n            for terminal in first_producao - {'EPSILON'}:\n                if tabela[nt_head][terminal] is not None:\n                    raise ValueError(f\"Conflito FIRST/FIRST em [{nt_head}, {terminal}]!\")\n                tabela[nt_head][terminal] = producao\n                \n            # Regra 2: FOLLOW\n            if 'EPSILON' in first_producao:\n                for terminal in follow[nt_head]:\n                    if tabela[nt_head][terminal] is not None:\n                        raise ValueError(f\"Conflito FIRST/FOLLOW em [{nt_head}, {terminal}]!\")\n                    tabela[nt_head][terminal] = producao\n\n    return tabela\n\ndef imprimir_tabela(tabela: dict):\n    \"\"\"Função auxiliar para exibir a tabela de forma legível no console.\"\"\"\n    nao_terminais = sorted(tabela.keys())\n    if not nao_terminais: return\n    \n    terminais = sorted(tabela[nao_terminais[0]].keys())\n    \n    # Imprimir cabeçalho\n    header = f\"{'Não-Terminal':&lt;15}\" + \"\".join([f\"{t:&lt;18}\" for t in terminais])\n    print(header)\n    print(\"-\" * len(header))\n    \n    # Imprimir linhas\n    for nt in nao_terminais:\n        row_str = f\"{nt:&lt;15}\"\n        for t in terminais:\n            producao = tabela[nt][t]\n            if producao is None:\n                cell = \" \"\n            else:\n                # Formata a produção como \"A -&gt; α\"\n                cell = f\"{nt} → {' '.join(producao)}\"\n            row_str += f\"{cell:&lt;18}\"\n        print(row_str)\n\n# --- Exemplo de uso para a gramática refatorada (Exemplo 2) ---\ngramatica_sem_conflito = {\n    'E': [['T', \"E'\"]],\n    \"E'\": [['+', 'E'], ['EPSILON']],\n    'T': [['int'], ['(', 'E', ')']]\n}\n\ntry:\n    tabela_final = construir_tabela_ll1(gramatica_sem_conflito, 'E')\n    print(\"Tabela de Análise LL(1) gerada com sucesso:\\n\")\n    imprimir_tabela(tabela_final)\nexcept ValueError as e:\n    print(f\"Erro ao gerar tabela: {e}\")",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parsers LL(1): Começando a Análise Sintática</span>"
    ]
  },
  {
    "objectID": "05-parsersLL1.html#um-exemplo-mais-completo-análise-sintática-ll1-para-expressões-aritméticas",
    "href": "05-parsersLL1.html#um-exemplo-mais-completo-análise-sintática-ll1-para-expressões-aritméticas",
    "title": "6  Parsers LL(1): Começando a Análise Sintática",
    "section": "6.9 Um Exemplo mais Completo: Análise Sintática LL(1) para Expressões Aritméticas",
    "text": "6.9 Um Exemplo mais Completo: Análise Sintática LL(1) para Expressões Aritméticas\nEm linguagens de programação e sistemas de cálculo, expressões aritméticas são fundamentais para realizar operações matemáticas. Um analisador sintático eficiente deve ser capaz de reconhecer e validar a estrutura dessas expressões, respeitando a precedência de operadores e o uso correto de parênteses.\n\n6.9.1 Problema\nConsidere a seguinte expressão aritmética que combina operações de adição e multiplicação com uso de parênteses:\n(id + id) * id\nEsta expressão representa um cálculo onde: - id representa identificadores ou valores numéricos - Os parênteses alteram a precedência natural dos operadores\n- A multiplicação (\\(\\times\\)) tem maior precedência que a adição (\\(+\\)) quando não há parênteses\n\n\n6.9.2 Tarefa\nDesenvolva um analisador sintático \\(LL(1)\\) para reconhecer expressões aritméticas com as seguintes características:\nRequisitos da Gramática: - Deve suportar operações de adição (\\(+\\)) e multiplicação (\\(\\times\\)) - Deve respeitar a precedência de operadores (multiplicação antes da adição) - Deve permitir o uso de parênteses para alterar a precedência - Os operandos são representados por identificadores (id)\nSua solução deve incluir:\n\nDefinição da gramática original usando a notação BNF tradicional para expressões aritméticas, mesmo que contenha recursão à esquerda\nVerificação e correção da gramática para torná-la adequada para análise \\(LL(1)\\):\n\nIdentificar problemas como recursão à esquerda\nAplicar transformações necessárias\nApresentar a gramática final corrigida\n\nCálculo dos conjuntos \\(FIRST\\) e \\(FOLLOW\\) para todos os não-terminais da gramática corrigida\nConstrução da tabela de derivação \\(LL(1)\\) completa, indicando as produções apropriadas para cada combinação de não-terminal e terminal\nAnálise sintática passo a passo da expressão (id + id) * id usando:\n\nAlgoritmo de análise preditiva com pilha\nDemonstração de cada passo da análise\nVerificação da aceitação da entrada\n\n\n\n\n6.9.3 Observações Importantes\n\nUse id para representar identificadores ou números na gramática\nUse $ como marcador de fim de entrada\nUse \\(\\varepsilon\\) (epsilon) para representar produções vazias\nA gramática inicial pode seguir o padrão clássico de expressões aritméticas:\n\n\\(E \\rightarrow\\) Expressões\n\\(T \\rightarrow\\) Termos\n\n\\(F \\rightarrow\\) Fatores\n\nConsidere que \\(\\times\\) e * são equivalentes para multiplicação\n\n\n\n6.9.4 Exemplo de Saída Esperada\nA análise deve demonstrar que a expressão (id + id) * id é aceita pela gramática, mostrando como a precedência dos operadores é respeitada e como os parênteses forçam a adição a ser executada antes da multiplicação.\nNota: Este exercício é fundamental para compreender os princípios de análise sintática descendente e a construção de parsers eficientes para expressões matemáticas em compiladores e interpretadores.\n\n\n6.9.5 Solução\n\n6.9.5.1 1. Definição da Gramática Inocente\n\\[\n\\begin{aligned}\nE &\\rightarrow E + T \\mid T \\\\\nT &\\rightarrow T \\times F \\mid F \\\\\nF &\\rightarrow (E) \\mid id\n\\end{aligned}\n\\]\n\n\n6.9.5.2 2. Verificação da Gramática\nEsta gramática não é \\(LL(1)\\) porque possui recursão à esquerda (ex: \\(E \\rightarrow E + T\\)), o que causaria um loop infinito em um analisador descendente. O primeiro passo é eliminar a recursão à esquerda. Aplicamos a regra de transformação padrão: uma produção da forma \\(A \\rightarrow A\\alpha \\mid \\beta\\) é convertida para \\(A \\rightarrow \\beta A'\\) e \\(A' \\rightarrow \\alpha A' \\mid \\varepsilon\\).\nGramática Corrigida para \\(LL(1)\\) (\\(G'\\))\n\\[\n\\begin{aligned}\nE &\\rightarrow T E' \\\\\nE' &\\rightarrow + T E' \\mid \\varepsilon \\\\\nT &\\rightarrow F T' \\\\\nT' &\\rightarrow \\times F T' \\mid \\varepsilon \\\\\nF &\\rightarrow (E) \\mid id\n\\end{aligned}\n\\]\nOnde \\(\\varepsilon\\) representa a cadeia vazia e \\(id\\) representa um identificador ou número. Agora a gramática é adequada para a análise \\(LL(1)\\).\n\n\n6.9.5.3 3. Cálculo dos Conjuntos \\(FIRST\\) e \\(FOLLOW\\)\nCom base na gramática corrigida (\\(G'\\)), calculamos os conjuntos \\(FIRST\\) e \\(FOLLOW\\).\n\n6.9.5.3.1 Conjunto \\(FIRST\\)\nO conjunto \\(FIRST(A)\\) contém os terminais que podem iniciar uma sentença derivada de \\(A\\).\n\\[\n\\begin{aligned}\n&FIRST(F) = \\{ (, id \\} \\\\\n&FIRST(T^{\\prime}) = \\{ \\times, \\varepsilon \\} \\\\\n&FIRST(T) = FIRST(F) = \\{ (, id \\} \\\\\n&FIRST(E^{\\prime}) = \\{ +, \\varepsilon \\} \\\\\n&FIRST(E) = FIRST(T) = \\{ (, id \\}\n\\end{aligned}\n\\]\n\n\n6.9.5.3.2 Conjunto \\(FOLLOW\\)\nO conjunto \\(FOLLOW(A)\\) contém os terminais que podem aparecer imediatamente após uma sentença derivada de \\(A\\).\n\\[\n\\begin{aligned}\nFOLLOW(E) &= \\{ \\$, ) \\} \\\\\nFOLLOW(E^{\\prime}) &= FOLLOW(E) = \\{ \\$, ) \\} \\\\\nFOLLOW(T) &= FIRST(E^{\\prime}) - \\{\\varepsilon\\} \\cup FOLLOW(E) = \\{ + \\} \\cup \\{ \\$, ) \\} = \\{ +, \\$, ) \\} \\\\\nFOLLOW(T^{\\prime}) &= FOLLOW(T) = \\{ +, \\$, ) \\} \\\\\nFOLLOW(F) &= FIRST(T^{\\prime}) - \\{\\varepsilon\\} \\cup FOLLOW(T) = \\{ \\times \\} \\cup \\{ +, \\$, ) \\} = \\{ \\times, +, \\$, ) \\}\n\\end{aligned}\n\\]\n\n\n\n6.9.5.4 4. Construção da Tabela de Derivação \\(LL(1)\\)\nUsando os conjuntos \\(FIRST\\) e \\(FOLLOW\\), construímos a tabela de derivação.\n\nPara uma produção \\(A \\rightarrow \\alpha\\):\n\nAdicione \\(A \\rightarrow \\alpha\\) em \\(Tabela[A, a]\\) para cada terminal \\(a \\in FIRST(\\alpha)\\).\nSe \\(\\varepsilon \\in FIRST(\\alpha)\\), adicione \\(A \\rightarrow \\alpha\\) em \\(Tabela[A, b]\\) para cada terminal \\(b \\in FOLLOW(A)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\n+\n×\n(\n)\n$\n\n\n\n\nE\n\\(E \\to T E'\\)\n\n\n\\(E \\to T E'\\)\n\n\n\n\nE’\n\n\\(E' \\to + T E'\\)\n\n\n\\(E' \\to \\varepsilon\\)\n\\(E' \\to \\varepsilon\\)\n\n\nT\n\\(T \\to F T'\\)\n\n\n\\(T \\to F T'\\)\n\n\n\n\nT’\n\n\\(T' \\to \\varepsilon\\)\n\\(T' \\to \\times F T'\\)\n\n\\(T' \\to \\varepsilon\\)\n\\(T' \\to \\varepsilon\\)\n\n\nF\n\\(F \\to id\\)\n\n\n\\(F \\to (E)\\)\n\n\n\n\n\nAs células vazias representam erros de sintaxe.\n\n\n6.9.5.5 5. Análise Sintática da Entrada (id + id) * id\nVamos rastrear a análise da string usando o algoritmo de análise preditiva com uma pilha.\n\n\n\nPilha\nEntrada\nAção\n\n\n\n\n$ E\n( id + id ) * id $\nE \\to T E'\n\n\n$ E' T\n( id + id ) * id $\nT \\to F T'\n\n\n$ E' T' F\n( id + id ) * id $\nF \\to (E)\n\n\n$ E' T' ) E (\n( id + id ) * id $\nmatch (\n\n\n$ E' T' ) E\nid + id ) * id $\nE \\to T E'\n\n\n$ E' T' ) E' T\nid + id ) * id $\nT \\to F T'\n\n\n$ E' T' ) E' T' F\nid + id ) * id $\nF \\to id\n\n\n$ E' T' ) E' T' id\nid + id ) * id $\nmatch id\n\n\n$ E' T' ) E' T'\n+ id ) * id $\nT' \\to \\varepsilon\n\n\n$ E' T' ) E'\n+ id ) * id $\nE' \\to + T E'\n\n\n$ E' T' ) E' T +\n+ id ) * id $\nmatch +\n\n\n$ E' T' ) E' T\nid ) * id $\nT \\to F T'\n\n\n$ E' T' ) E' T' F\nid ) * id $\nF \\to id\n\n\n$ E' T' ) E' T' id\nid ) * id $\nmatch id\n\n\n$ E' T' ) E' T'\n) * id $\nT' \\to \\varepsilon\n\n\n$ E' T' ) E'\n) * id $\nE' \\to \\varepsilon\n\n\n$ E' T' )\n) * id $\nmatch )\n\n\n$ E' T'\n* id $\nT' \\to \\times F T'\n\n\n$ E' T' F \\times\n* id $\nmatch \\times\n\n\n$ E' T' F\nid $\nF \\to id\n\n\n$ E' T' id\nid $\nmatch id\n\n\n$ E' T'\n$\nT' \\to \\varepsilon\n\n\n$ E'\n$\nE' \\to \\varepsilon\n\n\n$\n$\nAceito",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parsers LL(1): Começando a Análise Sintática</span>"
    ]
  },
  {
    "objectID": "05-parsersLL1.html#um-exemplo-com-linguagens-de-programação-laço-for-em-c",
    "href": "05-parsersLL1.html#um-exemplo-com-linguagens-de-programação-laço-for-em-c",
    "title": "6  Parsers LL(1): Começando a Análise Sintática",
    "section": "6.10 Um Exemplo Com Linguagens de Programação: Laço For em C",
    "text": "6.10 Um Exemplo Com Linguagens de Programação: Laço For em C\nConsidere o seguinte fragmento de código em linguagem C que implementa um laço simples:\nfor (i = 0; i &lt; 10; i++) {\n    x = x + 1;\n}\nEste código representa um laço for típico da linguagem C, que inicializa uma variável de controle, verifica uma condição a cada iteração e atualiza a variável de controle após cada execução do bloco.\n\n6.10.1 Tarefa\nCrie uma tabela de derivação LL(1) para analisar este tipo de estrutura de laço for na linguagem C, considerando:\n\nA estrutura completa do laço for com seus três componentes: inicialização, condição e incremento;\nExpressões aritméticas simples que podem aparecer na inicialização, condição e corpo do laço;\nBlocos de código contendo instruções de atribuição.\n\nSua solução deve incluir:\n\nA definição da gramática inocente para este fragmento da linguagem C.\nA correção da gramática para torná-la adequada para análise LL(1) (eliminando recursão à esquerda).\nO cálculo dos conjuntos FIRST e FOLLOW para a gramática corrigida.\nA construção da tabela de derivação LL(1).\nA análise passo a passo da entrada de exemplo usando a tabela construída.\n\n\n\n6.10.2 Observações\n\nConsidere que ID representa identificadores (como i, x).\nConsidere que em um representa literais numéricos (como 0, 10, 1).\nA gramática deve suportar expressões aritméticas com adição e multiplicação.\nA gramática deve ser capaz de analisar corretamente o exemplo fornecido.\n\n\n\n6.10.3 Solução\n\n6.10.3.1 Tabela de Derivação LL(1) para um Laço ‘for’\nO objetivo é criar uma tabela de derivação \\(LL(1)\\) para uma gramática simplificada de laços for da linguagem C.\n\n6.10.3.1.1 1. Verificação e Correção da Gramática\nVamos começar com uma gramática inocente (\\(G\\)) que possui recursão à esquerda, o que a tornará inadequada para a análise \\(LL(1)\\). Forçando a eliminação da recursão apenas para treinar a eliminação de recursão à esquerda.\n\n6.10.3.1.1.1 Gramática Inocente\n\\[\n\\begin{aligned}\n\\text{FOR\\_STMT} &\\rightarrow \\text{for} ( \\text{INIT} ; \\text{COND} ; \\text{INCR} ) \\text{BLOCK} \\\\\n\\text{INIT} &\\rightarrow \\text{ID} = \\text{EXPR} \\\\\n\\text{COND} &\\rightarrow \\text{EXPR} &lt; \\text{EXPR} \\\\\n\\text{INCR} &\\rightarrow \\text{ID}++ \\\\\n\\text{BLOCK} &\\rightarrow \\{ \\text{STMT} \\} \\\\\n\\text{STMT} &\\rightarrow \\text{ID} = \\text{EXPR} ; \\\\\n\\text{EXPR} &\\rightarrow \\text{EXPR} + \\text{TERM} \\mid \\text{TERM} \\\\\n\\text{TERM} &\\rightarrow \\text{TERM} * \\text{FACTOR} \\mid \\text{FACTOR} \\\\\n\\text{FACTOR} &\\rightarrow ( \\text{EXPR} ) \\mid \\text{ID} \\mid \\text{em um}\n\\end{aligned}\n\\]\nPara corrigir, eliminamos a recursão à esquerda nas regras EXPR e TERM.\n\n\n\n6.10.3.1.2 Gramática Corrigida para LL(1) (\\(G'\\))\n\\[\n\\begin{aligned}\n\\text{FOR\\_STMT} &\\rightarrow \\text{for} ( \\text{INIT} ; \\text{COND} ; \\text{INCR} ) \\text{BLOCK} \\\\\n\\text{INIT} &\\rightarrow \\text{ID} = \\text{EXPR} \\\\\n\\text{COND} &\\rightarrow \\text{EXPR} &lt; \\text{EXPR} \\\\\n\\text{INCR} &\\rightarrow \\text{ID}++ \\\\\n\\text{BLOCK} &\\rightarrow \\{ \\text{STMT} \\} \\\\\n\\text{STMT} &\\rightarrow \\text{ID} = \\text{EXPR} ; \\\\\n\\text{EXPR} &\\rightarrow \\text{TERM} \\, \\text{EXPR\\_TAIL} \\\\\n\\text{EXPR\\_TAIL} &\\rightarrow + \\text{TERM} \\, \\text{EXPR\\_TAIL} \\mid \\varepsilon \\\\\n\\text{TERM} &\\rightarrow \\text{FACTOR} \\, \\text{TERM\\_TAIL} \\\\\n\\text{TERM\\_TAIL} &\\rightarrow * \\text{FACTOR} \\, \\text{TERM\\_TAIL} \\mid \\varepsilon \\\\\n\\text{FACTOR} &\\rightarrow ( \\text{EXPR} ) \\mid \\text{ID} \\mid \\text{em um}\n\\end{aligned}\n\\]\nOnde \\(\\varepsilon\\) representa a cadeia vazia. Agora a gramática é adequada para a análise \\(LL(1)\\).\n\n\n\n6.10.3.2 2. Cálculo dos Conjuntos FIRST e FOLLOW\nCom base na gramática corrigida (\\(G'\\)), calculamos os conjuntos \\(FIRST\\) e \\(FOLLOW\\).\n\n6.10.3.2.1 Conjunto FIRST\nO conjunto FIRST(A) contém os terminais que podem iniciar uma sentença derivada de \\(A\\).\n\\[\n\\begin{aligned}\nFIRST(\\text{FACTOR}) &= \\{ (, \\text{ID}, \\text{em um} \\} \\\\\nFIRST(\\text{TERM\\_TAIL}) &= \\{ *, \\varepsilon \\} \\\\\nFIRST(\\text{TERM}) &= FIRST(\\text{FACTOR}) = \\{ (, \\text{ID}, \\text{em um} \\} \\\\\nFIRST(\\text{EXPR\\_TAIL}) &= \\{ +, \\varepsilon \\} \\\\\nFIRST(\\text{EXPR}) &= FIRST(\\text{TERM}) = \\{ (, \\text{ID}, \\text{em um} \\} \\\\\nFIRST(\\text{STMT}) &= \\{ \\text{ID} \\} \\\\\nFIRST(\\text{BLOCK}) &= \\{ \\{ \\} \\\\\nFIRST(\\text{INCR}) &= \\{ \\text{ID} \\} \\\\\nFIRST(\\text{COND}) &= FIRST(\\text{EXPR}) = \\{ (, \\text{ID}, \\text{em um} \\} \\\\\nFIRST(\\text{INIT}) &= \\{ \\text{ID} \\} \\\\\nFIRST(\\text{FOR\\_STMT}) &= \\{ \\text{for} \\}\n\\end{aligned}\n\\]\n\n\n6.10.3.2.2 Conjunto FOLLOW\nO conjunto FOLLOW(A) contém os terminais que podem aparecer imediatamente após uma sentença derivada de \\(A\\).\n\\[\n\\begin{aligned}\nFOLLOW(\\text{FOR\\_STMT}) &= \\{ \\$ \\} \\\\\nFOLLOW(\\text{INIT}) &= \\{ ; \\} \\\\\nFOLLOW(\\text{COND}) &= \\{ ; \\} \\\\\nFOLLOW(\\text{INCR}) &= \\{ ) \\} \\\\\nFOLLOW(\\text{BLOCK}) &= FOLLOW(\\text{FOR\\_STMT}) = \\{ \\$ \\} \\\\\nFOLLOW(\\text{STMT}) &= \\{ \\} \\} \\\\\nFOLLOW(\\text{EXPR}) &= \\{ ;, &lt;, ) \\} \\\\\nFOLLOW(\\text{EXPR\\_TAIL}) &= FOLLOW(\\text{EXPR}) = \\{ ;, &lt;, ) \\} \\\\\nFOLLOW(\\text{TERM}) &= FIRST(\\text{EXPR\\_TAIL}) - \\{\\varepsilon\\} \\cup FOLLOW(\\text{EXPR}) = \\{ +, ;, &lt;, ) \\} \\\\\nFOLLOW(\\text{TERM\\_TAIL}) &= FOLLOW(\\text{TERM}) = \\{ +, ;, &lt;, ) \\} \\\\\nFOLLOW(\\text{FACTOR}) &= FIRST(\\text{TERM\\_TAIL}) - \\{\\varepsilon\\} \\cup FOLLOW(\\text{TERM}) = \\{ *, +, ;, &lt;, ) \\}\n\\end{aligned}\n\\]\n\n\n\n\n6.10.3.3 3. Construção da Tabela de Derivação LL(1)\nUsando os conjuntos \\(FIRST\\) e \\(FOLLOW\\), construímos a tabela de derivação.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNão-Terminal\nfor\nID\nem um\n+\n*\n&lt;\n(\n)\n;\n{\n}\n$\n\n\n\n\nFOR_STMT\n→ for(...)BLOCK\n\n\n\n\n\n\n\n\n\n\n\n\n\nINIT\n\n→ ID=EXPR\n\n\n\n\n\n\n\n\n\n\n\n\nCOND\n\n→ EXPR&lt;EXPR\n→ EXPR&lt;EXPR\n\n\n\n→ EXPR&lt;EXPR\n\n\n\n\n\n\n\nINCR\n\n→ ID++\n\n\n\n\n\n\n\n\n\n\n\n\nBLOCK\n\n\n\n\n\n\n\n\n\n→ {STMT}\n\n\n\n\nSTMT\n\n→ ID=EXPR;\n\n\n\n\n\n\n\n\n\n\n\n\nEXPR\n\n→ TERM EXPR_T\n→ TERM EXPR_T\n\n\n\n→ TERM EXPR_T\n\n\n\n\n\n\n\nEXPR_TAIL\n\n\n\n→ +TERM EXPR_T\n\n→ ε\n\n→ ε\n→ ε\n\n\n\n\n\nTERM\n\n→ FACTOR TERM_T\n→ FACTOR TERM_T\n\n\n\n→ FACTOR TERM_T\n\n\n\n\n\n\n\nTERM_TAIL\n\n\n\n→ ε\n→ *FACTOR TERM_T\n→ ε\n\n→ ε\n→ ε\n\n\n\n\n\nFACTOR\n\n→ ID\n→ em um\n\n\n\n→ (EXPR)\n\n\n\n\n\n\n\n\nAs células vazias representam erros de sintaxe.\n\n\n\n6.10.3.4 4. Análise Sintática da Entrada for (i = 0; i &lt; 10; i++) { x = x + 1; }\nO rastreio abaixo mostra o estado da pilha (lida da esquerda para a direita, topo à esquerda) e da entrada a cada passo.\n\n\n\n\n\n\n\n\nPilha (Topo à Esquerda)\nEntrada Restante\nAção\n\n\n\n\nFOR_STMT $\nfor (i=0;...) $\n→ for(INIT;COND;INCR)BLOCK\n\n\nfor(...)BLOCK $\nfor (i=0;...) $\nMatch for\n\n\n(INIT;COND;INCR)BLOCK $\n(i=0;...) $\nMatch (\n\n\nINIT;COND;INCR)BLOCK $\ni=0;...) $\nINIT → ID=EXPR\n\n\nID=EXPR;COND;INCR)BLOCK $\ni=0;...) $\nMatch ID (i)\n\n\n=EXPR;COND;INCR)BLOCK $\n=0;...) $\nMatch =\n\n\nEXPR;COND;INCR)BLOCK $\n0;...) $\nEXPR → TERM EXPR_TAIL\n\n\nTERM EXPR_TAIL;COND...) $\n0;...) $\nTERM → FACTOR TERM_TAIL\n\n\nFACTOR TERM_TAIL EXPR_T...) $\n0;...) $\nFACTOR → em um\n\n\nem um TERM_TAIL EXPR_T...) $\n0;...) $\nMatch em um (0)\n\n\nTERM_TAIL EXPR_TAIL;...) $\n;i&lt;10;...) $\nTERM_TAIL → ε\n\n\nEXPR_TAIL;COND;INCR...) $\n;i&lt;10;...) $\nEXPR_TAIL → ε\n\n\n;COND;INCR)BLOCK $\n;i&lt;10;...) $\nMatch ;\n\n\nCOND;INCR)BLOCK $\ni&lt;10;...) $\nCOND → EXPR&lt;EXPR\n\n\nEXPR&lt;EXPR;INCR)BLOCK $\ni&lt;10;...) $\nEXPR → TERM EXPR_TAIL\n\n\nTERM EXPR_TAIL&lt;EXPR...) $\ni&lt;10;...) $\nTERM → FACTOR TERM_TAIL\n\n\nFACTOR TERM_TAIL EXPR_T...) $\ni&lt;10;...) $\nFACTOR → ID\n\n\nID TERM_TAIL EXPR_T...) $\ni&lt;10;...) $\nMatch ID (i)\n\n\nTERM_TAIL EXPR_TAIL&lt;...) $\n&lt;10;...) $\nTERM_TAIL → ε\n\n\nEXPR_TAIL&lt;EXPR;INCR...) $\n&lt;10;...) $\nEXPR_TAIL → ε\n\n\n&lt;EXPR;INCR)BLOCK $\n&lt;10;...) $\nMatch &lt;\n\n\nEXPR;INCR)BLOCK $\n10;...) $\nEXPR → TERM EXPR_TAIL\n\n\n…\n…\n(continua até o final da entrada)\n\n\nSTMT } $\nx=x+1;} $\nSTMT → ID=EXPR;\n\n\nID=EXPR; } $\nx=x+1;} $\nMatch ID (x)\n\n\n=EXPR; } $\n=x+1;} $\nMatch =\n\n\nEXPR; } $\nx+1;} $\nEXPR → TERM EXPR_TAIL\n\n\nTERM EXPR_TAIL; } $\nx+1;} $\nTERM → FACTOR TERM_TAIL\n\n\nFACTOR TERM_TAIL EXPR_T; } $\nx+1;} $\nFACTOR → ID\n\n\nID TERM_TAIL EXPR_T; } $\nx+1;} $\nMatch ID (x)\n\n\nTERM_TAIL EXPR_TAIL; } $\n+1;} $\nTERM_TAIL → ε\n\n\nEXPR_TAIL; } $\n+1;} $\nEXPR_TAIL → +TERM EXPR_TAIL\n\n\n+TERM EXPR_TAIL; } $\n+1;} $\nMatch +\n\n\nTERM EXPR_TAIL; } $\n1;} $\nTERM → FACTOR TERM_TAIL\n\n\nFACTOR TERM_TAIL EXPR_T; } $\n1;} $\nFACTOR → em um\n\n\nem um TERM_TAIL EXPR_T; } $\n1;} $\nMatch em um (1)\n\n\nTERM_TAIL EXPR_TAIL; } $\n;} $\nTERM_TAIL → ε\n\n\nEXPR_TAIL; } $\n;} $\nEXPR_TAIL → ε\n\n\n; } $\n;} $\nMatch ;\n\n\n} $\n} $\nMatch }\n\n\n$ $\n$ $\nAceito",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parsers LL(1): Começando a Análise Sintática</span>"
    ]
  },
  {
    "objectID": "06-first-follow.html",
    "href": "06-first-follow.html",
    "title": "7  Conjuntos FIRST e FOLLOW",
    "section": "",
    "text": "7.1 O Conjunto FIRST\nO conjunto \\(FIRST\\) de um símbolo não-terminal é o conjunto de todos os terminais que podem aparecer no início de qualquer string derivada desse símbolo, incluindo o símbolo vazio (\\(\\varepsilon\\)) se o não-terminal puder derivar a string vazia. Para os símbolos terminais, o elemento do conjunto \\(FIRST\\) será o próprio símbolo terminal.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conjuntos FIRST e FOLLOW</span>"
    ]
  },
  {
    "objectID": "06-first-follow.html#o-conjunto-first",
    "href": "06-first-follow.html#o-conjunto-first",
    "title": "7  Conjuntos FIRST e FOLLOW",
    "section": "",
    "text": "7.1.1 Regras de Criação do Conjunto FIRST\nPara definir o conjunto \\(FIRST(X)\\) para todos os símbolos não-terminais \\(X\\) de uma gramática que esteja definida por um conjunto de regras de produção, podemos seguir os seguintes passos:\n\nPara símbolos terminais: o conjunto \\(FIRST\\) é o próprio símbolo terminal. Ou seja, se \\(a\\) é um terminal, então \\(FIRST(a) = {a}\\).\nPara um símbolo não-terminal \\(X\\): olhe para cada regra de produção \\(X \\rightarrow \\alpha\\) e siga as seguintes regras:\n\nSe \\(\\alpha\\) é um terminal, adicione \\(\\alpha\\) ao conjunto \\(FIRST(X)\\).\nSe \\(\\alpha\\) começa com um símbolo não-terminal \\(Y\\), adicione \\(FIRST(Y)\\) ao \\(FIRST(X)\\), exceto pelo símbolo de vazio \\((\\varepsilon\\)) se ele estiver presente.\nSe \\(\\alpha\\) consiste apenas em não-terminais e todos eles podem derivar em vazio (diretamente ou indiretamente), adicione \\(\\varepsilon\\) ao conjunto \\(FIRST(X)\\).\n\n\nO símbolo vazio \\(\\varepsilon\\) pertence ao conjunto FIRST(X) se, e somente se, \\(X\\) pode derivar a string vazia (diretamente ou indiretamente).\nRepita esses passos até que os conjuntos \\(FIRST\\) de todos os símbolos não-terminais não possam ser alterado.\n\n\n7.1.2 Exemplo 1: Criação de Conjuntos FIRST\nConsidere a gramática definida pelo seguinte conjunto de regras de produção:\n\\[\n\\begin{array}{cc}\n1. &S \\rightarrow aB \\vert  bA \\\\\n2. &A \\rightarrow c \\vert  d \\\\\n3. &B \\rightarrow e \\vert  f \\\\\n\\end{array}\n\\]\nEste conjunto de regras de produção permite criar:\n\n\n\nSímbolo\nFIRST\nExplicação\n\n\n\n\nS\n{a, b}\nS pode ser derivado em “aB” ou “bA”\n\n\nA\n{c, d}\nA pode ser derivado em “c” ou “d”\n\n\nB\n{e, f}\nB pode ser derivado em “e” ou “f”\n\n\n\nLogo: \\(FIRST =\\{(S,\\{a, b\\}),(A,\\{c, d\\}),(B,\\{e, f\\})\\}\\), um conjunto de tuplas.\nAgora que entendemos o algoritmo, podemos tentar criar um pseudocódigo para encontrar os elementos do conjunto \\(First\\).\n\n\n7.1.3 Algoritmo para calcular o conjunto FIRST\n## Algoritmo para calcular o conjunto FIRST para símbolos não-terminais\n\n# Entrada: Um conjunto de regras de produção P\n# Saída: Um dicionário FIRST, onde FIRST[X] é o conjunto FIRST do símbolo não-terminal X\n\nfunção calcular_FIRST(gramática):\n    FIRST = {}  # Inicializa o dicionário FIRST\n\n    # Passo 1: Inicialização para não-terminais\n    para cada símbolo não-terminal X na gramática:\n        FIRST[X] &lt;- {}\n\n    # Passo 2: Iteração para não-terminais\n    mudou = verdadeiro\n    enquanto mudou:\n        mudou = falso\n        para cada regra de produção X \\rightarrow Y1 Y2 ... Yn na gramática:\n            k = 0\n            adicionou_epsilon = verdadeiro\n            enquanto k &lt; n e adicionou_epsilon:\n                adicionou_epsilon = falso\n                Yk = Y[k]\n\n                # Se Yk é terminal, adicionar Yk ao FIRST[X]\n                se Yk é terminal:\n                    se Yk não está em FIRST[X]:\n                        adicionar Yk a FIRST[X]\n                        mudou = verdadeiro\n                # Se Yk é não-terminal, adicionar FIRST[Yk] ao FIRST[X], exceto \\varepsilon\n                senão:\n                    para cada símbolo t em FIRST[Yk]:\n                        se t != \"\\varepsilon\":\n                            se t não está em FIRST[X]:\n                                adicionar t a FIRST[X]\n                                mudou = verdadeiro\n                        senão:\n                            adicionou_epsilon = verdadeiro\n                k = k + 1\n\n            # Se todos os Y1, Y2, ..., Yn podem derivar \\varepsilon, adicionar \\varepsilon ao FIRST[X]\n            se k == n e adicionou_epsilon:\n                se \"\\varepsilon\" não está em FIRST[X]:\n                    adicionar \"\\varepsilon\" a FIRST[X]\n                    mudou = verdadeiro\n\n    retornar FIRST\nEste pseudocódigo, poderia ser criado em python com um código parecido com este:\ndef calcular_FIRST(producoes):\n    FIRST = {}\n\n    # Passo 1: Inicialização para não-terminais\n    # Identificamos todos os símbolos não-terminais presentes nas produções\n    nao_terminais = {regra.split('\\rightarrow')[0].strip() for regra in producoes}\n\n    # Inicializamos o conjunto FIRST de cada não-terminal como um conjunto vazio\n    for nao_terminal in nao_terminais:\n        FIRST[nao_terminal] = set()\n\n    mudou = True\n    # O loop continua até que não haja mais mudanças nos conjuntos FIRST\n    while mudou:\n        mudou = False\n        # Iteramos por todas as produções da gramática\n        for producao in producoes:\n            partes = producao.split('\\rightarrow')\n            X = partes[0].strip()  # Não-terminal do lado esquerdo da produção\n            Y = partes[1].strip().split()  # Lista de símbolos do lado direito da produção\n\n            k = 0\n            adicionou_epsilon = True  # Flag para controlar a adição de \\varepsilon\n            # Iteramos sobre os símbolos do lado direito da produção\n            while k &lt; len(Y) and adicionou_epsilon:\n                adicionou_epsilon = False\n                Yk = Y[k]\n\n                # Se Yk é um não-terminal, adicionamos seus FIRST ao FIRST de X\n                if Yk in nao_terminais:\n                    for simbolo in FIRST[Yk]:\n                        if simbolo != \"\\varepsilon\":\n                            if simbolo not in FIRST[X]:\n                                FIRST[X].add(simbolo)\n                                mudou = True\n                        else:\n                            adicionou_epsilon = True\n                else:\n                    # Se Yk é um terminal, adicionamos Yk ao FIRST de X\n                    if Yk not in FIRST[X]:\n                        FIRST[X].add(Yk)\n                        mudou = True\n                    adicionou_epsilon = False  # Paramos de adicionar se encontramos um terminal\n                k += 1\n\n            # Se todos os símbolos Y1, Y2, ..., Yn podem derivar \\varepsilon, adicionamos \\varepsilon ao FIRST de X\n            if k == len(Y) and adicionou_epsilon:\n                if \"\\varepsilon\" not in FIRST[X]:\n                    FIRST[X].add(\"\\varepsilon\")\n                    mudou = True\n\n    return FIRST\n\n# Exemplo de uso\nproducoes = [\n    \"S \\rightarrow a B\",\n    \"S \\rightarrow b A\",\n    \"A \\rightarrow c\",\n    \"A \\rightarrow d\",\n    \"B \\rightarrow e\",\n    \"B \\rightarrow f\"\n]\n\nFIRST = calcular_FIRST(producoes)\nfor nao_terminal in FIRST:\n    print(f\"FIRST({nao_terminal}) = {FIRST[nao_terminal]}\")",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conjuntos FIRST e FOLLOW</span>"
    ]
  },
  {
    "objectID": "06-first-follow.html#o-conjunto-follow",
    "href": "06-first-follow.html#o-conjunto-follow",
    "title": "7  Conjuntos FIRST e FOLLOW",
    "section": "7.2 O Conjunto FOLLOW",
    "text": "7.2 O Conjunto FOLLOW\nO conjunto \\(FOLLOW\\) de um símbolo não-terminal é o conjunto de terminais que podem aparecer imediatamente à direita (após, follow) desse não-terminal em alguma forma sentencial derivada, ou o símbolo de fim de entrada ($) se o não-terminal puder aparecer no final de uma forma sentencial.\nPara definir o conjunto \\(FOLLOW(A)\\) para cada não-terminal \\(A\\), siga estes passos:\n\nColoque o símbolo de fim de entrada \\((\\$)\\) no \\(FOLLOW\\) do símbolo inicial da gramática. Ao colocar o símbolo de fim de entrada ($) no \\(FOLLOW\\) do símbolo inicial da gramática, garantimos que o analisador sintático reconheça a última derivação da gramática como válida. Isso significa que o analisador estará preparado para encontrar o símbolo (\\(\\$\\)) ao final da string de entrada, indicando que a análise foi concluída com sucesso. Em outras palavras, o símbolo (\\(\\$\\)) no \\(FOLLOW\\) do símbolo inicial representa a expectativa de que a string de entrada seja completamente processada e que não existam símbolos após a última derivada.\nPara cada produção da forma \\(A \\rightarrow \\alpha B \\beta\\), onde \\(B\\) é um não-terminal:\n\n\nSe \\(\\beta\\) não deriva \\(\\varepsilon\\) (a string vazia), adicione \\(FIRST(\\beta)\\) (sem \\(\\varepsilon\\)) a \\(FOLLOW(B)\\).\nSe \\(\\beta\\) deriva \\(\\varepsilon\\) (a string vazia) ou \\(\\beta\\) é a string vazia, adicione \\(FOLLOW(A)\\) a \\(FOLLOW(B)\\).\n\nRepita esses passos até que os conjuntos \\(FOLLOW\\) de todos os símbolos não-terminais não mudem mais.\n\n7.2.1 Exemplo 1: Criação de Conjuntos FOLLOW\nConsidere a gramática definida por:\n\\[\n\\begin{array}{cc}\n1. & S \\rightarrow aB \\vert  bA \\\\\n2. & A \\rightarrow c \\vert  d \\\\\n3. & B \\rightarrow e \\vert  f \\\\\n\\end{array}\n\\]\nConjunto FIRST:\n\n\\(FIRST(S) = \\{a, b\\}\\) (S pode derivar em \\(aB\\) ou \\(bA\\))\n\\(FIRST(A) = \\{c, d\\}\\) (A pode derivar em \\(c\\) ou \\(d\\))\n\\(FIRST(B) = \\{e, f\\}\\) (B pode derivar em \\(e\\) ou \\(f\\))\n\nConjunto FOLLOW:\nPara calcular o conjunto FOLLOW, seguimos as regras aplicadas à gramática do exemplo:\n\n\\(FOLLOW(S) = \\{\\$\\}\\)\n\n\\(S\\) é o símbolo inicial, então adicionamos o marcador de fim de entrada (\\(\\$\\)) ao seu conjunto FOLLOW.\n\n\\(FOLLOW(A) = \\{\\$\\}\\)\n\nAnalisamos as regras para encontrar onde \\(A\\) aparece no lado direito. Encontramos \\(A\\) na produção \\(S \\to bA\\).\nComo não há nenhum símbolo após \\(A\\) nesta produção, adicionamos todos os elementos de \\(FOLLOW(S)\\) ao conjunto \\(FOLLOW(A)\\).\nPortanto, \\(FOLLOW(A)\\) recebe \\(\\{\\$\\}\\).\n\n\\(FOLLOW(B) = \\{\\$\\}\\)\n\nAnalisamos as regras para encontrar onde \\(B\\) aparece no lado direito. Encontramos \\(B\\) na produção \\(S \\rightarrow aB\\).\nComo também não há nenhum símbolo após \\(B\\) nesta produção, adicionamos todos os elementos de \\(FOLLOW(S)\\) ao conjunto \\(FOLLOW(B)\\).\nPortanto, \\(FOLLOW(B)\\) também recebe \\(\\{\\$\\}\\).\n\n\nCriamos o conjunto \\(FIRST\\) porque este é necessário para a criação do conjunto \\(FOLLOW\\). Mas, neste momento nos interessa apenas o conjunto \\(FOLLOW\\). O conjunto resultante será:\n\n\n\n\n\n\n\n\nSímbolo\nFOLLOW\nExplicação\n\n\n\n\n\\(S\\)\n\\(\\{ \\$ \\}\\)\n\\(S\\) é o símbolo inicial, então \\(\\$\\) é adicionado ao seu conjunto FOLLOW.\n\n\n\\(A\\)\n\\(\\{ \\$ \\}\\)\n\\(A\\) aparece na produção \\(S \\to bA\\). Como \\(A\\) está no final, \\(FOLLOW(A)\\) herda o conteúdo de \\(FOLLOW(S)\\).\n\n\n\\(B\\)\n\\(\\{ \\$ \\}\\)\n\\(B\\) aparece na produção \\(S \\to aB\\). Como \\(B\\) está no final, \\(FOLLOW(B)\\) herda o conteúdo de \\(FOLLOW(S)\\).\n\n\n\n\n\n7.2.2 Algoritmo para calcular o conjunto FOLLOW\nAssim como fizemos com o \\(FIRST\\) podemos criar um algoritmo para criar o conjunto \\(FOLLOW\\):\n# Algoritmo para calcular o conjunto FIRST para símbolos não-terminais\n# Entrada: Um conjunto de regras de produção P\n# Saída: Um dicionário FIRST, onde FIRST[X] é o conjunto FIRST do símbolo não-terminal X\n\nfunção calcular_FIRST(producoes):\n    FIRST = {}  # Inicializa o dicionário FIRST\n    \n    # Identifica todos os não-terminais e terminais\n    nao_terminais = conjunto vazio\n    para cada producao nas producoes:\n        X = lado esquerdo da producao (antes de \"-&gt;\")\n        adicionar X aos nao_terminais\n    \n    # Passo 1: Inicialização - cria conjuntos vazios para cada não-terminal\n    para cada X em nao_terminais:\n        FIRST[X] = conjunto vazio\n    \n    # Passo 2: Iteração até convergência\n    mudou = verdadeiro\n    enquanto mudou:\n        mudou = falso\n        para cada producao nas producoes:\n            X = lado esquerdo da producao\n            simbolos = lado direito da producao (lista de símbolos)\n            \n            # Caso especial: produção vazia (X -&gt; epsilon)\n            se simbolos == [\"epsilon\"]:\n                se \"epsilon\" não está em FIRST[X]:\n                    adicionar \"epsilon\" a FIRST[X]\n                    mudou = verdadeiro\n                continuar para próxima produção\n            \n            # Percorre os símbolos do lado direito\n            k = 0\n            continuar = verdadeiro\n            enquanto k &lt; tamanho(simbolos) e continuar:\n                Yk = simbolos[k]\n                continuar = falso\n                \n                # Se Yk é terminal\n                se Yk não está em nao_terminais e Yk != \"epsilon\":\n                    se Yk não está em FIRST[X]:\n                        adicionar Yk a FIRST[X]\n                        mudou = verdadeiro\n                # Se Yk é não-terminal\n                senão se Yk está em nao_terminais:\n                    # Adiciona FIRST[Yk] - {epsilon} a FIRST[X]\n                    para cada simbolo em FIRST[Yk]:\n                        se simbolo != \"epsilon\":\n                            se simbolo não está em FIRST[X]:\n                                adicionar simbolo a FIRST[X]\n                                mudou = verdadeiro\n                    # Se epsilon está em FIRST[Yk], continua para próximo símbolo\n                    se \"epsilon\" está em FIRST[Yk]:\n                        continuar = verdadeiro\n                k = k + 1\n            \n            # Se todos os símbolos podem derivar epsilon\n            se continuar == verdadeiro:\n                se \"epsilon\" não está em FIRST[X]:\n                    adicionar \"epsilon\" a FIRST[X]\n                    mudou = verdadeiro\n    \n    retornar FIRST\nAgora que temos um pseudo código, podemos partir para o código em Python. Neste ponto, a atenta leitora precisa lembrar que vamos precisar do conjunto \\(FIRST\\) para encontrar o conjunto \\(FOLLOW\\). Logo começando pelo \\(FIRST\\), teremos:\ndef calcular_FIRST(producoes):\n    \"\"\"\n    Calcula o conjunto FIRST para todos os não-terminais de uma gramática.\n    \n    Args:\n        producoes: Lista de strings representando as produções (ex: \"S -&gt; a B\")\n    \n    Returns:\n        Dicionário onde as chaves são não-terminais e valores são conjuntos FIRST\n    \"\"\"\n    FIRST = {}\n    \n    # Identifica todos os não-terminais (símbolos do lado esquerdo)\n    nao_terminais = set()\n    for producao in producoes:\n        X = producao.split('-&gt;')[0].strip()\n        nao_terminais.add(X)\n    \n    # Passo 1: Inicialização - cria conjuntos vazios para cada não-terminal\n    for nao_terminal in nao_terminais:\n        FIRST[nao_terminal] = set()\n    \n    # Passo 2: Iteração até convergência\n    mudou = True\n    while mudou:\n        mudou = False\n        for producao in producoes:\n            partes = producao.split('-&gt;')\n            X = partes[0].strip()\n            simbolos = partes[1].strip().split()\n            \n            # Caso especial: produção vazia (X -&gt; epsilon)\n            if simbolos == ['epsilon']:\n                if 'epsilon' not in FIRST[X]:\n                    FIRST[X].add('epsilon')\n                    mudou = True\n                continue\n            \n            # Percorre os símbolos do lado direito\n            k = 0\n            continuar = True\n            while k &lt; len(simbolos) and continuar:\n                Yk = simbolos[k]\n                continuar = False\n                \n                # Se Yk é terminal\n                if Yk not in nao_terminais and Yk != 'epsilon':\n                    if Yk not in FIRST[X]:\n                        FIRST[X].add(Yk)\n                        mudou = True\n                # Se Yk é não-terminal\n                elif Yk in nao_terminais:\n                    # Adiciona FIRST[Yk] - {epsilon} a FIRST[X]\n                    for simbolo in FIRST[Yk]:\n                        if simbolo != 'epsilon':\n                            if simbolo not in FIRST[X]:\n                                FIRST[X].add(simbolo)\n                                mudou = True\n                    # Se epsilon está em FIRST[Yk], continua para próximo símbolo\n                    if 'epsilon' in FIRST[Yk]:\n                        continuar = True\n                k += 1\n            \n            # Se todos os símbolos podem derivar epsilon\n            if continuar:\n                if 'epsilon' not in FIRST[X]:\n                    FIRST[X].add('epsilon')\n                    mudou = True\n    \n    return FIRST\nFinalmente podemos ver o pseudocódigo e o código em Python para encontrar o conjunto \\(FOLLOW\\):\n# Algoritmo para calcular o conjunto FOLLOW para símbolos não-terminais\n# Entrada: Um conjunto de regras de produção P e o símbolo inicial S\n# Saída: Um dicionário FOLLOW, onde FOLLOW[X] é o conjunto FOLLOW do símbolo não-terminal X\n\nfunção calcular_FOLLOW(producoes, simbolo_inicial):\n    # Primeiro calcula o conjunto FIRST que será necessário\n    FIRST = calcular_FIRST(producoes)\n    FOLLOW = {}  # Inicializa o dicionário FOLLOW\n    \n    # Identifica todos os não-terminais\n    nao_terminais = conjunto vazio\n    para cada producao nas producoes:\n        X = lado esquerdo da producao (antes de \"-&gt;\")\n        adicionar X aos nao_terminais\n    \n    # Passo 1: Inicialização - cria conjuntos vazios para cada não-terminal\n    para cada X em nao_terminais:\n        FOLLOW[X] = conjunto vazio\n    \n    # Passo 2: Adiciona $ ao FOLLOW do símbolo inicial\n    FOLLOW[simbolo_inicial] = {\"$\"}\n    \n    # Passo 3: Iteração até convergência\n    mudou = verdadeiro\n    enquanto mudou:\n        mudou = falso\n        para cada producao nas producoes:\n            A = lado esquerdo da producao\n            alfa = lado direito da producao (lista de símbolos)\n            \n            # Para cada símbolo na produção\n            para i de 0 até tamanho(alfa) - 1:\n                B = alfa[i]\n                \n                # Se B é um não-terminal\n                se B está em nao_terminais:\n                    beta = alfa[i+1:fim]  # símbolos após B\n                    \n                    # Se existe sequência beta após B\n                    se beta não é vazia:\n                        # Calcula FIRST(beta)\n                        first_beta = calcular_first_da_sequencia(beta, FIRST, nao_terminais)\n                        \n                        # Adiciona FIRST(beta) - {epsilon} em FOLLOW(B)\n                        para cada simbolo em first_beta:\n                            se simbolo != \"epsilon\":\n                                se simbolo não está em FOLLOW[B]:\n                                    adicionar simbolo a FOLLOW[B]\n                                    mudou = verdadeiro\n                        \n                        # Se epsilon está em FIRST(beta), adiciona FOLLOW(A) em FOLLOW(B)\n                        se \"epsilon\" está em first_beta:\n                            para cada simbolo em FOLLOW[A]:\n                                se simbolo não está em FOLLOW[B]:\n                                    adicionar simbolo a FOLLOW[B]\n                                    mudou = verdadeiro\n                    \n                    # Se não há nada após B (B está no final)\n                    senão:\n                        # Adiciona FOLLOW(A) em FOLLOW(B)\n                        para cada simbolo em FOLLOW[A]:\n                            se simbolo não está em FOLLOW[B]:\n                                adicionar simbolo a FOLLOW[B]\n                                mudou = verdadeiro\n    \n    retornar FOLLOW\n\n# Função auxiliar para calcular FIRST de uma sequência de símbolos\nfunção calcular_first_da_sequencia(sequencia, FIRST, nao_terminais):\n    first_sequencia = conjunto vazio\n    \n    para cada simbolo na sequencia:\n        # Se é terminal, adiciona e para\n        se simbolo não está em nao_terminais:\n            adicionar simbolo a first_sequencia\n            parar o loop\n        # Se é não-terminal\n        senão:\n            # Adiciona FIRST[simbolo] - {epsilon}\n            para cada s em FIRST[simbolo]:\n                se s != \"epsilon\":\n                    adicionar s a first_sequencia\n            # Se epsilon não está em FIRST[simbolo], para\n            se \"epsilon\" não está em FIRST[simbolo]:\n                parar o loop\n    \n    # Se percorreu toda a sequência (todos derivam epsilon)\n    se percorreu toda a sequencia:\n        adicionar \"epsilon\" a first_sequencia\n    \n    retornar first_sequencia\nFinalmente o código em Python será:\ndef calcular_first_da_sequencia(sequencia, FIRST, nao_terminais):\n    \"\"\"\n    Calcula o FIRST de uma sequência de símbolos.\n    \n    Args:\n        sequencia: Lista de símbolos\n        FIRST: Dicionário com conjuntos FIRST já calculados\n        nao_terminais: Conjunto de não-terminais da gramática\n    \n    Returns:\n        Conjunto FIRST da sequência\n    \"\"\"\n    first_sequencia = set()\n    \n    for simbolo in sequencia:\n        # Se é terminal, adiciona e para\n        if simbolo not in nao_terminais:\n            first_sequencia.add(simbolo)\n            break\n        # Se é não-terminal\n        else:\n            # Adiciona FIRST[simbolo] - {epsilon}\n            for s in FIRST[simbolo]:\n                if s != 'epsilon':\n                    first_sequencia.add(s)\n            # Se epsilon não está em FIRST[simbolo], para\n            if 'epsilon' not in FIRST[simbolo]:\n                break\n    else:\n        # Se percorreu toda a sequência (todos derivam epsilon)\n        first_sequencia.add('epsilon')\n    \n    return first_sequencia\n\ndef calcular_FOLLOW(producoes, simbolo_inicial):\n    \"\"\"\n    Calcula o conjunto FOLLOW para todos os não-terminais de uma gramática.\n    \n    Args:\n        producoes: Lista de strings representando as produções\n        simbolo_inicial: Símbolo inicial da gramática\n    \n    Returns:\n        Dicionário onde as chaves são não-terminais e valores são conjuntos FOLLOW\n    \"\"\"\n    # Primeiro calcula o conjunto FIRST que será necessário\n    FIRST = calcular_FIRST(producoes)\n    FOLLOW = {}\n    \n    # Identifica todos os não-terminais\n    nao_terminais = set()\n    for producao in producoes:\n        X = producao.split('-&gt;')[0].strip()\n        nao_terminais.add(X)\n    \n    # Passo 1: Inicialização - cria conjuntos vazios para cada não-terminal\n    for nao_terminal in nao_terminais:\n        FOLLOW[nao_terminal] = set()\n    \n    # Passo 2: Adiciona $ ao FOLLOW do símbolo inicial\n    FOLLOW[simbolo_inicial].add('$')\n    \n    # Passo 3: Iteração até convergência\n    mudou = True\n    while mudou:\n        mudou = False\n        for producao in producoes:\n            partes = producao.split('-&gt;')\n            A = partes[0].strip()\n            alfa = partes[1].strip().split()\n            \n            # Para cada símbolo na produção\n            for i in range(len(alfa)):\n                B = alfa[i]\n                \n                # Se B é um não-terminal\n                if B in nao_terminais:\n                    beta = alfa[i+1:]  # símbolos após B\n                    \n                    # Se existe sequência beta após B\n                    if beta:\n                        # Calcula FIRST(beta)\n                        first_beta = calcular_first_da_sequencia(beta, FIRST, nao_terminais)\n                        \n                        # Adiciona FIRST(beta) - {epsilon} em FOLLOW(B)\n                        for simbolo in first_beta:\n                            if simbolo != 'epsilon':\n                                if simbolo not in FOLLOW[B]:\n                                    FOLLOW[B].add(simbolo)\n                                    mudou = True\n                        \n                        # Se epsilon está em FIRST(beta), adiciona FOLLOW(A) em FOLLOW(B)\n                        if 'epsilon' in first_beta:\n                            for simbolo in FOLLOW[A]:\n                                if simbolo not in FOLLOW[B]:\n                                    FOLLOW[B].add(simbolo)\n                                    mudou = True\n                    \n                    # Se não há nada após B (B está no final)\n                    else:\n                        # Adiciona FOLLOW(A) em FOLLOW(B)\n                        for simbolo in FOLLOW[A]:\n                            if simbolo not in FOLLOW[B]:\n                                FOLLOW[B].add(simbolo)\n                                mudou = True\n    \n    return FOLLOW\n\n# Exemplo de uso completo\nif __name__ == \"__main__\":\n    # Definindo a gramática\n    producoes = [\n        \"S -&gt; a B\",\n        \"S -&gt; b A\",\n        \"A -&gt; c\",\n        \"A -&gt; d\",\n        \"B -&gt; e\",\n        \"B -&gt; f\"\n    ]\n    simbolo_inicial = \"S\"\n    \n    # Calculando FIRST\n    print(\"Conjuntos FIRST:\")\n    FIRST = calcular_FIRST(producoes)\n    for nao_terminal in sorted(FIRST.keys()):\n        print(f\"  FIRST({nao_terminal}) = {{{', '.join(sorted(FIRST[nao_terminal]))}}}\")\n    \n    # Calculando FOLLOW\n    print(\"\\nConjuntos FOLLOW:\")\n    FOLLOW = calcular_FOLLOW(producoes, simbolo_inicial)\n    for nao_terminal in sorted(FOLLOW.keys()):\n        print(f\"  FOLLOW({nao_terminal}) = {{{', '.join(sorted(FOLLOW[nao_terminal]))}}}\")",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conjuntos FIRST e FOLLOW</span>"
    ]
  },
  {
    "objectID": "07-parserLR1.html",
    "href": "07-parserLR1.html",
    "title": "8  Parsers \\(LR(1)\\): Análise Sintática bottom-up",
    "section": "",
    "text": "8.1 bottom-up vs top-down: Uma Mudança de Paradigma\nPara entender profundamente os parsers \\(LR(1)\\), precisamos primeiro compreender a diferença fundamental entre as abordagens top-down e bottom-up.\nEssa diferença não é apenas conceitual — ela esconde implicações profundas:",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parsers $LR(1)$: Análise Sintática _bottom-up_</span>"
    ]
  },
  {
    "objectID": "07-parserLR1.html#bottom-up-vs-top-down-uma-mudança-de-paradigma",
    "href": "07-parserLR1.html#bottom-up-vs-top-down-uma-mudança-de-paradigma",
    "title": "8  Parsers \\(LR(1)\\): Análise Sintática bottom-up",
    "section": "",
    "text": "parser \\(LL(1)\\) - top-down: a criativa leitora pode imaginar que está tentando montar um quebra-cabeça começando pela imagem da caixa. Você sabe que precisa formar uma árvore, então começa buscando as peças de borda e vai preenchendo os detalhes de cima para baixo.\nparser \\(LR(1)\\) - bottom-up: agora imagine a abordagem oposta: você pega as peças individuais e vai juntando pequenos grupos que fazem sentido. Dois pedaços verdes formam uma folha, três folhas formam um galho, vários galhos formam a copa. Você constrói de baixo para cima até ter a árvore completa.\n\n\n\nPoder de reconhecimento: \\(LR(1)\\) reconhece praticamente todas as gramáticas livres de contexto determinísticas;\nTratamento de recursão: recursão à esquerda é natural e eficiente;\nDetecção de erros: erros são detectados no momento mais cedo possível;\nComplexidade: a construção das tabelas é mais complexa, mas o parsing é igualmente eficiente.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parsers $LR(1)$: Análise Sintática _bottom-up_</span>"
    ]
  },
  {
    "objectID": "07-parserLR1.html#o-algoritmo-shift-reduce",
    "href": "07-parserLR1.html#o-algoritmo-shift-reduce",
    "title": "8  Parsers \\(LR(1)\\): Análise Sintática bottom-up",
    "section": "8.2 O Algoritmo Shift-Reduce",
    "text": "8.2 O Algoritmo Shift-Reduce\nO parser \\(LR(1)\\) é usa uma pilha, um buffer de entrada e uma tabela de parser como pode ser visto na Figure 8.1.\n\n\n\n\n\n\n\n\n\n(a) Parser LR(1)\n\n\n\n\n\nFigure 8.1\n\n\n\nO coração de um parser \\(LR(1)\\) é o algoritmo Shift-Reduce, que utiliza uma pilha e um buffer de entrada. O nome vem das duas operações principais:\n\nShift: consome o próximo símbolo do buffer de entrada e transita para um novo estado, conforme especificado na tabela ACTION. Por exemplo, se a ação for ACTION[estado_atual, simbolo] = sN, o parser empilha o estado N no topo da sua pilha de estados e avança a entrada.\nReduce: substitui um conjunto de símbolos no topo da pilha, que corresponde ao lado direito de uma produção, pelo não-terminal do lado esquerdo da produção. Chamamos este conjunto de símbolos de handle. Um exemplo custuma clarear as ideias. Considere a gramática a seguir:\n\n1. S' → S\n2. S → a A B e\n3. A → A b\n4. A → b\n5. B → d\nNesta gramática eu já tomei o cuidado de numerar as produções e de incluir uma regra para aumentar a gramática, a regra S' → S. Para ilustrar como o parser usa a tabela ACTION/GOTO, a seguir, a amável leitora encontrará uma tabela parcial, previamente calculada para esta gramática, como exemplo do que veremos mais adiante:\n\n\n\nEstado\nACTION\n\n\n\n\n\nGOTO\n\n\n\n\n\n\n\na\nb\nd\ne\n$\n\nS\nA\nB\n\n\n0\ns2\n\n\n\n\n\n1\n\n\n\n\n1\n\n\n\n\nacc\n\n\n\n\n\n\n2\n\ns4\n\n\n\n\n\n3\n\n\n\n3\n\ns6\ns7\n\n\n\n\n\n5\n\n\n4\n\nr4\nr4\n\n\n\n\n\n\n\n\n5\n\n\n\ns8\n\n\n\n\n\n\n\n6\n\nr3\nr3\nr3\n\n\n\n\n\n\n\n7\n\n\n\nr5\n\n\n\n\n\n\n\n8\n\n\n\n\nr2\n\n\n\n\n\n\n\n\n8.2.1 Rastreamento Passo a Passo (Versão Canônica)\nO algoritmo percorre a string de entrada e aplica as operações de shift e reduce de acordo com as regras da tabela ACTION/GOTO. Para alinhar nossa análise com a implementação computacional padrão, a pilha conterá apenas os números dos estados. A lógica da operação de redução é a parte que requer atenção especial:\n\npara uma produção k: A → β, o parser desempilha um número de estados igual ao número de símbolos em β (ou seja, |β|).\napós desempilhar, o estado s que agora está no topo da pilha representa o ponto na análise imediatamente antes de β ser reconhecido.\no parser então consulta GOTO[s, A] para determinar o próximo estado s', que será empilhado.\n\nPara que a criativa leitora entenda, basta seguir o rastreamento para a string abbde$, utilizando uma pilha que armazena apenas estados.\n\n\n\n\n\n\n\n\n\nPasso\nPilha de Estados\nBuffer de Entrada\nAção\n\n\n\n\n1\n[0]\nabbde$\nACTION[0,a] = s2. Empilha estado 2.\n\n\n2\n[0, 2]\nbbde$\nACTION[2,b] = s4. Empilha estado 4.\n\n\n3\n[0, 2, 4]\nbde$\nACTION[4,b] = r4 (Reduce por A → b). |b|=1, desempilha 1 estado.\n\n\n4\n[0, 2]\nbde$\nPilha expõe estado 2. GOTO[2,A] = 3. Empilha estado 3.\n\n\n5\n[0, 2, 3]\nbde$\nACTION[3,b] = s6. Empilha estado 6.\n\n\n6\n[0, 2, 3, 6]\nde$\nACTION[6,d] = r3 (Reduce por A → A b). |A b|=2, desempilha 2 estados.\n\n\n7\n[0, 2]\nde$\nPilha expõe estado 2. GOTO[2,A] = 3. Empilha estado 3.\n\n\n8\n[0, 2, 3]\nde$\nACTION[3,d] = s7. Empilha estado 7.\n\n\n9\n[0, 2, 3, 7]\ne$\nACTION[7,e] = r5 (Reduce por B → d). |d|=1, desempilha 1 estado.\n\n\n10\n[0, 2, 3]\ne$\nPilha expõe estado 3. GOTO[3,B] = 5. Empilha estado 5.\n\n\n11\n[0, 2, 3, 5]\ne$\nACTION[5,e] = s8. Empilha estado 8.\n\n\n12\n[0, 2, 3, 5, 8]\n$\nACTION[8,$] = r2 (Reduce por S → a A B e). |a A B e|=4, desempilha 4 estados.\n\n\n13\n[0]\n$\nPilha expõe estado 0. GOTO[0,S] = 1. Empilha estado 1.\n\n\n14\n[0, 1]\n$\nACTION[1,$] = acc. Aceito.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parsers $LR(1)$: Análise Sintática _bottom-up_</span>"
    ]
  },
  {
    "objectID": "07-parserLR1.html#construção-da-tabela-lr1-um-exemplo-guiado",
    "href": "07-parserLR1.html#construção-da-tabela-lr1-um-exemplo-guiado",
    "title": "8  Parsers \\(LR(1)\\): Análise Sintática bottom-up",
    "section": "8.3 Construção da Tabela LR(1): Um Exemplo Guiado",
    "text": "8.3 Construção da Tabela LR(1): Um Exemplo Guiado\nEntender como o parser funciona, é apenas o primeiro estágio. A construção da tabela \\(LR(1)\\) é um processo algorítmico sistemático, detalhado e que requerer atenção e cuidado da atenta leitora. Para tentar entender este processo vamos construir uma tabela de parsing para uma gramática simples, acompanhando cada etapa em um passo a passo detalhado. Nossa gramática de exemplo terá apenas duas produções:\n1. E → E + n\n2. E → n\nO que estamos tentando fazer aqui é construir uma tabela de parsing que possa guiar o parser por meio da gramática. Para isso, precisamos seguir um processo sistemático, que envolve a criação de conjuntos de itens \\(LR(1)\\), a definição de estados e transições, e finalmente o preenchimento da tabela ACTION/GOTO. Este processo está sintetizado no pseudocódigo a seguir:\nALGORITMO CONSTRUIR_COLECAO_CANONICA_E_TABELA_LR1(G')\n\n// ENTRADA: Uma gramática aumentada G' com símbolo inicial S'\n// SAÍDA: As tabelas de parsing LR(1) ACTION e GOTO\n\n// Parte 1: Construção da Coleção Canônica de Itens LR(1)\n\n1.  Inicialize C, a coleção de conjuntos de itens, como um conjunto vazio.\n2.  Calcule I₀ = CLOSURE({[S' → •S, $]}).\n3.  Adicione I₀ a C.\n4.  Marque I₀ como \"não processado\".\n\n5.  ENQUANTO houver um conjunto Iᵢ em C marcado como \"não processado\":\n6.      Marque Iᵢ como \"processado\".\n7.      PARA CADA símbolo gramatical X (terminal ou não-terminal):\n8.          Calcule Iⱼ = GOTO(Iᵢ, X).\n9.          SE Iⱼ não for vazio E Iⱼ não estiver em C:\n10.             Adicione Iⱼ a C.\n11.             Marque Iⱼ como \"não processado\".\n12.         FIM SE\n13.     FIM PARA\n14. FIM ENQUANTO\n\n// Parte 2: Construção das Tabelas ACTION e GOTO a partir da Coleção C\n\n15. PARA CADA conjunto Iᵢ em C (onde i é o número do estado):\n16.     PARA CADA item no conjunto Iᵢ:\n\n17.         // Caso 1: Item de Shift\n18.         SE o item tem a forma [A → α • t β, a], onde t é um TERMINAL:\n19.             Calcule Iⱼ = GOTO(Iᵢ, t).\n20.             Seja j o índice de Iⱼ em C.\n21.             // Se ACTION[i, t] já estiver preenchido, há um conflito.\n22.             Defina ACTION[i, t] = \"shift j\".\n23.         FIM SE\n\n24.         // Caso 2: Item de Reduce\n25.         SE o item tem a forma [A → α •, a], onde A ≠ S':\n26.             Seja k o número da produção A → α.\n27.             // Se ACTION[i, a] já estiver preenchido, há um conflito.\n28.             Defina ACTION[i, a] = \"reduce k\".\n29.         FIM SE\n\n30.         // Caso 3: Item de Aceitação\n31.         SE o item tem a forma [S' → S •, $]:\n32.             Defina ACTION[i, $] = \"accept\".\n33.         FIM SE\n34.     FIM PARA\n\n35.     // Preenchimento da tabela GOTO para o estado i\n36.     PARA CADA NÃO-TERMINAL B:\n37.         Calcule Iⱼ = GOTO(Iᵢ, B).\n38.         Seja j o índice de Iⱼ em C.\n39.         SE Iⱼ não for vazio:\n40.             Defina GOTO[i, B] = j.\n41.         FIM SE\n42.     FIM PARA\n43. FIM PARA\n\n44. // --- Funções Auxiliares ---\n\n45. FUNÇÃO CLOSURE(I):\n46.    J = I\n47.    REPETIR\n48.        PARA CADA item [A → α • B β, a] em J:\n49.            PARA CADA produção B → γ na gramática G':\n50.                PARA CADA terminal b em FIRST(βa):\n51.                    Adicione o item [B → •γ, b] a J.\n52.                FIM PARA\n53.            FIM PARA\n54.        FIM PARA\n55.    ATÉ que nenhum novo item possa ser adicionado a J.\n56.    RETORNE J.\n\n57. FUNÇÃO GOTO(I, X):\n58.    Inicialize J como um conjunto vazio.\n59.    PARA CADA item [A → α • X β, a] em I:\n60.        Adicione o item [A → α X • β, a] a J.\n61.    FIM PARA\n62.    RETORNE CLOSURE(J).\nPara entender este processo, vamos seguir cada passo com nossa gramática de exemplo, começando por criar uma gramática adequada ao algoritmo que descrevemos no pseudocódigo acima e depois seguimos passo a passo.\n\n8.3.1 Passo 1: Gramática Aumentada\nPrimeiro, aumentamos a gramática adicionando uma nova produção. Esta produção ajudará a definir o estado de aceitação do parser.\n0. E' → E\n1. E → E + n\n2. E → n\n\n\n8.3.2 Passo 2: Itens \\(LR(1)\\) e a Operação CLOSURE\nAgora que temos a gramática aumentada, criamos o estado inicial do nosso analisador, o conjunto de itens I₀. Para isso, precisamos entender dois conceitos fundamentais: o Item LR(1) e a operação CLOSURE.\nPense em um item LR(1) como um marcador de progresso para o parser. Um item \\(LR(1)\\) nos dirá três coisas:\n\nQual regra estamos tentando reconhecer? (Ex: E → E + n);\nAté onde já avançamos? O ponto (•) indicará o que já foi visto e o que esperamos ver a seguir;\nO que esperamos encontrar depois que a regra for completada? Isto que esperamos encontrar chamamos de lookahead e, em geral, será o símbolo terminal que deverá aparecer na entrada para que a redução da regra seja válida.\n\nPor exemplo, quando encontrarmos o item [E → E • + n, $] ele significará:\n\nestamos tentando reconhecer a regra E → E + n;\njá reconhecemos um E;\nesperamos ver um + seguido por um n;\napós reconhecermos E + n e reduzirmos para E, esperamos que o próximo símbolo na entrada seja o fim da string ($).\n\nA operação CLOSURE, fechamento em inglês, irá garantir que um determinado estado contenha todas as informações sobre todas as regras que poderiam ser iniciadas a partir daquele ponto.\n\n\n\n\n\n\n\n\n\n(a) Itens LR(1)\n\n\n\n\n\nFigure 8.2\n\n\n\nA regra para a construção \\(LR(1)\\) canônica é precisa: para cada item da forma [A → α • B β, a] em um conjunto, onde B é um não-terminal, devemos adicionar ao conjunto um item [B → • γ, b] para cada produção B → γ. O novo lookahead b será cada símbolo terminal no conjunto FIRST(βa). É este cálculo rigoroso do lookahead que diferencia o \\(LR(1)\\) do seu primo mais simples, o \\(SLR(1)\\) que veremos adiante.\n\n8.3.2.1 Construindo I₀\nVamos construir o estado I₀ para nossa gramática aumentada com três regras (E' → E, E → E + n, E → n) aplicando a lógica completa da criação do conjunto de itens e da operação CLOSURE do \\(LR(1)\\).\n1. O Ponto de Partida (O Núcleo de I₀): todo o processo começa com um único item, derivado da primeira regra da gramática aumentada. O lookahead inicial é sempre $, o marcador de fim de entrada. Como nossa primeira regra é E' → E, o ponto está no início da produção e o lookahead é $, logo:\n\nnosso conjunto inicial será: { [E' → •E, $] }. Para enfatizar este conjunto diz que: Estamos no início de tudo. Esperamos encontrar uma estrutura que corresponda a uma Expressão (E), e depois dela, esperamos o fim da entrada ($).\n\n2. Primeira Aplicação da Regra CLOSURE: o algoritmo analisa o item no nosso conjunto:\n\nO item está no formato [A → α • B β, a], onde A = E', α = ε a string vazia, B = E, β = ε a string vazia, e a = $. Neste caso, o algoritmo vê que o ponto • está antes do não-terminal E. A regra CLOSURE é acionada.\nPrecisamos adicionar as produções de E. Para calcular o lookahead dos novos itens, calculamos FIRST(βa) que como β = ε, calculamos o FIRST(ε$) = {$}.\n\nPortanto, adicionamos os itens correspondentes às produções de E, com $ como lookahead. Na nossa gramática temos duas produções para E (E → E + n e E → n). Para criar os novos itens, o ponto (•) será sempre colocado no início das produções quando elas forem adicionadas durante a operação CLOSURE. Isso ocorre porque o ponto (•) representa o estado de ainda não começamos a reconhecer esta regra. logo:\n\n[E → •E + n, $];\n[E → •n, $].\n\nNosso conjunto de itens I₀ cresceu. Agora temos:\n\n[E' → •E, $];\n[E → •E + n, $];\n[E → •n, $].\n\n3. Segunda Aplicação da Regra CLOSURE (A Recursividade): a operação CLOSURE é recursiva. O algoritmo deve reexaminar o conjunto para ver se os novos itens adicionados exigem mais expansões. O procedimento será:\n\nanalisamos o novo item [E → •E + n, $];\nnovamente, o ponto • está antes do não-terminal E. A regra CLOSURE é acionada novamente.\ndesta vez, o item está na forma [A → α • B β, a], na qual A = E, α = ε, B = E, β = + n, e a = $.\ncalculamos o lookahead para as produções de E: FIRST(βa), neste caso, FIRST(+ n $) = {+}.\n\nComo o lookahead que encontramos é +, adicionamos às produções de E com este novo lookahead:\n\n[E → •E + n, +];\n[E → •n, +];\n\nA atenta leitora deve notar que estes novos itens foram adicionados ao mesmo conjunto I₀ que está em construção. A operação CLOSURE continua analisando todos os itens do conjunto, incluindo os recém-adicionados, até que nenhuma nova adição seja possível.\n4. Verificação de Estabilidade: o algoritmo continua e analisa os itens recém-adicionados (por exemplo, [E → •E + n, +]). Se aplicarmos a regra CLOSURE novamente, o lookahead calculado será FIRST(+ n +) = {+}. Os itens que seriam gerados ([E → •E + n, +] e [E → •n, +]) já estão no conjunto. Como nenhuma nova informação é adicionada, o processo CLOSURE para. Neste cenário a atenta leitora pode dizer que o conjunto está fechado e estável.\nO conjunto estado I₀ completo, que representa o ponto de partida do nosso parser \\(LR(1)\\), é o conjunto final de itens que calculamos:\n\n[E' → •E, $]\n[E → •E + n, $]\n[E → •n, $]\n[E → •E + n, +]\n[E → •n, +]\n\nEste estado indica que, no início da análise, o parser espera ver uma estrutura E que pode ser seguida pelo fim da entrada ($) ou por um operador +.\n\n\n\n8.3.3 Passo 3: A Operação GOTO e a Criação de Estados\nSe a operação CLOSURE indica tudo o que é possível em um único estado, a operação GOTO mostra como se mover entre os estados. A operação GOTO irá responder à pergunta: Se estamos no estado I e o próximo símbolo da entrada é X, para qual novo estado de conhecimento vértices vamos? Para isso, a operação GOTO(I, X) é um processo de duas etapas:\n\nAvanço do Ponto: primeiro, criamos um novo conjunto de itens pegando todos os itens no estado I nos quais o ponto • está imediatamente antes do símbolo X. Para cada um desses itens, movemos o ponto uma posição para a direita, passando por cima de X. Isso simboliza o consumo, o reconhecimento bem-sucedido do símbolo X.\nFechamento do Novo Conjunto: em seguida, aplicamos a operação CLOSURE a este novo conjunto de itens. Isso é fundamental para garantir que o novo estado para o qual chegamos também seja completo e contenha todas as produções que podem ser iniciadas a partir deste novo ponto de progresso.\n\nVamos aplicar esse processo passo a passo para construir os estados I₁ a I₄ a partir do nosso estado inicial I₀ completo.\nLembrando que o nosso estado I₀ foi definido como: { [E' → •E, $], [E → •E + n, $], [E → •n, $], [E → •E + n, +], [E → •n, +] }\n\nCálculo de GOTO(I₀, E) → I₁: estamos tentando responder à pergunta: O que acontece se, a partir do estado I₀, reconhecermos uma Expressão (E)? Neste caso, teremos:\n\nAvanço do Ponto: Procuramos em I₀ por itens com •E. Encontramos três: [E' → •E, $], [E → •E + n, $], e [E → •E + n, +]. Movemos o ponto em todos, criando o núcleo do novo estado: { [E' → E•, $], [E → E• + n, $], [E → E• + n, +] }\nFechamento (CLOSURE): Nos itens [E → E• + n, $] e [E → E• + n, +], o ponto está antes de um terminal (+), então a operação CLOSURE não adiciona novos itens. O conjunto está estável.\n\nEste novo conjunto é o nosso Estado I₁:\n\n[E' → E•, $] (Item de aceitação)\n[E → E• + n, $]\n[E → E• + n, +]\n\nInterpretação de I₁: vértices acabamos de analisar uma Expressão. Se o próximo símbolo for $, a análise termina (aceitação). Se for +, continuaremos a análise.\nCálculo de GOTO(I₀, n) → I₂: agora perguntamos: E se, a partir de I₀, o símbolo que virmos for um n?, teremos:\n\nAvanço do Ponto: procuramos em I₀ por itens com •n. Encontramos dois: [E → •n, $] e [E → •n, +]. Movemos o ponto em ambos: { [E → n•, $], [E → n•, +] }\nFechamento (CLOSURE): o ponto está no final da produção em ambos os itens. Nenhuma expansão é necessária.\n\nO conjunto está estável. Este é o nosso Estado I₂:\n\n[E → n•, $] (Item de redução)\n[E → n•, +] (Item de redução)\n\nInterpretação de I₂: vértices acabamos de analisar um n. O parser deve agora reduzir n para E se o próximo símbolo for $ ou +.\nCálculo de GOTO(I₁, +) → I₃: continuamos o processo. O que acontece se estivermos no estado I₁ e virmos um +?\n\nAvanço do Ponto: procuramos em I₁ por itens com •+. Encontramos dois: [E → E• + n, $] e [E → E• + n, +]. Movemos o ponto: {[E → E + •n, $], [E → E + •n, +]}\nFechamento (CLOSURE): O ponto está antes do terminal n. Nenhuma expansão é necessária.\n\nO conjunto está estável. Este é o nosso Estado I₃:\n\n[E → E + •n, $]\n[E → E + •n, +]\n\nInterpretação de I₃: Vimos uma Expressão seguida de +. Agora, esperamos um n para completar a regra, e após a redução, o símbolo seguinte poderá ser $ ou +.\nCálculo de GOTO(I₃, n) → I₄: finalmente, o que acontece se estivermos em I₃ e virmos um n?\n\nAvanço do Ponto: Procuramos em I₃ por itens com •n: [E → E + •n, $] e [E → E + •n, +]. Movemos o ponto: { [E → E + n•, $], [E → E + n•, +] }\nFechamento (CLOSURE): O ponto está no final. Nenhuma expansão.\n\nO conjunto está estável. Este é o nosso Estado I₄:\n\n[E → E + n•, $] (Item de redução)\n[E → E + n•, +] (Item de redução)\n\nInterpretação de I₄: “Vimos a sequência E + n. O parser deve agora reduzir E + n para E se o próximo símbolo for $ ou +.”\n\nEste processo continua até que nenhum novo estado possa ser criado. A operação GOTO constrói efetivamente o autômato de estados, que é o mapa completo que o parser usará para navegar pela análise da entrada.\n\n8.3.3.1 O Autômato LR(1): A Máquina de Estados por Trás do parser\nO processo que acabamos de ver nas seções de CLOSURE e GOTO não é um exercício puramente abstrato. Na verdade, estamos construindo o componente central de nosso parser: um Autômato Finito Determinístico (AFD), também conhecido como máquina de estados finitos. É este autômato que irá guiar todas as decisões de shift do analisador. Para entender isso, a atenta leitora deve fazer a seguinte associação:\n\nOs Estados: cada conjunto de itens \\(LR(1)\\) que calculamos (\\(I_0\\), \\(I_1\\), \\(I_2\\), etc.) corresponde a um único estado no nosso autômato. Cada estado encapsula todo o conhecimento que o parser tem sobre o progresso da análise sintática até aquele momento. Ou seja, quais regras podem estar sendo reconhecidas e o que se espera ver a seguir. \\(I_0\\) é, por definição, o estado inicial do autômato.\nAs Transições: A função \\(GOTO(I_i, X) = I_j\\) define formalmente as transições entre os estados. Se o autômato está no estado \\(I_i\\) e o próximo símbolo na entrada, ou no topo da pilha após uma redução, é \\(X\\), ele se move para o estado \\(I_j\\). As operações de shift do parser nada mais são do que seguir essas transições com os terminais da entrada.\n\nNo nosso exemplo, a operação \\(GOTO(I_0, n) = I_2\\) significa que há uma transição do estado inicial \\(I_0\\) para o estado \\(I_2\\) ao ler o símbolo \\(n\\). O diagrama de transição que veremos mais adiante é a visualização exata deste autômato.\nO parser \\(LR(1)\\) não é apenas um Autômato Finito Determinístico, ele é um Autômato de Pilha Determinístico (APD). A diferença é fundamental:\n\nO Autômato Finito: o conjunto de estados \\(I\\) e transições \\(GOTO\\) atua como a “unidade de controle” ou o “cérebro” do parser. Ele olha para o estado atual e o próximo símbolo e decide para onde ir.\nA Pilha serve como a memória do parser. Ela armazena o histórico de estados visitados. Sem a pilha, o parser não teria como lembrar o aninhamento de estruturas — por exemplo, quantos parênteses foram abertos e ainda não fechados.\n\nQuando o parser executa uma ação de reduce \\(A \\rightarrow \\beta\\), ele desempilha um número de estados correspondente ao tamanho de \\(\\beta\\). O estado que então aparece no topo da pilha revela o contexto em que a regra foi encontrada, permitindo que o parser consulte a tabela GOTO (\\(GOTO[estado\\_do\\_topo, A]\\)) para saber qual o próximo estado.\nPortanto, um parser \\(LR(1)\\) é a combinação elegante de um Autômato Finito, que reconhece os padrões locais, com uma pilha, que gerencia a estrutura hierárquica da gramática.\n\n\n\n8.3.4 Passo 4: Construindo a Tabela ACTION/GOTO\nDepois de termos construído todos os conjuntos de itens (os estados I₀ a I₄), o passo final é traduzir essa informação em uma tabela de decisões. Esta tabela é o verdadeiro cérebro do parser, dizendo-lhe exatamente o que fazer a cada momento. Ela é dividida em duas partes:\n\nACTION: esta parte da tabela é consultada para símbolos terminais (n, +, $). Ela dita uma de quatro possíveis ações:\n\nsN (shift N): empilhe o terminal atual e vá para o estado N;\nrK (reduce K): reduza os símbolos no topo da pilha usando a produção de número K;\nacc (accept): a análise foi um sucesso;\ncélula vazia: indica um erro de sintaxe.\n\nGOTO: esta parte da tabela é consultada para símbolos não-terminais (E). Ela nos diz para qual estado ir após uma ação de reduce. Vamos preencher a tabela linha por linha, analisando cada um dos nossos estados (I₀ a I₄) e as transições GOTO que calculamos.\n\n\nPreenchendo a Linha do Estado 0 (I₀):\n\na transição GOTO(I₀, n) = I₂ resulta em ACTION[0, n] = s2;\na transição GOTO(I₀, E) = I₁ resulta em GOTO[0, E] = 1.\n\n\n\n\nEstado\nACTION\n\n\nGOTO\n\n\n\n\n\nn\n+\n$\nE\n\n\n0\ns2\n\n\n1\n\n\n\nPreenchendo a Linha do Estado 1 (I₁):\n\n\na transição GOTO(I₁, +) = I₃ resulta em ACTION[1, +] = s3;\no item [E' → E•, $] indica aceitação no lookahead $ ⇒ ACTION[1, $] = acc.\n\n\n\nEstado\nACTION\n\n\nGOTO\n\n\n\n\n\nn\n+\n$\nE\n\n\n1\n\ns3\nacc\n\n\n\n\n\n\nPreenchendo a Linha do Estado 2 (I₂):\n\no estado I₂ é { [E → n•, $], [E → n•, +] };\n\nambos são itens de redução para a produção E → n (produção nº 2);\n\no primeiro item, com lookahead $, gera a ação ACTION[2, $] = r2;\no segundo item, com lookahead +, gera a ação ACTION[2, +] = r2.\n\n\n\n\nEstado\nACTION\n\n\nGOTO\n\n\n\n\n\nn\n+\n$\nE\n\n\n2\n\nr2\nr2\n\n\n\n\nPreenchendo a Linha do Estado 3 (I₃):\n\na transição GOTO(I₃, n) = I₄ resulta em ACTION[3, n] = s4.\n\n\n\n\nEstado\nACTION\n\n\nGOTO\n\n\n\n\n\nn\n+\n$\nE\n\n\n3\ns4\n\n\n\n\n\n\nPreenchendo a Linha do Estado 4 (I₄):\n\no estado I₄ é { [E → E + n•, $], [E → E + n•, +] };\nambos são itens de redução para a produção E → E + n (produção nº 1);\no primeiro item, com lookahead $, gera a ação ACTION[4, $] = r1;\no segundo item, com lookahead +, gera a ação ACTION[4, +] = r1.\n\n\n\n\nEstado\nACTION\n\n\nGOTO\n\n\n\n\n\nn\n+\n$\nE\n\n\n4\n\nr1\nr1\n\n\n\n\nTabela Consolidada: juntando todas as linhas que preenchemos, obtemos a tabela completa e canônica, que guiará o parser de forma inequívoca durante a análise de qualquer string de entrada.\n\n\n\nEstado\nACTION\n\n\nGOTO\n\n\n\n\n\nn\n+\n$\nE\n\n\n0\ns2\n\n\n1\n\n\n1\n\ns3\nacc\n\n\n\n2\n\nr2\nr2\n\n\n\n3\ns4\n\n\n\n\n\n4\n\nr1\nr1\n\n\n\n\n\nUma vez definidos os estados e as transições, podemos construir o diagrama de transição do nosso autômato \\(LR(1)\\) como pode ser visto na Figure 8.3.\n\n\n\n\n\n\nDiagrama de transição do autômato LR(1)\n\n\n\n\nFigure 8.3",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parsers $LR(1)$: Análise Sintática _bottom-up_</span>"
    ]
  },
  {
    "objectID": "07-parserLR1.html#conflitos-em-parsers-lr1-uma-análise-detalhada",
    "href": "07-parserLR1.html#conflitos-em-parsers-lr1-uma-análise-detalhada",
    "title": "8  Parsers \\(LR(1)\\): Análise Sintática bottom-up",
    "section": "8.4 Conflitos em Parsers \\(LR(1)\\): Uma Análise Detalhada",
    "text": "8.4 Conflitos em Parsers \\(LR(1)\\): Uma Análise Detalhada\nApesar de seu poder, os parsers \\(LR(1)\\) podem encontrar situações em que a próxima ação não é única. Esses cenários, chamados de conflitos, são detectados durante a construção da tabela ACTION. Um conflito ocorre quando uma única célula da tabela (ACTION[estado, terminal]) precisa conter mais de uma ação válida. Isso significa que a gramática, como está escrita, é ambígua para o algoritmo \\(LR(1)\\).\n\n8.4.1 Conflito Shift-Reduce\nEsta é a forma mais comum de conflito. Um conflito Shift-Reduce ocorre quando o parser, em um determinado estado e com um determinado símbolo de lookahead, tem uma escolha válida entre empilhar esse símbolo (shift) e reduzir uma regra que já está completa no topo da pilha (reduce).\nCausa Raiz: um conflito de shift-reduce é identificado se, em um mesmo estado I, coexistem dois tipos de itens:\n\nUm item de redução: [A → α•, a], onde o ponto está no final da produção;\nUm item de shift: [B → β•aγ, b], onde o ponto está antes do mesmo terminal a que é o lookahead do item de redução.\n\nSe o próximo símbolo na entrada for a, o parser não sabe se deve reduzir por A → α ou empilhar a.\n\n8.4.1.1 Análise do Exemplo Clássico: O “dangling else”\nO exemplo clássico é a ambiguidade do else em linguagens como C ou Java. Considere esta gramática simplificada:\nSTMT → if EXPR then STMT\n | if EXPR then STMT else STMT\nAgora, imagine que o parser analisou a entrada if EXPR then STMT e o próximo símbolo no buffer é else. Ele se encontrará em um estado, digamos Ik, que conterá (entre outros) os seguintes itens:\n\n[STMT → if EXPR then STMT •, else]\n[STMT → if EXPR then STMT • else STMT, $]\n\nVamos analisar a situação quando o lookahead é else:\n\nO item 1 está completo (• no final). Ele diz ao parser: “A regra STMT → if EXPR then STMT já foi vista. Se o próximo token for else, uma ação válida é reduzir o que temos na pilha.” Isso implicaria que o if-then interno é uma declaração completa por si só.\nO item 2 tem o ponto antes de else. Ele diz ao parser: “Se o próximo token for else, uma ação válida é empilhá-lo (shift) e continuar a análise para formar uma declaração if-then-else.”\n\nO Conflito: A célula ACTION[Ik, else] precisa conter tanto uma ação de reduce (baseada no item 1) quanto uma de shift (baseada no item 2). O parser está paralisado: o else pertence ao if mais interno (shift) ou o if interno já acabou (reduce)?\n\n\n\n8.4.2 Conflito reduce-Reduce\nEste conflito é menos comum, mas geralmente indica um problema mais sério na gramática.\nDefinição: Ocorre quando o parser, em um mesmo estado e para o mesmo símbolo de lookahead, encontra duas ou mais regras completas diferentes que poderiam ser reduzidas.\nCausa Raiz: Um conflito de reduce-reduce é identificado se, em um mesmo estado I, existem dois ou mais itens de redução distintos que compartilham o mesmo símbolo de lookahead.\n\n[A → α•, a]\n[B → β•, a]\n\nSe o próximo símbolo na entrada for a, o parser não sabe se deve reduzir usando a regra A → α ou a regra B → β.\n\n8.4.2.1 Análise do Exemplo\nConsidere uma gramática ambígua onde uma expressão pode ser tanto um comando quanto um valor:\nS → C | V\nC → id\nV → id\nApós o parser ler um id, ele chegará a um estado Ik. A operação CLOSURE fará com que este estado contenha ambos os itens completos (supondo que o lookahead seja $, por exemplo):\n\n[C → id•, $]\n[V → id•, $]\n\nO Conflito: Quando o parser estiver no estado Ik e o lookahead for $, ele terá duas opções de redução:\n\nO item 1 diz: “Reduza usando a regra C → id”.\nO item 2 diz: “Reduza usando a regra V → id”.\n\nA célula ACTION[Ik, $] precisa conter duas ações de reduce diferentes. Isso é impossível. A gramática, como definida, não fornece contexto suficiente para o parser saber se id deveria ser um Comando ou um Valor.\n\n\n\n8.4.3 Resolução de Conflitos\nQuando conflitos surgem durante a construção da tabela ACTION, precisamos estabelecer regras de decisão manuais. A resolução envolve três etapas: identificação, análise e aplicação de uma estratégia de resolução.\n\n8.4.3.1 Identificação de Conflitos na Construção da Tabela\nDurante a construção da tabela ACTION, um conflito ocorre quando tentamos preencher uma célula que já possui uma ação. Por exemplo:\n\nConflito shift-Reduce: A célula ACTION[i, a] precisa conter tanto s_j quanto r_k\nConflito reduce-Reduce: A célula ACTION[i, a] precisa conter r_k e r_m onde k ≠ m\n\n\n\n8.4.3.2 Estratégias Manuais de Resolução\n\nPreferência por shift (Estratégia Padrão): quando há um conflito shift-reduce, geralmente preferimos o shift. Esta estratégia resolve naturalmente o problema clássico do “dangling else”, o else sempre se associa ao if mais próximo.\nOrdenação por Precedência: atribuímos níveis de precedência aos operadores e regras:\n\n\nOperadores de multiplicação/divisão: precedência \\(2\\);\nOperadores de adição/subtração: precedência \\(1\\);\nEm conflito, escolhemos a ação com maior precedência.\n\n\nAssociatividade: para operadores de mesma precedência:\n\n\nAssociatividade à esquerda: preferir reduce;\nAssociatividade à direita: preferir shift.\n\n\nResolução por Análise de Contexto: examine os itens \\(LR(1)\\) que causam o conflito. Se um item tem lookahead mais específico ou representa uma derivação mais provável sintaticamente, priorize-o.\n\n\n\n8.4.3.3 Exemplo Prático: Resolvendo o “Dangling Else”\nConsidere o estado com conflito:\n\nItem 1: [STMT → if EXPR then STMT •, else] (sugere reduce);\nItem 2: [STMT → if EXPR then STMT • else STMT, $] (sugere shift);\n\nResolução Manual: marcamos na tabela que, para ACTION[estado, else], sempre escolhemos shift. Isso garante que o else se associe ao if mais interno.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parsers $LR(1)$: Análise Sintática _bottom-up_</span>"
    ]
  },
  {
    "objectID": "07-parserLR1.html#implementação-do-lr1em-python",
    "href": "07-parserLR1.html#implementação-do-lr1em-python",
    "title": "8  Parsers \\(LR(1)\\): Análise Sintática bottom-up",
    "section": "8.5 Implementação do LR(1)em Python",
    "text": "8.5 Implementação do LR(1)em Python\nA seguir, uma implementação completa de um parser \\(LR(1)\\) com resolução de conflitos:\nfrom typing import List, Tuple, Dict, Set, Optional\nfrom dataclasses import dataclass\nfrom collections import defaultdict\n\n@dataclass(frozen=True)\nclass LR1Item:\n    \"\"\"Representa um item LR(1): [A → α•β, a]\"\"\"\n    lhs: str    # Lado esquerdo da produção\n    before_dot: tuple  # Símbolos antes do ponto\n    after_dot: tuple  # Símbolos após o ponto\n    lookahead: str   # Símbolo de lookahead\n \n    def __str__(self):\n        before = ' '.join(self.before_dot) if self.before_dot else ''\n        after = ' '.join(self.after_dot) if self.after_dot else ''\n        return f\"[{self.lhs} → {before}•{after}, {self.lookahead}]\"\n\n@dataclass\nclass Production:\n    \"\"\"Representa uma produção da gramática\"\"\"\n    lhs: str\n    rhs: tuple\n\nclass LR1Parser:\n    def __init__(self):\n        self.grammar = {}\n        self.terminals = set()\n        self.nonterminals = set()\n        self.start_symbol = None\n        self.original_start = None\n        self.productions = []\n        self.first_sets = {}\n        self.states = []\n        self.action_table = {}\n        self.goto_table = {}\n \n    def add_production(self, lhs: str, rhs: List[str]):\n        \"\"\"Adiciona uma produção à gramática\"\"\"\n        if lhs not in self.grammar:\n            self.grammar[lhs] = []\n \n        rhs_tuple = tuple(rhs)\n        self.grammar[lhs].append(rhs_tuple)\n        self.productions.append(Production(lhs, rhs_tuple))\n \n        self.nonterminals.add(lhs)\n \n        for symbol in rhs:\n            if symbol not in self.grammar:\n                self.terminals.add(symbol)\n \n        if not self.start_symbol:\n            self.start_symbol = lhs\n            self.original_start = lhs\n\n    def augment_grammar(self):\n        \"\"\"Aumenta a gramática com S' → S\"\"\"\n        if self.original_start is None:\n            raise ValueError(\"Grammar not set\")\n        new_start = self.original_start + \"'\"\n        self.add_production(new_start, [self.original_start])\n        self.start_symbol = new_start\n\n    def compute_first(self):\n        \"\"\"Calcula os conjuntos FIRST para todos os não-terminais de forma iterativa\"\"\"\n        self.first_sets = {nt: set() for nt in self.nonterminals}\n        \n        changed = True\n        while changed:\n            changed = False\n            for nt in self.nonterminals:\n                old_size = len(self.first_sets[nt])\n                \n                for rhs in self.grammar.get(nt, []):\n                    if not rhs:  # Produção vazia\n                        self.first_sets[nt].add('ε')\n                        continue\n                    \n                    # Para cada símbolo da produção\n                    all_have_epsilon = True\n                    for sym in rhs:\n                        if sym in self.terminals:\n                            self.first_sets[nt].add(sym)\n                            all_have_epsilon = False\n                            break\n                        else:  # É não-terminal\n                            # Adiciona FIRST(sym) - {ε}\n                            self.first_sets[nt].update(self.first_sets[sym] - {'ε'})\n                            if 'ε' not in self.first_sets[sym]:\n                                all_have_epsilon = False\n                                break\n                    \n                    # Se todos os símbolos derivam ε, adiciona ε\n                    if all_have_epsilon:\n                        self.first_sets[nt].add('ε')\n                \n                # Verifica se houve mudança\n                if len(self.first_sets[nt]) &gt; old_size:\n                    changed = True\n\n    def calculate_first(self, symbols: tuple) -&gt; Set[str]:\n        \"\"\"Calcula o conjunto FIRST para uma sequência de símbolos\"\"\"\n        if not symbols:\n            return {'ε'}\n \n        first = set()\n        \n        for sym in symbols:\n            if sym in self.terminals:\n                first.add(sym)\n                break\n            else:  # É não-terminal\n                first.update(self.first_sets[sym] - {'ε'})\n                if 'ε' not in self.first_sets[sym]:\n                    break\n        else:\n            # Todos os símbolos derivam ε\n            first.add('ε')\n \n        return first\n\n    def closure(self, items: Set[LR1Item]) -&gt; Set[LR1Item]:\n        \"\"\"Calcula o fechamento de um conjunto de itens LR(1)\"\"\"\n        closure_set = set(items)\n        changed = True\n \n        while changed:\n            changed = False\n            new_items = set()\n \n            for item in list(closure_set):\n                # Se não há símbolos após o ponto, continue\n                if not item.after_dot:\n                    continue\n \n                next_symbol = item.after_dot[0]\n                # Se o próximo símbolo não é não-terminal, continue\n                if next_symbol not in self.nonterminals:\n                    continue\n \n                # Calcula β (resto após o símbolo)\n                beta = item.after_dot[1:]\n                # Calcula FIRST(βa)\n                beta_a = beta + (item.lookahead,)\n                lookaheads = self.calculate_first(beta_a)\n \n                # Para cada produção do não-terminal\n                for production in self.grammar.get(next_symbol, []):\n                    # Para cada lookahead em FIRST(βa)\n                    for la in lookaheads:\n                        # O marcador 'ε' nunca é um lookahead real. \n                        # A lógica de calculate_first já garante que o lookahead correto (item.lookahead)\n                        # foi incluído no conjunto 'lookaheads' se 'beta' era anulável.\n                        if la == 'ε':\n                            continue\n\n                        new_item = LR1Item(\n                            lhs=next_symbol,\n                            before_dot=(),\n                            after_dot=production,\n                            lookahead=la\n                        )\n\n                        if new_item not in closure_set:\n                            new_items.add(new_item)\n                            changed = True\n                \n                closure_set.update(new_items)\n         return closure_set\n\n    def goto(self, items: Set[LR1Item], symbol: str) -&gt; Set[LR1Item]:\n        \"\"\"Calcula GOTO(items, symbol)\"\"\"\n        goto_set = set()\n \n        for item in items:\n            if item.after_dot and item.after_dot[0] == symbol:\n                new_item = LR1Item(\n                    lhs=item.lhs,\n                    before_dot=item.before_dot + (symbol,),\n                    after_dot=item.after_dot[1:],\n                    lookahead=item.lookahead\n                )\n                goto_set.add(new_item)\n \n        if not goto_set:\n            return set()\n \n        return self.closure(goto_set)\n\n    def build_states(self):\n        \"\"\"Constrói todos os estados do autômato LR(1)\"\"\"\n        self.compute_first()\n        self.terminals.add('$')\n\n        # Estado inicial: I₀ = CLOSURE({[S' → •S, $]})\n        augmented_prod = self.grammar[self.start_symbol][0]\n        start_item = LR1Item(\n            lhs=self.start_symbol,\n            before_dot=(),\n            after_dot=augmented_prod,\n            lookahead='$'\n        )\n \n        initial_state = self.closure({start_item})\n        self.states = [initial_state]\n \n        # Constrói os demais estados usando GOTO\n        i = 0\n        while i &lt; len(self.states):\n            current_state = self.states[i]\n \n            # Encontra todos os símbolos possíveis após o ponto\n            symbols = set()\n            for item in current_state:\n                if item.after_dot:\n                    symbols.add(item.after_dot[0])\n \n            # Cria novos estados via GOTO\n            for symbol in symbols:\n                new_state = self.goto(current_state, symbol)\n                if new_state and new_state not in self.states:\n                    self.states.append(new_state)\n \n            i += 1\n\n    def build_parsing_table(self):\n        \"\"\"Constrói a tabela ACTION/GOTO seguindo o algoritmo LR(1) canônico\"\"\"\n        conflicts = []\n \n        for i, state in enumerate(self.states):\n            # Para cada item no estado\n            for item in state:\n                if item.after_dot:\n                    # Item da forma [A → α•aβ, b] onde 'a' é terminal\n                    symbol = item.after_dot[0]\n                    if symbol in self.terminals:\n                        goto_state = self.goto(state, symbol)\n                        if goto_state in self.states:\n                            j = self.states.index(goto_state)\n                            key = (i, symbol)\n                            \n                            if key in self.action_table:\n                                existing = self.action_table[key]\n                                if existing[0] == 'reduce':\n                                    conflicts.append(f\"Conflito S/R em estado {i}, símbolo '{symbol}': preferindo shift\")\n                                self.action_table[key] = ('shift', j)\n                            else:\n                                self.action_table[key] = ('shift', j)\n                else:\n                    # Item da forma [A → α•, a] - item de redução\n                    if (item.lhs == self.start_symbol and \n                        item.before_dot == (self.original_start,) and \n                        item.lookahead == '$'):\n                        # Accept: [S' → S•, $]\n                        key = (i, '$')\n                        self.action_table[key] = ('accept', None)\n                    else:\n                        # Reduce: encontra o índice da produção\n                        for prod_idx, prod in enumerate(self.productions):\n                            if (prod.lhs == item.lhs and \n                                prod.rhs == item.before_dot):\n                                \n                                key = (i, item.lookahead)\n                                action = ('reduce', prod_idx)\n                                \n                                if key in self.action_table:\n                                    existing = self.action_table[key]\n                                    if existing[0] == 'shift':\n                                        conflicts.append(f\"Conflito S/R em estado {i}, símbolo '{item.lookahead}': mantendo shift\")\n                                        continue\n                                    elif existing[0] == 'reduce':\n                                        conflicts.append(f\"Conflito R/R em estado {i}, símbolo '{item.lookahead}': mantendo primeira redução\")\n                                        continue\n                                \n                                self.action_table[key] = action\n                                break\n\n            # Preenche GOTO para não-terminais\n            for symbol in self.nonterminals:\n                goto_state = self.goto(state, symbol)\n                if goto_state in self.states:\n                    j = self.states.index(goto_state)\n                    self.goto_table[(i, symbol)] = j\n \n        return conflicts\n\n    def print_states(self):\n        \"\"\"Imprime todos os estados construídos\"\"\"\n        for i, state in enumerate(self.states):\n            print(f\"\\nEstado I{i}:\")\n            for item in sorted(state, key=lambda x: (x.lhs, x.before_dot, x.after_dot, x.lookahead)):\n                print(f\"  {item}\")\n\n    def parse(self, input_tokens: List[str]) -&gt; bool:\n        \"\"\"Analisa uma string de entrada usando a tabela construída\"\"\"\n        stack = [0]  # Pilha de estados\n        symbols = []  # Pilha de símbolos para debug\n        input_tokens = input_tokens.copy()\n        input_tokens.append('$')\n        i = 0\n \n        print(f\"{'Passo':&lt;6} {'Pilha':&lt;20} {'Entrada':&lt;15} {'Ação':&lt;30}\")\n        print(\"-\" * 75)\n \n        step = 1\n        while True:\n            state = stack[-1]\n            token = input_tokens[i]\n \n            action = self.action_table.get((state, token))\n \n            if action is None:\n                print(f\"Erro: sem ação para estado {state} e token '{token}'\")\n                return False\n \n            stack_str = str(stack)\n            input_str = ' '.join(input_tokens[i:])\n \n            if action[0] == 'shift':\n                action_str = f\"shift para estado {action[1]}\"\n                print(f\"{step:&lt;6} {stack_str:&lt;20} {input_str:&lt;15} {action_str:&lt;30}\")\n                stack.append(action[1])\n                symbols.append(token)\n                i += 1\n \n            elif action[0] == 'reduce':\n                prod_idx = action[1]\n                prod = self.productions[prod_idx]\n                rhs_str = ' '.join(prod.rhs) if prod.rhs else 'ε'\n                prod_str = f\"{prod.lhs} → {rhs_str}\"\n                action_str = f\"reduce por {prod_str}\"\n                print(f\"{step:&lt;6} {stack_str:&lt;20} {input_str:&lt;15} {action_str:&lt;30}\")\n \n                # Remove estados e símbolos da pilha\n                for _ in range(len(prod.rhs)):\n                    if symbols:\n                        symbols.pop()\n                    if stack:\n                        stack.pop()\n \n                # Adiciona o não-terminal produzido\n                symbols.append(prod.lhs)\n \n                # Consulta GOTO\n                goto_state = self.goto_table.get((stack[-1], prod.lhs))\n                if goto_state is None:\n                    print(f\"Erro: sem entrada GOTO para estado {stack[-1]} e símbolo {prod.lhs}\")\n                    return False\n                stack.append(goto_state)\n \n            elif action[0] == 'accept':\n                print(f\"{step:&lt;6} {stack_str:&lt;20} {input_str:&lt;15} {'Accept!':&lt;30}\")\n                return True\n \n            step += 1\n            if step &gt; 100:  # Proteção contra loops infinitos\n                print(\"Limite de passos excedido\")\n                return False\n \n        return False\n\n# Exemplo de uso\nif __name__ == \"__main__\":\n    parser = LR1Parser()\n \n    # Define a gramática E → E + n | n\n    parser.add_production('E', ['E', '+', 'n'])\n    parser.add_production('E', ['n'])\n\n    # Aumenta a gramática\n    parser.augment_grammar()\n\n    # Constrói o parser\n    print(\"Construindo estados do autômato LR(1)...\")\n    parser.build_states()\n\n    print(f\"Estados construídos: {len(parser.states)}\")\n    \n    # Imprime todos os estados\n    parser.print_states()\n\n    # Constrói a tabela de parsing\n    print(\"\\nConstruindo tabela ACTION/GOTO...\")\n    conflicts = parser.build_parsing_table()\n\n    if conflicts:\n        print(f\"\\nConflitos encontrados: {len(conflicts)}\")\n        for conflict in conflicts:\n            print(f\" - {conflict}\")\n    else:\n        print(\"\\nNenhum conflito encontrado.\")\n \n    # Mostra as produções numeradas\n    print(\"\\nProduções da gramática aumentada:\")\n    for idx, prod in enumerate(parser.productions):\n        rhs_str = ' '.join(prod.rhs) if prod.rhs else 'ε'\n        print(f\" {idx}: {prod.lhs} → {rhs_str}\")\n\n    # Testa o parser\n    print(\"\\n\" + \"=\"*75)\n    print(\"Analisando 'n + n':\")\n    print(\"=\"*75)\n    result = parser.parse(['n', '+', 'n'])\n    print(f\"\\nResultado: {'Aceito' if result else 'Rejeitado'}\")\n    \n    print(\"\\n\" + \"=\"*75)\n    print(\"Analisando 'n + n + n':\")\n    print(\"=\"*75)\n    result = parser.parse(['n', '+', 'n', '+', 'n'])\n    print(f\"\\nResultado: {'Aceito' if result else 'Rejeitado'}\")",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parsers $LR(1)$: Análise Sintática _bottom-up_</span>"
    ]
  },
  {
    "objectID": "07-parserLR1.html#exercícios",
    "href": "07-parserLR1.html#exercícios",
    "title": "8  Parsers \\(LR(1)\\): Análise Sintática bottom-up",
    "section": "8.6 Exercícios",
    "text": "8.6 Exercícios\n\n8.6.1 Exercício 1: Gramática de Parênteses Balanceados\nDesenvolva um parser \\(LR(1)\\) para reconhecer strings de parênteses balanceados considerando a seguinte gramática:\nS → ( S ) S\nS → ε\n\nAumente a gramática adicionando a produção inicial S' → S.\nConstrua os estados do autômato \\(LR(1)\\):    - Calcule o estado inicial \\(I_0\\) usando a operação CLOSURE    - Determine todos os estados usando a operação GOTO    - Liste todos os itens \\(LR(1)\\) em cada estado\nConstrua a tabela ACTION/GOTO completa\nTeste o parser com as seguintes strings:    - ();    - (());    - ()();    - (()(())).\n\nDica para começar:\nO estado inicial \\(I_0\\) é derivado do núcleo [S' → •S, $]. Ao aplicar a operação CLOSURE neste item, o lookahead para as produções de S é FIRST(ε$), que resulta em $. Como nenhuma das produções de S começa com um não-terminal, a operação CLOSURE se estabiliza rapidamente. O estado inicial \\(I_0\\) correto é:\n[S' → •S, $]\n[S → •( S ) S, $]\n[S → •, $]\nNote que esta gramática tem uma produção vazia (S → ε), o que significa que o item [S → •, $] já é um item de redução. Isso levará a uma ação de reduce no estado \\(I_0\\) para o lookahead $.\n\n\n8.6.2 Exercício 2: Expressões Aritméticas com as Quatro Operações\nDesenvolva um parser \\(LR(1)\\) para expressões aritméticas com as quatro operações básicas. A gramática para este exercício é a seguinte:\nE → E + T\nE → E - T\nE → T\nT → T * F\nT → T / F\nT → F\nF → ( E )\nF → em um\nTarefas:\n\nAumente a gramática com E' → E.\nConstrua pelo menos os 5 primeiros estados do autômato \\(LR(1)\\):    - Estado \\(I_0\\): estado inicial    - Estados resultantes de \\(GOTO(I_0, E)\\), \\(GOTO(I_0, T)\\), \\(GOTO(I_0, F)\\)    - Estados resultantes de \\(GOTO(I_0, ()\\) e \\(GOTO(I_0, em um)\\)\nIdentifique possíveis conflitos na gramática:    - Esta gramática não possui conflitos se construída corretamente    - A hierarquia E → T → F garante a precedência correta dos operadores\nConstrua a tabela ACTION/GOTO para os estados que você calculou.\nTrace a análise da expressão em um + em um * em um usando sua tabela parcial.\n\nEstrutura do Estado Inicial \\(I_0\\):\nPara construir o estado \\(I_0\\) canônico, lembre-se de aplicar a regra CLOSURE recursivamente. O processo começa com o núcleo [E' → •E, $], que gera um conjunto base de itens com lookahead $. Em seguida, cada item com um não-terminal após o ponto (•) irá gerar novos itens com lookaheads específicos.\nPor exemplo:\n\nO núcleo [E' → •E, $] gera o item [E → •E + T, $].\nEste item, por sua vez, ao aplicar CLOSURE, gera novos itens para as produções de E com lookahead FIRST(+ T $) = {+}.\n\nO estado \\(I_0\\) completo é grande. A seguir, uma amostra representativa de sua estrutura, mostrando os itens gerados por diferentes lookaheads:\n/* Itens gerados a partir do lookahead inicial '$' */\n[E' → •E, $]\n[E → •E + T, $]\n[E → •E - T, $]\n[E → •T, $]\n[T → •T * F, $]\n[T → •T / F, $]\n[T → •F, $]\n[F → •( E ), $]\n[F → •em um, $]\n\n/* Itens adicionais gerados a partir de [E → •E + T, $] com lookahead '+' */\n[E → •E + T, +]\n[E → •E - T, +]\n[E → •T, +]\n[T → •T * F, +]\n[T → •T / F, +]\n[T → •F, +]\n[F → •( E ), +]\n[F → •em um, +]\n\n/* E assim por diante para os lookaheads '-', '*', '/' ... */\nObservações sobre Precedência:\n\nA estrutura da gramática implementa naturalmente a precedência:   - * e / têm maior precedência (nível T)   - + e - têm menor precedência (nível E)\nA associatividade à esquerda é garantida pela recursão à esquerda nas produções\n\nString de Teste Adicional: ( em um + em um ) * em um\nPara esta string, observe como os parênteses forçam a avaliação da soma antes da multiplicação, demonstrando que a gramática respeita corretamente a precedência definida pelos parênteses.\n\n\n8.6.3 Exercício 3: Linguagem de Comandos\nDesenvolva um parser \\(LR(1)\\) para a mini-linguagem de comandos a seguir:\nPROG → CMDS\nCMDS → CMDS CMD | CMD\nCMD → if EXPR then CMD else CMD\n | if EXPR then CMD\n | while EXPR do CMD\n | begin CMDS end\n | assign id = EXPR\nEXPR → EXPR &lt; EXPR | EXPR + EXPR | id | em um\nTarefas:\n\nIdentifique o conflito Shift-Reduce causado pelo “dangling else”\nResolva o conflito dando preferência ao shift ;\nConstrua os primeiros estados do autômato \\(LR(1)\\).\n\nDica para o Estado Inicial (I₀):\nAssim como no exercício anterior, o estado I₀ será grande devido à propagação de lookaheads. O processo começa com [PROG → •CMDS, $]. A aplicação de CLOSURE neste item irá gerar itens para as produções de CMDS com lookahead $.\nPor sua vez, um item como [CMDS → •CMDS CMD, $] irá gerar novos itens para CMDS e CMD com lookaheads derivados de FIRST(CMD $). O conjunto FIRST(CMD) inclui terminais como {if, while, begin, assign}.\nAbaixo está uma amostra para ilustrar o conceito, mas a construção completa exigirá aplicar a regra CLOSURE até a estabilização:\n/* Itens base com lookahead '$' */\n[PROG → •CMDS, $]\n[CMDS → •CMDS CMD, $]\n[CMDS → •CMD, $]\n[CMD → •if EXPR then CMD else CMD, $]\n[CMD → •if EXPR then CMD, $]\n/* ...e as outras produções de CMD com lookahead '$' */\n\n/* Itens gerados a partir de [CMDS → •CMDS CMD, $], \n   com lookaheads de FIRST(CMD) = {if, while, begin, assign} */\n[CMDS → •CMDS CMD, if]\n[CMDS → •CMD, if]\n[CMD → •if EXPR then CMD else CMD, if]\n/* ...e assim por diante para 'if' e os outros terminais */\nString de Teste: if x &lt; 5 then begin assign y = x + 1 end\nA solução completa requer cuidado com conflitos e decisões sobre precedência e associatividade.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parsers $LR(1)$: Análise Sintática _bottom-up_</span>"
    ]
  },
  {
    "objectID": "08-parserSLR1.html",
    "href": "08-parserSLR1.html",
    "title": "9  Parsers \\(SLR(1)\\): A Ponte Entre Simplicidade e Poder",
    "section": "",
    "text": "9.1 A Simplificação Fundamental: de Itens \\(LR(1)\\) para Itens \\(LR(0)\\)\nA diferença mais importante entre \\(SLR(1)\\) e \\(LR(1)\\) está na estrutura dos itens. Enquanto o \\(LR(1)\\) usa itens da forma \\([A \\rightarrow \\alpha \\bullet \\beta, a]\\) com lookahead específico, o \\(SLR(1)\\) trabalha com itens \\(LR(0)\\) mais simples: \\([A \\rightarrow \\alpha \\bullet \\beta]\\), sem lookahead embutido.\nEsta simplificação tem consequências profundas:\nPara ilustrar, considere dois itens que seriam distintos no \\(LR(1)\\), como apresentado na figura Figure 9.1:\nNa Figura Figure 9.1, dois itens \\(LR(1)\\) com diferentes lookaheads se fundem em um único item \\(LR(0)\\) no \\(SLR(1)\\). Isso reduz a complexidade, mas pode introduzir conflitos.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Parsers $SLR(1)$: A Ponte Entre Simplicidade e Poder</span>"
    ]
  },
  {
    "objectID": "08-parserSLR1.html#a-simplificação-fundamental-de-itens-lr1-para-itens-lr0",
    "href": "08-parserSLR1.html#a-simplificação-fundamental-de-itens-lr1-para-itens-lr0",
    "title": "9  Parsers \\(SLR(1)\\): A Ponte Entre Simplicidade e Poder",
    "section": "",
    "text": "Redução drástica de estados: gramáticas que geram centenas de estados \\(LR(1)\\) podem ter apenas dezenas de estados \\(SLR(1)\\);\nConstrução mais simples: o algoritmo de construção é significativamente mais direto;\nMenor poder de reconhecimento: algumas gramáticas \\(LR(1)\\) geram conflitos no \\(SLR(1)\\).\n\n\n\n\n\n\n\n\nColapso de Itens \\(LR(1)\\) para LR(0)\n\n\n\n\nFigure 9.1",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Parsers $SLR(1)$: A Ponte Entre Simplicidade e Poder</span>"
    ]
  },
  {
    "objectID": "08-parserSLR1.html#o-papel-dos-conjuntos-follow",
    "href": "08-parserSLR1.html#o-papel-dos-conjuntos-follow",
    "title": "9  Parsers \\(SLR(1)\\): A Ponte Entre Simplicidade e Poder",
    "section": "9.2 O Papel dos Conjuntos \\(FOLLOW\\)",
    "text": "9.2 O Papel dos Conjuntos \\(FOLLOW\\)\nSe os itens \\(LR(0)\\) não carregam lookahead, como o parser decide quando reduzir? A resposta para esta questão pode ser encontrada nos conjuntos \\(FOLLOW\\). Para cada não-terminal \\(A\\) na gramática, \\(FOLLOW(A)\\) contém todos os terminais que podem aparecer imediatamente após \\(A\\) em alguma forma sentencial.\nComo vimos antes no Capítulo Chapter 7, a construção do conjunto \\(FOLLOW\\) segue regras precisas:\n\nse \\(S\\) é o símbolo inicial, então \\(\\$ \\in FOLLOW(S)\\);\nse existe uma produção \\(A \\rightarrow \\alpha B \\beta\\), então \\(FIRST(\\beta) - \\{\\epsilon\\} \\subseteq FOLLOW(B)\\);\nse existe uma produção \\(A \\rightarrow \\alpha B\\) ou \\(A \\rightarrow \\alpha B \\beta\\) onde \\(\\epsilon \\in FIRST(\\beta)\\), então \\(FOLLOW(A) \\subseteq FOLLOW(B)\\).\n\nEsta informação global substitui a propagação local de lookaheads do \\(LR(1)\\), tornando a construção muito mais eficiente.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Parsers $SLR(1)$: A Ponte Entre Simplicidade e Poder</span>"
    ]
  },
  {
    "objectID": "08-parserSLR1.html#o-algoritmo-de-construção-slr1",
    "href": "08-parserSLR1.html#o-algoritmo-de-construção-slr1",
    "title": "9  Parsers \\(SLR(1)\\): A Ponte Entre Simplicidade e Poder",
    "section": "9.3 O Algoritmo de Construção \\(SLR(1)\\)",
    "text": "9.3 O Algoritmo de Construção \\(SLR(1)\\)\nO processo de construção de um parser \\(SLR(1)\\) segue três fases distintas:\n\n9.3.1 Fase 1: Preparação da Gramática\nA primeira fase estabelece os fundamentos necessários para a construção do parser. Começamos aumentando a gramática original adicionando uma nova produção \\(S' \\rightarrow S\\), onde \\(S\\) é o símbolo inicial original. Este aumento nos permite detectar a aceitação da entrada de forma inequívoca. Em seguida, calculamos os conjuntos \\(FIRST\\) para todos os símbolos da gramática, que nos informam quais terminais podem aparecer no início de qualquer sequência derivável de um símbolo. Por fim, para o \\(SLR(1)\\), calculamos os conjuntos \\(FOLLOW\\) para todos os não-terminais, que contêm os terminais que podem aparecer imediatamente após cada não-terminal em qualquer forma sentencial.\n\n\n9.3.2 Fase 2: Construção da Coleção Canônica de Itens \\(LR(0)\\)\nEsta fase constrói o autômato finito que reconhece os prefixos viáveis da gramática. Começamos com o estado inicial \\(I_0\\), que contém o closure do item \\([S' \\rightarrow \\bullet S]\\). A função \\(CLOSURE\\) expande um conjunto de itens adicionando todos os itens que podem ser alcançados quando encontramos um não-terminal imediatamente após o ponto. Para cada estado e cada símbolo gramatical, aplicamos a função \\(GOTO\\), que move o ponto sobre o símbolo e calcula o closure do resultado. Novos estados são adicionados à coleção até que nenhum novo estado seja descoberto. Note que, diferentemente do \\(LR(1)\\), trabalhamos apenas com itens \\(LR(0)\\) sem lookahead embutido, o que resulta em significativamente menos estados.\n\n\n9.3.3 Fase 3: Construção das Tabelas ACTION e GOTO\nA fase final transforma o autômato de itens \\(LR(0)\\) nas tabelas de parsing. Para cada estado, examinamos seus itens: se um item tem a forma \\([A \\rightarrow \\alpha \\bullet t\\beta]\\) onde \\(t\\) é terminal, geramos uma ação de shift; se um item tem a forma \\([A \\rightarrow \\alpha \\bullet]\\) (ponto no final), geramos ações de reduce para todos os terminais em \\(FOLLOW(A)\\) - esta é a diferença fundamental do \\(SLR(1)\\) em relação ao \\(LR(1)\\), usando informação global ao invés de lookaheads específicos; o item especial \\([S' \\rightarrow S\\bullet]\\) gera a ação de aceitação. A tabela \\(GOTO\\) é preenchida para não-terminais, indicando para qual estado transitar após uma redução. Esta abordagem pode gerar conflitos em gramáticas que seriam \\(LR(1)\\) mas não \\(SLR(1)\\); os conjuntos \\(FOLLOW\\) são uma aproximação conservadora dos lookaheads verdadeiramente necessários.\n\n\n9.3.4 Pseudocódigo do Algoritmo \\(SLR(1)\\)\nAs regras de criação de parsers \\(SLR(1)\\) podem ser formalizadas no seguinte pseudocódigo:\nALGORITMO CONSTRUIR_PARSER_SLR1(G)\n\n// ENTRADA: Uma gramática G\n// SAÍDA: As tabelas de parsing $SLR(1)$ ACTION e GOTO\n\n// Fase 1: Preparação\n1. Aumentar a gramática: adicionar S' → S\n2. Calcular conjuntos FIRST para todos os símbolos\n3. Calcular conjuntos FOLLOW para todos os não-terminais\n\n// Fase 2: Construção da Coleção Canônica de Itens LR(0)\n4. Inicializar C = {}\n5. I₀ = CLOSURE({[S' → •S]}) // Note: sem _lookahead_!\n6. Adicionar I₀ a C\n7. Marcar I₀ como \"não processado\"\n\n8. ENQUANTO houver um conjunto I em C marcado como \"não processado\":\n9.   Marcar I como \"processado\"\n10.   PARA CADA símbolo gramatical X:\n11.     J = GOTO(I, X)\n12.     SE J não for vazio E J não estiver em C:\n13.       Adicionar J a C\n14.       Marcar J como \"não processado\"\n15.     FIM SE\n16.   FIM PARA\n17. FIM ENQUANTO\n\n// Fase 3: Construção das Tabelas ACTION e GOTO\n18. PARA CADA estado i em C:\n19.   PARA CADA item em I_i:\n20.     // Caso 1: Item de _shift_\n21.     SE o item tem a forma [A → α•tβ], onde t é TERMINAL:\n22.       J = GOTO(I_i, t)\n23.       ACTION[i, t] = \"_shift_ j\", onde I_j = J\n24.     FIM SE\n    \n25.     // Caso 2: Item de Reduce (aqui está a diferença mais importante!)\n26.     SE o item tem a forma [A → α•] E A ≠ S':\n27.       PARA CADA terminal a em FOLLOW(A): // Usa FOLLOW!\n28.         ACTION[i, a] = \"reduce k\", onde k é o número da produção\n29.       FIM PARA\n30.     FIM SE\n    \n31.     // Caso 3: Aceitação\n32.     SE o item é [S' → S•]:\n33.       ACTION[i, $] = \"accept\"\n34.     FIM SE\n35.   FIM PARA\n  \n36.   // Tabela GOTO para não-terminais\n37.   PARA CADA não-terminal B:\n38.     SE GOTO(I_i, B) = I_j E j ≠ vazio:\n39.       GOTO[i, B] = j\n40.     FIM SE\n41.   FIM PARA\n42. FIM PARA\n\n// Funções Auxiliares\n\n43. FUNÇÃO CLOSURE(I): // Versão LR(0) - mais simples!\n44.   J = I\n45.   REPETIR\n46.     PARA CADA item [A → α•Bβ] em J:\n47.       PARA CADA produção B → γ:\n48.         Adicionar [B → •γ] a J // Sem _lookahead_!\n49.       FIM PARA\n50.     FIM PARA\n51.   ATÉ que nenhum novo item seja adicionado\n52.   RETORNAR J\n\n53. FUNÇÃO GOTO(I, X):\n54.   J = {}\n55.   PARA CADA item [A → α•Xβ] em I:\n56.     Adicionar [A → αX•β] a J\n57.   FIM PARA\n58.   RETORNAR CLOSURE(J)",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Parsers $SLR(1)$: A Ponte Entre Simplicidade e Poder</span>"
    ]
  },
  {
    "objectID": "08-parserSLR1.html#exemplo-guiado-construindo-um-parser-slr1-completo",
    "href": "08-parserSLR1.html#exemplo-guiado-construindo-um-parser-slr1-completo",
    "title": "9  Parsers \\(SLR(1)\\): A Ponte Entre Simplicidade e Poder",
    "section": "9.4 Exemplo Guiado: Construindo um Parser \\(SLR(1)\\) Completo",
    "text": "9.4 Exemplo Guiado: Construindo um Parser \\(SLR(1)\\) Completo\nPara que a esforçada leitora possa entender o processo de criação de parsers \\(SLR(1)\\), vamos construir um parser \\(SLR(1)\\) para a mesma gramática usada no Capítulo Chapter 8; isso vai permitir que a leitora veja as diferenças e semelhanças entre os dois tipos de parser\nE → E + n\nE → n\n\n9.4.1 Passo 1: Gramática Aumentada e Conjuntos Preliminares\nPrimeiro, aumentamos a gramática:\n0. E' → E\n1. E → E + n\n2. E → n\nAgora calculamos os conjuntos necessários:\nConjuntos \\(FIRST\\): são muito simples nesta gramática:\n\n\\(FIRST(E) = \\{n\\}\\);\n\\(FIRST(E') = \\{n\\}\\).\n\nConjuntos \\(FOLLOW\\): para calcular \\(FOLLOW\\), aplicamos as regras de criação do conjunto \\(FOLLOW\\) sistematicamente:\n\n\\(E'\\) é o símbolo inicial, então \\(\\$ \\in FOLLOW(E')\\);\nda produção \\(E' \\rightarrow E\\): como não há nada após \\(E\\), \\(FOLLOW(E') \\subseteq FOLLOW(E)\\), logo \\(\\$ \\in FOLLOW(E)\\);\nda produção \\(E \\rightarrow E + n\\): o símbolo \\(+\\) aparece após \\(E\\), então \\(+ \\in FOLLOW(E)\\).\n\nResultado:\n\n\\(FOLLOW(E') = \\{\\$\\}\\);\n\\(FOLLOW(E) = \\{\\$, +\\}\\).\n\n\n\n9.4.2 Passo 2: Construção dos Estados \\(LR(0)\\)\nEstado \\(I_0\\) (Estado Inicial): começamos com o núcleo \\(\\{[E' \\rightarrow \\bullet E]\\}\\) e aplicamos CLOSURE:\nNúcleo: [E' → •E]\n\nAplicando CLOSURE:\n- E está após •, então adicionamos produções de E:\n [E → •E + n]\n [E → •n]\n\nEstado I₀ completo:\n [E' → •E]\n [E → •E + n]\n [E → •n]\nA atenta leitora deve notar a diferença fundamental: no \\(LR(1)\\), teríamos múltiplas versões desses itens com diferentes lookaheads. No \\(SLR(1)\\), cada item aparece apenas uma vez.\nCalculando as Transições:\n\\(GOTO(I_0, E) = I_1\\):\nMovendo • sobre E nos itens relevantes:\n [E' → E•]\n [E → E• + n]\n\nEstado I₁:\n [E' → E•]\n [E → E• + n]\n\\(GOTO(I_0, n) = I_2\\):\nMovendo • sobre n:\n [E → n•]\n\nEstado I₂:\n [E → n•]\n\\(GOTO(I_1, +) = I_3\\):\nMovendo • sobre +:\n [E → E + •n]\n\nEstado I₃:\n [E → E + •n]\n\\(GOTO(I_3, n) = I_4\\):\nMovendo • sobre n:\n [E → E + n•]\n\nEstado $I_4$:\n [E → E + n•]\n\n\n9.4.3 Passo 3: Comparação com \\(LR(1)\\)\nA diferença é digna de nota. No \\(LR(1)\\) canônico, o estado \\(I_0\\) continha \\(5\\) itens devido aos diferentes lookaheads. No \\(SLR(1)\\), temos apenas \\(3\\). Esta economia se propaga por todos os estados:\n\n\n\nParser\nEstados Totais\nItens em \\(I_0\\)\n\n\n\n\n\\(LR(1)\\)\n5\n5\n\n\n\\(SLR(1)\\)\n5\n3\n\n\n\nNeste exemplo simples, o número de estados coincidiu, mas em gramáticas maiores, a diferença pode ser dramática. Um exemplo mais complexo, que exploraremos a seguir, deixará essa diferença mais evidente.\n\n\n9.4.4 Passo 4: Construção da Tabela ACTION/GOTO\nAgora construímos a tabela usando os conjuntos \\(FOLLOW\\):\nEstado 0 (\\(I_0\\)):\n\n\\([E \\rightarrow \\bullet n]\\) com • antes de \\(n\\) (terminal) → \\(ACTION[0, n] = s2\\);\n\\(GOTO(I_0, E) = I_1\\) → \\(GOTO[0, E] = 1\\).\n\nEstado 1 (\\(I_1\\)):\n\n\\([E' \\rightarrow E\\bullet]\\) é item de aceitação → \\(ACTION[1, \\$] = acc\\);\n\\([E \\rightarrow E\\bullet + n]\\) com • antes de \\(+\\) → \\(ACTION[1, +] = s3\\).\n\nEstado 2 (\\(I_2\\)):\n\n\\([E \\rightarrow n\\bullet]\\) é item de redução;\nUsamos \\(FOLLOW(E) = \\{\\$, +\\}\\);\n→ \\(ACTION[2, \\$] = r2\\) e \\(ACTION[2, +] = r2\\).\n\nEstado 3 (\\(I_3\\)):\n\n\\([E \\rightarrow E + \\bullet n]\\) com • antes de \\(n\\) → \\(ACTION[3, n] = s4\\).\n\nEstado 4 (\\(I_4\\)):\n\n\\([E \\rightarrow E + n\\bullet]\\) é item de redução;\nUsamos \\(FOLLOW(E) = \\{\\$, +\\}\\);\n→ \\(ACTION[4, \\$] = r1\\) e \\(ACTION[4, +] = r1\\).\n\n\n\n9.4.5 Tabela Final \\(SLR(1)\\)\n\n\n\nEstado\nACTION\n\n\nGOTO\n\n\n\n\n\nn\n+\n$\nE\n\n\n0\ns2\n\n\n1\n\n\n1\n\ns3\nacc\n\n\n\n2\n\nr2\nr2\n\n\n\n3\ns4\n\n\n\n\n\n4\n\nr1\nr1\n\n\n\n\nObservação: a tabela resultante é idêntica à tabela \\(LR(1)\\) para esta gramática! Isso ocorre porque esta gramática simples é \\(SLR(1)\\). A diferença aparece quando a gramática tem sutilezas que o \\(SLR(1)\\) não consegue capturar.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Parsers $SLR(1)$: A Ponte Entre Simplicidade e Poder</span>"
    ]
  },
  {
    "objectID": "08-parserSLR1.html#implementação-em-python",
    "href": "08-parserSLR1.html#implementação-em-python",
    "title": "9  Parsers \\(SLR(1)\\): A Ponte Entre Simplicidade e Poder",
    "section": "9.5 Implementação em Python",
    "text": "9.5 Implementação em Python\nfrom typing import Set, Dict, List, Tuple, Optional\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict\n\n@dataclass(frozen=True)\nclass LR0Item:\n  \"\"\"Item LR(0): [A → α•β]\"\"\"\n  lhs: str\n  before_dot: tuple\n  after_dot: tuple\n  \n  def __str__(self):\n    before = ' '.join(self.before_dot) if self.before_dot else ''\n    after = ' '.join(self.after_dot) if self.after_dot else ''\n    return f\"[{self.lhs} → {before}•{after}]\"\n\n@dataclass\nclass Production:\n  \"\"\"Produção da gramática\"\"\"\n  lhs: str\n  rhs: tuple\n\nclass SLR1Parser:\n  def __init__(self):\n    self.grammar = {}\n    self.terminals = set()\n    self.nonterminals = set()\n    self.start_symbol = None\n    self.original_start = None\n    self.productions = []\n    self.first_sets = {}\n    self.follow_sets = {}\n    self.states = []\n    self.action_table = {}\n    self.goto_table = {}\n  \n  def add_production(self, lhs: str, rhs: List[str]):\n    \"\"\"Adiciona uma produção à gramática\"\"\"\n    if lhs not in self.grammar:\n      self.grammar[lhs] = []\n    \n    rhs_tuple = tuple(rhs)\n    self.grammar[lhs].append(rhs_tuple)\n    self.productions.append(Production(lhs, rhs_tuple))\n    \n    self.nonterminals.add(lhs)\n    \n    for symbol in rhs:\n      if symbol not in self.grammar and symbol != 'ε':\n        self.terminals.add(symbol)\n    \n    if not self.start_symbol:\n      self.start_symbol = lhs\n      self.original_start = lhs\n  \n  def augment_grammar(self):\n    \"\"\"Aumenta a gramática com S' → S\"\"\"\n    new_start = self.original_start + \"'\"\n    self.add_production(new_start, [self.original_start])\n    self.start_symbol = new_start\n  \n  def compute_first(self):\n    \"\"\"Calcula conjuntos FIRST\"\"\"\n    self.first_sets = {nt: set() for nt in self.nonterminals}\n    \n    # Para terminais, FIRST é o próprio terminal\n    for t in self.terminals:\n      self.first_sets[t] = {t}\n    \n    changed = True\n    while changed:\n      changed = False\n      for nt in self.nonterminals:\n        old_size = len(self.first_sets[nt])\n        \n        for rhs in self.grammar.get(nt, []):\n          if not rhs or rhs == ('ε',):\n            self.first_sets[nt].add('ε')\n            continue\n          \n          all_have_epsilon = True\n          for sym in rhs:\n            if sym in self.terminals:\n              self.first_sets[nt].add(sym)\n              all_have_epsilon = False\n              break\n            else:\n              self.first_sets[nt].update(\n                self.first_sets[sym] - {'ε'}\n              )\n              if 'ε' not in self.first_sets[sym]:\n                all_have_epsilon = False\n                break\n          \n          if all_have_epsilon:\n            self.first_sets[nt].add('ε')\n        \n        if len(self.first_sets[nt]) &gt; old_size:\n          changed = True\n  \n  def compute_follow(self):\n    \"\"\"Calcula conjuntos FOLLOW\"\"\"\n    self.follow_sets = {nt: set() for nt in self.nonterminals}\n    \n    # $ está em FOLLOW do símbolo inicial\n    self.follow_sets[self.start_symbol].add('$')\n    \n    changed = True\n    while changed:\n      changed = False\n      \n      for nt in self.nonterminals:\n        for rhs in self.grammar.get(nt, []):\n          for i, symbol in enumerate(rhs):\n            if symbol in self.nonterminals:\n              old_size = len(self.follow_sets[symbol])\n              \n              # Símbolos após o não-terminal\n              beta = rhs[i + 1:]\n              \n              if beta:\n                # FIRST(β) - {ε} ⊆ FOLLOW(symbol)\n                first_beta = self.calculate_first_of_sequence(beta)\n                self.follow_sets[symbol].update(\n                  first_beta - {'ε'}\n                )\n                \n                # Se ε ∈ FIRST(β), então FOLLOW(nt) ⊆ FOLLOW(symbol)\n                if 'ε' in first_beta:\n                  self.follow_sets[symbol].update(\n                    self.follow_sets[nt]\n                  )\n              else:\n                # symbol está no final: FOLLOW(nt) ⊆ FOLLOW(symbol)\n                self.follow_sets[symbol].update(\n                  self.follow_sets[nt]\n                )\n              \n              if len(self.follow_sets[symbol]) &gt; old_size:\n                changed = True\n  \n  def calculate_first_of_sequence(self, symbols: tuple) -&gt; Set[str]:\n    \"\"\"Calcula FIRST de uma sequência de símbolos\"\"\"\n    if not symbols:\n      return {'ε'}\n    \n    result = set()\n    for sym in symbols:\n      if sym in self.terminals:\n        result.add(sym)\n        break\n      else:\n        result.update(self.first_sets[sym] - {'ε'})\n        if 'ε' not in self.first_sets[sym]:\n          break\n    else:\n      result.add('ε')\n    \n    return result\n  \n  def closure(self, items: Set[LR0Item]) -&gt; Set[LR0Item]:\n    \"\"\"Calcula CLOSURE de um conjunto de itens LR(0)\"\"\"\n    closure_set = set(items)\n    changed = True\n    \n    while changed:\n      changed = False\n      new_items = set()\n      \n      for item in closure_set:\n        if not item.after_dot:\n          continue\n        \n        next_symbol = item.after_dot[0]\n        if next_symbol not in self.nonterminals:\n          continue\n        \n        for production in self.grammar.get(next_symbol, []):\n          new_item = LR0Item(\n            lhs=next_symbol,\n            before_dot=(),\n            after_dot=production\n          )\n          \n          if new_item not in closure_set:\n            new_items.add(new_item)\n            changed = True\n      \n      closure_set.update(new_items)\n    \n    return closure_set\n  \n  def goto(self, items: Set[LR0Item], symbol: str) -&gt; Set[LR0Item]:\n    \"\"\"Calcula GOTO(items, symbol)\"\"\"\n    goto_set = set()\n    \n    for item in items:\n      if item.after_dot and item.after_dot[0] == symbol:\n        new_item = LR0Item(\n          lhs=item.lhs,\n          before_dot=item.before_dot + (symbol,),\n          after_dot=item.after_dot[1:]\n        )\n        goto_set.add(new_item)\n    \n    if not goto_set:\n      return set()\n    \n    return self.closure(goto_set)\n  \n  def build_states(self):\n    \"\"\"Constrói a coleção canônica de conjuntos de itens LR(0)\"\"\"\n    self.compute_first()\n    self.compute_follow()\n    self.terminals.add('$')\n    \n    # Estado inicial\n    augmented_prod = self.grammar[self.start_symbol][0]\n    start_item = LR0Item(\n      lhs=self.start_symbol,\n      before_dot=(),\n      after_dot=augmented_prod\n    )\n    \n    initial_state = self.closure({start_item})\n    self.states = [initial_state]\n    \n    # Constrói estados usando GOTO\n    i = 0\n    while i &lt; len(self.states):\n      current_state = self.states[i]\n      \n      # Símbolos possíveis após o ponto\n      symbols = set()\n      for item in current_state:\n        if item.after_dot:\n          symbols.add(item.after_dot[0])\n      \n      for symbol in symbols:\n        new_state = self.goto(current_state, symbol)\n        if new_state and new_state not in self.states:\n          self.states.append(new_state)\n      \n      i += 1\n  \n  def build_parsing_table(self):\n    \"\"\"Constrói a tabela ACTION/GOTO usando SLR(1)\"\"\"\n    conflicts = []\n    \n    for i, state in enumerate(self.states):\n      for item in state:\n        if item.after_dot:\n          # Item de _shift_: [A → α•aβ]\n          symbol = item.after_dot[0]\n          if symbol in self.terminals:\n            goto_state = self.goto(state, symbol)\n            if goto_state in self.states:\n              j = self.states.index(goto_state)\n              key = (i, symbol)\n              \n              if key in self.action_table:\n                existing = self.action_table[key]\n                if existing[0] == 'reduce':\n                  conflicts.append(\n                    f\"Conflito S/R em estado {i}, \"\n                    f\"símbolo '{symbol}': preferindo _shift_\"\n                  )\n                self.action_table[key] = ('_shift_', j)\n              else:\n                self.action_table[key] = ('_shift_', j)\n        else:\n          # Item de reduce: [A → α•]\n          if (item.lhs == self.start_symbol and \n            item.before_dot == (self.original_start,)):\n            # Accept\n            key = (i, '$')\n            self.action_table[key] = ('accept', None)\n          else:\n            # Reduce usando FOLLOW\n            for prod_idx, prod in enumerate(self.productions):\n              if (prod.lhs == item.lhs and \n                prod.rhs == item.before_dot):\n                \n                # Para cada símbolo em FOLLOW(A)\n                for symbol in self.follow_sets[item.lhs]:\n                  key = (i, symbol)\n                  action = ('reduce', prod_idx)\n                  \n                  if key in self.action_table:\n                    existing = self.action_table[key]\n                    if existing[0] == '_shift_':\n                      conflicts.append(\n                        f\"Conflito S/R em estado {i}, \"\n                        f\"símbolo '{symbol}': mantendo _shift_\"\n                      )\n                      continue\n                    elif existing[0] == 'reduce':\n                      if existing != action:\n                        conflicts.append(\n                          f\"Conflito R/R em estado {i}, \"\n                          f\"símbolo '{symbol}'\"\n                        )\n                      continue\n                  \n                  self.action_table[key] = action\n                break\n    \n    # Preenche GOTO para não-terminais\n    for i, state in enumerate(self.states):\n      for symbol in self.nonterminals:\n        goto_state = self.goto(state, symbol)\n        if goto_state in self.states:\n          j = self.states.index(goto_state)\n          self.goto_table[(i, symbol)] = j\n    \n    return conflicts\n  \n  def print_states(self):\n    \"\"\"Imprime os estados do autômato\"\"\"\n    for i, state in enumerate(self.states):\n      print(f\"\\nEstado I{i}:\")\n      for item in sorted(state, key=lambda x: str(x)):\n        print(f\" {item}\")\n  \n  def print_follow_sets(self):\n    \"\"\"Imprime os conjuntos FOLLOW\"\"\"\n    print(\"\\nConjuntos FOLLOW:\")\n    for nt in sorted(self.nonterminals):\n      symbols = sorted(self.follow_sets[nt])\n      print(f\" FOLLOW({nt}) = {{{', '.join(symbols)}}}\")\n  \n  def parse(self, input_tokens: List[str]) -&gt; bool:\n    \"\"\"Analisa uma string usando a tabela SLR(1)\"\"\"\n    stack = [0]\n    input_tokens = input_tokens.copy()\n    input_tokens.append('$')\n    i = 0\n    \n    print(f\"{'Passo':&lt;6} {'Pilha':&lt;20} {'Entrada':&lt;15} {'Ação':&lt;30}\")\n    print(\"-\" * 75)\n    \n    step = 1\n    while True:\n      state = stack[-1]\n      token = input_tokens[i]\n      \n      action = self.action_table.get((state, token))\n      \n      if action is None:\n        print(f\"Erro: sem ação para estado {state} e token '{token}'\")\n        return False\n      \n      stack_str = str(stack)\n      input_str = ' '.join(input_tokens[i:])\n      \n      if action[0] == '_shift_':\n        action_str = f\"_shift_ para estado {action[1]}\"\n        print(f\"{step:&lt;6} {stack_str:&lt;20} {input_str:&lt;15} {action_str:&lt;30}\")\n        stack.append(action[1])\n        i += 1\n        \n      elif action[0] == 'reduce':\n        prod_idx = action[1]\n        prod = self.productions[prod_idx]\n        rhs_str = ' '.join(prod.rhs) if prod.rhs else 'ε'\n        prod_str = f\"{prod.lhs} → {rhs_str}\"\n        action_str = f\"reduce por {prod_str}\"\n        print(f\"{step:&lt;6} {stack_str:&lt;20} {input_str:&lt;15} {action_str:&lt;30}\")\n        \n        # Remove estados da pilha\n        for _ in range(len(prod.rhs)):\n          if stack:\n            stack.pop()\n        \n        # Consulta GOTO\n        goto_state = self.goto_table.get((stack[-1], prod.lhs))\n        if goto_state is None:\n          print(f\"Erro: sem GOTO para estado {stack[-1]} e símbolo {prod.lhs}\")\n          return False\n        stack.append(goto_state)\n        \n      elif action[0] == 'accept':\n        print(f\"{step:&lt;6} {stack_str:&lt;20} {input_str:&lt;15} {'Accept!':&lt;30}\")\n        return True\n      \n      step += 1\n      if step &gt; 100:\n        print(\"Limite de passos excedido\")\n        return False\n    \n    return False\n\n# Exemplo de uso\nif __name__ == \"__main__\":\n  parser = SLR1Parser()\n  \n  # Define a gramática E → E + n | n\n  parser.add_production('E', ['E', '+', 'n'])\n  parser.add_production('E', ['n'])\n  \n  # Aumenta a gramática\n  parser.augment_grammar()\n  \n  # Constrói o parser\n  print(\"Construindo estados do autômato SLR(1)...\")\n  parser.build_states()\n  \n  print(f\"Estados construídos: {len(parser.states)}\")\n  \n  # Imprime os estados\n  parser.print_states()\n  \n  # Imprime conjuntos FOLLOW\n  parser.print_follow_sets()\n  \n  # Constrói a tabela\n  print(\"\\nConstruindo tabela ACTION/GOTO...\")\n  conflicts = parser.build_parsing_table()\n  \n  if conflicts:\n    print(f\"\\nConflitos encontrados: {len(conflicts)}\")\n    for conflict in conflicts:\n      print(f\" - {conflict}\")\n  else:\n    print(\"\\nNenhum conflito encontrado.\")\n  \n  # Mostra as produções\n  print(\"\\nProduções da gramática aumentada:\")\n  for idx, prod in enumerate(parser.productions):\n    rhs_str = ' '.join(prod.rhs) if prod.rhs else 'ε'\n    print(f\" {idx}: {prod.lhs} → {rhs_str}\")\n  \n  # Testa o parser\n  print(\"\\n\" + \"=\"*75)\n  print(\"Analisando 'n + n':\")\n  print(\"=\"*75)\n  result = parser.parse(['n', '+', 'n'])\n  print(f\"\\nResultado: {'Aceito' if result else 'Rejeitado'}\")\n  \n  print(\"\\n\" + \"=\"*75)\n  print(\"Analisando 'n + n + n':\")\n  print(\"=\"*75)\n  result = parser.parse(['n', '+', 'n', '+', 'n'])\n  print(f\"\\nResultado: {'Aceito' if result else 'Rejeitado'}\")",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Parsers $SLR(1)$: A Ponte Entre Simplicidade e Poder</span>"
    ]
  },
  {
    "objectID": "08-parserSLR1.html#um-estudo-de-caso-uma-gramática-lr1-não-slr1",
    "href": "08-parserSLR1.html#um-estudo-de-caso-uma-gramática-lr1-não-slr1",
    "title": "9  Parsers \\(SLR(1)\\): A Ponte Entre Simplicidade e Poder",
    "section": "9.6 Um Estudo de Caso: Uma Gramática \\(LR(1)\\) Não-\\(SLR(1)\\)",
    "text": "9.6 Um Estudo de Caso: Uma Gramática \\(LR(1)\\) Não-\\(SLR(1)\\)\nPara entender as limitações do \\(SLR(1)\\) e a necessidade de métodos mais potentes como o \\(LR(1)\\), vamos analisar uma gramática que é um exemplo clássico de ser \\(LR(1)\\), mas não \\(SLR(1)\\). Ela foi projetada para criar um cenário em que a informação global dos conjuntos \\(FOLLOW\\) é insuficiente, mas a informação local dos lookaheads do \\(LR(1)\\) é precisa o bastante para resolver a ambiguidade.\nS → a A d\nS → b B d\nS → a B e\nS → b A e\nA → c\nB → c\nEsta gramática descreve uma linguagem simples com quatro sentenças possíveis: acd, bcd, abe e bae. O problema para o parser surge após ler o terminal c, pois, nesse momento, ele não sabe se esse c veio de um não-terminal A ou B. A decisão de qual produção usar para a redução (A → c ou B → c) dependerá do contexto que o \\(SLR(1)\\) não consegue enxergar.\n\n9.6.1 Preparação: Gramática Aumentada e Conjuntos Preliminares\nPrimeiro, aumentamos a gramática e numeramos as produções:\n0. S' → S\n1. S → a A d\n2. S → b B d\n3. S → a B e\n4. S → b A e\n5. A → c\n6. B → c\n\n9.6.1.1 Conjuntos \\(FIRST\\)\nOs conjuntos \\(FIRST\\) são diretos de calcular:\n\n\\(FIRST(S) = \\{a, b\\}\\)\n\\(FIRST(A) = \\{c\\}\\)\n\\(FIRST(B) = \\{c\\}\\)\n\n\n\n9.6.1.2 Conjuntos \\(FOLLOW\\)\nAqui está o ponto central do problema para o \\(SLR(1)\\). Vamos calcular os conjuntos \\(FOLLOW\\):\n\n\\(S'\\) é o símbolo inicial, então \\(\\$ \\in FOLLOW(S')\\) e, por consequência, \\(\\$ \\in FOLLOW(S)\\).\nDa produção S → a A d, o terminal d segue A, então \\(d \\in FOLLOW(A)\\).\nDa produção S → b A e, o terminal e segue A, então \\(e \\in FOLLOW(A)\\).\nDa produção S → b B d, o terminal d segue B, então \\(d \\in FOLLOW(B)\\).\nDa produção S → a B e, o terminal e segue B, então \\(e \\in FOLLOW(B)\\).\n\nAplicando as regras, obtemos:\n\n\\(FOLLOW(S) = \\{\\$\\}\\)\n\\(FOLLOW(A) = \\{d, e\\}\\)\n\\(FOLLOW(B) = \\{d, e\\}\\)\n\nNote que \\(FOLLOW(A)\\) e \\(FOLLOW(B)\\) são idênticos. É esta sobreposição que o \\(SLR(1)\\) não conseguirá resolver.\n\n\n\n9.6.2 Construção da Tabela \\(SLR(1)\\)\n\n9.6.2.1 Estados do Autômato \\(LR(0)\\)\nVamos construir os estados mais relevantes para a análise.\nEstado \\(I_0\\) (inicial):\nCLOSURE({[S' → •S]}) =\n[S' → •S]\n[S → •a A d]\n[S → •b B d]\n[S → •a B e]\n[S → •b A e]\nEstado \\(I_1\\): \\(GOTO(I_0, a)\\)\nCLOSURE({[S → a•A d], [S → a•B e]}) =\n[S → a•A d]\n[S → a•B e]\n[A → •c] // Adicionado pelo item 1\n[B → •c] // Adicionado pelo item 2\nEstado \\(I_2\\): \\(GOTO(I_0, b)\\)\nCLOSURE({[S → b•B d], [S → b•A e]}) =\n[S → b•B d]\n[S → b•A e]\n[A → •c] // Adicionado pelo item 2\n[B → •c] // Adicionado pelo item 1\nEstado \\(I_6\\): \\(GOTO(I_1, c)\\) ou \\(GOTO(I_2, c)\\)\nCLOSURE({[A → c•], [B → c•]}) =\n[A → c•]\n[B → c•]\nEste estado, \\(I_6\\), é o estado problemático.\n\n\n9.6.2.2 Tabela \\(SLR(1)\\) com Conflitos\nAo construir a tabela para o estado \\(I_6\\), encontramos um conflito.\n\nO item [A → c•] (produção 5) nos diz para reduzir. As ações de redução são inseridas para cada terminal no conjunto \\(FOLLOW(A)\\). Como \\(FOLLOW(A) = \\{d, e\\}\\), teríamos:\n\\(ACTION[6, d] = r5\\)\n\\(ACTION[6, e] = r5\\)\nO item [B → c•] (produção 6) também nos diz para reduzir. As ações são inseridas para cada terminal em \\(FOLLOW(B)\\). Como \\(FOLLOW(B) = \\{d, e\\}\\), teríamos:\n\\(ACTION[6, d] = r6\\)\n\\(ACTION[6, e] = r6\\)\n\nCONFLITO REDUCE-REDUCE no Estado 6: para os terminais d e e, a tabela teria duas entradas:\n\nNa célula \\(ACTION[6, d]\\), temos um conflito entre reduce 5 e reduce 6.\nNa célula \\(ACTION[6, e]\\), temos um conflito entre reduce 5 e reduce 6.\n\nO parser \\(SLR(1)\\) não sabe se, ao ver um c, deve reduzi-lo para A ou para B. Ele usa a informação global de que tanto A quanto B podem ser seguidos por d ou e, gerando o conflito.\n\n\n\n9.6.3 Construção da Tabela \\(LR(1)\\) Canônica\nAgora, vamos ver como a propagação de lookaheads específicos do \\(LR(1)\\) resolve esse problema.\n\n9.6.3.1 Estados do Autômato \\(LR(1)\\)\nEstado \\(I_0\\) (inicial):\n[S' → •S, $]\n[S → •a A d, $]\n[S → •b B d, $]\n[S → •a B e, $]\n[S → •b A e, $]\nEstado \\(I_1\\): \\(GOTO(I_0, a)\\)\n\nNúcleo: {[S → a•A d, $], [S → a•B e, $]}\nAplicando CLOSURE:\nDo item [S → a•A d, $], o símbolo após A é d. O lookahead para as produções de A será \\(FIRST(d\\$) = \\{d\\}\\). Adicionamos: [A → •c, d].\nDo item [S → a•B e, $], o símbolo após B é e. O lookahead para as produções de B será \\(FIRST(e\\$) = \\{e\\}\\). Adicionamos: [B → •c, e].\nEstado \\(I_1\\) completo:\n\n[S → a•A d, $]\n[S → a•B e, $]\n[A → •c, d]\n[B → •c, e]\nEstado \\(I_2\\): \\(GOTO(I_0, b)\\)\n\nSeguindo a mesma lógica:\n\n[S → b•B d, $]\n[S → b•A e, $]\n[B → •c, d]\n[A → •c, e]\nAgora, observe o que acontece quando fazemos a transição com c. Criamos dois estados distintos, onde o \\(SLR(1)\\) criava apenas um.\nEstado \\(I_6\\): \\(GOTO(I_1, c)\\)\n[A → c•, d]\n[B → c•, e]\nNeste estado, que alcançamos após ver um a e depois um c, as ações são:\n\nSe o próximo símbolo for d, a única ação possível é reduce 5 (A → c), pois o lookahead do item é d.\nSe o próximo símbolo for e, a única ação possível é reduce 6 (B → c), pois o lookahead do item é e.\n\nNão há conflito!\nEstado \\(I_9\\): \\(GOTO(I_2, c)\\)\n[B → c•, d]\n[A → c•, e]\nNeste estado, que alcançamos após ver um b e depois um c, as ações são:\n\nSe o próximo símbolo for d, a única ação possível é reduce 6 (B → c).\nSe o próximo símbolo for e, a única ação possível é reduce 5 (A → c). Não há conflito!\n\n\n\n\n9.6.4 Tabela Final \\(LR(1)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstado\nACTION\n\n\n\n\n\nGOTO\n\n\n\n\n\n\n\na\nb\nc\nd\ne\n$\nS\nA\nB\n\n\n0\ns1\ns2\n\n\n\n\n3\n\n\n\n\n1\n\n\ns6\n\n\n\n\n4\n5\n\n\n2\n\n\ns9\n\n\n\n\n8\n7\n\n\n3\n\n\n\n\n\nacc\n\n\n\n\n\n4\n\n\n\ns10\n\n\n\n\n\n\n\n5\n\n\n\n\ns11\n\n\n\n\n\n\n6\n\n\n\nr5\nr6\n\n\n\n\n\n\n7\n\n\n\ns12\n\n\n\n\n\n\n\n8\n\n\n\n\ns13\n\n\n\n\n\n\n9\n\n\n\nr6\nr5\n\n\n\n\n\n\n10\n\n\n\n\n\nr1\n\n\n\n\n\n11\n\n\n\n\n\nr3\n\n\n\n\n\n12\n\n\n\n\n\nr2\n\n\n\n\n\n13\n\n\n\n\n\nr4\n\n\n\n\n\n\n\n\n9.6.5 Por Que o Conflito Ocorre no \\(SLR(1)\\)\n\nProblema do \\(SLR(1)\\): O parser chega ao estado \\(I_6\\) (LR(0)) após ler c. Para decidir a redução, ele consulta os conjuntos \\(FOLLOW(A)\\) e \\(FOLLOW(B)\\). Como ambos contêm {d, e}, ele não consegue decidir entre reduzir para A ou para B.\nSolução do \\(LR(1)\\): O parser \\(LR(1)\\) sabe muito mais.\n\n\nSe ele chegou ao estado \\(I_6\\) (LR(1)), ele sabe que veio da sequência a c. Neste contexto, o c deve ser seguido por d para formar A (acd) ou por e para formar B (abe). Os lookaheads d e e são específicos para cada redução e não se sobrepõem.\nSe ele chegou ao estado \\(I_9\\) (LR(1)), ele sabe que veio da sequência b c. O raciocínio é análogo.\n\nA precisão do \\(LR(1)\\) ao propagar os lookaheads permite que ele divida o que era um único estado conflituoso no \\(SLR(1)\\) em múltiplos estados não conflituosos, cada um representando um contexto de análise distinto.## Vantagens e Limitações do \\(SLR(1)\\)\n\n\n9.6.6 Vantagens\n\nSimplicidade de Implementação: o algoritmo é direto e eficiente;\nTabelas Compactas: significativamente menores que \\(LR(1)\\);\nEficiência de Construção: tempo \\(O(n^2)\\) vs \\(O(n^3)\\) do \\(LR(1)\\);\nSuficiente para Muitas Linguagens: funciona bem para gramáticas de linguagens de programação simples.\n\n\n\n9.6.7 Limitações\n\nPoder de Reconhecimento Limitado: nem toda gramática \\(LR(1)\\) é \\(SLR(1)\\);\nConflitos Espúrios: o uso de \\(FOLLOW\\) pode gerar conflitos desnecessários;\nInadequado para Gramáticas Complexas: linguagens modernas frequentemente excedem suas capacidades.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Parsers $SLR(1)$: A Ponte Entre Simplicidade e Poder</span>"
    ]
  },
  {
    "objectID": "08-parserSLR1.html#o-lugar-do-slr1-na-hierarquia",
    "href": "08-parserSLR1.html#o-lugar-do-slr1-na-hierarquia",
    "title": "9  Parsers \\(SLR(1)\\): A Ponte Entre Simplicidade e Poder",
    "section": "9.7 O Lugar do \\(SLR(1)\\) na Hierarquia",
    "text": "9.7 O Lugar do \\(SLR(1)\\) na Hierarquia\nO \\(SLR(1)\\) ocupa uma posição intermediária na hierarquia dos parsers:\n\\[LL(1) \\subset $SLR(1)$ \\subset LALR(1) \\subset $LR(1)\\]$\nOnde:\n\n\\(LL(1)\\): parsers preditivos top-down;\n\\(SLR(1)\\): Simple LR com conjuntos \\(FOLLOW\\);\n\\(LALR(1)\\): Look-Ahead LR, combina estados \\(LR(1)\\) similares;\n\\(LR(1)\\): canônico, máximo poder determinístico.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Parsers $SLR(1)$: A Ponte Entre Simplicidade e Poder</span>"
    ]
  },
  {
    "objectID": "08-parserSLR1.html#exercícios",
    "href": "08-parserSLR1.html#exercícios",
    "title": "9  Parsers \\(SLR(1)\\): A Ponte Entre Simplicidade e Poder",
    "section": "9.8 Exercícios",
    "text": "9.8 Exercícios\n\n9.8.1 Exercício 1: Gramática com Conflito Potencial\nConsidere a gramática:\nS → A a | b\nA → c | ε\n\nAumente a gramática e calcule os conjuntos \\(FIRST\\) e \\(FOLLOW\\);\nConstrua os estados \\(LR(0)\\) do autômato;\nConstrua a tabela \\(SLR(1)\\) e identifique possíveis conflitos;\nAnalise se esta gramática é \\(SLR(1)\\).\n\nDica: Preste atenção especial ao conjunto \\(FOLLOW(A)\\) e como ele afeta as ações de redução.\n\n\n9.8.2 Exercício 2: Implementação de Gramática de Declarações\nImplemente um parser \\(SLR(1)\\) para:\nDECL → TYPE VARS ;\nTYPE → int | float\nVARS → VARS , id | id\n\nConstrua a coleção canônica de conjuntos de itens \\(LR(0)\\);\nCalcule os conjuntos \\(FOLLOW\\) necessários;\nImplemente o parser usando o código fornecido como base;\nTeste com as strings: int x ;, float x , y , z ;\n\nObservação: Esta gramática é perfeitamente \\(SLR(1)\\) e não deve gerar conflitos.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Parsers $SLR(1)$: A Ponte Entre Simplicidade e Poder</span>"
    ]
  },
  {
    "objectID": "10-semantico.html",
    "href": "10-semantico.html",
    "title": "10  Analisadores Semânticos: a determinação do significado",
    "section": "",
    "text": "10.1 Perspectiva Histórica: A Evolução da Análise Semântica\nA análise semântica, o centro nervoso do compilador para significado e a coerência, não nasceu pronta. Sua história é uma jornada que acompanha a própria evolução das linguagens de programação, partindo de verificações simples e dispersas para se tornar uma estrutura formal e sofisticada. A incansável leitora poderá traçar essa trajetória desde os primórdios da computação até os complexos sistemas de tipos que usamos hoje. A Figura Figure 10.1 ilustra essa evolução.\nNos primeiros dias da compilação, com linguagens como o FORTRAN I (1957), a análise semântica era uma atividade ad-hoc. Não havia uma fase distinta especificamente destinada a análise de sentido; as verificações de tipo e coerência eram feitas de forma improvisada, entrelaçadas diretamente com a lógica de geração de código. O objetivo era fazer o programa funcionar, e a separação de responsabilidades era uma ambição distante. Vamos fixar o marco inicial desta jornada de criação e evolução no ALGOL 60 (1960). O projeto de linguagem de programação que fez a primeira tentativa séria de separar a sintaxe (a forma) da semântica (o significado). Mais importante, o ALGOL 60 introduziu o conceito revolucionário da estrutura de blocos e, consequentemente, do escopo léxico, forçando os compiladores a entenderem em que bloco de código uma determinada variável era válida. Sem dúvidas uma das primeiras tarefas verdadeiramente semânticas atribuídas a um compilador.\nA necessidade de rigor, programas que rodassem sempre da mesma forma e com os mesmos resultados, levou a uma era de formalização na década seguinte. Em vez de manter um conjunto regras desconexas espalhadas pelo código do compilador, os pesquisadores buscaram uma base teórica sólida. Que explicassem, justificassem e formalizassem a análise semântica. Neste interim, Robert Floyd (1967) contribuiu ao associar a semântica dos programas à lógica formal através de asserções e invariantes. Contudo, o grande salto foi dado por Donald Knuth (1968) com suas Gramáticas de Atributos. Pela primeira vez, havia um formalismo poderoso e sistemático para descrever e implementar a análise semântica. As regras semânticas podiam agora ser anexadas diretamente às regras da gramática sintática, permitindo que informações como tipos de dados fluíssem pela árvore sintática de forma estruturada. A linguagem Pascal, projetada por Niklaus Wirth (1971), foi um exemplo brilhante dessa nova abordagem, demonstrando como uma análise semântica completa e estruturada poderia ser elegantemente implementada em um único passo. Neste ponto, tínhamos a base teórica, Floyd, uma formatação prática, Knuth, e uma linguagem que a utilizava, Wirth.\nCom fundamentos sólidos estabelecidos, a fase seguinte foi de sistematização e popularização. O livro Principles of Compiler Design de Aho e Ullman (1977), o famoso Livro do Dragão, estabeleceu a estrutura canônica dos compiladores, consolidando a análise semântica como uma fase bem definida. Paralelamente, a linguagem C, desenvolvida por Dennis Ritchie (1972) e padronizada neste período, demonstrava que um sistema de tipos simples mas rigoroso poderia ser extremamente eficaz, influenciando profundamente o design de compiladores com sua abordagem pragmática de verificação de tipos em tempo de compilação. Foi nesse período que uma das ideias mais poderosas da teoria das linguagens surgiu: a inferência de tipos. O algoritmo W, desenvolvido por Robin Milner (1978) para a linguagem ML, permitiu que o compilador deduzisse os tipos das variáveis automaticamente, sem a necessidade de anotações explícitas, combinando segurança e flexibilidade de uma forma inédita, um contraste marcante com a filosofia da linguagem C, que exigia declarações explícitas mas oferecia controle direto sobre a representação em memória. Ao mesmo tempo, linguagens como Ada (1983) exploravam o outro extremo, implementando sistemas de tipos extremamente ricos e complexos, projetados para a construção de software robusto e de missão crítica, enquanto o C mantinha sua posição como a escolha para programação de sistemas, nos quais a previsibilidade e o controle fino da memória e performance eram mais valorizados que a expressividade do sistema de tipos.\nA partir de meados da década de 1980, a complexidade das linguagens explodiu, trazendo consigo desafios semânticos sem precedentes. O C++ (1985), com a introdução de templates e sobrecarga de funções e operadores, exigiu que os analisadores semânticos resolvessem nomes e tipos em contextos muito mais elaborados. O Java (1995) popularizou um modelo híbrido, realizando verificações de tipo rigorosas tanto em tempo de compilação quanto em tempo de execução através da JVM. Paralelamente, a tradição funcional, representada pelo Haskell (1990), continuou a empurrar as fronteiras da teoria, introduzindo conceitos como type classes e polimorfismo paramétrico avançado, que hoje influenciam o design de muitas linguagens modernas.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analisadores Semânticos: a determinação do significado</span>"
    ]
  },
  {
    "objectID": "10-semantico.html#perspectiva-histórica-a-evolução-da-análise-semântica",
    "href": "10-semantico.html#perspectiva-histórica-a-evolução-da-análise-semântica",
    "title": "10  Analisadores Semânticos: a determinação do significado",
    "section": "",
    "text": "Linha do Tempo da Análise Semântica\n\n\n\n\nFigure 10.1",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analisadores Semânticos: a determinação do significado</span>"
    ]
  },
  {
    "objectID": "10-semantico.html#da-necessidade-à-solução-por-que-gramáticas-de-atributos",
    "href": "10-semantico.html#da-necessidade-à-solução-por-que-gramáticas-de-atributos",
    "title": "10  Analisadores Semânticos: a determinação do significado",
    "section": "10.2 Da Necessidade à Solução: Por Que Gramáticas de Atributos?",
    "text": "10.2 Da Necessidade à Solução: Por Que Gramáticas de Atributos?\nA evolução histórica da análise semântica que acabamos de traçar revela um padrão: à medida que as linguagens de programação se tornavam mais expressivas, as verificações semânticas necessárias cresciam em número e complexidade. Contudo, um problema fundamental permanecia sem solução satisfatória: como especificar formalmente essas verificações?\nAs gramáticas livres de contexto (GLC), poderosas para descrever a sintaxe, revelavam-se insuficientes para capturar regras semânticas. Considere a regra toda variável deve ser declarada antes de seu uso. Uma GLC pode garantir que x = y + z; seja uma atribuição sintaticamente válida, mas não pode verificar se y e z foram previamente declarados. Este tipo de restrição é sensível ao contexto e depende de informações que estão fora da estrutura local da produção.\nDurante os anos 1960, a análise semântica nos compiladores era implementada como código ad-hoc espalhado pelo parser. Cada compilador tinha sua própria abordagem, muitas vezes não documentada e difícil de manter. Não havia uma especificação formal da semântica, apenas a implementação. Isso criava problemas sérios:\n\nNão-portabilidade: reaproveitar código entre compiladores era praticamente impossível;\nVerificação impossível: sem especificação formal, não havia como provar que a implementação estava correta;\nManutenção custosa: modificar ou estender as regras semânticas exigia alterações profundas no código do compilador;\nFalta de clareza: a semântica da linguagem estava implícita no código, não explícita em documentação.\n\nA comunidade de compiladores necessitava de um formalismo que:\n\nEstendesse naturalmente as GLCs, mantendo sua elegância matemática;\nPermitisse especificar como informação contextual flui pela árvore sintática;\nFosse implementável de forma sistemática e automática;\nServisse tanto como especificação quanto como base para implementação.\n\nA resposta veio em 1968, quando Donald Knuth introduziu as Gramáticas de Atributos. Sua ideia central era engenhosamente simples: se as produções da gramática descrevem a estrutura do programa, por que não anotar essas mesmas produções com regras que descrevem o significado? Em vez de criar um formalismo completamente novo, Knuth estendeu as GLCs adicionando duas características:\n\nAtributos: propriedades associadas aos símbolos gramaticais, como o tipo de uma expressão;\nRegras Semânticas: equações que definem como calcular atributos a partir de outros atributos.\n\nEsta abordagem unificava sintaxe e semântica sob um único formalismo matemático. A análise semântica deixava de ser código disperso para se tornar um conjunto de equações anexadas à gramática, verificável, modular e automaticamente implementável. Era a ponte que faltava entre a teoria formal das linguagens e a prática da construção de compiladores.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analisadores Semânticos: a determinação do significado</span>"
    ]
  },
  {
    "objectID": "10-semantico.html#a-análise-semântica-no-contexto-do-compilador",
    "href": "10-semantico.html#a-análise-semântica-no-contexto-do-compilador",
    "title": "10  Analisadores Semânticos: a determinação do significado",
    "section": "10.3 A Análise Semântica no Contexto do Compilador",
    "text": "10.3 A Análise Semântica no Contexto do Compilador\nConsiderando a estrutura de compilação canônica, como definida por Aho (1977), e histórica que estamos adotando ao longo de todo este livro, a análise semântica será a terceira fase do front-end de um compilador, posicionando-se logo após a análise sintática. Como pode ser visto na Figura Figure 10.2. A principal informação de entrada do analisador semântico será a Árvore de Sintática Abstrata, termo importado do inglês Abstract Syntactic Tree, AST, a representação hierárquica da estrutura do programa que foi gerada pelo analisador sintático.\n\n\n\n\n\n\nFases do Compilador\n\n\n\n\nFigure 10.2\n\n\n\nÉ importante que a sagaz leitora compreenda que, nos compiladores modernos, a análise semântica raramente é um módulo completamente separado que será executado após o término da análise sintática. Neste texto, usaremos esta divisão por motivos didáticos e um pouco de preguiça. Ok, confesso, muita preguiça. Mas, a verdade é que na maior parte dos compiladores modernos, a análise semântica está implementada como um conjunto de procedimentos, conhecidos como ações semânticas, que serão invocados pelo analisador sintático em momentos específicos e importantes da geração da AST. Por exemplo, em um analisador sintático de descida recursiva, \\(LL(k)\\), as chamadas para as rotinas de verificação semântica podem, simplesmente, ser inseridas no corpo das funções que fazem o parse de cada regra da gramática. Em um analisador \\(LR(k)\\), bottom-up, uma ação semântica pode ser disparada sempre que o parser realizar uma redução, ou seja, quando reconhece o lado direito de uma produção da gramática.\nEssa integração entre as fases sintática e semântica está fundamentada em uma decisão de projeto: a arquitetura do compilador será de uma única passagem, one-pass, ou de múltiplas passagens, multi-pass? São opções diferentes com impacto profundo na complexidade, desempenho e flexibilidade do compilador.\nCompiladores de uma passagem: intercalam a análise léxica, sintática, semântica e até a geração de código em uma única travessia do código-fonte. São extremamente rápidos, mas impõem restrições à linguagem, como a exigência de que todas as entidades sejam declaradas antes de serem usadas.\nCompiladores de múltiplas passagens: utilizam a AST como uma representação intermediária entre as fases. A primeira passagem constrói a AST, e passagens subsequentes a percorrem para realizar a análise semântica, otimizações e, finalmente, a geração de código. Essa abordagem é mais modular, flexível e adequada para linguagens complexas como C++ ou Java, que permitem declarações em qualquer ordem. A AST, nesse contexto, não é apenas uma estrutura de dados, mas uma estrutura de dados que desacopla a sintaxe do significado.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analisadores Semânticos: a determinação do significado</span>"
    ]
  },
  {
    "objectID": "10-semantico.html#semântica-estática-vs.-dinâmica-o-limite-da-compilação",
    "href": "10-semantico.html#semântica-estática-vs.-dinâmica-o-limite-da-compilação",
    "title": "10  Analisadores Semânticos: a determinação do significado",
    "section": "10.4 Semântica Estática vs. Dinâmica: O Limite da Compilação",
    "text": "10.4 Semântica Estática vs. Dinâmica: O Limite da Compilação\nAs regras semânticas de uma linguagem se dividem em duas categorias, que definem o que pode e o que não pode ser verificado antes da execução do programa. Estas categorias são:\nSemântica Estática: compreende todas as regras de significado que podem ser verificadas em tempo de compilação. Essas regras são chamadas de estáticas porque a verificação ocorre através da análise do código-fonte, sem necessidade de execução. A análise semântica concentra-se primariamente em impor essas regras estáticas, que podem ser categorizadas em:\n1. Verificações de Escopo e Declarações:\n\nToda variável deve ser declarada antes de seu uso;\nIdentificadores não podem ser redeclarados no mesmo escopo;\nVariáveis locais ocultam variáveis de escopos externos conforme as regras da linguagem.\n\n2. Verificações de Tipos:\n\nOs tipos dos operandos de uma expressão devem ser compatíveis com o operador;\nO tipo da expressão atribuída deve ser compatível com o tipo da variável receptora;\nExpressões condicionais (if, while) devem resultar em valores booleanos;\nOperações de indexação de arrays devem usar índices inteiros.\n\n3. Verificações de Sub-rotinas:\n\nFunções devem ser chamadas com o número correto de argumentos;\nOs tipos dos argumentos passados devem corresponder aos tipos dos parâmetros formais;\nO tipo do valor retornado deve corresponder ao tipo de retorno declarado;\nFunções não-void devem ter pelo menos um comando return em todos os caminhos de execução.\n\n4. Verificações de Fluxo de Controle:\n\nComandos break e continue só podem aparecer dentro de laços;\nComandos return só podem aparecer dentro de funções;\nLabels referenciados por goto devem existir no escopo da função;\nEm linguagens com switch, valores de case devem ser únicos e do tipo compatível.\n\n5. Verificações de Unicidade e Consistência:\n\nCampos de estruturas/classes não podem ter nomes duplicados;\nEnumerações não podem ter valores duplicados (quando aplicável);\nModificadores de acesso (public, private) devem ser mutuamente exclusivos;\nSobrecarga de funções deve ter assinaturas distintas.\n\n6. Verificações de Acessibilidade:\n\nMembros private só podem ser acessados dentro da própria classe;\nMembros protected seguem regras de visibilidade conforme hierarquia de herança;\nVariáveis const/final não podem ser modificadas após inicialização.\n\nA leitora deve observar que essas regras são frequentemente dependentes de contexto, característica que as gramáticas livres de contexto, por si sós, não conseguem capturar. Uma expressão como x + y é sintaticamente válida independente de x e y terem sido declarados ou de seus tipos serem compatíveis com o operador +. É a análise semântica que impõe essas restrições contextuais, fazendo dela, em essência, uma verificação de contexto sensível.\nSemântica Dinâmica: refere-se aos aspectos do significado do programa que só podem ser determinados em tempo de execução. Diferentemente da semântica estática, essas propriedades dependem dos valores concretos que os dados assumem durante a execução, informação geralmente não disponível durante a compilação. O compilador insere verificações explícitas no código gerado que serão executadas junto com a lógica do programa, detectando violações e gerando exceções ou encerrando a execução.\n1. Erros Aritméticos:\n\nDivisão por zero: impossível detectar estaticamente quando o divisor é uma variável;\nOverflow/Underflow aritmético: quando o resultado de uma operação excede os limites do tipo;\nOperações com valores especiais: operações envolvendo NaN, Infinity em ponto flutuante;\nRaiz quadrada de número negativo: em linguagens sem suporte nativo a números complexos.\n\nint divisor = obterValorDoUsuario();\nint resultado = 100 / divisor; // Erro se divisor == 0\n\nint a = INT_MAX;\nint b = a + 1; // Overflow: comportamento pode ser indefinido\n2. Erros de Acesso à Memória:\n\nDesreferenciamento de ponteiro nulo: acessar memória através de ponteiro não inicializado;\nAcesso fora dos limites de array: índice negativo ou maior/igual ao tamanho do array;\nDangling pointers: acessar memória já liberada;\nDouble free: liberar a mesma região de memória múltiplas vezes;\nMemory leaks: falha em liberar memória alocada (detectável por ferramentas especializadas).\n\nint[] numeros = new int[10];\nint indice = calcularIndice();\nint valor = numeros[indice]; // Erro se indice &lt; 0 ou indice &gt;= 10\n\nString texto = null;\nint tamanho = texto.length(); // NullPointerException\n3. Erros de Tipo Dinâmico: em linguagens com tipagem dinâmica ou conversões inseguras:\n\nConversão de tipo inválida (casting incorreto);\nChamada de método inexistente em objetos de tipo incorreto;\nOperações entre tipos incompatíveis não detectadas estaticamente.\n\ndef processar(obj):\n  return obj.metodo() # Erro se obj não possui metodo()\n\nresultado = \"texto\" / 2 # TypeError em**Python**\nObject obj = \"String\";\nInteger num = (Integer) obj; // ClassCastException em tempo de execução\n4. Erros de Recursos e Limites do Sistema:\n\nStack overflow: recursão muito profunda ou alocação excessiva na pilha;\nHeap exhaustion: falha ao alocar memória dinâmica;\nDeadlock: threads esperando indefinidamente por recursos mutuamente bloqueados;\nTimeout: operações que excedem limites de tempo estabelecidos;\nLimites de arquivo/rede: disco cheio, conexão perdida, arquivo não encontrado.\n\nvoid recursaoInfinita() {\n  int array[10000];\n  recursaoInfinita(); // Estoura a pilha\n}\n\nint* p = malloc(SIZE_MAX); // Falha se memória insuficiente\n5. Violações de Invariantes e Contratos:\n\nAsserções falhando: condições que deveriam sempre ser verdadeiras;\nPré-condições violadas: função chamada com argumentos inválidos;\nPós-condições não satisfeitas: função não produz resultado esperado;\nInvariantes de classe quebradas: estado interno inconsistente.\n\nassert(saldo &gt;= 0); // Falha se invariante violada\n\nvoid sacar(double valor) {\n  assert(valor &gt; 0 && valor &lt;= saldo); // Pré-condição\n  saldo -= valor;\n  assert(saldo &gt;= 0); // Pós-condição\n}\n6. Erros de Concorrência:\n\nRace conditions: resultado depende do timing de threads concorrentes;\nDeadlocks: impasse circular na aquisição de recursos;\nStarvation: thread nunca obtém acesso a recurso necessário;\nAtomicidade violada: operação que deveria ser atômica é interrompida.\n\n\n10.4.1 Indecidibilidade e Trade-offs\nA leitora perspicaz notará que muitos desses problemas são indecidíveis estaticamente. O Problema da Parada de Turing prova que não existe algoritmo geral que possa determinar se um programa arbitrário terminará ou entrará em loop infinito. Consequentemente:\n\nNão é possível determinar estaticamente se um divisor será zero sem executar o código;\nNão é possível prever todos os índices de array que serão usados em tempo de execução;\nNão é possível detectar todos os deadlocks potenciais através de análise estática.\n\nEste fato fundamental força um trade-off no design de linguagens:\nVerificações Agressivas (Linguagens Seguras):\n\nExemplos: Java, Python, Rust;\nCaracterísticas: inserem verificações extensivas em tempo de execução;\nCusto: overhead de performance (5-15% tipicamente);\nBenefício: programas nunca apresentam comportamento indefinido; erros são capturados e reportados.\n\nVerificações Mínimas (Linguagens de Sistema):\n\nExemplos: C, C++ (sem flags de sanitização);\nCaracterísticas: assumem que o programador está correto; comportamento indefinido em erros;\nCusto: programas podem corromper memória silenciosamente, criar vulnerabilidades de segurança;\nBenefício: máxima performance; controle total sobre o hardware.\n\nVerificações Opcionais (Abordagem Híbrida):\n\nExemplos: C++ com sanitizers, Rust com unsafe, Python com -O;\nCaracterísticas: verificações podem ser habilitadas/desabilitadas;\nUso típico: verificações ativadas em desenvolvimento/debug, desativadas em produção.\n\nA análise semântica dinâmica representa, portanto, a última linha de defesa do programa contra comportamento incorreto. Enquanto a semântica estática previne erros que podem ser provados impossíveis, a semântica dinâmica detecta erros que só se manifestam sob condições específicas de execução, transformando falhas catastróficas em exceções controláveis.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analisadores Semânticos: a determinação do significado</span>"
    ]
  },
  {
    "objectID": "10-semantico.html#gramáticas-de-atributos-formalizando-a-análise-semântica",
    "href": "10-semantico.html#gramáticas-de-atributos-formalizando-a-análise-semântica",
    "title": "10  Analisadores Semânticos: a determinação do significado",
    "section": "10.5 Gramáticas de Atributos: Formalizando a Análise Semântica",
    "text": "10.5 Gramáticas de Atributos: Formalizando a Análise Semântica\nVimos que as gramáticas livres de contexto (GLC) são excelentes para descrever a estrutura sintática de uma linguagem, mas insuficientes para capturar regras contextuais, como a verificação de tipos. Para formalizar a análise semântica de maneira sistemática e rigorosa, precisamos de uma ferramenta mais poderosa. Aqui voltamos a ideia de Donald Knuth.\n\n10.5.1 Da Intuição ao Formalismo: Atributos e Regras Semânticas\nUma Gramática de Atributos é, em sua essência, uma Gramática Livre de Contexto na qual cada símbolo gramatical, terminal ou não-terminal, é anotado com um conjunto de atributos, e cada produção da gramática é acompanhada por um conjunto de regras semânticas que especificam como calcular os valores desses atributos.\n\nAtributos: São propriedades associadas aos vértices da árvore sintática. Eles podem carregar informações como o tipo de uma expressão, o valor de uma constante, uma string de código gerado, ou um ponteiro para a Tabela de Símbolos.\nRegras Semânticas: São equações que definem o valor de um atributo em termos dos valores de outros atributos nos vértices vizinhos da árvore sintática (pai, filhos ou irmãos).\n\nEsse formalismo transforma a análise semântica em um processo de avaliação de atributos sobre a árvore sintática, um processo conhecido como “decoração” da árvore.\n\n\n10.5.2 Atributos Sintetizados e Herdados: O Fluxo de Informação\nOs atributos são classificados com base na direção em que a informação flui pela árvore sintática. Como pode ser visto na Figura Figure 10.3.\n\n\n\n\n\n\nAtributos Sintetizados e Herdados\n\n\n\n\nFigure 10.3\n\n\n\nAtributos Sintetizados: um atributo de um vértice é sintetizado se seu valor é calculado a partir dos valores dos atributos de seus filhos. A informação flui de baixo para cima na árvore sintática, das folhas para a raiz. Eles são ideais para agregar informações.\nExemplo: calcular o valor de uma expressão aritmética. Considere a produção expr -&gt; expr1 + term. O atributo valor de expr é naturalmente sintetizado pela soma dos atributos valor de expr1 e term.\n\nRegra Semântica: expr.valor = expr1.valor + term.valor\n\nAtributos Herdados: um atributo de um vértice é herdado se seu valor é calculado a partir dos valores dos atributos de seu pai ou de seus irmãos. A informação flui de cima para baixo ou lateralmente na árvore. Eles são perfeitos para propagar informações de contexto.\nExemplo: propagar o tipo em uma declaração de variáveis. Considere a produção declaracao -&gt; tipo lista_vars. O tipo de cada variável em lista_vars depende do tipo especificado no início da declaração.\n\nRegra Semântica: lista_vars.tipo_herdado = tipo.tipo_sintetizado\n\nO poder das gramáticas de atributos reside na interação entre esses dois tipos de fluxo, permitindo que informações contextuais, herdadas, influenciem cálculos locais que, por sua vez, são agregados e passados para cima, os atributos sintetizados.\n\n\n10.5.3 Definição Formal E Exemplos Práticos\nPara transformar a análise semântica em um processo sistemático e guiado pela sintaxe, utilizamos o formalismo das Gramáticas de Atributos. Formalmente, uma Gramática de Atributos (AG) é uma tupla \\(AG = \\{G, A, R\\}\\), na qual cada componente tem um papel bem definido:\n\n\\(G\\): A Gramática Livre de Contexto (GLC): a fundação sintática, definida por \\(G = \\{N, T, P, S\\}\\), na qual:\n\n\\(N\\) é o conjunto de símbolos não-terminais (variáveis sintáticas como expr, declaracao).\n\\(T\\) é o conjunto de símbolos terminais (tokens como id, int, +).\n\\(P\\) é o conjunto de produções ou regras gramaticais (ex: \\(E \\rightarrow E + T\\)).\n\\(S\\) é o símbolo inicial da gramática.\n\n\nA GLC descreve a forma, mas não o significado.\n\n\\(A\\): O Conjunto de Atributos: o componente que carrega a informação semântica. O conjunto \\(A\\) é a união de todos os atributos associados a cada símbolo gramatical definido como: \\(A = \\bigcup_{X \\in N \\cup T} A(X)\\).\n\nUm atributo é uma propriedade ou valor associado a um vértice na árvore sintática, como o tipo de uma variável (tipo), o valor de uma expressão (valor), ou o código gerado (codigo).\nPara cada símbolo \\(X\\), seu conjunto de atributos \\(A(X)\\) é dividido em dois grupos disjuntos, \\(A(X) = I(X) \\cup S(X)\\):\n\\(I(X)\\): Atributos Herdados (Inherited), cujo valor em um vértice é determinado a partir dos atributos de seu vértice-pai ou de seus irmãos. Eles propagam informação de cima para baixo e lateralmente na árvore, distribuindo o contexto.\n\\(S(X)\\): Atributos Sintetizados (Synthesized), cujo valor em um vértice é calculado a partir dos atributos de seus vértices-filhos. Eles agregam informação de baixo para cima, das folhas em direção à raiz.\n\n\\(R\\): O Conjunto de Regras Semânticas: o cérebro da operação, associando a cada produção \\(p \\in P\\) um conjunto de equações que definem como os atributos são calculados. Para uma produção \\(p: X_0 \\rightarrow X_1 X_2 \\dots X_n\\), as regras em \\(R\\) especificam como calcular:\n\n\nOs atributos sintetizados de \\(X_0\\) a partir dos atributos de \\(X_1, \\dots, X_n\\).\nOs atributos herdados de \\(X_1, \\dots, X_n\\) a partir dos atributos de \\(X_0\\) ou de outros símbolos \\(X_j\\) na mesma produção.\n\n\n10.5.3.1 Exemplo 1: Declaração de Variáveis\nVamos definir uma Gramática de Atributos para processar declarações simples como float x, y;, na qual o tipo deve ser distribuído para cada variável na lista.\n1. Gramática (\\(G\\)): começamos com a seguinte gramática livre de contexto:\n\\[\n\\begin{align*}\n\\text{Declaracao} & \\rightarrow \\text{Tipo} \\quad \\text{ListaIDs} \\\\\n\\text{Tipo} & \\rightarrow \\textbf{int} \\mid \\textbf{float} \\\\\n\\text{ListaIDs} & \\rightarrow \\text{ListaIDs}_1, \\textbf{id} \\mid \\textbf{id}\n\\end{align*}\n\\]\n2. Análise do Fluxo de Informação: para transformar esta gramática em uma Gramática de Atributos, precisamos primeiro identificar como a informação flui pela árvore sintática que pode ser vista na Figura Figure 10.4:\n\n\n\n\n\n\nÁrvore Sintática de Declaração\n\n\n\n\nFigure 10.4\n\n\n\nNa AST da Figura Figure 10.4, podemos observar dois fluxos principais de informação:\n\nInformação que sobe (sintetizada): o tipo declarado (int ou float) é reconhecido nas folhas da árvore (tokens int ou float) e precisa ser propagado para cima até o símbolo Tipo.\nInformação que desce (herdada): uma vez que o tipo foi determinado, ele precisa ser distribuído para todos os identificadores na ListaIDs. Como a lista pode ter múltiplos elementos separados por vírgulas, o tipo deve “fluir para baixo” através da recursão da lista.\n\n3. Definição dos Atributos (\\(A\\)): com base na análise do fluxo, definimos:\nPara o símbolo Tipo:\n\nAtributo sintetizado tipo: armazena o tipo reconhecido ('int' ou 'float'). A informação flui das folhas, tokens terminais, para cima, logo é sintetizado.\n\nPara o símbolo ListaIDs:\n\nAtributo herdado tipo_herdado: recebe o tipo que deve ser atribuído a todos os identificadores. A informação do tipo vem de fora, do pai Declaracao, e deve ser propagada através da lista, caracterizando um atributo herdado.\n\nPara o terminal id:\n\nAtributo sintetizado lexval: contém o nome do identificador, fornecido pelo analisador léxico. Este valor vem da análise léxica e é uma propriedade inerente ao token.\n\n4. Construção das Regras Semânticas (\\(R\\)): com os atributos definidos, podemos agora estabelecer as regras semânticas que governam como esses atributos são calculados e propagados. As regras semânticas serão sistematicamente derivadas para cada produção da gramática:\nProdução 1: \\(\\text{Declaracao} \\rightarrow \\text{Tipo} \\quad \\text{ListaIDs}\\). Esta produção conecta a determinação do tipo, filho esquerdo, com sua distribuição, filho direito. A regra semântica estabelece a ponte:\n\\[\\text{ListaIDs.tipo\\_herdado} = \\text{Tipo.tipo}\\]\nO valor sintetizado de Tipo, que subiu da folha, agora deve descer como atributo herdado para ListaIDs. Esta é a transição crítica entre fluxo ascendente e descendente que sempre irá requerer atenção.\nProdução 2: \\(\\text{Tipo} \\rightarrow \\textbf{int}\\)\n\\[\\text{Tipo.tipo} = \\text{'integer'}\\]\nQuando reconhecemos o token int, sintetizamos o atributo tipo com o valor semântico correspondente. Esta é uma regra de tradução do léxico para o semântico.\nProdução 3: \\(\\text{Tipo} \\rightarrow \\textbf{float}\\)\n\\[\\text{Tipo.tipo} = \\text{'real'}\\]\nAnálogo à produção anterior, mas para o tipo float.\nProdução 4: \\(\\text{ListaIDs} \\rightarrow \\text{ListaIDs}_1, \\textbf{id}\\)\nEsta produção recursiva é o núcleo da propagação do tipo através da lista:\n\\[\n\\begin{align*}\n\\text{ListaIDs}_1\\text{.tipo\\_herdado} &= \\text{ListaIDs.tipo\\_herdado} \\\\\n\\text{adicionar\\_tipo}(\\text{id.lexval}, &\\text{ListaIDs.tipo\\_herdado})\n\\end{align*}\n\\]\nRaciocínio detalhado:\n\nPrimeira regra: o atributo herdado do pai (ListaIDs) deve ser copiado para o filho recursivo (ListaIDs₁). Isso garante que o tipo continue fluindo através da recursão, alcançando todos os identificadores da lista.\nSegunda regra: para o identificador atual (id), executamos a ação semântica de adicionar seu nome (id.lexval) e tipo (ListaIDs.tipo_herdado) à Tabela de Símbolos.\n\nProdução 5: \\(\\text{ListaIDs} \\rightarrow \\textbf{id}\\)\nEsta é a produção base da recursão:\n\\[\\text{adicionar\\_tipo}(\\text{id.lexval}, \\text{ListaIDs.tipo\\_herdado})\\]\nQuando chegamos ao último, ou ao único, identificador da lista, simplesmente adicionamos suas informações à Tabela de Símbolos. Não há recursão para propagar.\n5. Tabela Resumida:\n\n\n\n\n\n\n\nProdução\nRegras Semânticas\n\n\n\n\n\\(\\text{Declaracao} \\rightarrow \\text{Tipo} \\text{ListaIDs}\\)\n\\(\\text{ListaIDs.tipo\\_herdado} = \\text{Tipo.tipo}\\)\n\n\n\\(\\text{Tipo} \\rightarrow \\textbf{int}\\)\n\\(\\text{Tipo.tipo} = \\text{'integer'}\\)\n\n\n\\(\\text{Tipo} \\rightarrow \\textbf{float}\\)\n\\(\\text{Tipo.tipo} = \\text{'real'}\\)\n\n\n\\(\\text{ListaIDs} \\rightarrow \\text{ListaIDs}_1, \\textbf{id}\\)\n\\(\\text{ListaIDs}_1\\text{.tipo\\_herdado} = \\text{ListaIDs.tipo\\_herdado}\\)  adicionar_tipo(id.lexval, ListaIDs.tipo_herdado)\n\n\n\\(\\text{ListaIDs} \\rightarrow \\textbf{id}\\)\nadicionar_tipo(id.lexval, ListaIDs.tipo_herdado)\n\n\n\n6. Exemplo de Avaliação: para a declaração float x, y;, a árvore sintática anotada seria construída conforme ilustrado na Figura Figure 10.5, na qual, temos:\n\nO atributo Tipo.tipo é sintetizado com valor 'real';\nEste valor é passado como ListaIDs.tipo_herdado;\nO tipo é propagado recursivamente através da lista;\nCada identificador é adicionado à Tabela de Símbolos com o tipo correto.\n\n\n\n\n\n\n\nExemplo de Gramática de Atributos\n\n\n\n\nFigure 10.5\n\n\n\n\n\n10.5.3.2 Exemplo 2: Estrutura de Tomada de Decisão (if-then-else)\nVamos construir uma Gramática de Atributos para processar estruturas condicionais como if (x &gt; 0) then y = 1 else y = -1.\n1. Gramática Base (\\(G\\)):\n\\[\n\\begin{align*}\n\\text{Comando} & \\rightarrow \\textbf{if} \\, \\text{Expr} \\, \\textbf{then} \\, \\text{Comando}_1 \\, \\textbf{else} \\, \\text{Comando}_2 \\\\\n\\text{Expr} & \\rightarrow \\text{Expr}_1 \\, \\text{oprel} \\, \\text{Expr}_2 \\\\\n\\text{Expr} & \\rightarrow \\textbf{id} \\mid \\textbf{num}\n\\end{align*}\n\\]\n2. Análise do Fluxo de Informação: para processar estruturas condicionais, precisamos identificar como a informação flui pela árvore sintática:\n\n\n\n\n\n\nÁrvore Sintática de If-Then-Else\n\n\n\n\nFigure 10.6\n\n\n\n\nInformação que desce (herdada): cada comando precisa saber qual rótulo usar como destino para continuar a execução após sua conclusão. Isso é necessário para estruturas aninhadas. A expressão booleana precisa conhecer os rótulos de destino para quando a condição for verdadeira ou falsa.\nInformação que sobe (sintetizada): cada componente da estrutura tem características que precisam ser agregadas, como informações de tipo ou outras propriedades semânticas.\n\n3. Definição dos Atributos (\\(A\\))\nPara o símbolo Comando:\n\nAtributo herdado proximo: rótulo para no qual o fluxo deve ir após executar este comando.\nJustificativa: o destino após o comando é determinado pelo contexto (comando pai), não pelo próprio comando.\n\nPara o símbolo Expr:\n\nAtributo herdado verdadeiro: rótulo para saltar quando a expressão for verdadeira.\nAtributo herdado falso: rótulo para saltar quando a expressão for falsa.\nJustificativa: os destinos dos saltos são determinados pela estrutura de controle que contém a expressão.\nAtributo sintetizado local: nome da variável ou temporária que armazena o resultado (para expressões não booleanas).\n\nPara os terminais id e num:\n\nAtributo sintetizado lexval: valor léxico do identificador ou número.\n\n4. Construção das Regras Semânticas (\\(R\\))\nAssumimos a existência da função auxiliar:\n\nnovo_rotulo(): gera um rótulo único (L1, L2, etc.)\n\nProdução 1:\n\\[\\text{Comando} \\rightarrow \\textbf{if} \\, \\text{Expr} \\, \\textbf{then} \\, \\text{Comando}_1 \\, \\textbf{else} \\, \\text{Comando}_2\\]\nEsta produção implementa a estrutura condicional completa:\n\\[\n\\begin{align*}\n\\text{Expr.verdadeiro} &:= \\text{novo\\_rotulo()} \\\\\n\\text{Expr.falso} &:= \\text{novo\\_rotulo()} \\\\\n\\text{Comando}_1\\text{.proximo} &:= \\text{Comando.proximo} \\\\\n\\text{Comando}_2\\text{.proximo} &:= \\text{Comando.proximo}\n\\end{align*}\n\\]\nPara entender o raciocínio por trás dessas regras:\n\nCriação de rótulos: geramos dois rótulos únicos, um para o comando then (Expr.verdadeiro) e outro para o comando else (Expr.falso).\nPropagação do proximo: ambos os comandos (then e else) devem convergir para o mesmo ponto de continuação, que é herdado do contexto externo. Isso garante que, independentemente do caminho tomado, a execução continue no mesmo ponto.\nFluxo de controle: a expressão usa seus atributos herdados verdadeiro e falso para determinar os destinos dos saltos condicionais.\n\nProdução 2:\n\\[\\text{Expr} \\rightarrow \\text{Expr}_1 \\, \\text{oprel} \\, \\text{Expr}_2\\]\nEsta produção define como os atributos herdados verdadeiro e falso são usados para especificar a semântica do desvio.\n\\[\\text{realizar\\_desvio\\_condicional}(\\text{Expr}_1\\text{.local, oprel.lexval, Expr}_2\\text{.local, Expr.verdadeiro, Expr.falso})\\]\nA regra invoca uma ação semântica que representa a lógica do desvio. Ela usa os locais das subexpressões e os rótulos herdados para indicar que, se a condição Expr₁.local oprel.lexval Expr₂.local for verdadeira, o fluxo deve ir para Expr.verdadeiro; caso contrário, para Expr.falso.\nProdução 3: a partir da regra:\n\\[\\text{Expr} \\rightarrow \\textbf{id}\\]\nTeremos: \\[\\text{Expr.local} = \\text{id.lexval}\\]\nUm identificador não requer processamento adicional; seu valor está em sua entrada na Tabela de Símbolos. Apenas registramos sua localização.\nProdução 4: considerando a regra:\n\\[\\text{Expr} \\rightarrow \\textbf{num}\\]\nTeremos:\n\\[\\text{Expr.local} = \\text{num.lexval}\\]\nProcesso análogo ao identificador; constantes são usadas diretamente.\n5. Tabela de Regras Semânticas:\n\n\n\n\n\n\n\nProdução\nRegras Semânticas\n\n\n\n\n\\(\\text{Comando} \\rightarrow \\textbf{if} \\, \\text{Expr} \\, \\textbf{then} \\, \\text{Cmd}_1 \\, \\textbf{else} \\, \\text{Cmd}_2\\)\n\\(\\text{Expr.verdadeiro} := \\text{novo\\_rotulo()}\\)  \\(\\text{Expr.falso} := \\text{novo\\_rotulo()}\\)  \\(\\text{Cmd}_1\\text{.proximo} := \\text{Cmd.proximo}\\)  \\(\\text{Cmd}_2\\text{.proximo} := \\text{Cmd.proximo}\\)\n\n\n\\(\\text{Expr} \\rightarrow \\text{Expr}_1 \\, \\text{oprel} \\, \\text{Expr}_2\\)\nrealizar_desvio_condicional(Expr₁.local, oprel.lexval, Expr₂.local, Expr.verdadeiro, Expr.falso)\n\n\n\\(\\text{Expr} \\rightarrow \\textbf{id}\\)\n\\(\\text{Expr.local} = \\text{id.lexval}\\)\n\n\n\\(\\text{Expr} \\rightarrow \\textbf{num}\\)\n\\(\\text{Expr.local} = \\text{num.lexval}\\)\n\n\n\n6. Exemplo de Avaliação: para o comando if (x &gt; 0) then y = 1 else y = -1 com Comando.proximo = L_fim:\n\nExpr.verdadeiro recebe um novo rótulo L1\nExpr.falso recebe um novo rótulo L2\nComando₁.proximo recebe L_fim (ambos os ramos convergem para o mesmo destino)\nComando₂.proximo recebe L_fim\n\nA árvore sintática deve ser anotada com estes valores, o que permitirá que a estrutura condicional seja processada corretamente com os destinos apropriados para cada um dos caminho de execução.\n\n\n10.5.3.3 Exemplo 3: Laço de Repetição (while)\nFinalmente, vamos construir uma Gramática de Atributos para laços como while (x &gt; 0) do x = x - 1, em que o fluxo de controle deve retornar ao início após cada iteração.\n1. Gramática Base (\\(G\\)):\n\\[\n\\begin{align*}\n\\text{Comando} & \\rightarrow \\textbf{while} \\, \\text{Expr} \\, \\textbf{do} \\, \\text{Comando}_1 \\\\\n\\text{Expr} & \\rightarrow \\text{Expr}_1 \\, \\text{oprel} \\, \\text{Expr}_2 \\\\\n\\text{Expr} & \\rightarrow \\textbf{id} \\mid \\textbf{num} \\\\\n\\text{Comando} & \\rightarrow \\textbf{id} \\, \\text{=} \\, \\text{Expr}\n\\end{align*}\n\\]\n2. Análise do Fluxo de Informação: para laços de repetição, a análise do fluxo de informação é crucial para garantir que o controle retorne ao início do laço após cada iteração. A árvore sintática para um comando while pode ser vista na Figura Figure 10.7:\n\n\n\n\n\n\nÁrvore Sintática de While\n\n\n\n\nFigure 10.7\n\n\n\n\nInformação que desce (herdada): o comando do corpo do laço precisa saber para que ponto do código deve ir após sua execução, como voltar ao início do laço. A expressão precisa saber para que ponto saltar em caso verdadeiro, início do corpo, e falso, após o laço.\nInformação que sobe (sintetizada): cada componente tem propriedades que precisam ser agregadas.\nPonto fundamental: o laço cria um ciclo no fluxo de controle - após executar o corpo, devemos retornar à avaliação da condição.\n\n3. Definição dos Atributos (\\(A\\)): avaliando símbolo por símbolo, teremos:\nPara o símbolo Comando (em geral):\n\nAtributo herdado proximo: rótulo para continuar após este comando. Determina o fluxo após a conclusão do comando.\n\nPara o símbolo Expr:\n\nAtributo herdado verdadeiro: rótulo quando a condição é verdadeira (entra no corpo).\nAtributo herdado falso: rótulo quando a condição é falsa (sai do laço). Os dois controlam o fluxo baseado na avaliação da condição.\nAtributo sintetizado local: localização do resultado da expressão.\n\n4. Construção das Regras Semânticas (\\(R\\)): vamos definir as regras individualmente para cada produção.\nProdução 1:\n\\[\\text{Comando} \\rightarrow \\textbf{while} \\, \\text{Expr} \\, \\textbf{do} \\, \\text{Comando}_1\\]\nEsta é a produção central, que implementa a semântica do laço:\n\\[\n\\begin{align*}\n\\text{inicio\\_laco} &:= \\text{novo\\_rotulo()} \\\\\n\\text{Expr.verdadeiro} &:= \\text{novo\\_rotulo()} \\\\\n\\text{Expr.falso} &:= \\text{Comando.proximo} \\\\\n\\text{Comando}_1\\text{.proximo} &:= \\text{inicio\\_laco}\n\\end{align*}\n\\]\nObservando o raciocínio por trás dessas regras veremos:\n\nCriação do rótulo de início (inicio_laco): este é o ponto para o qual voltaremos após cada iteração. Este é um valor gerado localmente na regra para controlar o fluxo; não é um atributo formal do símbolo Comando.\nConfiguração dos destinos da expressão:\n\nExpr.verdadeiro: cria um novo rótulo que marca o início do corpo do laço.\nExpr.falso: aponta para Comando.proximo, ou seja, o código após o laço. Quando a condição falha, saímos do laço.\n\nFechamento do ciclo (Comando₁.proximo := inicio_laco): esta é a regra fundamental que implementa a semântica do laço. Dizemos ao corpo do laço que, após sua execução, ele deve retornar ao inicio_laco, criando assim o ciclo de repetição.\nDistinção importante: no if-then-else, após executar o “then”, saltamos para proximo (para frente). No while, após executar o corpo, saltamos para inicio_laco (para trás), criando o ciclo.\n\nProdução 2:\n\\[\\text{Comando} \\rightarrow \\textbf{id} \\, \\text{=} \\, \\text{Expr}\\]\nComando de atribuição simples (necessário para o corpo do laço). Os atributos são processados conforme apropriado para uma atribuição.\nProdução 3:\n\\[\\text{Expr} \\rightarrow \\text{Expr}_1 \\, \\text{oprel} \\, \\text{Expr}_2\\]\nExpressão relacional (idêntica ao exemplo anterior). Os atributos herdados verdadeiro e falso são usados pela ação semântica para especificar o desvio.\nProdução 4: a partir da regra:\n\\[\\text{Expr} \\rightarrow \\textbf{id}\\]\nteremos:\n\\[\\text{Expr.local} = \\text{id.lexval}\\]\nProdução 5: a partir da regra:\n\\[\\text{Expr} \\rightarrow \\textbf{num}\\]\nteremos:\n\\[\n\\text{Expr.local} = \\text{num.lexval}\n\\]\n5. Tabela de Regras Semânticas:\n\n\n\n\n\n\n\nProdução\nRegras Semânticas\n\n\n\n\n\\(\\text{Comando} \\rightarrow \\textbf{while} \\, \\text{Expr} \\, \\textbf{do} \\, \\text{Cmd}_1\\)\n\\(\\text{inicio\\_laco} := \\text{novo\\_rotulo()}\\)  \\(\\text{Expr.verdadeiro} := \\text{novo\\_rotulo()}\\)  \\(\\text{Expr.falso} := \\text{Cmd.proximo}\\)  \\(\\text{Cmd}_1\\text{.proximo} := \\text{inicio\\_laco}\\)\n\n\n\\(\\text{Comando} \\rightarrow \\textbf{id} \\, \\text{=} \\, \\text{Expr}\\)\nrealizar_atribuicao(id.lexval, Expr.local)\n\n\n\\(\\text{Expr} \\rightarrow \\text{Expr}_1 \\, \\text{oprel} \\, \\text{Expr}_2\\)\nrealizar_desvio_condicional(Expr₁.local, oprel.lexval, Expr₂.local, Expr.verdadeiro, Expr.falso)\n\n\n\\(\\text{Expr} \\rightarrow \\textbf{id}\\)\n\\(\\text{Expr.local} = \\text{id.lexval}\\)\n\n\n\\(\\text{Expr} \\rightarrow \\textbf{num}\\)\n\\(\\text{Expr.local} = \\text{num.lexval}\\)\n\n\n\n6. Exemplo de Avaliação:\nPara o comando while (x &gt; 0) do x = x - 1 com Comando.proximo = L_fim:\n\nUm novo rótulo L_inicio é gerado para o laço;\nExpr.verdadeiro recebe um novo rótulo L_corpo;\nExpr.falso recebe L_fim (sai do laço);\nComando₁.proximo recebe L_inicio (retorna ao início - ciclo!).\n\nDiferença fundamental entre o if e o while:\nNo if-then-else, após executar o “then”, saltamos para proximo (para frente):\n\\[\\text{Comando}_1\\text{.proximo} = \\text{Comando.proximo}\\]\nNo while, após executar o corpo, saltamos para inicio_laco (para trás):\n\\[\\text{Comando}_1\\text{.proximo} = \\text{inicio\\_laco}\\]\nEsta diferença nas regras semânticas é o que implementa comportamentos completamente distintos: execução condicional única versus repetição em ciclo. A árvore sintática anotada com estes atributos captura perfeitamente a semântica do laço de repetição.\n\n\n\n10.5.4 Avaliação de Atributos: Grafos de Dependência e Ordenação Topológica\nAs regras semânticas de uma gramática de atributos formam um sistema de equações. Contudo, para resolver esse sistema, precisamos estabelecer uma ordem de avaliação, pois o cálculo de um atributo frequentemente depende do valor de outro. O Grafo de Dependências é a ferramenta formal que nos permite visualizar e determinar essa ordem.\nPara uma determinada árvore sintática, o grafo de dependências é um grafo direcionado \\(D = (V, E)\\) que torna explícitas as interdependências entre os atributos:\n\nVértices (\\(V\\)): Cada vértice no grafo de dependências corresponde a um atributo de um vértice na árvore sintática.\nArestas (\\(E\\)): Existe uma aresta direcionada do vértice \\((n_1, a_1)\\) para \\((n_2, a_2)\\) se, e somente se, o valor do atributo a2 depende diretamente do valor do atributo a1.\n\n\n\n\n\n\n\nGrafo de Dependências para x = a + b\n\n\n\n\nFigure 10.8: Exemplo de árvore sintática e seu grafo de dependências correspondente\n\n\n\nUma gramática de atributos é considerada não-circular se, para qualquer árvore sintática que ela possa gerar, seu grafo de dependências for acíclico (não contiver ciclos).\nO problema de encontrar a sequência correta de avaliação é resolvido pela ordenação topológica do grafo. O algoritmo geral consiste em construir o grafo, verificar se há ciclos e, em caso negativo, percorrê-lo em uma ordem topológica para calcular cada atributo.\n\n\n10.5.5 Classes de Gramáticas de Atributos e o Impacto na Eficiência\nEmbora a definição de uma gramática de atributos seja muito flexível, essa flexibilidade tem um custo. Regras semânticas irrestritas podem levar a dependências complexas, difíceis de avaliar ou, no pior caso, a dependências circulares que tornam a avaliação impossível. Para gerenciar essa complexidade, as gramáticas de atributos são classificadas em uma hierarquia baseada nas restrições impostas sobre o fluxo de informação. Em uma estrutura aninhada que pode ser vista na Figura Figure 10.9, cada classe é um subconjunto estrito da anterior, com propriedades específicas que impactam diretamente a implementação do compilador.\n\n\n\n\n\n\nClasses de Gramáticas de Atributos\n\n\n\n\nFigure 10.9\n\n\n\nEssa classificação é de suma importância para o design de linguagens e compiladores, pois representa um balanço direto entre o poder expressivo das regras semânticas e a eficiência do algoritmo de avaliação.\n\n10.5.5.1 Gramáticas S-Atribuídas\nA classe mais simples e restrita. Uma gramática é S-atribuída se ela utiliza apenas atributos sintetizados.\n\nCaracterística: A informação flui estritamente de baixo para cima (bottom-up) na árvore sintática.\nImplicação no Compilador: São muito fáceis de avaliar. Uma simples travessia em pós-ordem da árvore é suficiente para calcular todos os atributos. Elas se integram perfeitamente com analisadores sintáticos bottom-up (como LALR, usado por ferramentas como YACC/Bison), nos quais a ação semântica pode ser executada assim que uma produção é reduzida.\nComplexidade: Avaliação em tempo linear, \\(O(n)\\).\n\n\n\n10.5.5.2 Gramáticas L-Atribuídas\nEsta é a classe mais importante para a maioria das linguagens de programação modernas. Uma gramática é L-atribuída se, para cada produção \\(A \\rightarrow X_1 X_2 \\dots X_n\\), os atributos herdados de um símbolo \\(X_j\\) no lado direito dependem apenas:\n\nDos atributos herdados de \\(A\\) (o pai).\nDe quaisquer atributos dos símbolos à sua esquerda (\\(X_1, X_2, \\dots, X_{j-1}\\)).\n\n\nCaracterística: A informação pode fluir para baixo (herdada) e para cima (sintetizada), mas o fluxo de dependências herdadas segue estritamente a ordem da escrita, da esquerda para a direita. O “L” vem de Left-to-right.\nImplicação no Compilador: Esta restrição é o “ponto ideal” (sweet spot) do design de compiladores. Ela garante que todos os atributos possam ser avaliados em uma única travessia em profundidade e da esquerda para a direita da árvore sintática. Isso as torna perfeitas para analisadores de descida recursiva (top-down).\nComplexidade: Avaliação em tempo linear, \\(O(n)\\).\n\n\n\n10.5.5.3 Gramáticas Não-Circulares\nEsta é a classe mais geral de gramáticas de atributos que ainda é utilizável.\n\nCaracterística: A única restrição é que o grafo de dependências não pode ter ciclos para nenhuma árvore sintática possível. No entanto, as dependências podem ser complexas, com atributos dependendo de irmãos à sua direita, por exemplo.\nImplicação no Compilador: A ordem de avaliação não é fixa e pode mudar para cada árvore sintática. O compilador não pode usar uma simples travessia; ele é forçado a construir o grafo de dependências explicitamente e executar uma ordenação topológica.\nComplexidade: A avaliação é mais cara, geralmente quadrática (\\(O(n^2)\\)) ou pior, dependendo da estrutura do grafo. Linguagens com funcionalidades semânticas muito complexas, como templates em C++, podem exigir essa flexibilidade.\n\n\n\n10.5.5.4 Gramáticas Circulares\n\nCaracterística: Existe pelo menos uma árvore sintática que gera um grafo de dependências com um ciclo. Isso representa uma definição semântica recursiva sem caso base (ex: a = b + 1 e b = a * 2).\nImplicação no Compilador: São semanticamente malformadas e inúteis na prática, pois a avaliação de atributos é impossível. Pior ainda, o problema de determinar se uma gramática de atributos geral é circular ou não é indecidível.\n\nA tabela a seguir resume estas classes, suas características e o impacto na complexidade da análise semântica.\n\n\n\n\n\n\n\n\n\nClasse\nCaracterística\nComplexidade de Avaliação\nExemplo de Uso\n\n\n\n\nS-atribuída\nApenas atributos sintetizados\n\\(O(n)\\)\nCalculadoras, expressões\n\n\nL-atribuída\nRestrições em atributos herdados\n\\(O(n)\\)\nMaioria das linguagens (C, Java)\n\n\nAbsolutamente não-circular\nOrdem fixa para todas as árvores\n\\(O(n)\\)\nSubclasse de L-atribuídas\n\n\nNão-circular\nOrdem depende da árvore\n\\(O(n^2)\\)\nAda, C++ (templates)\n\n\nCircular\nPossui ciclos\nIndecidível\nNão utilizável\n\n\n\n\n\n\n10.5.6 Definição Formal e Exemplo Prático\nUma Definição Dirigida por Sintaxe (SDD) é a especificação formal de uma gramática de atributos. É uma generalização que associa regras semânticas a cada produção de uma GLC. Uma SDD especifica o que deve ser calculado, mas não como ou em que ordem. A ordem de avaliação é determinada pelas dependências entre os atributos.\nCom base nos tipos de atributos que utilizam, as SDDs podem ser classificadas em categorias importantes que impactam diretamente a implementação do compilador:\nGramáticas S-Atribuídas: São SDDs que utilizam apenas atributos sintetizados. A avaliação desses atributos pode ser feita de forma natural durante uma travessia em pós-ordem da árvore sintática. Isso as torna perfeitamente compatíveis com analisadores sintáticos bottom-up (como os \\(LR\\)), nos quais as ações semânticas podem ser executadas no momento em que uma produção é reduzida.\nGramáticas L-Atribuídas: São uma classe mais ampla de SDDs que permitem tanto atributos sintetizados quanto herdados, mas com uma restrição: para uma produção A→X₁X₂…Xₙ, um atributo herdado de Xⱼ só pode depender:\n\nDos atributos herdados de A; e\nDe quaisquer atributos, herdados ou sintetizados, dos símbolos à sua esquerda, X₁,…,Xⱼ₋₁.\n\nEssa restrição garante que os atributos possam ser avaliados em uma única travessia em profundidade e da esquerda para a direita da árvore sintática. Isso as torna ideais para implementação em conjunto com analisadores sintáticos top-down (como os \\(LL\\) e os de descida recursiva).\n\n\n10.5.7 Esquemas de Tradução Dirigida por Sintaxe (SDT)\nEnquanto uma SDD é uma especificação, um Esquema de Tradução Dirigida por Sintaxe (SDT) é uma implementação concreta. Um SDT é uma GLCna qual as ações semânticas (fragmentos de código) são embutidas diretamente no lado direito das produções. Essas ações são executadas no momento em que o analisador sintático reconhece aquela parte da produção.\nExemplo de SDT para uma calculadora simples:\n\n\n\nProdução\nAção Semântica\n\n\n\n\nE→E₁+T\n{ E.valor = E₁.valor + T.valor; }\n\n\nE→T\n{ E.valor = T.valor; }\n\n\nT→T₁*F\n{ T.valor = T₁.valor * F.valor; }\n\n\nT→F\n{ T.valor = F.valor; }\n\n\nF→(E)\n{ F.valor = E.valor; }\n\n\nF→num\n{ F.valor = num.lexval; }\n\n\n\nEste é um exemplo de uma gramática S-atribuída, pois todas as ações calculam um atributo para o símbolo à esquerda da produção a partir dos símbolos à direita.\nO formalismo das gramáticas de atributos e dos SDTs transforma a análise semântica, que poderia ser uma tarefa ad-hoc e propensa a erros, em um processo sistemático e declarativo. Ele nos permite raciocinar sobre o fluxo de informações contextuais de maneira estruturada, guiada diretamente pela sintaxe da linguagem, personificando o princípio da composicionalidade: o significado do todo é derivado do significado de suas partes.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analisadores Semânticos: a determinação do significado</span>"
    ]
  },
  {
    "objectID": "10-semantico.html#catálogo-de-verificações-semânticas-comuns",
    "href": "10-semantico.html#catálogo-de-verificações-semânticas-comuns",
    "title": "10  Analisadores Semânticos: a determinação do significado",
    "section": "10.6 Catálogo de Verificações Semânticas Comuns",
    "text": "10.6 Catálogo de Verificações Semânticas Comuns\nA análise semântica é a última linha de defesa do front-end contra programas incorretos. Ela é responsável por capturar uma ampla gama de erros que são sintaticamente invisíveis, mas semanticamente inválidos. Esta seção cataloga as verificações mais comuns realizadas por um analisador semântico, com exemplos práticos de código que falhariam em cada verificação.\n\n10.6.1 Verificações Relacionadas a Nomes e Escopos\nEstas verificações garantem que os identificadores sejam usados de acordo com as regras de escopo da linguagem. A Tabela de Símbolos é a ferramenta central para realizar essas checagens.\nUso de Identificador Não Declarado: Talvez o erro semântico mais fundamental. O compilador deve garantir que todo identificador (variável, função, etc.) tenha sido declarado antes de ser utilizado.\nExemplo de Erro:\nint main() {\n  int a = 5;\n  int b = a + c; // ERRO: 'c' não foi declarado.\n  return 0;\n}\nDeclaração Múltipla de Identificador no Mesmo Escopo: A maioria das linguagens proíbe a redeclaração de um mesmo identificador dentro do mesmo escopo.\nExemplo de Erro:\nvoid minhaFuncao() {\n  int x = 10;\n  String x = \"hello\"; // ERRO: 'x' já está definido neste escopo.\n}\n\n\n10.6.2 Verificações de Tipo\nO coração da análise semântica, a verificação de tipos garante que os dados sejam usados de maneira consistente com suas definições.\nIncompatibilidade em Atribuições: O tipo da expressão do lado direito de uma atribuição deve ser compatível (ou poder ser coagido) com o tipo da variável do lado esquerdo.\nExemplo de Erro:\nint idade;\nidade = \"vinte\"; // ERRO: Não é possível converter tipo 'string' para 'int'.\nIncompatibilidade em Operandos de Expressões: Os operandos de um operador devem ter tipos que são válidos para aquele operador.\nExemplo de Erro:\nresultado = \"Total: \" - 10 # ERRO: Operador '-' não suportado entre 'str' e 'int'.\nTipo Incorreto em Estruturas de Controle: A expressão de controle em estruturas como if, while e for deve resultar em um tipo booleano (ou um tipo que possa ser coagido para booleano).\nExemplo de Erro (em uma linguagem como Java):\nint contador = 0;\nif (contador) { // ERRO: 'contador' é int, mas um booleano é esperado.\n  //...\n}\n\n\n10.6.3 Verificações Relacionadas a Sub-rotinas (Funções/Métodos)\nEssas verificações garantem que as funções sejam chamadas corretamente, respeitando suas assinaturas.\nNúmero Incorreto de Argumentos em Chamada: A chamada de uma função deve fornecer exatamente o número de argumentos que sua declaração exige.\nExemplo de Erro:\nvoid imprimirSoma(int a, int b) {\n  std::cout &lt;&lt; a + b;\n}\n\nint main() {\n  imprimirSoma(10); // ERRO: Poucos argumentos para a função.\n  return 0;\n}\nTipo Incorreto de Argumentos em Chamada: Os tipos dos argumentos passados em uma chamada devem ser compatíveis com os tipos dos parâmetros na declaração da função.\nExemplo de Erro:\nvoid imprimirNome(std::string nome) {\n  std::cout &lt;&lt; nome;\n}\n\nint main() {\n  imprimirNome(123); // ERRO: Tipo de argumento inválido (esperado string, obteve int).\n  return 0;\n}\nRetorno de Valor Incompatível: O valor retornado por uma função deve ser compatível com o tipo de retorno declarado na sua assinatura.\nExemplo de Erro:\npublic int obterIdade() {\n  return \"jovem\"; // ERRO: Tipo de retorno incompatível.\n}\n\n\n10.6.4 Verificações de Controle de Fluxo\nEssas regras garantem que as construções de controle de fluxo sejam usadas em contextos apropriados.\nbreak ou continue Fora de um Laço: As palavras-chave break e continue só são permitidas dentro do corpo de estruturas de repetição (como while, for, do-while).\nExemplo de Erro:\nvoid verificar(int x) {\n  if (x &gt; 10) {\n    break; // ERRO: 'break' statement not within loop or switch.\n  }\n}\nUso de Rótulos (Labels) Inexistentes: Em linguagens que suportam goto, o rótulo de destino deve existir dentro do escopo da função atual.\nExemplo de Erro:\nvoid processar() {\n  //...\n  goto erro_fatal; // ERRO: Rótulo 'erro_fatal' não definido.\n}",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analisadores Semânticos: a determinação do significado</span>"
    ]
  },
  {
    "objectID": "10-semantico.html#o-produto-final-a-árvore-de-sintaxe-abstrata-atribuída",
    "href": "10-semantico.html#o-produto-final-a-árvore-de-sintaxe-abstrata-atribuída",
    "title": "10  Analisadores Semânticos: a determinação do significado",
    "section": "10.7 O Produto Final: A Árvore de Sintaxe Abstrata Atribuída",
    "text": "10.7 O Produto Final: A Árvore de Sintaxe Abstrata Atribuída\nO resultado de uma análise semântica bem-sucedida, em uma passagem ou em múltiplas passagens, não será percebido por um valor booleano indicando sucesso ou falha. O produto tangível desta fase será uma Árvore de Sintaxe Abstrata “Atribuída”, ou em alguns livros árvore sintática abstrata anotada, ou ainda árvore sintática abstrata decorada.\nO processo de decorar a árvore consiste em percorrer seus vértices e enriquecê-los com informações semânticas que foram inferidas durante a análise. Este é um processo de atribuição de informação aos vértices da árvore sintática. Por exemplo:\n\num vértice que representa o uso de uma variável pode ser decorado com um ponteiro para a entrada correspondente na Tabela de Símbolos;\num vértice de expressão (como + ou *) pode ser decorado com o tipo resultante da operação (ex.: int ou float);\num vértice de declaração de tipo pode ser decorado com informações sobre o layout de memória daquele tipo.\n\nEssa AST atribuída é uma estrutura de dados rica. Ela contém não apenas a estrutura sintática do programa, mas também todo o entendimento contextual que o compilador adquiriu durante o processo de compilação. Ela serve como a única fonte de verdade para as fases subsequentes do processo de compilação, como a geração de código intermediário e a otimização. Nesse sentido, a análise semântica não apenas detecta e elimina erros, mas enriquece a representação do programa com informações que o fortalecem e o preparam para a transformação final em código executável.\nA AST é a principal e mais importante estrutura de dados usada como entrada pelo analisador semântico. Mas, não é a única.\nPara que um compilador possa realizar verificações semânticas, como garantir que uma variável foi declarada antes de ser usada ou que o tipo de uma expressão é válido, ele precisa de um mecanismo para registrar e recuperar informações sobre os identificadores encontrados no código-fonte. Essa “memória” do compilador é a Tabela de Símbolos. Os processos de análise léxica e sintática precisaram consultar esta tabela para identificar palavras reservadas e tipos básicos, mas é o analisador semântico que fará o uso mais intensivo e complexo desta estrutura.\nA Tabela de Símbolos é uma estrutura de dados fundamental que associa cada identificador, ou símbolo, a um conjunto de informações, os atributos que citamos antes, que pertencem ao símbolo. Esta tabela funciona como um dicionário em que as chaves são os nomes dos símbolos e os valores são os metadados coletados sobre eles.\nPraticamente todas as fases do compilador interagem com a Tabela de Símbolos:\n\nAnálise Léxica: ao encontrar um identificador, o analisador léxico pode inseri-lo na Tabela de Símbolos se ele ainda não estiver presente. Ou para classificar um determinado lexema como uma palavra reservada, o analisador léxico consulta a tabela;\nAnálise Sintática/Semântica: o analisador semântico consulta a Tabela de Símbolos constantemente para verificar declarações, tipos e escopos. Ele também a preenche com informações obtidas das declarações.\nGeração de Código e Otimização: as fases do back-end usam a tabela para obter informações como o tipo de uma variável e seu endereço de memória para gerar o código de máquina apropriado.\n\nA Tabela de Símbolos está detalhada na seção Chapter 14",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analisadores Semânticos: a determinação do significado</span>"
    ]
  },
  {
    "objectID": "10-semantico.html#verificação-de-tipos-type-checking",
    "href": "10-semantico.html#verificação-de-tipos-type-checking",
    "title": "10  Analisadores Semânticos: a determinação do significado",
    "section": "10.8 Verificação de Tipos, Type Checking",
    "text": "10.8 Verificação de Tipos, Type Checking\nA verificação de tipos é, sem dúvida, a tarefa central da análise semântica. Ela consiste em percorrer a AST e aplicar um conjunto de regras para garantir que os tipos de dados sejam usados de maneira consistente e segura. Um tipo pode ser formalmente definido como um conjunto de valores e um conjunto de operações permitidas sobre esses valores. A verificação de tipos assegura que um programa não tente realizar operações absurdas, como somar uma string a um objeto de arquivo ou calcular a raiz quadrada de um booleano.\n\n10.8.1 A Importância dos Tipos: Prevenindo o “Absurdo” Computacional\nOs sistemas de tipos impõem uma disciplina que traz enormes benefícios, principalmente a detecção de uma vasta classe de erros antes mesmo da execução do programa. Essa verificação pode ocorrer em momentos distintos, o que nos leva a uma das mais importantes classificações de linguagens de programação.\n\n10.8.1.1 Tipagem Estática vs. Dinâmica\nTipagem Estática: A verificação de tipos é realizada em tempo de compilação. O compilador analisa o código-fonte, infere ou lê as anotações de tipo e valida todas as operações. Se um erro de tipo é encontrado, a compilação falha. Linguagens como Java, C++, C#, Rust e Haskell são estaticamente tipadas.\nVantagens:\n\nDetecção Precoce de Erros: Erros de tipo são encontrados durante o desenvolvimento, não em produção.\nPerformance: O compilador pode gerar código de máquina altamente otimizado, pois não precisa inserir verificações de tipo em tempo de execução nem carregar “tags” de tipo junto com os dados.\nDocumentação e Manutenibilidade: As declarações de tipo servem como uma forma de documentação, tornando o código mais fácil de entender e refatorar.\n\nDesvantagens: Pode ser menos flexível e exigir mais código explícito (boilerplate) do programador.\nTipagem Dinâmica: A verificação de tipos é adiada para tempo de execução. As variáveis não possuem tipos fixos, mas os valores que elas contêm sim. Antes de cada operação, o sistema de tempo de execução verifica se os tipos dos valores são compatíveis. Se não forem, um erro é lançado durante a execução. Linguagens como Python, JavaScript, Ruby e LISP são dinamicamente tipadas.\nVantagens:\n\nFlexibilidade: permite estruturas de dados heterogêneas e um estilo de programação mais fluido, ideal para prototipagem rápida.\nMenos Verboso: geralmente, não requer anotações de tipo explícitas.\n\nDesvantagens:\n\nErros em Tempo de Execução: Erros de tipo só são descobertos quando o código é efetivamente executado, o que pode ocorrer tardiamente no ciclo de vida do software.\noverhead de Performance: A necessidade de verificar tipos em tempo de execução e de armazenar informações de tipo com cada valor impõe uma penalidade de desempenho.\n\n\n\n10.8.1.2 Tipagem Forte vs. Fraca\nEssa distinção, muitas vezes confundida com estática vs. dinâmica, refere-se ao rigor com que a linguagem impõe suas regras de tipo.\nTipagem Forte: Uma linguagem fortemente tipada não permite que operações sejam realizadas entre tipos incompatíveis, prevenindo comportamento indefinido. Ela pode permitir conversões implícitas (coerção), mas apenas de maneiras bem-definidas e seguras. Por exemplo, emPython (fortemente tipado), 3 + \"olá\" resulta em um TypeError em tempo de execução, pois a operação não está definida.\nTipagem Fraca: Uma linguagem fracamente tipada possui regras de tipo mais permissivas e pode tentar realizar operações mesmo entre tipos incompatíveis, muitas vezes tratando os dados como sequências de bits. Em C (fracamente tipado), é possível usar um *cast para tratar um ponteiro para um inteiro como um ponteiro para uma estrutura, contornando o sistema de tipos e abrindo a porta para erros difíceis de depurar e vulnerabilidades de segurança.\nÉ importante que a atenta leitora perceba que essas duas classificações são ortogonais. Temos:\n\nEstática e Forte: Java, Rust, Haskell;\nEstática e Fraca: C, C++;\nDinâmica e Forte: Python, Ruby;\nDinâmica e Fraca: Tcl, JavaScript.\n\nA escolha de um sistema de tipos reflete uma filosofia de design da linguagem, um balanço entre segurança, flexibilidade, performance e expressividade.\n\n\n\n10.8.2 Equivalência de Tipos: Quando T1 é igual a T2?\nPara verificar se uma operação é válida, o compilador precisa primeiro determinar se os tipos envolvidos são equivalentes. Existem duas abordagens principais para isso:\nEquivalência por Nome (Nominal): dois tipos são considerados equivalentes se, e somente se, eles têm o mesmo nome. Cada declaração de um novo tipo (ex.: com typedef em C, type em Pascal, ou class em Java) cria um tipo único e distinto dos outros, mesmo que suas estruturas subjacentes sejam idênticas. Essa é a abordagem mais comum em linguagens modernas como Java, C# e C++. Esta abordagem é mais segura e reflete melhor a intenção do programador.\nExemplo:\n// Definição da classe para representar um Ponto\nclass Ponto {\n  int x;\n  int y;\n\n  public Ponto(int x, int y) {\n    this.x = x;\n    this.y = y;\n  }\n}\n\n// Definição da classe para representar um Vetor\nclass Vetor {\n  int x;\n  int y;\n\n  public Vetor(int x, int y) {\n    this.x = x;\n    this.y = y;\n  }\n}\n\n// Classe principal para testar a compatibilidade\npublic class TesteTipos {\n  public static void main(String[] args) {\n    Ponto p = new Ponto(10, 20);\n    Vetor v = new Vetor(5, -3);\n\n    // A linha a seguir causará um ERRO DE COMPILAÇÃO\n    // p = v; \n    // Erro típico: \"incompatible types: Vetor cannot be converted to Ponto\"\n  }\n}\nApesar de Ponto e Vetor serem estruturalmente idênticas, ambas possuem dois campos int chamados x e y, o compilador Java as considera como tendo tipos completamente incompatíveis. A tentativa de atribuir um objeto Vetor a uma variável do tipo Ponto (p = v;) falha com um erro claro, como: error: incompatible types: Vetor cannot be converted to Ponto. Isso ocorre porque, para o Java, o que importa é o nome do tipo. Ponto é um nome, Vetor é outro. Em Java a equivalência é nominal.\nEquivalência Estrutural: dois tipos são equivalentes se eles têm a mesma estrutura interna. Os nomes dos tipos são ignorados; o compilador expande as definições de tipo e compara suas formas recursivamente. Essa abordagem é mais flexível, mas pode levar a equivalências acidentais entre tipos que são conceitualmente diferentes. É usada em linguagens como Go e TypeScript.\nExemplo: no exemplo acima, sob equivalência estrutural, Ponto e Vetor seriam considerados o mesmo tipo, pois ambos são estruturas com dois campos inteiros chamados x e y. A atribuição v = p; seria válida.\n\n\n10.8.3 Compatibilidade de Tipos: Coerção e Conversão (Casting)\nMesmo que dois tipos não sejam equivalentes, eles podem ser compatíveis. A compatibilidade permite que um valor de um tipo seja usado em um contexto que espera outro.\nConversão (Casting): é uma mudança de tipo explícita, solicitada pelo programador. O cast informa ao compilador: “Eu sei o que estou fazendo; trate este valor como se fosse deste outro tipo”. O compilador insere o código necessário para realizar a conversão.\nExemplo em C:\nint i;\nfloat f = 3.14; \ni = (int)f;\nCoerção (Coercion): é uma conversão de tipo implícita, realizada automaticamente pelo compilador para que uma operação seja válida. Isso acontece em “promoções numéricas”, por exemplo.\nExemplo:\ndouble d = 5;\nO compilador automaticamente promove o literal inteiro 5 para o valor de ponto flutuante 5.0 antes da atribuição. A coerção aumenta a conveniência, mas pode mascarar erros graves se as regras não forem bem compreendidas.\n\n\n10.8.4 Lidando com Polimorfismo: “Muitas Formas”\nPolimorfismo, da expressão em grego para muitas formas, é a capacidade de uma única interface, como uma função ou um operador, ser usada com diferentes tipos de dados. Existem três tipos principais de polimorfismo que um sistema de tipos deve gerenciar:\n\nPolimorfismo Ad-hoc (Sobrecarga): ocorre quando múltiplas funções ou métodos compartilham o mesmo nome, mas têm assinaturas (tipos e/ou número de parâmetros) diferentes. O compilador (ou o sistema de tempo de execução) seleciona a implementação correta com base nos tipos dos argumentos fornecidos na chamada. O exemplo clássico é o operador +, que realiza adição para números e concatenação para strings.\nPolimorfismo Paramétrico (Generics): permite que uma função ou tipo de dado seja escrito de forma genérica, para que possa lidar com valores de forma idêntica, sem depender de seu tipo. O código é parametrizado por uma ou mais variáveis de tipo. Um exemplo é uma função reverse que pode reverter uma lista de qualquer tipo, pois o algoritmo de reversão não depende do tipo dos elementos da lista.\nPolimorfismo de Subtipo (Herança): é a base da programação orientada a objetos. Permite que um objeto de uma classe derivada (subtipo) seja usado em qualquer contexto que espere um objeto da classe base. Por exemplo, uma função que espera um objeto do tipo Animal pode receber, sem problemas, um objeto do tipo Cachorro, assumindo que Cachorro herda de Animal.\n\n\n\n10.8.5 A Base Formal dos Sistemas de Tipos\nAté agora, discutimos a verificação de tipos de forma intuitiva. No entanto, para que um compilador possa executar essa tarefa de maneira automática e correta, nossa intuição precisa ser traduzida para um sistema matemático rigoroso. A teoria dos tipos nos fornece exatamente esse formalismo, permitindo não apenas especificar as regras de tipo de uma linguagem, mas também provar que um programa é seguro com relação a tipos, conceito chamado de type-safe, antes de sua execução.\nO pilar deste sistema é uma afirmação conhecida como julgamento de tipo, em inglês: typing judgment, representada pela seguinte notação:\n\\[\\Gamma \\vdash e : \\tau\\]\nA atenta leitora não deve se intimidar com os símbolos. Essa expressão formaliza uma ideia bastante simples e pode ser lida da seguinte forma: “No contexto \\(\\Gamma\\), é possível provar que a expressão \\(e\\) tem o tipo \\(\\tau\\)”. Vamos dissecar cada componente:\n\n\\(\\Gamma\\) (Gama): é o contexto de tipagem, também chamado de ambiente. Na prática, ele é a nossa Tabela de Símbolos. É um mapa que armazena as informações de tipo que já conhecemos, como as associações entre nomes de variáveis e seus respectivos tipos (por exemplo, \\(\\{ x: \\text{int}, y: \\text{bool} \\}\\)).\n\\(\\vdash\\) (Turnstile, ou “|-”, ou ainda catraca): é o símbolo de derivação. Ele separa as nossas premissas (o que já sabemos, o contexto \\(\\Gamma\\)) da nossa conclusão, a afirmação de que \\(e\\) tem o tipo \\(\\tau\\).\n\\(e : \\tau\\): é a afirmação central que queremos provar: a expressão \\(e\\) tem o tipo \\(\\tau\\).\n\nCom essa estrutura, podemos definir as regras de tipo de uma linguagem como regras de inferência. Elas nos dizem como derivar novos julgamentos de tipo a partir de julgamentos existentes. A estrutura de uma regra de inferência é sempre a mesma: se as premissas (acima da linha) forem verdadeiras, então podemos concluir que a afirmação (abaixo da linha) também é verdadeira.\nVejamos as três regras fundamentais para uma linguagem funcional simples:\nRegra da Variável (O Axioma): esta é a regra mais básica, nosso ponto de partida. Ela formaliza a consulta à Tabela de Símbolos.\n\\[\\frac{x : \\tau \\in \\Gamma}{\\Gamma \\vdash x : \\tau}\\]\nLeitura: se o contexto \\(\\Gamma\\) nos informa que a variável x está associada ao tipo \\(\\tau\\), então podemos concluir que, neste contexto, x tem o tipo \\(\\tau\\). Este é o nosso caso base: a prova de que o tipo de uma variável é simplesmente o tipo que lhe foi declarado.\nRegra da Abstração (Definição de Função): esta regra nos ensina a determinar o tipo de uma função (em cálculo lambda, uma “abstração”).\n\\[\\frac{\\Gamma, x : \\tau_1 \\vdash e : \\tau_2}{\\Gamma \\vdash \\lambda x : \\tau_1 . e : \\tau_1 \\rightarrow \\tau_2}\\]\nLeitura: para descobrir o tipo de uma função \\(\\lambda x.e\\), primeiro assumimos que seu argumento x tem um tipo \\(\\tau_1\\). Adicionamos essa suposição (\\(x : \\tau_1\\)) ao nosso contexto, criando um novo contexto \\(\\Gamma, x : \\tau_1\\). Se, nesse novo contexto, conseguirmos provar que o corpo da função \\(e\\) tem o tipo \\(\\tau_2\\), então podemos concluir que a função inteira tem o tipo \\(\\tau_1 \\rightarrow \\tau_2\\), uma função que recebe \\(\\tau_1\\) e retorna \\(\\tau_2\\).\nRegra da Aplicação (Chamada de Função): esta regra governa como verificamos a chamada de uma função.\n\\[\\frac{\\Gamma \\vdash e_1 : \\tau_1 \\rightarrow \\tau_2 \\quad \\Gamma \\vdash e_2 : \\tau_1}{\\Gamma \\vdash e_1 \\, e_2 : \\tau_2}\\]\nLeitura: para verificar a chamada de função \\(e_1 \\, e_2\\), precisamos provar duas coisas em nosso contexto \\(\\Gamma\\): primeiro, que \\(e_1\\) é de fato uma função do tipo \\(\\tau_1 \\rightarrow \\tau_2\\), e segundo, que o argumento \\(e_2\\) que estamos passando para ela tem o tipo \\(\\tau_1\\). Se ambas as premissas forem verdadeiras, podemos concluir que a expressão inteira da chamada de função resulta em um valor do tipo \\(\\tau_2\\).\n\n\n10.8.6 Construindo uma Árvore de Derivação: Um Exemplo Prático\nPara tornar essas regras abstratas mais concretas, vamos construir uma árvore de derivação de tipos passo a passo. Nosso objetivo será provar formalmente que a expressão (λx:int. x + 1) 5 tem o tipo int, partindo de um contexto vazio Γ = {}.\nUma árvore de derivação é uma representação visual de como as regras de inferência se encaixam. A expressão final que queremos provar fica na raiz (embaixo), e as premissas formam os galhos que crescem para cima, terminando em axiomas (regras que não têm premissas).\nAntes de começar, precisamos de mais duas regras simples para lidar com literais e operadores:\nRegra da Constante (ou Literal): Um literal já carrega seu próprio tipo. \\[\\frac{c \\text{ é um literal de tipo } \\tau}{\\Gamma \\vdash c : \\tau}\\]\nRegra do Operador (exemplo para +): Se somarmos dois inteiros, o resultado é um inteiro. \\[\\frac{\\Gamma \\vdash e_1 : \\text{int} \\quad \\Gamma \\vdash e_2 : \\text{int}}{\\Gamma \\vdash e_1 + e_2 : \\text{int}}\\]\nAgora, vamos provar que {} ⊢ (λx:int. x + 1) 5 : int. Construímos a prova de baixo para cima, aplicando a regra que corresponde à estrutura da expressão em cada passo.\n\nO Objetivo Final: A expressão (λx:int. x + 1) 5 é uma aplicação de função. Portanto, a última regra a ser usada é a Regra da Aplicação. Para que a conclusão seja ... : int, a regra nos diz que a função deve ter o tipo τ₁ → int e o argumento deve ter o tipo τ₁. Nossa hipótese é que τ₁ seja int. Então, precisamos provar duas premissas:\n\n{} ⊢ (λx:int. x + 1) : int → int\n{} ⊢ 5 : int\n\nProvando a Premissa 2 (o argumento): Provar {} ⊢ 5 : int é simples. Usamos a Regra da Constante. Este é um galho finalizado (um axioma).\nProvando a Premissa 1 (a função): A expressão λx:int. x + 1 é uma abstração de função. A única regra que se aplica é a Regra da Abstração. Para provar que a função tem o tipo int → int, precisamos adicionar a hipótese do argumento (x:int) ao contexto e provar que o corpo da função (x + 1) tem o tipo int nesse novo contexto. Nossa nova meta é:\n\n{x:int} ⊢ x + 1 : int\n\nProvando o Corpo da Função: A expressão x + 1 é uma operação de soma. Aplicamos a Regra do Operador +. Para isso, precisamos provar suas duas premissas no contexto atual ({x:int}):\n\n{x:int} ⊢ x : int\n{x:int} ⊢ 1 : int\n\nProvando as Folhas da Árvore:\n\nPara provar {x:int} ⊢ x : int, usamos a Regra da Variável. Como x:int está no contexto, a regra se aplica diretamente.\nPara provar {x:int} ⊢ 1 : int, usamos a **Regra da Constante`.\n\n\nComo conseguimos provar todas as premissas até chegar a axiomas, a derivação inteira é válida!\n\n10.8.6.1 A Árvore de Derivação Completa\nVisualmente, a prova completa se parece com esta árvore, na qual lemos de cima (axiomas) para baixo (conclusão):\n\nRegra da Variável         Regra da Constante\n-----------------        ------------------\n{x:int} ⊢ x : int         {x:int} ⊢ 1 : int\n------------------------------------------------------ Regra do Operador (+)\n    {x:int} ⊢ x + 1 : int\n------------------------------------------------------ Regra da Abstração    ---------------- Regra da Constante\n{} ⊢ (λx:int. x + 1) : int → int                    {} ⊢ 5 : int\n-------------------------------------------------------------------------------------------------- Regra da Aplicação\n               {} ⊢ (λx:int. x + 1) 5 : int\n\n\n\n\n10.8.7 Exemplo Adicional: Verificação de Tipos em C++\nPara demonstrar como o mesmo formalismo se aplica a linguagens imperativas, vamos analisar um trecho de código C++ muito comum. O objetivo não é mais encontrar o tipo de uma expressão única, mas sim provar que uma sequência de comandos é bem-tipada, passo a passo, atualizando o contexto de tipagem (Γ) a cada declaração. Considere o código:\nint x = 10;\nint y = x + 5;\nVamos começar com um contexto vazio, Γ₀ = {}.\n\n10.8.7.1 Análise da Linha 1: int x = 10;\nPara validar uma declaração com inicialização, precisamos de uma regra específica.\nRegra da Declaração (Simplificada): Uma declaração T v = e; é bem-tipada se a expressão e tiver o tipo T. O efeito de uma declaração bem-tipada é adicionar a nova variável v com seu tipo T ao contexto para as linhas seguintes. \\[\\frac{\\Gamma \\vdash e : T}{\\Gamma \\vdash (\\texttt{T v = e;}) : \\textbf{ok}}\\]\nNa qual, ok significa que a declaração é semanticamente válida/bem-tipada.\nPara validar int x = 10; no contexto Γ₀, aplicamos esta regra:\n\nObjetivo: Provar que Γ₀ ⊢ (int x = 10;) : ok.\nPremissa Necessária: Pela regra, precisamos primeiro provar que Γ₀ ⊢ 10 : int.\nProva da Premissa: Usando a Regra da Constante, sabemos que o literal 10 tem o tipo int. A premissa é verdadeira.\n\nComo a premissa é válida, a declaração int x = 10; é bem-tipada. O efeito colateral desta análise é a criação de um novo contexto para a próxima linha: Γ₁ = Γ₀ ∪ {x:int}.\n\n\n10.8.7.2 Análise da Linha 2: int y = x + 5;\nAgora, analisamos a segunda linha no novo contexto Γ₁ = {x:int}.\n\nObjetivo: Provar que Γ₁ ⊢ (int y = x + 5;) : ok.\nPremissa Necessária: Usando a mesma Regra da Declaração, precisamos provar que a expressão do lado direito tem o tipo int, ou seja, Γ₁ ⊢ x + 5 : int.\nProva da Premissa: A expressão x + 5 é uma soma. Usamos a Regra do Operador +, que por sua vez exige duas novas premissas:\n\nΓ₁ ⊢ x : int\nΓ₁ ⊢ 5 : int\n\nProva das Sub-premissas:\n\nΓ₁ ⊢ x : int é verdadeiro pela Regra da Variável, pois x:int existe em nosso contexto Γ₁.\nΓ₁ ⊢ 5 : int é verdadeiro pela Regra da Constante.\n\n\nComo todas as premissas foram satisfeitas, a expressão x + 5 de fato tem o tipo int. Consequentemente, a declaração int y = x + 5; é bem-tipada.\nA árvore de derivação para a segunda linha seria:\nRegra da Variável    Regra da Constante\n-------------------  ------------------\nΓ₁ ⊢ x : int       Γ₁ ⊢ 5 : int\n------------------------------------------- Regra do Operador (+)\n      Γ₁ ⊢ x + 5 : int\n------------------------------------------- Regra da Declaração\n     Γ₁ ⊢ (int y = x + 5;) : ok\nEste exemplo mostra como o sistema de julgamento de tipos funciona como um mecanismo formal que não só valida expressões, mas também modela a forma como o contexto (o conhecimento do compilador sobre as variáveis declaradas) evolui ao longo da análise de um programa imperativo.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analisadores Semânticos: a determinação do significado</span>"
    ]
  },
  {
    "objectID": "10-semantico.html#exercícios-da-seção",
    "href": "10-semantico.html#exercícios-da-seção",
    "title": "10  Analisadores Semânticos: a determinação do significado",
    "section": "10.9 Exercícios da Seção",
    "text": "10.9 Exercícios da Seção\n\n10.9.1 Questão 1: Considerando que estamos trabalhando em uma liguagem de programação na qual o contexto pode ser formado de objetos \\(O\\), métodos \\(M\\) e classes \\(C\\). Qual a regra de dedução para o julgamento de tipo para a adição de inteiros?\nO julgamento de tipo para adição de inteiros pode ser expresso como:\n\\[\\frac{O,M,C \\vdash e_1 : \\text{Int} \\quad O,M,C \\vdash e_2 : \\text{Int}}{O,M,C \\vdash e_1 + e_2 : \\text{Int}}\\]\nLeitura da regra: Se no ambiente de objetos \\(O\\), métodos \\(M\\) e classes \\(C\\), a expressão \\(e_1\\) tem tipo Int e a expressão \\(e_2\\) tem tipo Int, então a expressão \\(e_1 + e_2\\) tem tipo Int.\nEm outras palavras, para que uma operação de adição seja bem tipada, ambos os operandos devem ter tipo inteiro, e o resultado também será um inteiro.\n\n\n10.9.2 Questão 2: No mesmo ambiente da questão um. Como seria o julgamento de tipo para instruções IF?\nO julgamento de tipo para instruções condicionais (IF) pode ser expresso como:\n\\[\\frac{O,M,C \\vdash e_1 : \\text{Bool} \\quad O,M,C \\vdash e_2 : T_2 \\quad O,M,C \\vdash e_3 : T_3 \\quad T_2 \\sqcup T_3 = T}{O,M,C \\vdash \\text{if } e_1 \\text{ then } e_2 \\text{ else } e_3 : T}\\]\nLeitura da regra: Se a condição \\(e_1\\) tem tipo Bool, o ramo then \\(e_2\\) tem tipo \\(T_2\\), o ramo else \\(e_3\\) tem tipo \\(T_3\\), e o menor limite superior (join) de \\(T_2\\) e \\(T_3\\) é \\(T\\), então toda a expressão if tem tipo \\(T\\).\n\n\n10.9.3 Questão 3: O que a seguinte regra significa? Como lemos ela?\nA regra apresentada é:\n\\[\\frac{O,M,C \\vdash e_1 : T_1 \\quad O,M,C \\vdash e_2 : T_2 \\quad \\vdots \\quad O,M,C \\vdash e_n : T_n}{O,M,C \\vdash \\{ e_1; e_2; \\ldots e_n; \\} : T_n} \\quad \\text{[Sequence]}\\]\nSignificado da regra: Esta é a regra de tipagem para sequências de expressões. Ela especifica que uma sequência de expressões tem o tipo da última expressão na sequência.\nComo ler: Se no contexto de tipagem \\(O,M,C\\) temos que:\n\na expressão \\(e_1\\) tem tipo \\(T_1\\);\na expressão \\(e_2\\) tem tipo \\(T_2\\);\ne assim por diante até a expressão \\(e_n\\) que tem tipo \\(T_n\\)\n\nEntão a sequência inteira { e₁; e₂; ... eₙ; } tem tipo \\(T_n\\) (o tipo da última expressão).\nIntuição: Em muitas linguagens, uma sequência de comandos é executada em ordem, e o valor retornado é o valor da última expressão. Por exemplo:\n{\n  x &lt;- 5;\n  y &lt;- 10;\n  x + y\n}\nEsta sequência tem tipo Int porque a última expressão x + y tem tipo Int, independentemente dos tipos das expressões anteriores.\n\n\n10.9.4 Questão 4: Considere uma linguagem de programação que suporta variáveis, funções e expressões aritméticas com tipos inteiro, real e string. Crie um sistema de tipos com Cálculo de Sequentes e escreva as regra de tipagem para a declaração de variáveis, operações aritméticas, strings e para a chamada de função.\n\n10.9.4.1 Regras de Tipagem para Declaração de Variável e Chamada de Função\n\n10.9.4.1.1 1. Axiomas da Linguagem\nAxiomas são regras de tipagem sem premissas - definem os tipos dos valores primitivos da linguagem.\n\n10.9.4.1.1.1 Axioma para literais inteiros:\n\\[\\frac{}{O, M, C \\vdash n : \\text{Int}}\\]\nonde \\(n \\in \\mathbb{Z}\\) (qualquer número inteiro)\nExemplo: 42 : Int, -7 : Int, 0 : Int\n\n\n10.9.4.1.1.2 Axioma para literais reais:\n\\[\\frac{}{O, M, C \\vdash r : \\text{Real}}\\]\nonde \\(r \\in \\mathbb{R}\\) (qualquer número real)\nExemplo: 3.14 : Real, -0.5 : Real, 2.0 : Real\n\n\n10.9.4.1.1.3 Axioma para literais de string:\n\\[\\frac{}{O, M, C \\vdash s : \\text{String}}\\]\nonde \\(s\\) é uma sequência de caracteres entre aspas\nExemplo: \"hello\" : String, \"\" : String, \"123\" : String\n\n\n\n10.9.4.1.2 Axioma para variáveis no ambiente:\n\\[\\frac{C(x) = T}{O, M, C \\vdash x : T}\\]\nSe a variável \\(x\\) está no ambiente \\(C\\) com tipo \\(T\\), então \\(x\\) tem tipo \\(T\\).\n\n\n\n10.9.4.2 2. Regras de Tipagem para Operações Aritméticas\n\n10.9.4.2.1 Adição de inteiros:\n\\[\\frac{O, M, C \\vdash e_1 : \\text{Int} \\quad O, M, C \\vdash e_2 : \\text{Int}}{O, M, C \\vdash e_1 + e_2 : \\text{Int}}\\]\n\n\n10.9.4.2.2 Adição de reais:\n\\[\\frac{O, M, C \\vdash e_1 : \\text{Real} \\quad O, M, C \\vdash e_2 : \\text{Real}}{O, M, C \\vdash e_1 + e_2 : \\text{Real}}\\]\n\n\n10.9.4.2.3 Adição mista (coerção de Int para Real):\n\\[\\frac{O, M, C \\vdash e_1 : \\text{Int} \\quad O, M, C \\vdash e_2 : \\text{Real}}{O, M, C \\vdash e_1 + e_2 : \\text{Real}}\\]\n\\[\\frac{O, M, C \\vdash e_1 : \\text{Real} \\quad O, M, C \\vdash e_2 : \\text{Int}}{O, M, C \\vdash e_1 + e_2 : \\text{Real}}\\]\nNota: As mesmas regras aplicam-se para subtração, multiplicação e divisão.\n\n\n10.9.4.2.4 Concatenação de strings:\n\\[\\frac{O, M, C \\vdash e_1 : \\text{String} \\quad O, M, C \\vdash e_2 : \\text{String}}{O, M, C \\vdash e_1 \\oplus e_2 : \\text{String}}\\]\nonde \\(\\oplus\\) representa o operador de concatenação.\n\n\n\n10.9.4.3 3. Regra de Tipagem para Declaração de Variável\nA declaração de uma variável envolve associar um nome a um tipo e, opcionalmente, a uma expressão de inicialização.\n\n10.9.4.3.1 Forma geral da declaração:\nx : T &lt;- e\nonde x é o nome da variável, T é o tipo declarado e e é a expressão de inicialização.\n\n\n10.9.4.3.2 Regra de tipagem:\n\\[\\frac{O, M, C \\vdash e : T' \\quad T' \\leq T \\quad O, M, C[x \\mapsto T] \\vdash e_{corpo} : T_{corpo}}{O, M, C \\vdash (x : T \\leftarrow e; e_{corpo}) : T_{corpo}}\\]\nLeitura da regra:\n\nA expressão de inicialização \\(e\\) deve ter tipo \\(T'\\);\nO tipo \\(T'\\) deve ser um subtipo de \\(T\\) (ou igual a \\(T\\));\nNo contexto estendido onde \\(x\\) tem tipo \\(T\\), o corpo subsequente \\(e_{corpo}\\) tem tipo \\(T_{corpo}\\);\nEntão toda a construção tem tipo \\(T_{corpo}\\).\n\nVariante simplificada (apenas declaração sem corpo subsequente):\n\\[\\frac{O, M, C \\vdash e : T' \\quad T' \\leq T}{O, M, C \\vdash (x : T \\leftarrow e) : T}\\]\nExemplo:\nx : Int &lt;- 5 + 3\nNeste caso, 5 + 3 tem tipo Int, que é compatível com o tipo declarado Int.\nOutro exemplo com coerção:\ny : Real &lt;- 10\nAqui, 10 tem tipo Int, mas Int \\(\\leq\\) Real, então a declaração é válida.\n\n\n\n10.9.4.4 4. Regra de Tipagem para Chamada de Função\nA chamada de uma função envolve verificar que os argumentos passados são compatíveis com os parâmetros esperados.\n\n10.9.4.4.1 Forma geral da chamada:\nf(e₁, e₂, ..., eₙ)\nonde f é o nome da função e \\(e_1, e_2, \\ldots, e_n\\) são os argumentos.\n\n\n10.9.4.4.2 Regra de tipagem:\n\\[\\frac{M(f) = (T_1, T_2, \\ldots, T_n) \\rightarrow T_{ret} \\quad O, M, C \\vdash e_1 : T_1' \\quad T_1' \\leq T_1 \\quad \\ldots \\quad O, M, C \\vdash e_n : T_n' \\quad T_n' \\leq T_n}{O, M, C \\vdash f(e_1, e_2, \\ldots, e_n) : T_{ret}}\\]\nLeitura da regra:\n\nA função \\(f\\) deve estar definida no ambiente de métodos \\(M\\) com assinatura \\((T_1, T_2, \\ldots, T_n) \\rightarrow T_{ret}\\);\nCada argumento \\(e_i\\) deve ter tipo \\(T_i'\\);\nCada tipo de argumento \\(T_i'\\) deve ser subtipo (ou igual) ao tipo de parâmetro esperado \\(T_i\\);\nSe todas as premissas são satisfeitas, a chamada tem tipo \\(T_{ret}\\) (tipo de retorno da função).\n\nExemplo 1: suponha que temos a função:\ndef soma(a : Int, b : Int) : Int { a + b }\nPara a chamada soma(5, 10): - Verificamos que 5 tem tipo Int e Int \\(\\leq\\) Int - Verificamos que 10 tem tipo Int e Int \\(\\leq\\) Int - Portanto, soma(5, 10) tem tipo Int\nExemplo 2 (com coerção): suponha a função:\ndef media(a : Real, b : Real) : Real { (a + b) / 2.0 }\nPara a chamada media(5, 3.5):\n\nVerificamos que 5 tem tipo Int e Int \\(\\leq\\) Real;\nVerificamos que 3.5 tem tipo Real e Real \\(\\leq\\) Real;\nPortanto, media(5, 3.5) tem tipo Real.\n\n\n\n\n10.9.4.5 4. Hierarquia de Tipos\nPara que a relação de subtipagem \\(\\leq\\) funcione, definimos:\n\\[\\text{Int} \\leq \\text{Real}\\]\nIsso permite que inteiros sejam usados onde reais são esperados (coerção implícita).\nReflexividade e transitividade também são válidas:\n\n\\(T \\leq T\\) para qualquer tipo \\(T\\);\nSe \\(T_1 \\leq T_2\\) e \\(T_2 \\leq T_3\\), então \\(T_1 \\leq T_3\\).\n\n\n\n10.9.4.6 O Motor da Inferência: Algoritmo de Hindley-Milner\nAs regras acima são ótimas para verificar tipos que já estão anotados. Mas, o que acontece em linguagens como ML ou Haskell, nas quais podemos escrever declarações como let f x = x + 1 e o compilador descobre sozinho que f tem o tipo int -&gt; int? Isso é inferência de tipos, e o motor por trás dessa arte é, em grande parte, movido pelo algoritmo W, desenvolvido por Robin Milner.\nO algoritmo W automatiza o processo de encontrar os tipos, mesmo quando eles não são declarados. Sua ideia central é usar variáveis de tipo, símbolos como \\(\\alpha, \\beta, \\gamma\\) que funcionam como placeholders para tipos desconhecidos e um processo chamado unificação. Unificação é, em essência, um algoritmo para resolver equações entre tipos.\nQuando o algoritmo W analisa uma expressão, ele gera um conjunto de equações de tipo. Por exemplo, para f x = x + 1, ele diria:\n\nx deve ter um tipo \\(\\alpha\\).\n1 tem o tipo int.\nO operador + tem o tipo (int, int) -&gt; int.\nPortanto, x (que é \\(\\alpha\\)) deve ser compatível com int. Equação: \\(\\alpha = \\text{int}\\).\nO resultado de x + 1 é int.\nA função f recebe x (cujo tipo é \\(\\alpha\\)) e retorna o resultado de x + 1 (cujo tipo é int).\nPortanto, o tipo de f é \\(\\alpha \\rightarrow \\text{int}\\).\nResolvendo a equação do passo 4, substituímos \\(\\alpha\\) por int, e o algoritmo conclui que o tipo de f é \\(\\text{int} \\rightarrow \\text{int}\\).\n\nO pseudocódigo do algoritmo formaliza exatamente esse processo de geração e resolução de restrições de tipo:\nalgoritmo W(Γ, e):\n  caso e de:\n    // Variável: simplesmente a procuramos no ambiente.\n    x → \n      se x : σ ∈ Γ então\n        retornar (instanciar(σ), ∅)\n      senão\n        erro \"variável não ligada\"\n    \n    // Função: criamos uma nova variável de tipo α para o argumento,\n    // e descobrimos o tipo do corpo nesse novo contexto.\n    λx.e₁ →\n      α ← nova_variavel_tipo()\n      (τ₁, S₁) ← W(Γ ∪ {x : α}, e₁)\n      retornar (S₁(α) → τ₁, S₁)\n    \n    // Aplicação: o passo mais complexo. Descobrimos os tipos da\n    // função (e₁) e do argumento (e₂), e então unificamos.\n    e₁ e₂ →\n      (τ₁, S₁) ← W(Γ, e₁)\n      (τ₂, S₂) ← W(S₁(Γ), e₂)\n      α ← nova_variavel_tipo()\n      // A equação a ser resolvida: τ₁ deve ser igual a τ₂ → α\n      S₃ ← unificar(S₂(τ₁), τ₂ → α)\n      // O resultado final é o tipo de α, após aplicar todas as\n      // substituições (soluções) encontradas.\n      retornar (S₃(α), S₃ ∘ S₂ ∘ S₁)\nNo pseudocódigo, as variáveis \\(S\\) representam as substituições, as soluções parciais para as equações de tipo, que são compostas ao longo do processo.\nO Algoritmo W é poderoso e tem histórico de uso em linguagens funcionais descendentes do ML. O sistema de tipos do Haskell evoluiu para incluir extensões mais avançadas, como classes de tipos, a sua base para a inferência de tipos ainda é o Algoritmo W. Isso permite que o compilador do Haskell, o Glasgow Haskell Compiler, GHC, infira os tipos da maioria das funções e expressões de forma automática. Fora do domínio das linguagens funcionais, o Algoritmo W também influenciou sistemas de tipos em linguagens como TypeScript, que trazem a inferência de tipos para o mundo da programação orientada a objetos e funcional. Finalmente, precisamos destacar a linguagem Rust, conhecida por seu foco em segurança de memória e concorrência, que possui um sistema de inferência de tipos. Embora seja baseado no sistema Hindley-Milner, ele é significativamente estendido para lidar com o sistema de lifetimes e traits característicos da linguagem. Portanto, a curiosa leitora não pode dizer que o Rust utiliza uma implementação pura do Algoritmo W, mas sim um sistema que evoluiu a partir de suas fundações.\nFinalmente, na Chapter 13 discutimos o sistema de tipos Hindley-Milner e o Algoritmo W em mais detalhes.\n\n\n\n10.9.5 Complexidade da Verificação e Inferência de Tipos\nA complexidade para garantir a segurança de tipos varia drasticamente com o poder do sistema de tipos:\n\nTipagem Monomórfica Simples: em linguagens como Pascal ou C, a verificação de tipos é muito eficiente. Ela pode ser feita em tempo linear, \\(O(n)\\), correspondendo a uma única travessia na AST.\nInferência de Hindley-Milner: o algoritmo W, apesar de sua expressividade, ainda é considerado eficiente. Em teoria, seu pior caso é exponencial, mas com otimizações na estrutura de dados de unificação, ele se comporta de forma quase linear na prática. Porém, a atenta leitora deve observar que a observação empírica não muda a complexidade exponencial deste algoritmo, \\(O(n^3)\\).\nSistemas Mais Complexos: à medida que adicionamos funcionalidades ao sistema de tipos, a complexidade pode explodir. A inferência de tipos na presença de subtipos (comuns em programação orientada a objetos) torna o problema PSPACE-completo, como provado por Benjamin Pierce. Se adicionarmos características ainda mais poderosas, como as type classes do Haskell em sua forma mais geral, o problema da inferência de tipos se torna indecidível — ou seja, não existe um algoritmo que possa resolver todos os casos. Isso demonstra um dos balanços mais fundamentais no design de linguagens: a troca entre o poder de expressividade do sistema de tipos e a capacidade do compilador de verificá-lo e inferi-lo de forma automática e eficiente.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analisadores Semânticos: a determinação do significado</span>"
    ]
  },
  {
    "objectID": "10-semantico.html#implementação-prática-da-teoria-à-prática",
    "href": "10-semantico.html#implementação-prática-da-teoria-à-prática",
    "title": "10  Analisadores Semânticos: a determinação do significado",
    "section": "10.10 Implementação Prática: Da Teoria à Prática",
    "text": "10.10 Implementação Prática: Da Teoria à Prática\nAté este ponto, discutimos os fundamentos teóricos da análise semântica e das gramáticas de atributos. Um diagrama em blocos da estrutura do analisador semântico pode ser visto na Figura Figure 10.10.\n\n\n\n\n\n\nFigure 10.10\n\n\n\nAgora, a persistente leitora deverá aplicar esses conceitos construindo um analisador semântico completo para uma mini-linguagem de expressões aritméticas. Se acompanhar detalhada e cuidadosamente este exercício demonstrará como a teoria se traduz em código executável, consolidando o entendimento através da prática. Porém, esta é uma implementação simplificada, destinada a ilustrar os conceitos principais. Sendo assim, existem alguns pontos importantes que a leitora deve observar:\nSeparação de Responsabilidades: o analisador semântico é completamente independente do parser. Ele recebe uma AST como entrada e a anota com informações semânticas. Esta modularidade é essencial em compiladores reais. Uma opção a favor da didática.\nRecuperação de Erros: o analisador que criaremos não irá interromper a execução no primeiro erro; ele continuará analisando e acumulará todos os erros encontrados. Isso fornece o objetivo é fornecer informações mais completas ao programador.\nDecoração da AST: os vértices da AST serão anotados com tipos inferidos (tipo_inferido). Esta AST decorada poderia ser passada para a fase de geração de código intermediário, que usaria essas informações para gerar instruções apropriadas.\nLimitações: por simplicidade, nossa implementação não cobre aspectos importantes como:\n\nAnálise de fluxo de dados (variáveis não inicializadas, código morto);\nOtimizações baseadas em tipos (constant folding);\nMensagens de erro com informações de localização;\nSuporte a estruturas de dados compostas (arrays, structs).\n\n\n10.10.1 Definição da Mini-Linguagem: Langarit\nIniciaremos definindo uma linguagem simples, mas suficientemente rica para demonstrar os principais conceitos da análise semântica. Nossa linguagem, Langarit, suportará:\n\nDeclarações de variáveis com tipos int e float;\nExpressões aritméticas com operadores +, -, *, /;\nAtribuições de valores a variáveis;\nLiterais numéricos inteiros e de ponto flutuante.\n\nExemplos de programas válidos em Langarit:\nint x;\nfloat y;\nx = 10;\ny = 3.14;\nx = x + 5;\ny = y * 2.0;\nExemplos de programas com erros semânticos:\nx = 10;     # ERRO: x não foi declarado\nint x;\nx = 3.14;    # ERRO: tentativa de atribuir float a int\nint y;\ny = x / 0;    # ERRO: divisão por zero (semântica dinâmica, mas detectável em literais)\n\n\n10.10.2 Gramática Livre de Contexto para Langarit\nNossa linguagem precisa de uma sintaxe de Langarit, vamos criar uma gramática livre de contexto para Langarit. Lembre-se, esta gramática descreve apenas a forma dos programas válidos, sem considerar o significado.\n\\[\n\\begin{align*}\n\\text{Programa} & \\rightarrow \\text{Declaracoes} \\quad \\text{Comandos} \\\\\n\\text{Declaracoes} & \\rightarrow \\text{Declaracao} \\quad \\text{Declaracoes} \\mid \\epsilon \\\\\n\\text{Declaracao} & \\rightarrow \\text{Tipo} \\quad \\textbf{id} \\; \\textbf{;} \\\\\n\\text{Tipo} & \\rightarrow \\textbf{int} \\mid \\textbf{float} \\\\\n\\text{Comandos} & \\rightarrow \\text{Comando} \\quad \\text{Comandos} \\mid \\epsilon \\\\\n\\text{Comando} & \\rightarrow \\textbf{id} \\; \\textbf{=} \\; \\text{Expr} \\; \\textbf{;} \\\\\n\\text{Expr} & \\rightarrow \\text{Expr} \\; \\textbf{+} \\; \\text{Term} \\mid \\text{Expr} \\; \\textbf{-} \\; \\text{Term} \\mid \\text{Term} \\\\\n\\text{Term} & \\rightarrow \\text{Term} \\; \\textbf{*} \\; \\text{Factor} \\mid \\text{Term} \\; \\textbf{/} \\; \\text{Factor} \\mid \\text{Factor} \\\\\n\\text{Factor} & \\rightarrow \\textbf{(} \\; \\text{Expr} \\; \\textbf{)} \\mid \\textbf{id} \\mid \\textbf{num\\_int} \\mid \\textbf{num\\_float}\n\\end{align*}\n\\]\nObserve como a estrutura desta gramática, cuja hierarquia pode ser vista na Figura Figure 10.11, já impõe regras de precedência e associatividade. Ao separar Expr (adição/subtração), Term (multiplicação/divisão) e Factor (literais, variáveis), garantimos que as operações em Term sejam avaliadas antes das de Expr. A recursão à esquerda nas regras (Expr -&gt; Expr + Term) define a associatividade da esquerda para a direita para os operadores.\n\n\n\n\n\n\nFigure 10.11\n\n\n\n\n\n10.10.3 Gramática de Atributos para Langarit\nUma vez que temos uma gramática livre de contexto, estenderemos a gramática sintática com atributos e regras semânticas. Neste momento, nossos objetivos são:\n\nVerificar declarações: toda variável usada foi previamente declarada;\nVerificar tipos: operações são realizadas entre tipos compatíveis;\nInferir tipos: determinar o tipo resultante de cada expressão;\nAnotar a AST: decorar a árvore com informações de tipo.\n\nCom estes objetivos em mente, definimos os atributos e as regras semânticas. Começando com os atributos. Nossa estratégia de análise semântica seguirá dois fluxos principais de informação:\nUm fluxo contextual, semelhante ao herdado: as declarações de variáveis irão popular a Tabela de Símbolos. Esta tabela funcionará como nosso contexto, sendo consultada posteriormente quando uma variável for usada em uma expressão ou atribuição.\nUm fluxo sintetizado, bottom-up: para as expressões, o tipo será sintetizado a partir das folhas da árvore. Começaremos com os tipos dos números e variáveis e, subindo pela AST, usaremos a função promover_tipo para calcular o tipo resultante de cada subexpressão até chegarmos à raiz da expressão. Assim, teremos:\n\n\n\nSímbolo\nAtributos Sintetizados\nAtributos Herdados\n\n\n\n\nTipo\ntipo (string: “int” ou “float”)\n-\n\n\nExpr\ntipo (string)\n-\n\n\nTerm\ntipo (string)\n-\n\n\nFactor\ntipo (string)\n-\n\n\nid\nlexema (string), tipo (string)\n-\n\n\nnum_int\nvalor (int), tipo = “int”\n-\n\n\nnum_float\nvalor (float), tipo = “float”\n-\n\n\n\nPodemos definir as regras semânticas associadas a cada produção da gramática:\nPara Declarações:\n\n\n\n\n\n\n\nProdução\nRegras Semânticas\n\n\n\n\n\\(\\text{Declaracao} \\rightarrow \\text{Tipo} \\; \\textbf{id} \\; \\textbf{;}\\)\ntabela_simbolos.adicionar(id.lexema, Tipo.tipo)  Se id.lexema já existe: ERRO\n\n\n\nPara Expressões Aritméticas:\n\n\n\n\n\n\n\nProdução\nRegras Semânticas\n\n\n\n\n\\(\\text{Expr} \\rightarrow \\text{Expr}_1 \\; \\textbf{+} \\; \\text{Term}\\)\nExpr.tipo = promover_tipo(Expr1.tipo, Term.tipo)\n\n\n\\(\\text{Expr} \\rightarrow \\text{Expr}_1 \\; \\textbf{-} \\; \\text{Term}\\)\nExpr.tipo = promover_tipo(Expr1.tipo, Term.tipo)\n\n\n\\(\\text{Expr} \\rightarrow \\text{Term}\\)\nExpr.tipo = Term.tipo\n\n\n\\(\\text{Term} \\rightarrow \\text{Term}_1 \\; \\textbf{*} \\; \\text{Factor}\\)\nTerm.tipo = promover_tipo(Term1.tipo, Factor.tipo)\n\n\n\\(\\text{Term} \\rightarrow \\text{Term}_1 \\; \\textbf{/} \\; \\text{Factor}\\)\nTerm.tipo = promover_tipo(Term1.tipo, Factor.tipo)\n\n\n\\(\\text{Term} \\rightarrow \\text{Factor}\\)\nTerm.tipo = Factor.tipo\n\n\n\\(\\text{Factor} \\rightarrow \\textbf{(} \\; \\text{Expr} \\; \\textbf{)}\\)\nFactor.tipo = Expr.tipo\n\n\n\\(\\text{Factor} \\rightarrow \\textbf{id}\\)\nFactor.tipo = tabela_simbolos.consultar(id.lexema)  Se não encontrado: ERRO\n\n\n\\(\\text{Factor} \\rightarrow \\textbf{num\\_int}\\)\nFactor.tipo = \"int\"\n\n\n\\(\\text{Factor} \\rightarrow \\textbf{num\\_float}\\)\nFactor.tipo = \"float\"\n\n\n\nPara Atribuições:\n\n\n\n\n\n\n\nProdução\nRegras Semânticas\n\n\n\n\n\\(\\text{Comando} \\rightarrow \\textbf{id} \\; \\textbf{=} \\; \\text{Expr} \\; \\textbf{;}\\)\ntipo_var = tabela_simbolos.consultar(id.lexema)  Se não encontrado: ERRO  Se tipo_var == \"int\" e Expr.tipo == \"float\": ERRO (perda de precisão)  Se tipo_var == \"float\" e Expr.tipo == \"int\": OK (promoção implícita)\n\n\n\nFunção de Promoção de Tipos:\nA função promover_tipo(tipo1, tipo2) implementa as regras de coerção:\n\npromover_tipo(\"int\", \"int\") retorna \"int\"\npromover_tipo(\"float\", \"float\") retorna \"float\"\npromover_tipo(\"int\", \"float\") retorna \"float\"\npromover_tipo(\"float\", \"int\") retorna \"float\"\n\n\n\n10.10.4 Implementação emPython\nImplementaremos o analisador semântico como um percurso da AST gerada por um parser. Por simplicidade, assumiremos que já temos a AST construída. Contudo, antes de escrever o analisador, precisamos de uma forma de representar a Árvore Sintática Abstrata emPython. Usaremos dataclasses para criar uma hierarquia de classes simples e legível, em que cada classe corresponde a um conceito da nossa gramática, um programa, uma declaração, uma expressão binária, etc..\n\n10.10.4.1 Estrutura dos vértices da AST\nfrom dataclasses import dataclass\nfrom typing import Optional, List, Union\n\n@dataclass\nclass NoAST:\n  \"\"\"Classe base para todos os vértices da AST\"\"\"\n  pass\n\n@dataclass\nclass Programa(NoAST):\n  declaracoes: List['Declaracao']\n  comandos: List['Comando']\n\n@dataclass\nclass Declaracao(NoAST):\n  tipo: str # \"int\" ou \"float\"\n  nome: str\n\n@dataclass\nclass Comando(NoAST):\n  pass\n\n@dataclass\nclass Atribuicao(Comando):\n  variavel: str\n  expressao: 'Expressao'\n\n@dataclass\nclass Expressao(NoAST):\n  tipo_inferido: Optional[str] = None # Preenchido pelo analisador semântico\n\n@dataclass\nclass ExpressaoBinaria(Expressao):\n  esquerda: Expressao\n  operador: str # \"+\", \"-\", \"*\", \"/\"\n  direita: Expressao\n\n@dataclass\nclass Variavel(Expressao):\n  nome: str\n\n@dataclass\nclass LiteralInt(Expressao):\n  valor: int\n  tipo_inferido: str = \"int\"\n\n@dataclass\nclass LiteralFloat(Expressao):\n  valor: float\n  tipo_inferido: str = \"float\"\n\n\n10.10.4.2 Tabela de Símbolos\nA Tabela de Símbolos (Chapter 14) é a estrutura de dados que servirá como a memória do nosso analisador. É ela que armazena o contexto necessário para as verificações semânticas, associando o nome de cada variável ao seu tipo (int ou float). Nossa implementação usará uma pilha de dicionários para gerenciar escopos, embora neste exemplo inicial utilizemos apenas o escopo global.\nclass TabelaSimbolos:\n  \"\"\"\n  **Tabela de Símbolos** para armazenar informações de variáveis.\n  Suporta escopos aninhados através de uma pilha de dicionários.\n  \"\"\"\n  def __init__(self):\n    self.escopos = [{}] # Lista de dicionários; escopo global no índice 0\n  \n  def adicionar(self, nome: str, tipo: str) -&gt; None:\n    \"\"\"\n    Adiciona uma variável ao escopo atual.\n    Lança exceção se a variável já foi declarada no escopo atual.\n    \"\"\"\n    escopo_atual = self.escopos[-1]\n    if nome in escopo_atual:\n      raise Exception(f\"Erro semântico: variável '{nome}' já declarada neste escopo\")\n    escopo_atual[nome] = tipo\n  \n  def consultar(self, nome: str) -&gt; Optional[str]:\n    \"\"\"\n    Busca o tipo de uma variável percorrendo os escopos do mais interno ao mais externo.\n    Retorna None se a variável não foi encontrada.\n    \"\"\"\n    for escopo in reversed(self.escopos):\n      if nome in escopo:\n        return escopo[nome]\n    return None\n  \n  def entrar_escopo(self) -&gt; None:\n    \"\"\"Cria um novo escopo (para blocos aninhados)\"\"\"\n    self.escopos.append({})\n  \n  def sair_escopo(self) -&gt; None:\n    \"\"\"Remove o escopo mais interno\"\"\"\n    if len(self.escopos) &gt; 1:\n      self.escopos.pop()\n\n\n10.10.4.3 Analisador Semântico\nO coração da nossa implementação é a classe AnalisadorSemantico. Ela percorrerá a AST aplicando as regras que definimos. A estrutura do código seguirá o padrão de projeto Visitor, no qual criamos um método específico, _analisar_..., para cada tipo de vértice da árvore. Neste caso, usamos uma abordagem em duas passagens para simplificar a implementação:\nPrimeiro, percorreremos todas as declaracoes para popular a Tabela de Símbolos. Depois, em uma segunda passagem, analisaremos os comandos, usando a tabela já preenchida para realizar as verificações de tipo e uso.\nclass AnalisadorSemantico:\n  \"\"\"\n  Analisador semântico que percorre a AST, verifica regras semânticas\n  e anota os vértices com informações de tipo.\n  \"\"\"\n  def __init__(self):\n    self.tabela = TabelaSimbolos()\n    self.erros = []\n  \n  def analisar(self, programa: Programa) -&gt; bool:\n    \"\"\"\n    Ponto de entrada principal. Analisa um programa completo.\n    Retorna True se não houver erros semânticos.\n    \"\"\"\n    try:\n      self._analisar_programa(programa)\n      return len(self.erros) == 0\n    except Exception as e:\n      self.erros.append(str(e))\n      return False\n  \n  def _analisar_programa(self, programa: Programa) -&gt; None:\n    \"\"\"Analisa declarações seguidas de comandos\"\"\"\n    # Primeira passagem: processar todas as declarações\n    for decl in programa.declaracoes:\n      self._analisar_declaracao(decl)\n    \n    # Segunda passagem: processar comandos\n    for cmd in programa.comandos:\n      self._analisar_comando(cmd)\n  \n  def _analisar_declaracao(self, decl: Declaracao) -&gt; None:\n    \"\"\"Adiciona variável à **Tabela de Símbolos**\"\"\"\n    try:\n      self.tabela.adicionar(decl.nome, decl.tipo)\n    except Exception as e:\n      self.erros.append(str(e))\n  \n  def _analisar_comando(self, cmd: Comando) -&gt; None:\n    \"\"\"Despacha análise para o tipo específico de comando\"\"\"\n    if isinstance(cmd, Atribuicao):\n      self._analisar_atribuicao(cmd)\n    else:\n      raise Exception(f\"Tipo de comando desconhecido: {type(cmd)}\")\n  \n  def _analisar_atribuicao(self, atrib: Atribuicao) -&gt; None:\n    \"\"\"Verifica compatibilidade de tipos em atribuição\"\"\"\n    # Verificar se a variável foi declarada\n    tipo_var = self.tabela.consultar(atrib.variavel)\n    if tipo_var is None:\n      self.erros.append(\n        f\"Erro semântico: variável '{atrib.variavel}' não declarada\"\n      )\n      return\n    \n    # Inferir tipo da expressão\n    tipo_expr = self._analisar_expressao(atrib.expressao)\n    \n    # Verificar compatibilidade\n    if tipo_var == \"int\" and tipo_expr == \"float\":\n      self.erros.append(\n        f\"Erro semântico: não é possível atribuir float a variável int '{atrib.variavel}'\"\n      )\n    # int para float é permitido (promoção implícita)\n  \n  def _analisar_expressao(self, expr: Expressao) -&gt; str:\n    \"\"\"\n    Analisa uma expressão e retorna seu tipo.\n    Anota o vértice da AST com o tipo inferido.\n    \"\"\"\n    if isinstance(expr, LiteralInt):\n      return \"int\"\n    \n    elif isinstance(expr, LiteralFloat):\n      return \"float\"\n    \n    elif isinstance(expr, Variavel):\n      tipo = self.tabela.consultar(expr.nome)\n      if tipo is None:\n        self.erros.append(\n          f\"Erro semântico: variável '{expr.nome}' não declarada\"\n        )\n        expr.tipo_inferido = \"int\" # Tipo padrão para recuperação de erro\n        return \"int\"\n      expr.tipo_inferido = tipo\n      return tipo\n    \n    elif isinstance(expr, ExpressaoBinaria):\n      tipo_esq = self._analisar_expressao(expr.esquerda)\n      tipo_dir = self._analisar_expressao(expr.direita)\n      \n      # Verificar divisão por zero em literais (semântica dinâmica parcialmente estática)\n      if expr.operador == \"/\" and isinstance(expr.direita, (LiteralInt, LiteralFloat)):\n        if expr.direita.valor == 0:\n          self.erros.append(\"Erro semântico: divisão por zero detectada\")\n      \n      # A linha a seguir é a implementação em código da regra semântica:\n      # Expr.tipo = promover_tipo(Expr1.tipo, Term.tipo)\n      tipo_resultado = self._promover_tipo(tipo_esq, tipo_dir)\n      expr.tipo_inferido = tipo_resultado\n      return tipo_resultado\n    \n    else:\n      raise Exception(f\"Tipo de expressão desconhecido: {type(expr)}\")\n  \n  def _promover_tipo(self, tipo1: str, tipo2: str) -&gt; str:\n    \"\"\"\n    Implementa regras de promoção de tipos em operações aritméticas.\n    float + int -&gt; float\n    int + int -&gt; int\n    \"\"\"\n    if tipo1 == \"float\" or tipo2 == \"float\":\n      return \"float\"\n    return \"int\"\n  \n  def imprimir_erros(self) -&gt; None:\n    \"\"\"Imprime todos os erros semânticos encontrados\"\"\"\n    if not self.erros:\n      print(\"Nenhum erro semântico encontrado.\")\n    else:\n      print(f\"Encontrados {len(self.erros)} erro(s) semântico(s):\")\n      for i, erro in enumerate(self.erros, 1):\n        print(f\" {i}. {erro}\")\n\n\n\n10.10.5 Exemplo de Uso Completo\ndef exemplo_programa_valido():\n  \"\"\"Programa correto: todas as variáveis declaradas, tipos compatíveis\"\"\"\n  programa = Programa(\n    declaracoes=[\n      Declaracao(tipo=\"int\", nome=\"x\"),\n      Declaracao(tipo=\"float\", nome=\"y\"),\n    ],\n    comandos=[\n      Atribuicao(\n        variavel=\"x\",\n        expressao=LiteralInt(valor=10)\n      ),\n      Atribuicao(\n        variavel=\"y\",\n        expressao=LiteralFloat(valor=3.14)\n      ),\n      Atribuicao(\n        variavel=\"x\",\n        expressao=ExpressaoBinaria(\n          esquerda=Variavel(nome=\"x\"),\n          operador=\"+\",\n          direita=LiteralInt(valor=5)\n        )\n      ),\n      Atribuicao(\n        variavel=\"y\",\n        expressao=ExpressaoBinaria(\n          esquerda=Variavel(nome=\"y\"),\n          operador=\"*\",\n          direita=LiteralFloat(valor=2.0)\n        )\n      ),\n    ]\n  )\n  \n  analisador = AnalisadorSemantico()\n  sucesso = analisador.analisar(programa)\n  \n  print(\"=== Programa Válido ===\")\n  analisador.imprimir_erros()\n  print(f\"Análise: {'SUCESSO' if sucesso else 'FALHOU'}\\n\")\n  return sucesso\n\ndef exemplo_variavel_nao_declarada():\n  \"\"\"Erro: uso de variável não declarada\"\"\"\n  programa = Programa(\n    declaracoes=[\n      Declaracao(tipo=\"int\", nome=\"x\"),\n    ],\n    comandos=[\n      Atribuicao(\n        variavel=\"y\", # Erro: y não foi declarada\n        expressao=LiteralInt(valor=10)\n      ),\n    ]\n  )\n  \n  analisador = AnalisadorSemantico()\n  sucesso = analisador.analisar(programa)\n  \n  print(\"=== Variável Não Declarada ===\")\n  analisador.imprimir_erros()\n  print(f\"Análise: {'SUCESSO' if sucesso else 'FALHOU'}\\n\")\n  return sucesso\n\ndef exemplo_incompatibilidade_tipo():\n  \"\"\"Erro: tentativa de atribuir float a variável int\"\"\"\n  programa = Programa(\n    declaracoes=[\n      Declaracao(tipo=\"int\", nome=\"x\"),\n    ],\n    comandos=[\n      Atribuicao(\n        variavel=\"x\",\n        expressao=LiteralFloat(valor=3.14) # Erro: float para int\n      ),\n    ]\n  )\n  \n  analisador = AnalisadorSemantico()\n  sucesso = analisador.analisar(programa)\n  \n  print(\"=== Incompatibilidade de Tipo ===\")\n  analisador.imprimir_erros()\n  print(f\"Análise: {'SUCESSO' if sucesso else 'FALHOU'}\\n\")\n  return sucesso\n\ndef exemplo_divisao_por_zero():\n  \"\"\"Erro: divisão por zero detectada estaticamente\"\"\"\n  programa = Programa(\n    declaracoes=[\n      Declaracao(tipo=\"int\", nome=\"x\"),\n    ],\n    comandos=[\n      Atribuicao(\n        variavel=\"x\",\n        expressao=ExpressaoBinaria(\n          esquerda=LiteralInt(valor=10),\n          operador=\"/\",\n          direita=LiteralInt(valor=0) # Erro: divisão por zero\n        )\n      ),\n    ]\n  )\n  \n  analisador = AnalisadorSemantico()\n  sucesso = analisador.analisar(programa)\n  \n  print(\"=== Divisão por Zero ===\")\n  analisador.imprimir_erros()\n  print(f\"Análise: {'SUCESSO' if sucesso else 'FALHOU'}\\n\")\n  return sucesso\n\ndef exemplo_promocao_tipo():\n  \"\"\"Promoção automática de int para float\"\"\"\n  programa = Programa(\n    declaracoes=[\n      Declaracao(tipo=\"float\", nome=\"resultado\"),\n      Declaracao(tipo=\"int\", nome=\"x\"),\n    ],\n    comandos=[\n      Atribuicao(\n        variavel=\"x\",\n        expressao=LiteralInt(valor=5)\n      ),\n      Atribuicao(\n        variavel=\"resultado\",\n        expressao=ExpressaoBinaria(\n          esquerda=Variavel(nome=\"x\"), # int\n          operador=\"+\",\n          direita=LiteralFloat(valor=3.14) # float\n        ) # Resultado: float (promoção de x)\n      ),\n    ]\n  )\n  \n  analisador = AnalisadorSemantico()\n  sucesso = analisador.analisar(programa)\n  \n  print(\"=== Promoção de Tipo (int -&gt; float) ===\")\n  analisador.imprimir_erros()\n  print(f\"Análise: {'SUCESSO' if sucesso else 'FALHOU'}\")\n  \n  # Verificar tipo inferido\n  expr = programa.comandos[1].expressao\n  print(f\"Tipo inferido da expressão: {expr.tipo_inferido}\\n\")\n  return sucesso\n\n# Executar todos os exemplos\nif __name__ == \"__main__\":\n  exemplo_programa_valido()\n  exemplo_variavel_nao_declarada()\n  exemplo_incompatibilidade_tipo()\n  exemplo_divisao_por_zero()\n  exemplo_promocao_tipo()\n\n\n10.10.6 Saída Esperada\n=== Programa Válido ===\nNenhum erro semântico encontrado.\nAnálise: SUCESSO\n\n=== Variável Não Declarada ===\nEncontrados 1 erro(s) semântico(s):\n 1. Erro semântico: variável 'y' não declarada\nAnálise: FALHOU\n\n=== Incompatibilidade de Tipo ===\nEncontrados 1 erro(s) semântico(s):\n 1. Erro semântico: não é possível atribuir float a variável int 'x'\nAnálise: FALHOU\n\n=== Divisão por Zero ===\nEncontrados 1 erro(s) semântico(s):\n 1. Erro semântico: divisão por zero detectada\nAnálise: FALHOU\n\n=== Promoção de Tipo (int -&gt; float) ===\nNenhum erro semântico encontrado.\nAnálise: SUCESSO\nTipo inferido da expressão: float\n\n\n10.10.7 Exercícios Propostos\nA implementação acima fornece uma base sólida, mas pode ser estendida de várias maneiras instrutivas:\n1. Adicionar Operadores Relacionais: estenda a gramática e o analisador para suportar operadores de comparação (&lt;, &gt;, &lt;=, &gt;=, ==, !=). Esses operadores devem:\n\nAceitar operandos numéricos (int ou float);\nSempre retornar tipo booleano;\nRequer adicionar o tipo bool à linguagem.\n\n2. Implementar Comandos Condicionais: adicione suporte para if-else:\n\nA condição deve ser do tipo booleano;\nAmbos os ramos devem ser semanticamente válidos;\nConsidere como escopos aninhados afetam a Tabela de Símbolos.\n\n3. Suportar Múltiplas Declarações na Mesma Linha: modifique a gramática para permitir: int x, y, z;:\n\nTodas as variáveis devem receber o mesmo tipo;\nNomes duplicados na mesma declaração devem ser rejeitados.\n\n4. Adicionar Verificação de Inicialização: implemente análise de fluxo de dados para detectar uso de variáveis não inicializadas:\n\nVariáveis devem ser atribuídas antes de serem usadas em expressões;\nRequer rastrear o estado de inicialização de cada variável\n\n5. Implementar Funções e Escopo: Estenda a linguagem com funções:\n\nParâmetros e variáveis locais;\nVerificação de tipos de argumentos em chamadas;\nVerificação de retorno em funções não-void;\nEscopos aninhados (função dentro de função).\n\n6. Gerar Mensagens de Erro Mais Informativas: melhore a qualidade das mensagens de erro adicionando:\n\nNúmeros de linha e coluna;\nSugestões de correção (ex: “Você quis dizer ‘variavel’ em vez de ‘varaivel’?”);\nContexto do erro (mostrar a linha de código).",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analisadores Semânticos: a determinação do significado</span>"
    ]
  },
  {
    "objectID": "10-semantico.html#a-ponte-para-a-geração-de-código-intermediário",
    "href": "10-semantico.html#a-ponte-para-a-geração-de-código-intermediário",
    "title": "10  Analisadores Semânticos: a determinação do significado",
    "section": "10.11 A Ponte para a Geração de Código Intermediário",
    "text": "10.11 A Ponte para a Geração de Código Intermediário\nA análise semântica marca o fim da fase de análise do compilador. Neste ponto, o programa-fonte foi validado como sintática e semanticamente correto. A AST, agora ricamente Atribuída com informações de tipo e escopo, está pronta para ser entregue à fase de síntese, cujo primeiro passo é a geração de código intermediário (CI).\n\n10.11.1 Da Análise à Síntese: O Papel da AST Atribuída\nA transição da análise para a síntese é um momento crítico no processo de compilação. O compilador deixa de entender o código-fonte e começa a construir o programa-alvo. Gerar código de máquina diretamente a partir da AST é possível, mas apresenta duas grandes desvantagens:\n\nComplexidade: A estrutura hierárquica e de alto nível da AST está muito distante da natureza linear e de baixo nível do código de máquina. A tradução direta seria complexa e propensa a erros.\nAcoplamento: Um gerador de código que vai direto da AST para o código de máquina acopla fortemente o front-end (dependente da linguagem) ao back-end (dependente da arquitetura). Isso tornaria a portabilidade do compilador para novas máquinas um pesadelo, exigindo uma reescrita completa do gerador de código.\n\nA solução é usar uma Representação Intermediária (RI) ou Código Intermediário (CI). O CI é uma linguagem abstrata, mais simples que a linguagem-fonte, mas mais rica que a linguagem de máquina. Ele atua como uma ponte, permitindo que o front-end traduza a AST Atribuída para essa linguagem universal, e que o back-end traduza o CI para o código de máquina específico da arquitetura-alvo. Essa abordagem promove a modularidade e a portabilidade.\n\n\n10.11.2 Código de Três Endereços (TAC): Uma Representação Linear\nUma das formas mais populares de código intermediário é o Código de Três Endereços (TAC). Sua principal característica é que cada instrução possui, no máximo, um operador e três endereços (dois para operandos e um para o resultado). Um “endereço” pode ser um nome de variável, uma constante ou uma variável temporária gerada pelo compilador.\nO formato geral é: resultado = operando1 op operando2.\nO TAC transforma a estrutura hierárquica da AST em uma sequência linear de instruções simples, muito semelhante a uma linguagem de montagem abstrata. Algumas instruções típicas do TAC incluem:\n\nAtribuição Binária: x = y + z;\nAtribuição Unária: x = -y;\nAtribuição de Cópia: x = y;\nSalto Incondicional: goto L;\nSalto Condicional: if x &lt; y goto L;\nChamada de Procedimento: param x1, call p, n;\nAcesso a Array: x = y[i], x[i] = y.\n\n\n\n10.11.3 Exemplo de Tradução: Gerando TAC a partir da AST Atribuída\nO processo de geração de TAC é tipicamente implementado como uma travessia recursiva da AST Atribuída, geralmente em pós-ordem. Para cada vértice da árvore, o compilador gera uma sequência de instruções TAC.\nVamos considerar a tradução da instrução w = (a + b) * c;, assumindo que todas as variáveis são do tipo float.\n\nAST Atribuída (Simplificada): A análise semântica já validou os tipos e decorou a árvore. O vértice da atribuição, o + e o * estão todos anotados com o tipo float.\n\n    =\n  / \\\nid(w) * [type: float]\n    /  \\\n    + [type: float] id(c)\n  / \\\nid(a) id(b)\n\nTravessia e Geração de TAC: O gerador de código percorre a árvore recursivamente.\n\nEle desce até o vértice +. Para gerar o código para +, ele primeiro precisa gerar o código para seus filhos, a e b.\nOs operandos a e b não requerem geração de código; seus valores serão usados diretamente.\nAo retornar ao vértice +, o gerador de código cria uma nova variável temporária, t1, para armazenar o resultado da soma. Ele emite a primeira instrução TAC: t1 = a + b\nAgora, o gerador de código sobe para o vértice *. Ele já processou o filho esquerdo (cujo resultado está em t1) e agora processa o filho direito, c.\nCom os resultados de ambos os filhos (t1 e c), ele cria uma segunda variável temporária, t2, e emite a instrução de multiplicação: t2 = t1 * c\nFinalmente, ele sobe para o vértice raiz, =. O resultado da expressão do lado direito está em t2. Ele então emite a instrução de atribuição final: w = t2\n\nCódigo de Três Endereços Resultante: A sequência linear de instruções TAC para a expressão original é:\n\nt1 = a + b\nt2 = t1 * c\nw = t2\nQue pode ser vista na Figura Figure 10.12.\n\n\n\n\n\n\nFigure 10.12\n\n\n\nA informação da AST Atribuída é fundamental aqui. O atributo de tipo em cada vértice de operador (+, *) informa ao gerador de código qual instrução específica emitir (ex.: ADD_FLOAT vs. ADD_INT). As referências dos vértices de identificador (a, b, c, w) à Tabela de Símbolos fornecem os endereços de memória que serão usados no código final. A AST Atribuída, portanto, não é apenas uma representação validada, mas um projeto detalhado a partir do qual a fase de síntese pode construir o programa executável.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analisadores Semânticos: a determinação do significado</span>"
    ]
  },
  {
    "objectID": "10-semantico.html#exercícios",
    "href": "10-semantico.html#exercícios",
    "title": "10  Analisadores Semânticos: a determinação do significado",
    "section": "10.12 Exercícios",
    "text": "10.12 Exercícios\n\n10.12.1 Exercício 1: Semântica Básica\nDado o programa:\nx := 3;\ny := x * 2;\nx := x + y;\nTrace a execução usando semântica denotacional, mostrando o estado após cada comando.\nSolução:\n\n\\(\\sigma_0 = \\{\\}\\) (estado inicial);\n\\(\\sigma_1 = [\\![x := 3]\\!]\\sigma_0 = \\{x \\mapsto 3\\}\\);\n\\(\\sigma_2 = [\\![y := x * 2]\\!]\\sigma_1 = \\{x \\mapsto 3, y \\mapsto 6\\}\\);\n\\(\\sigma_3 = [\\![x := x + y]\\!]\\sigma_2 = \\{x \\mapsto 9, y \\mapsto 6\\}\\).\n\n\n\n10.12.2 Exercício 2: Análise de Tipos\nVerifique se o seguinte programa está bem tipado:\nint x = 5;\nbool b = (x &gt; 0);\nint y = if b then x + 1 else x - 1;\nSolução:\n\n\\(x : \\text{int}\\);\n\\(x &gt; 0 : \\text{bool}\\) (comparação de inteiros);\n\\(b : \\text{bool}\\);\n\\(x + 1 : \\text{int}\\);\n\\(x - 1 : \\text{int}\\);\n\\(\\text{if } b \\text{ then } \\text{int} \\text{ else } \\text{int} : \\text{int}\\);\n\\(y : \\text{int}\\).\n\nPrograma bem tipado!\n\n\n10.12.3 Exercício 3: Otimização\nIdentifique otimizações possíveis para:\nx := 10;\ny := x + 0;\nz := y * 1;\nw := z - z;\nSolução: aplicando regras semânticas de identidade:\n\n\\(y := x + 0\\) pode ser simplificado para \\(y := x\\);\n\\(z := y * 1\\) pode ser simplificado para \\(z := y\\);\n\\(w := z - z\\) pode ser simplificado para \\(w := 0\\).\n\nCódigo otimizado:\nx := 10;\ny := x;\nz := y;\nw := 0;\nOu ainda mais otimizado:\nx := 10;\ny := 10;\nz := 10;\nw := 0;",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analisadores Semânticos: a determinação do significado</span>"
    ]
  },
  {
    "objectID": "10a-semantico.html",
    "href": "10a-semantico.html",
    "title": "11  Fundamentos Matemáticos da Semântica",
    "section": "",
    "text": "11.0.1 As Três Abordagens Semânticas\nExistem três formas principais de definir formalmente o significado de um programa de computador, cada uma com uma perspectiva e um propósito distintos. Elas não são excludentes, mas sim ferramentas diferentes para responder à pergunta fundamental: o que este código realmente significa?.\nExiste uma pergunta que deve estar incomodando a curiosa leitora: qual tipo de semântica formal é utilizada por compiladores industriais como GCC, LLVM e MSVC? A resposta direta é que eles não implementam uma semântica formal (denotacional, axiomática ou operacional) de maneira explícita e rigorosa. Em vez disso, a abordagem deles é guiada por um pragmatismo de engenharia, fundamentado em uma especificação informal que se assemelha em espírito à semântica operacional.\nO significado de uma linguagem como C++ ou C é definido por um padrão, um documento extenso em linguagem natural que descreve o comportamento de uma máquina abstrata. O trabalho do compilador é traduzir o código fonte para um código de máquina que se comporte de acordo com as regras dessa máquina abstrata. As otimizações são validadas não por provas matemáticas formais, mas por um raciocínio rigoroso de que elas preservam o comportamento observável do programa, conforme definido no padrão.\nA escolha por não usar uma semântica denotacional completa no backend desses compiladores se deve a razões práticas e fundamentais:\nApesar de não ser implementada diretamente, a semântica formal, especialmente a denotacional, exerce uma influência indispensável e profunda em quase todos os aspectos de um compilador moderno. Ela funciona como a física teórica para a engenharia: os engenheiros não resolvem as equações de Schrödinger para projetar um chip, mas a mecânica quântica é o fundamento de toda a eletrônica moderna.\nExiste uma classe de compiladores, principalmente acadêmicos ou para nichos de altíssima criticidade, que levam a semântica formal ao extremo. O mais famoso é o CompCert, um compilador para a linguagem C, escrito e verificado usando o assistente de provas Coq.\nCada passo da compilação, da análise sintática à geração de código assembly, vem acompanhado de uma prova matemática de que a semântica do programa foi preservada. Isso fornece um nível de confiança impossível de ser alcançado apenas com testes. Não é surpresa que a CompCert seja usada em indústrias como aviação, pela Airbus, e sistemas nucleares, nas quais um erro no compilador poderia ter consequências catastróficas.\nA tendência é uma aproximação cada vez maior entre o mundo pragmático e o formal. O sucesso do Rust provou que é possível embutir garantias formais, como segurança de memória, em uma linguagem de sistemas de alto desempenho. O WebAssembly (Wasm), um padrão moderno para código executável na web e fora dela, possui uma especificação de referência totalmente formal, permitindo implementações verificadas.\nPara quem estuda compiladores, aprender semântica denotacional e outras formalidades não tem como objetivo implementá-las diretamente em algum compilador. O objetivo é adquirir o arcabouço intelectual para compreender profundamente o que os programas significam, raciocinar sobre a correção de transformações e, finalmente, estar apto a criar as próximas inovações em linguagens e compiladores. Neste documento, focaremos na semântica denotacional por sua importância central neste entendimento.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentos Matemáticos da Semântica</span>"
    ]
  },
  {
    "objectID": "10a-semantico.html#construindo-a-semântica-de-uma-linguagem-imperativa-a-linguagem-imp",
    "href": "10a-semantico.html#construindo-a-semântica-de-uma-linguagem-imperativa-a-linguagem-imp",
    "title": "11  Fundamentos Matemáticos da Semântica",
    "section": "11.1 Construindo a Semântica de uma Linguagem Imperativa: A Linguagem IMP",
    "text": "11.1 Construindo a Semântica de uma Linguagem Imperativa: A Linguagem IMP\nA Linguagem IMP é uma linguagem de programação de sistemas desenvolvida por Edgar T. Irons no final dos anos 1960 e começo dos anos 1970 para a National Security Agency, NSA. Vamos definir formalmente a semântica de IMP, uma linguagem imperativa simples mas completa.\nA sintaxe de IMP é definida pela seguinte gramática:\n// Expressões Aritméticas\nAExp ::= n                    // numeral\n       | x                    // variável\n       | AExp + AExp         // adição\n       | AExp - AExp         // subtração  \n       | AExp * AExp         // multiplicação\n\n// Expressões Booleanas\nBExp ::= true | false        // constantes booleanas\n       | AExp = AExp         // igualdade\n       | AExp ≤ AExp         // menor ou igual\n       | ¬BExp               // negação\n       | BExp ∧ BExp         // conjunção\n       | BExp ∨ BExp         // disjunção\n\n// Comandos\nCmd ::= skip                 // comando vazio\n      | x := AExp            // atribuição\n      | Cmd ; Cmd            // sequência\n      | if BExp then Cmd else Cmd   // condicional\n      | while BExp do Cmd    // repetição\nA linguagem IMP serve como um caso de estudo perfeito para entender a análise semântica porque é simples o suficiente para ser compreendida, mas completa o suficiente para demonstrar os conceitos fundamentais.\nPara entender a semântica de IMP, precisamos fazer algumas decisões de projeto importantes. Vamos explorar essas decisões e suas justificativas.\n\n11.1.1 Decisão 1: Três Categorias Sintáticas\nComeçamos dividindo a linguagem IMP em três categorias sintáticas distintas:\nAExp: Expressões Aritméticas (produzem números)\nBExp: Expressões Booleanas (produzem verdadeiro/falso)\nCmd:  Comandos (modificam o estado do programa)\nA atenta leitora verá que esta divisão não é arbitrária. Cada categoria tem um papel semântico diferente. Expressões aritméticas calculam valores numéricos mas não modificam nada. Expressões booleanas tomam decisões mas não calculam números. Comandos modificam o estado mas não retornam valores diretamente.\nPara formalizar matematicamente essas três categorias, precisamos primeiro definir os domínios semânticos sobre os quais elas operam. O domínio de valores básicos de IMP é dado por:\n\\[D_{val} = \\mathbb{Z} \\cup \\mathbb{B} \\cup \\{\\perp\\}\\]\nNesta definição, \\(\\mathbb{Z}\\) representa o conjunto dos números inteiros, \\(\\mathbb{B} = \\{\\text{true}, \\text{false}\\}\\) representa os valores booleanos, e \\(\\perp\\) (bottom) representa computação indefinida ou erro. Com este domínio estabelecido, podemos definir as funções semânticas para cada categoria sintática:\n\\[[\\![\\cdot]\\!]_{aexp} : \\text{AExp} \\times \\text{Estado} \\rightarrow \\mathbb{Z} \\cup \\{\\perp\\}\\]\n\\[[\\![\\cdot]\\!]_{bexp} : \\text{BExp} \\times \\text{Estado} \\rightarrow \\mathbb{B} \\cup \\{\\perp\\}\\]\n\\[[\\![\\cdot]\\!]_{cmd} : \\text{Cmd} \\rightarrow (\\text{Estado} \\rightarrow \\text{Estado})\\]\nEssa separação permite detectar erros como tentar usar um número em um ponto no qual se espera um booleano:\n// ERRO: x + 3 não é booleano\nif x + 3 then y := 5 else skip\nNa hora em que criamos a gramática poderíamos ter escolhido usar uma única categoria, por exemplo Expressão para englobar tudo, como em linguagens como Scheme. Neste ponto, rejeitamos essa abordagem porque uma única categoria sintática dificulta a detecção de erros de tipo, complica a definição semântica e não reflete claramente a intenção do programador, reduzindo as metáforas e analogias que podemos usar para explicar o significado de cada construção.\n\n\n11.1.2 Decisão 2: Uso de Funções Matemáticas para Semântica\nNosso objetivo é entender a análise semântica de maneira rigorosa e precisa. Para isso, escolhemos definir a semântica de IMP usando funções matemáticas. Essa decisão é fundamental para garantir clareza, precisão e a capacidade de provar propriedades sobre programas. A função semântica para expressões aritméticas, que já introduzimos, pode ser lida como: a função semântica para expressões aritméticas recebe uma expressão e um estado, e produz um inteiro ou um valor indefinido.\nO rigor da matemática é essencial porque a linguagem natural é ambígua. Esta ambiguidade pode levar a interpretações errôneas. Para destacar isso, vamos comparar duas definições possíveis para a adição. Uma descrição informal seria Some os valores das duas expressões, enquanto a definição formal é \\([\\![e_1 + e_2]\\!]_{aexp}\\sigma = [\\![e_1]\\!]_{aexp}\\sigma + [\\![e_2]\\!]_{aexp}\\sigma\\). A notação matemática não deixa espaço para interpretação.\nAlém disso, com as definições matemáticas, podemos provar propriedades sobre programas. Por exemplo, será possível provar que x := 5; y := x sempre resulta em y contendo o valor 5. Finalmente, a notação matemática é diretamente traduzível em código. Isso significa que as definições que criamos podem ser implementadas diretamente no compilador, garantindo que o comportamento do compilador corresponda exatamente à semântica formal que definimos. No caso da definição formal da adição que acabamos de ver, poderíamos ter uma implementação em Python como:\ndef eval_aexp(expr, state):\n    if isinstance(expr, Numeral):\n        return expr.value\n    elif isinstance(expr, Variable):\n        return state[expr.name]\n    elif isinstance(expr, Addition):\n        return eval_aexp(expr.left, state) + eval_aexp(expr.right, state)\n    # ... outros casos\nA atenta leitora não deve perder de vista que poderíamos descrever a semântica apenas em linguagem natural. Isso, com certeza, funcionaria para exemplos pequenos, mas se tornaria inconsistente, impreciso e muito extenso para casos complexos.\n\n\n11.1.3 Decisão 3: Estado como Função Matemática\nUma parte crucial da semântica de qualquer linguagem imperativa é o conceito de estado. O estado representa o conteúdo da memória do programa em um dado momento, ou seja, o valor atual de todas as variáveis. Para mantermos nossa coerência, precisamos de uma maneira precisa e matemática para representar esse estado. Neste caso, escolhemos representar o estado como uma função matemática que mapeia variáveis para seus valores atuais:\n\\[\\sigma : \\text{Var} \\rightarrow \\mathbb{Z}\\]\nNesta definição, \\(\\sigma(x)\\) retorna o valor atual da variável \\(x\\).\nNa realidade, variáveis são identificadores, nomes, que atribuímos a endereços de memória. Mas para análise semântica, não precisamos desses detalhes de implementação. O importante é a relação variável → valor. A notação \\(\\sigma[x \\mapsto v]\\) representa um novo estado que é idêntico a \\(\\sigma\\), exceto que \\(x\\) agora tem valor \\(v\\). Isso captura formalmente o que acontece em uma atribuição. Podemos definir essa operação de atualização precisamente:\n\\[\\sigma[x \\mapsto v](y) = \\begin{cases} v & \\text{se } y = x \\\\ \\sigma(y) & \\text{se } y \\neq x \\end{cases}\\]\nVamos ver como isso funciona na prática. Considere o comando x := 20 em um estado inicial onde x vale 5 e y vale 10. Neste caso, teremos um estado inicial \\(\\sigma = \\{x \\mapsto 5, y \\mapsto 10\\}\\). Após a execução de x := 20, o estado final será formalmente dado por:\n\\[\\sigma' = \\sigma[x \\mapsto 20] = \\{x \\mapsto 20, y \\mapsto 10\\}\\]\nA notação deixa claro que \\(y\\) não foi alterado, apenas \\(x\\).\nPara todas as decisões que tomamos existem algumas alternativas possíveis. Neste caso, poderíamos usar uma lista de pares (variável, valor) ou uma estrutura de dados mais complexa como um dicionário. Embora essas representações sejam úteis na implementação, a notação funcional é mais limpa matematicamente e evita questões sobre ordem ou duplicatas.\n\n\n11.1.4 Decisão 4: Definição Composicional\nO princípio da composicionalidade é fundamental para a semântica denotacional. Ele afirma que o significado de uma construção sintática é determinado pelos significados de suas partes constituintes. Isso nos permite construir o significado de programas complexos a partir do significado de suas partes menores. Neste caso, cada regra semântica é definida em termos dos componentes do construtor. Por exemplo:\n\\[[\\![e_1 + e_2]\\!]_{aexp}\\sigma = [\\![e_1]\\!]_{aexp}\\sigma + [\\![e_2]\\!]_{aexp}\\sigma\\]\nO princípio da composicionalidade nos permite raciocinar sobre a semântica de expressões complexas de forma modular. Isso reflete como o compilador realmente processa o código. Considere o programa (3 + 5) * 2 e sua árvore de análise:\n    *\n   / \\\n  +   2\n / \\\n3   5\nA avaliação procede de baixo para cima: primeiro \\([\\![3]\\!] = 3\\), depois \\([\\![5]\\!] = 5\\), em seguida \\([\\![3 + 5]\\!] = [\\![3]\\!] + [\\![5]\\!] = 8\\), depois \\([\\![2]\\!] = 2\\), e finalmente \\([\\![(3 + 5) * 2]\\!] = [\\![3 + 5]\\!] \\times [\\![2]\\!] = 16\\).\nAlém disso, existe uma vantagem inestimável: a modularidade. Podemos adicionar novos operadores sem reescrever toda a semântica. Por exemplo, se quisermos adicionar divisão, podemos fazer isso facilmente:\n\\[[\\![e_1 / e_2]\\!]_{aexp}\\sigma = [\\![e_1]\\!]_{aexp}\\sigma \\div [\\![e_2]\\!]_{aexp}\\sigma\\]\n\n\n11.1.5 Decisão 5: Semântica Completa de Expressões\nCom os fundamentos estabelecidos, podemos agora definir formalmente o significado completo de todas as expressões em IMP. Para expressões aritméticas, começamos com os casos base. Numerais mapeiam diretamente para seus valores matemáticos: \\([\\![n]\\!]_{aexp}\\sigma = valor(n)\\), onde \\(valor : \\text{Numeral} \\rightarrow \\mathbb{Z}\\) converte a representação sintática em valor matemático. Variáveis obtêm seus valores consultando o estado atual: \\([\\![x]\\!]_{aexp}\\sigma = \\sigma(x)\\).\nAs operações binárias seguem o princípio da composicionalidade. A subtração é definida como \\([\\![e_1 - e_2]\\!]_{aexp}\\sigma = [\\![e_1]\\!]_{aexp}\\sigma - [\\![e_2]\\!]_{aexp}\\sigma\\), e a multiplicação como \\([\\![e_1 * e_2]\\!]_{aexp}\\sigma = [\\![e_1]\\!]_{aexp}\\sigma \\times [\\![e_2]\\!]_{aexp}\\sigma\\). Uma propriedade fundamental das expressões aritméticas é que elas são puras, não modificam o estado. Isso garante que \\([\\![e]\\!]_{aexp}\\sigma = [\\![e]\\!]_{aexp}\\sigma\\), uma igualdade que parece trivial mas tem implicações profundas para otimização.\nPara expressões booleanas, as constantes são diretas: \\([\\![\\text{true}]\\!]_{bexp}\\sigma = \\text{true}\\) e \\([\\![\\text{false}]\\!]_{bexp}\\sigma = \\text{false}\\). As comparações avaliam primeiro suas subexpressões aritméticas: \\([\\![e_1 = e_2]\\!]_{bexp}\\sigma\\) é true se e somente se \\([\\![e_1]\\!]_{aexp}\\sigma = [\\![e_2]\\!]_{aexp}\\sigma\\). Similarmente, \\([\\![e_1 \\leq e_2]\\!]_{bexp}\\sigma\\) é true se e somente se \\([\\![e_1]\\!]_{aexp}\\sigma \\leq [\\![e_2]\\!]_{aexp}\\sigma\\).\nOs operadores lógicos seguem a semântica padrão da lógica booleana: \\([\\![\\neg b]\\!]_{bexp}\\sigma = \\neg([\\![b]\\!]_{bexp}\\sigma)\\), \\([\\![b_1 \\land b_2]\\!]_{bexp}\\sigma = [\\![b_1]\\!]_{bexp}\\sigma \\land [\\![b_2]\\!]_{bexp}\\sigma\\), e \\([\\![b_1 \\lor b_2]\\!]_{bexp}\\sigma = [\\![b_1]\\!]_{bexp}\\sigma \\lor [\\![b_2]\\!]_{bexp}\\sigma\\). Assim como expressões aritméticas, expressões booleanas são puras, avaliam um valor sem modificar o estado.\n\n\n11.1.6 Decisão 6: Tratamento do Comando Skip\nO comando skip é um comando especial que não faz nada. Sua inclusão na linguagem pode parecer trivial, mas tem implicações semânticas importantes. Vamos definir sua semântica:\n\\[[\\![\\text{skip}]\\!]_{cmd}\\sigma = \\sigma\\]\nO skip funciona como o elemento neutro da composição sequencial. Para que a semântica seja uma estrutura algébrica completa, precisamos de elementos identidade. Para nossa linguagem IMP, skip será esse elemento. Isso significa que para qualquer comando \\(S\\), teremos:\n\\[[\\![S ; \\text{skip}]\\!]_{cmd}\\sigma = [\\![\\text{skip}]\\!]_{cmd}([\\![S]\\!]_{cmd}\\sigma) = [\\![S]\\!]_{cmd}\\sigma\\]\nO funcionamento do skip é análogo a como o \\(0\\) é a identidade da adição: \\(x + 0 = x\\). Ao transformar ou otimizar código, skip serve como placeholder:\n// Antes da otimização\nif x &gt; 0 then y := 5 else skip\n\n// Depois de alguma transformação que remove o ramo then\nif x &gt; 0 then skip else skip\n\n// Finalmente otimizado para\nskip\nPodemos usar o skip para simplificar a semântica de condicionais e decisões, garantindo que sempre haja um comando em cada ramo. Isso evita a necessidade de tratar casos especiais, como em:\nif error then \n    print(\"Erro encontrado\")\nelse \n    skip\n\n\n11.1.7 Decisão 7: Sequência como Composição de Funções\nPara dar significado a uma sequência de comandos, como S₁; S₂, precisamos de uma forma que capture rigorosamente a noção de que um comando é executado após o outro. A abordagem escolhida para este caso será definir a semântica da sequência como sendo uma composição de funções. A curiosa leitora verá que esta decisão, embora pareça abstrata, é uma das mais elegantes e poderosas que podemos tomar. Formalmente, definiremos a semântica da sequência da seguinte maneira:\n\\[[\\![S_1 ; S_2]\\!]_{cmd}\\sigma = [\\![S_2]\\!]_{cmd}([\\![S_1]\\!]_{cmd}\\sigma)\\]\nA atenta leitora pode, inicialmente, achar a notação um pouco contra-intuitiva, já que S₂ aparece antes de S₁ na fórmula. No entanto, esta é a notação padrão para a composição de funções, \\(g(f(x))\\), em que a função mais interna, \\(f(x)\\), é aplicada primeiro. A interpretação é direta e alinhada com nossa intuição sobre a execução de programas: primeiro, executamos o comando S₁ no estado atual σ, o que produz um novo estado intermediário. Em seguida, pegamos este novo estado e o usamos como entrada para a execução do comando S₂, produzindo o estado final.\nVamos detalhar isso com um exemplo prático para que não reste nenhuma dúvida. Considere a sequência de comandos x := 5; y := x + 3, partindo de um estado inicial \\(\\sigma_0\\) em que x e y valem 0. A aplicação da nossa regra semântica se desdobra da seguinte forma: a função semântica para toda a sequência, \\([\\![\\text{x := 5; y := x + 3}]\\!]_{cmd}\\), aplicada ao estado inicial \\(\\sigma_0\\), é equivalente a aplicar a função de y := x + 3 ao resultado da aplicação da função de x := 5 a \\(\\sigma_0\\). O primeiro passo, \\([\\![\\text{x := 5}]\\!]_{cmd}\\sigma_0\\), transforma o estado inicial em um novo estado \\(\\{x \\mapsto 5, y \\mapsto 0\\}\\). Este novo estado é, então, usado como argumento para a segunda função, \\([\\![\\text{y := x + 3}]\\!]_{cmd}\\), que lê o valor de x como 5, calcula a soma 5 + 3, e produz o estado final \\(\\{x \\mapsto 5, y \\mapsto 8\\}\\).\nUma das grandes vantagens desta abordagem funcional é que a composição de funções é associativa. Isso significa que a semântica de (S₁; S₂) ; S₃ é idêntica à de S₁; (S₂; S₃). Essa propriedade matemática garante que não precisamos nos preocupar com parênteses em longas sequências de comandos, o que reflete perfeitamente como escrevemos e entendemos código imperativo. Embora pudéssemos ter optado por uma abordagem alternativa, como representar a sequência de comandos como uma lista e iterar sobre ela, a escolha pela composição binária é matematicamente mais elegante e robusta. Ela facilita enormemente a realização de provas formais sobre programas, especialmente aquelas que utilizam o método de indução matemática sobre a estrutura dos comandos.\n\n\n11.1.8 Decisão 8: Semântica da Atribuição\nA atribuição é o comando mais fundamental que modifica o estado do programa. Sua semântica captura a essência da computação imperativa:\n\\[[\\![x := e]\\!]_{cmd}\\sigma = \\sigma[x \\mapsto [\\![e]\\!]_{aexp}\\sigma]\\]\nA sagaz leitora perceberá que esta definição expressa precisamente o processo de atribuição: primeiro, avalia-se a expressão \\(e\\) no estado atual \\(\\sigma\\), obtendo um valor; em seguida, o estado é atualizado para mapear a variável \\(x\\) para este valor, mantendo todas as outras variáveis inalteradas.\nConsidere um exemplo com estado explícito. Para o comando x := y + 1 executado em um estado \\(\\sigma = \\{x \\mapsto 3, y \\mapsto 5\\}\\), primeiro avaliamos a expressão do lado direito: \\([\\![\\text{y + 1}]\\!]_{aexp}\\sigma = \\sigma(y) + 1 = 5 + 1 = 6\\). O estado resultante será \\(\\sigma' = \\sigma[x \\mapsto 6] = \\{x \\mapsto 6, y \\mapsto 5\\}\\), observe que o valor de y permanece inalterado.\nUma propriedade importante da atribuição é o comportamento de sobreposição em atribuições sucessivas à mesma variável. Para a sequência x := e₁ ; x := e₂, temos:\n\\[[\\![x := e_1 ; x := e_2]\\!]_{cmd}\\sigma = \\sigma[x \\mapsto [\\![e_2]\\!]_{aexp}(\\sigma[x \\mapsto [\\![e_1]\\!]_{aexp}\\sigma])]\\]\nAgora, se a expressão \\(e_2\\) não faz referência a \\(x\\), então o valor calculado em \\(e_1\\) é irrelevante para o resultado final, e podemos simplificar para:\n\\[[\\![x := e_1 ; x := e_2]\\!]_{cmd}\\sigma = \\sigma[x \\mapsto [\\![e_2]\\!]_{aexp}\\sigma]\\]\nEsta observação justifica matematicamente a eliminação de atribuições mortas, uma otimização comum em compiladores.\n\n\n11.1.9 Decisão 9: Semântica do Condicional\nO condicional realiza seleção de controle baseada em uma expressão booleana. Sua semântica é definida por casos:\n\\[[\\![\\text{if } b \\text{ then } S_1 \\text{ else } S_2]\\!]_{cmd}\\sigma = \\begin{cases} [\\![S_1]\\!]_{cmd}\\sigma & \\text{se } [\\![b]\\!]_{bexp}\\sigma = \\text{true} \\\\ [\\![S_2]\\!]_{cmd}\\sigma & \\text{se } [\\![b]\\!]_{bexp}\\sigma = \\text{false} \\\\ \\perp & \\text{se } [\\![b]\\!]_{bexp}\\sigma = \\perp \\end{cases}\\]\nA interpretação é natural: avalia-se primeiro a condição \\(b\\) no estado atual; se o resultado for verdadeiro, executa-se \\(S_1\\); se for falso, executa-se \\(S_2\\); se a avaliação da condição resulta em erro (representado por \\(\\perp\\)), o resultado de todo o condicional é indefinido.\nVejamos um exemplo concreto. Para o comando if x &gt; 0 then y := 1 else y := -1 executado em um estado \\(\\sigma = \\{x \\mapsto 5, y \\mapsto 0\\}\\), primeiro avaliamos a condição: \\([\\![\\text{x &gt; 0}]\\!]_{bexp}\\sigma = (5 &gt; 0) = \\text{true}\\). Como a condição é verdadeira, executamos o ramo then: \\([\\![\\text{y := 1}]\\!]_{cmd}\\sigma = \\{x \\mapsto 5, y \\mapsto 1\\}\\).\nEm muitos programas, um dos ramos do condicional pode ser simplesmente skip. Neste caso, a semântica se simplifica para:\n\\[[\\![\\text{if } b \\text{ then } S \\text{ else skip}]\\!]_{cmd}\\sigma = \\begin{cases} [\\![S]\\!]_{cmd}\\sigma & \\text{se } [\\![b]\\!]_{bexp}\\sigma = \\text{true} \\\\ \\sigma & \\text{se } [\\![b]\\!]_{bexp}\\sigma = \\text{false} \\end{cases}\\]\nEsta definição captura precisamente a semântica de seleção condicional. O tratamento explícito do caso de erro (\\(\\perp\\)) na definição geral garante que programas com condições malformadas sejam adequadamente rejeitados.\n\n\n11.1.10 Decisão 10: Valores vs. Efeitos Colaterais\nNa arquitetura semântica da linguagem IMP, estabelecemos uma distinção fundamental e intransigente: expressões produzem valores, enquanto comandos produzem efeitos colaterais. Uma expressão aritmética ou booleana em IMP é “pura”; sua avaliação calcula um resultado, seja um número ou um valor de verdade, mas jamais altera o estado do programa. Em contrapartida, um comando modifica o estado, alterando o valor de variáveis, mas não produz um valor que possa ser usado em outro cálculo.\nA sagaz leitora perceberá que esta decisão de projeto simplifica enormemente o raciocínio sobre o comportamento dos programas. Como as expressões são livres de efeitos colaterais, a igualdade \\([\\![\\text{x + y}]\\!]_{aexp}\\sigma = [\\![\\text{x + y}]\\!]_{aexp}\\sigma\\) é sempre verdadeira. O valor de x + y depende unicamente do estado atual, e sua avaliação não o modifica. Isso tem implicações diretas e poderosas para a otimização de código. Se uma mesma expressão, como (x + y), aparece múltiplas vezes em um trecho de código, o compilador pode, com total segurança, calculá-la uma única vez, armazenar o resultado em uma variável temporária e reutilizá-lo. Essa otimização, conhecida como eliminação de subexpressão comum, só é segura porque a pureza das expressões garante que o valor de x e y não mudará entre as avaliações.\nPara contrastar, considere a complexidade introduzida por linguagens como C, que rejeitaram esta separação. Em C, uma expressão como (x++) + (x++) não apenas calcula um valor, mas também modifica a variável x como um efeito colateral. O resultado desta expressão é, de fato, indefinido pela especificação da linguagem, pois não fica claro qual dos incrementos ocorre primeiro. Ao proibir efeitos colaterais em expressões, IMP evita completamente essa classe de ambiguidades perigosas, optando por um modelo mais simples e previsível. A alternativa de permitir expressões com efeitos colaterais, embora possa oferecer uma notação mais concisa em alguns casos, introduz uma complexidade significativa, exigindo regras intrincadas sobre ordem de avaliação e “pontos de sequência”, tornando o raciocínio sobre o código e a implementação de otimizações uma tarefa muito mais árdua.\n\n\n11.1.11 Decisão 11: Semântica de Loops via Pontos Fixos\nO tratamento de laços de repetição, como o comando while, representa o desafio mais profundo na definição da semântica de uma linguagem imperativa. O problema fundamental reside na possibilidade de não-terminação. Um laço como while true do skip nunca para de executar, e nossa semântica precisa de uma maneira formal e rigorosa para capturar esse comportamento infinito. Uma abordagem recursiva simples, que funcionou para outras estruturas, falhará aqui, pois a definição do significado de um laço parece depender de si mesma, levando a uma regressão infinita.\nA solução para este impasse é uma das ideias mais elegantes da ciência da computação: definir o significado do comando while como o menor ponto fixo de uma função de ordem superior. Em vez de definir o significado diretamente, definimos uma função transformadora, que chamaremos de \\(F\\). Esta função \\(F\\) recebe como entrada uma função candidata a ser o significado do laço, \\(f\\), e produz uma nova função que representa uma aproximação melhor desse significado. A definição de \\(F\\) captura a lógica de uma única iteração do laço:\n\\[F(f) = \\lambda\\sigma. \\begin{cases} f([\\![S]\\!]_{cmd}\\sigma) & \\text{se } [\\![b]\\!]_{bexp}\\sigma = \\text{true} \\\\ \\sigma & \\text{se } [\\![b]\\!]_{bexp}\\sigma = \\text{false} \\end{cases}\\]\nEsta fórmula expressa que, para um estado \\(\\sigma\\), se a condição \\(b\\) for falsa, o laço termina e o estado não se altera. Se a condição for verdadeira, o corpo do laço \\(S\\) é executado, produzindo um novo estado, e então aplicamos a função \\(f\\) a este novo estado para continuar a execução. O significado do comando while b do S é, então, a função que não é alterada quando aplicada a \\(F\\), ou seja, o ponto fixo de \\(F\\), denotado por \\(\\text{fix}(F)\\).\nA razão pela qual os pontos fixos são a ferramenta correta é que eles nos permitem construir o significado do laço de forma iterativa, começando do nada. Iniciamos com a função \\(\\bot\\), que representa a indefinição total, o “não sei nada”. A primeira aproximação, \\(F^1(\\bot)\\), nos dá o significado de laços que executam no máximo uma vez. A segunda, \\(F^2(\\bot)\\), captura laços que executam no máximo duas vezes, e assim por diante. O limite desta cadeia de aproximações, \\(\\bigcup_{n=0}^{\\infty} F^n(\\bot)\\), é o menor ponto fixo e representa o significado completo e exato do laço para todos os casos que terminam. Crucialmente, se para um determinado estado inicial o laço nunca termina, o resultado da aplicação desta função limite será \\(\\perp\\), o que captura formalmente a não-terminação.\nQualquer outra alternativa apresenta limitações. Descrever o laço com uma definição operacional simples como execute o corpo enquanto a condição for verdadeira é informal e não lida com a não-terminação de maneira matemática. Tentar contar o número de iterações nos prenderia a detalhes de implementação, e simplesmente ignorar a não-terminação seria assumir que todos os programas terminam, o que é factualmente incorreto e impediria a análise de uma vasta classe de programas, como servidores e sistemas operacionais, que são projetados para rodar indefinidamente. A abordagem de pontos fixos, portanto, não é apenas uma escolha técnica, mas uma necessidade fundamental para uma semântica precisa e completa.\n\n\n11.1.12 Decisão 12: Separação Entre Sintaxe e Semântica\nUma decisão de projeto fundamental, que permeia toda a nossa discussão, é a rigorosa separação em camadas entre a sintaxe e a semântica. De um lado, definimos a sintaxe da linguagem IMP, a forma dos programas que podem ser escritos, através de uma gramática formal em EBNF. Do outro, definimos a semântica, o que esses programas significam, utilizando um arcabouço completamente distinto: o de funções matemáticas. Essas duas camadas são deliberadamente independentes, conectadas apenas pela função de interpretação \\([\\![\\cdot]\\!]\\) que mapeia uma na outra.\nEsta separação não é um mero capricho acadêmico; ela oferece vantagens profundas para o projeto e a implementação de linguagens e compiladores. A mais importante delas é a flexibilidade para atribuir múltiplas semânticas a uma única sintaxe. O que fizemos até agora foi definir a semântica denotacional, que é ideal para análise e otimização. No entanto, poderíamos, com a mesma facilidade, definir uma semântica operacional para a mesma sintaxe, descrevendo como um programa executa passo a passo, o que seria mais útil para construir um interpretador. Ou ainda, uma semântica axiomática, focada em provar propriedades do programa. A sintaxe permanece estável, servindo como uma fundação sobre a qual diferentes interpretações de significado podem ser construídas.\nAlém disso, esta separação é o que torna as otimizações de compiladores possíveis e seguras. Duas sequências de código podem ser sintaticamente muito diferentes, mas se pudermos provar que suas semânticas são equivalentes, o compilador tem a liberdade de substituir uma pela outra. Por exemplo, o código x := 5; x := 5; y := 10 é sintaticamente mais longo que x := 5; y := 10, mas, em muitos contextos, seus significados denotacionais são idênticos, justificando a eliminação da atribuição redundante. Por fim, a abstração semântica nos liberta dos detalhes de implementação. Nossa definição de significado não faz menção a como os números são representados em bits, como as variáveis são alocadas na memória ou qual a ordem de avaliação de subexpressões. Esses são problemas para a fase de geração de código, e a separação garante que possamos raciocinar sobre o significado do programa em um nível muito mais alto e limpo.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentos Matemáticos da Semântica</span>"
    ]
  },
  {
    "objectID": "10b-semantico.html",
    "href": "10b-semantico.html",
    "title": "12  Julgamento de Tipos em Linguagens Imperativas",
    "section": "",
    "text": "12.1 Do Elegante ao Pragmático: Por Que o Algoritmo W Não Basta\nA transição do mundo funcional para o imperativo revela um choque fundamental de filosofias de design. O Algoritmo W, com sua elegância matemática e capacidade de inferência completa, foi projetado para linguagens funcionais puras onde o estado é imutável e as funções são valores de primeira classe. Contudo, as linguagens imperativas introduzem características que tornam a inferência completa de tipos não apenas difícil, mas frequentemente indesejável:\nMutabilidade: em linguagens funcionais, uma vez que x recebe um tipo, ele permanece inalterado. Em linguagens imperativas, x pode ser reatribuído múltiplas vezes, e seu tipo pode precisar ser conhecido explicitamente para cada ponto do programa.\nEfeitos colaterais: funções imperativas não são puras. Elas podem modificar estado global, realizar I/O, lançar exceções. O tipo de uma função deve capturar não apenas sua assinatura, mas potencialmente seus efeitos.\nPonteiros e referências: C e C++ introduzem ponteiros, que criam um segundo nível de indireção no sistema de tipos. Um int* não é apenas “um tipo que aponta para int”, mas carrega semântica sobre propriedade de memória, alinhamento e aritmética de ponteiros.\nOrdem de declaração: muitas linguagens imperativas permitem que funções sejam chamadas antes de serem declaradas, ou que tipos sejam usados antes de sua definição completa. Isso exige múltiplas passagens ou análise diferida.\nPerformance previsível: em linguagens de sistemas como C e C++, o programador precisa de controle fino sobre representação de dados e layout de memória. Inferência automática poderia gerar código com características de performance surpreendentes.\nA consequência dessas diferenças é que a maioria das linguagens imperativas adota uma filosofia pragmática: verificação de tipos estrita combinada com inferência limitada e previsível. O objetivo não é maximizar a inferência, mas sim encontrar um equilíbrio entre conveniência do programador, clareza do código e previsibilidade do compilador.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Julgamento de Tipos em Linguagens Imperativas</span>"
    ]
  },
  {
    "objectID": "10b-semantico.html#taxonomia-dos-sistemas-de-tipos-imperativos",
    "href": "10b-semantico.html#taxonomia-dos-sistemas-de-tipos-imperativos",
    "title": "12  Julgamento de Tipos em Linguagens Imperativas",
    "section": "12.2 Taxonomia dos Sistemas de Tipos Imperativos",
    "text": "12.2 Taxonomia dos Sistemas de Tipos Imperativos\nAntes de mergulhar nos detalhes de cada linguagem, é útil estabelecer uma taxonomia das abordagens principais. As linguagens imperativas podem ser classificadas ao longo de vários eixos:\n\n12.2.1 Eixo 1: Momento da Verificação\n\nEstática pura: toda verificação em tempo de compilação (C, Rust)\nEstática com escape: verificação em compilação, mas com unsafe ou casts (C++, Rust com unsafe)\nHíbrida: verificação em compilação e runtime (Java, C#)\nGradual: opcional em compilação, sempre em runtime (Python com type hints, TypeScript)\nDinâmica pura: apenas em runtime (JavaScript, Python sem hints)\n\n\n\n12.2.2 Eixo 2: Equivalência de Tipos\n\nNominal: tipos são iguais se têm o mesmo nome (Java, C#, C++)\nEstrutural: tipos são iguais se têm a mesma estrutura (Go, TypeScript)\nMista: nominal para tipos nomeados, estrutural para anônimos (OCaml)\n\n\n\n12.2.3 Eixo 3: Inferência\n\nNenhuma: tipos sempre explícitos (C até C++03)\nLocal: inferência em expressões simples (C++11 auto, Java var)\nFluxo-sensitiva: inferência através de branches (TypeScript, Flow)\nCompleta: inferência global (ML, Haskell, parcialmente em Rust)",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Julgamento de Tipos em Linguagens Imperativas</span>"
    ]
  },
  {
    "objectID": "10b-semantico.html#python-do-dinâmico-ao-gradual",
    "href": "10b-semantico.html#python-do-dinâmico-ao-gradual",
    "title": "12  Julgamento de Tipos em Linguagens Imperativas",
    "section": "12.3 Python: Do Dinâmico ao Gradual",
    "text": "12.3 Python: Do Dinâmico ao Gradual\nPython representa uma abordagem radicalmente diferente. Historicamente uma linguagem puramente dinâmica, ela evoluiu para um sistema de tipagem gradual com a PEP 484 (2014).\n\n12.3.1 Python Puro: Duck Typing\nNo Python tradicional, não há julgamento de tipos em tempo de compilação:\ndef soma(a, b):\n    return a + b\n\nresultado = soma(3, 5)        # Funciona: 8\nresultado = soma(\"oi\", \"lá\")  # Funciona: \"oilá\"\nresultado = soma(3, \"lá\")     # TypeError em runtime\nO sistema de tipos é baseado em duck typing: “se anda como pato e faz quack como pato, então é um pato”. O que importa não é o tipo nominal de um objeto, mas sim quais operações ele suporta.\nFormalmente, não há julgamento de tipo estático. O “tipo” de uma expressão só é conhecido em runtime:\n\\[\\text{typeof}(e, \\sigma) = \\tau\\]\nOnde \\(\\sigma\\) é o estado do programa (valores concretos) e \\(\\tau\\) é o tipo descoberto em runtime.\n\n\n12.3.2 Python com Type Hints: Tipagem Gradual\nA PEP 484 introduziu anotações de tipo opcionais:\ndef soma(a: int, b: int) -&gt; int:\n    return a + b\n\n# Type checker (mypy) pode detectar erro:\nresultado: int = soma(3, \"olá\")  # Type error em análise estática\nO sistema de julgamento de tipos para Python anotado usa equivalência estrutural com subtipagem:\nRegra de Subtipagem (Subsumption):\n\\[\\frac{\\Gamma \\vdash e : T_1 \\quad T_1 &lt;: T_2}{\\Gamma \\vdash e : T_2}\\]\nSe e tem tipo T₁ e T₁ é subtipo de T₂, então e pode ser usado onde T₂ é esperado.\nExemplo prático:\nfrom typing import Protocol\n\nclass Desenhavel(Protocol):\n    def desenhar(self) -&gt; None: ...\n\nclass Circulo:\n    def desenhar(self) -&gt; None:\n        print(\"○\")\n\nclass Quadrado:\n    def desenhar(self) -&gt; None:\n        print(\"□\")\n\ndef renderizar(obj: Desenhavel) -&gt; None:\n    obj.desenhar()\n\n# Type checker aceita ambos:\nrenderizar(Circulo())   # OK: Circulo é estruturalmente compatível\nrenderizar(Quadrado())  # OK: Quadrado é estruturalmente compatível\nO julgamento aqui é:\n\\[\\frac{\\text{Circulo} &lt;: \\text{Desenhavel}}{\\Gamma \\vdash \\text{Circulo()} : \\text{Desenhavel}}\\]\nCaracterísticas únicas do sistema Python:\n\nVerificação é opcional: código sem anotações é válido\nStructural subtyping: protocols definem interfaces estruturais\nTipo Any: escape hatch que é compatível com tudo\nUnion types: int | str permite múltiplos tipos\nType narrowing: refinamento de tipos em branches\n\ndef processar(valor: int | str) -&gt; int:\n    if isinstance(valor, int):\n        # Aqui, type checker sabe que valor: int\n        return valor * 2\n    else:\n        # Aqui, type checker sabe que valor: str\n        return len(valor)",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Julgamento de Tipos em Linguagens Imperativas</span>"
    ]
  },
  {
    "objectID": "10b-semantico.html#c-tipagem-nominal-minimalista",
    "href": "10b-semantico.html#c-tipagem-nominal-minimalista",
    "title": "12  Julgamento de Tipos em Linguagens Imperativas",
    "section": "12.4 C: Tipagem Nominal Minimalista",
    "text": "12.4 C: Tipagem Nominal Minimalista\nC representa o extremo oposto: um sistema de tipos estático, fraco e nominal, projetado para estar o mais próximo possível do hardware.\n\n12.4.1 Sistema de Tipos de C\nO sistema de tipos de C é deliberadamente minimalista. As regras de julgamento são simples, mas permissivas:\nRegra de Declaração de Variável:\n\\[\\frac{T \\in \\{\\text{int, float, char, ...}\\}}{\\Gamma \\vdash (T \\, x;) : \\text{ok}, \\quad \\Gamma' = \\Gamma \\cup \\{x : T\\}}\\]\nRegra de Coerção Aritmética:\n\\[\\frac{\\Gamma \\vdash e_1 : T_1 \\quad \\Gamma \\vdash e_2 : T_2 \\quad T_1, T_2 \\in \\text{NumericTypes}}{\\Gamma \\vdash e_1 + e_2 : \\text{promote}(T_1, T_2)}\\]\nOnde promote segue a hierarquia: char &lt; short &lt; int &lt; long &lt; float &lt; double\nExemplo:\nint x = 10;\nfloat y = 3.14;\ndouble z = x + y;  // x promovido para float, resultado promovido para double\n\n\n12.4.2 A Fraqueza Controlada: Casts e Ponteiros\nO que torna C “fracamente tipado” é sua permissividade com casts e aritmética de ponteiros:\nint x = 42;\nfloat* p = (float*)&x;  // Cast inseguro: reinterpreta bits\n*p = 3.14;              // Comportamento indefinido\n\nchar* str = \"Hello\";\nint offset = str[2];    // 'l' interpretado como int\nRegra de Cast (Conversão Explícita):\n\\[\\frac{\\Gamma \\vdash e : T_1}{\\Gamma \\vdash (T_2)e : T_2}\\]\nEsta regra é perigosamente permissiva: ela aceita qualquer cast, transferindo a responsabilidade de correção para o programador. O compilador confia que você sabe o que está fazendo.\nExemplo de julgamento em C:\nPara o código:\nint x = 10;\nint* p = &x;\nint y = *p + 5;\nA derivação seria:\n\\[\\frac{\n  \\frac{}{\\Gamma_0 \\vdash 10 : \\text{int}} \\quad \\Gamma_1 = \\{x : \\text{int}\\}\n}{\n  \\Gamma_1 \\vdash (x = 10) : \\text{ok}\n}\\]\n\\[\\frac{\n  \\frac{x : \\text{int} \\in \\Gamma_1}{\\Gamma_1 \\vdash \\&x : \\text{int*}} \\quad \\Gamma_2 = \\Gamma_1 \\cup \\{p : \\text{int*}\\}\n}{\n  \\Gamma_2 \\vdash (p = \\&x) : \\text{ok}\n}\\]\n\\[\\frac{\n  \\frac{p : \\text{int*} \\in \\Gamma_2}{\\Gamma_2 \\vdash *p : \\text{int}} \\quad\n  \\frac{}{\\Gamma_2 \\vdash 5 : \\text{int}}\n}{\n  \\Gamma_2 \\vdash *p + 5 : \\text{int}\n}\\]",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Julgamento de Tipos em Linguagens Imperativas</span>"
    ]
  },
  {
    "objectID": "10b-semantico.html#c-templates-e-dedução-de-tipos",
    "href": "10b-semantico.html#c-templates-e-dedução-de-tipos",
    "title": "12  Julgamento de Tipos em Linguagens Imperativas",
    "section": "12.5 C++: Templates e Dedução de Tipos",
    "text": "12.5 C++: Templates e Dedução de Tipos\nC++ estende C com um sistema de tipos significativamente mais rico, mas mantém compatibilidade com a fraqueza controlada de C.\n\n12.5.1 Templates: Duck Typing em Tempo de Compilação\nTemplates em C++ são essencialmente duck typing estático. Um template é instanciado apenas quando usado, e o compilador verifica se todas as operações requeridas existem:\ntemplate&lt;typename T&gt;\nT max(T a, T b) {\n    return a &gt; b ? a : b;\n}\n\nint x = max(3, 5);        // T deduzido como int\ndouble y = max(3.1, 2.9); // T deduzido como double\nRegra de Instanciação de Template:\n\\[\\frac{\n  \\Gamma \\vdash e_1 : T \\quad \\Gamma \\vdash e_2 : T \\quad\n  T \\text{ suporta } &gt;\n}{\n  \\Gamma \\vdash \\text{max}(e_1, e_2) : T\n}\\]\nEsta verificação acontece por instanciação: o compilador substitui T por cada tipo concreto usado e então verifica se as operações são válidas.\n\n\n12.5.2 Dedução de Tipos com auto\nC++11 introduziu auto, que realiza inferência de tipo local:\nauto x = 42;           // x : int\nauto y = 3.14;         // y : double\nauto z = x + y;        // z : double\nauto f = [](int x) {   // f : lambda com tipo deduzido\n    return x * 2;\n};\nRegra de Dedução para auto:\n\\[\\frac{\\Gamma \\vdash e : \\tau}{\\Gamma \\vdash (\\text{auto } x = e) : \\text{ok}, \\quad \\Gamma' = \\Gamma \\cup \\{x : \\tau\\}}\\]\nO tipo de x é exatamente o tipo de e, sem coerções automáticas.\n\n\n12.5.3 Concepts (C++20): Restrições Explícitas\nC++20 introduziu concepts para adicionar restrições explícitas a templates:\ntemplate&lt;typename T&gt;\nconcept Numeric = std::is_arithmetic_v&lt;T&gt;;\n\ntemplate&lt;Numeric T&gt;\nT quadrado(T x) {\n    return x * x;\n}\n\nint a = quadrado(5);      // OK\ndouble b = quadrado(3.14); // OK\n// std::string c = quadrado(\"oi\"); // ERRO: string não satisfaz Numeric\nRegra de Concept:\n\\[\\frac{\n  \\Gamma \\vdash T \\models C \\quad \\Gamma, x : T \\vdash e : \\tau\n}{\n  \\Gamma \\vdash (\\text{concept } C\\, T \\Rightarrow e) : \\tau\n}\\]\nOnde \\(T \\models C\\) significa “T satisfaz as restrições do concept C”.\n\n\n12.5.4 Exemplo Completo de Julgamento em C++\nPara o código:\ntemplate&lt;typename T&gt;\nauto soma_dobro(T a, T b) -&gt; decltype(a + b) {\n    return (a + b) * 2;\n}\n\nauto resultado = soma_dobro(3, 5);\nA derivação envolve múltiplas etapas:\n\nDedução do parâmetro de template: \\[\\frac{\\Gamma \\vdash 3 : \\text{int} \\quad \\Gamma \\vdash 5 : \\text{int}}{T = \\text{int}}\\]\nDedução do tipo de retorno (usando decltype): \\[\\frac{\\Gamma \\vdash (a + b) : \\text{int}}{\\text{decltype}(a + b) = \\text{int}}\\]\nVerificação do corpo: \\[\\frac{\n  \\Gamma, a : \\text{int}, b : \\text{int} \\vdash (a + b) * 2 : \\text{int}\n}{\n  \\Gamma \\vdash \\text{soma\\_dobro}&lt;\\text{int}&gt; : (\\text{int}, \\text{int}) \\rightarrow \\text{int}\n}\\]\nDedução de auto: \\[\\frac{\\Gamma \\vdash \\text{soma\\_dobro}(3, 5) : \\text{int}}{\\Gamma \\vdash \\text{auto resultado} : \\text{int}}\\]",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Julgamento de Tipos em Linguagens Imperativas</span>"
    ]
  },
  {
    "objectID": "10b-semantico.html#rust-hindley-milner-encontra-sistemas",
    "href": "10b-semantico.html#rust-hindley-milner-encontra-sistemas",
    "title": "12  Julgamento de Tipos em Linguagens Imperativas",
    "section": "12.6 Rust: Hindley-Milner Encontra Sistemas",
    "text": "12.6 Rust: Hindley-Milner Encontra Sistemas\nRust é a linguagem imperativa moderna que mais se aproxima da elegância dos sistemas de tipos funcionais, mas com extensões cruciais para programação de sistemas.\n\n12.6.1 Sistema Base: Inferência Hindley-Milner\nO sistema de tipos de Rust é baseado em Hindley-Milner, permitindo inferência quase completa:\nfn soma(a: i32, b: i32) -&gt; i32 {\n    a + b\n}\n\nlet x = 5;           // Tipo inferido: i32\nlet y = soma(x, 10); // Tipo inferido: i32\nContudo, Rust adiciona três extensões fundamentais:\n\n\n12.6.2 1. Trait-Based Constraints\nTraits em Rust são similares a type classes em Haskell, mas com semântica de sistemas:\ntrait Adicionavel {\n    fn adicionar(&self, outro: &Self) -&gt; Self;\n}\n\nfn soma_generica&lt;T: Adicionavel&gt;(a: T, b: T) -&gt; T {\n    a.adicionar(&b)\n}\nRegra de Trait Bound:\n\\[\\frac{\n  \\Gamma \\vdash T : \\text{trait } C \\quad\n  \\Gamma, x : T \\vdash e : \\tau\n}{\n  \\Gamma \\vdash \\text{fn}(x : T \\text{ where } T : C) \\rightarrow \\tau : (T \\rightarrow \\tau)\n}\\]\n\n\n12.6.3 2. Ownership e Lifetimes\nA característica mais distintiva de Rust é seu sistema de ownership e lifetimes, que são parte integrante do sistema de tipos:\nfn primeiro&lt;'a&gt;(s: &'a str) -&gt; &'a str {\n    s.split_whitespace().next().unwrap_or(\"\")\n}\nRegra de Lifetime:\n\\[\\frac{\n  \\Gamma; \\Delta \\vdash e : \\tau \\quad\n  \\Delta \\vdash \\tau : 'a \\quad\n  'a \\text{ outlives todas as refs em } \\tau\n}{\n  \\Gamma; \\Delta \\vdash e : \\tau_{/'a}\n}\\]\nOnde \\(\\Delta\\) é o contexto de lifetimes e \\(\\tau_{/'a}\\) é o tipo \\(\\tau\\) anotado com lifetime \\('a\\).\nExemplo de julgamento com lifetimes:\nfn maior&lt;'a&gt;(s1: &'a str, s2: &'a str) -&gt; &'a str {\n    if s1.len() &gt; s2.len() { s1 } else { s2 }\n}\n\nlet resultado;\n{\n    let texto1 = String::from(\"curto\");\n    let texto2 = String::from(\"mais longo\");\n    resultado = maior(&texto1, &texto2); // ERRO: texto1 não vive o suficiente\n}\nA derivação falharia:\n\\[\\frac{\n  \\Gamma; \\{\\text{texto1} : 'b\\} \\vdash \\&\\text{texto1} : \\&'b \\text{str}\n}{\n  \\text{ERRO: } 'b \\not\\sqsupseteq 'a \\text{ (lifetime esperado)}\n}\\]\n\n\n12.6.4 3. Inferência Bidirecional\nRust usa inferência bidirecional para combinar anotações explícitas com dedução:\nlet v: Vec&lt;_&gt; = vec![1, 2, 3]; // Vec&lt;i32&gt; inferido\nlet soma = v.iter().sum();      // Tipo de retorno inferido do contexto\nRegra de Inferência Bidirecional:\n\nModo de síntese (\\(\\Gamma \\vdash e \\Rightarrow \\tau\\)): deduz o tipo de e\nModo de checagem (\\(\\Gamma \\vdash e \\Leftarrow \\tau\\)): verifica se e tem tipo \\(\\tau\\)\n\n\\[\\frac{\n  \\Gamma \\vdash e \\Rightarrow \\tau_1 \\quad\n  \\tau_1 = \\tau_2\n}{\n  \\Gamma \\vdash e \\Leftarrow \\tau_2\n}\\]\n\n\n12.6.5 Sistema Completo de Rust\nO sistema de tipos de Rust pode ser formalizado como uma tupla:\n\\[\\text{RustTypes} = (\\text{HM}, \\text{Traits}, \\text{Ownership}, \\text{Lifetimes})\\]\nExemplo de derivação completa:\nfn processar&lt;'a, T: Clone&gt;(dados: &'a [T]) -&gt; Vec&lt;T&gt; {\n    dados.iter().cloned().collect()\n}\nA derivação verifica:\n\nTrait bound: T : Clone\nLifetime: 'a para a referência de slice\nInferência: tipo de retorno é Vec&lt;T&gt;\nOwnership: cloned() cria cópias, satisfazendo ownership\n\n\\[\\frac{\n  \\begin{gathered}\n  T : \\text{Clone} \\\\\n  \\Gamma; \\{'a\\} \\vdash \\text{dados} : \\&'a [T] \\\\\n  \\Gamma \\vdash \\text{iter}() : \\text{Iter}&lt;\\&T&gt; \\\\\n  \\Gamma \\vdash \\text{cloned}() : \\text{Iter}&lt;T&gt; \\\\\n  \\Gamma \\vdash \\text{collect}() : \\text{Vec}&lt;T&gt;\n  \\end{gathered}\n}{\n  \\Gamma \\vdash \\text{processar} : \\forall 'a, T : \\text{Clone}. \\, \\&'a[T] \\rightarrow \\text{Vec}&lt;T&gt;\n}\\]",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Julgamento de Tipos em Linguagens Imperativas</span>"
    ]
  },
  {
    "objectID": "10b-semantico.html#casos-especiais-typescript-go-e-swift",
    "href": "10b-semantico.html#casos-especiais-typescript-go-e-swift",
    "title": "12  Julgamento de Tipos em Linguagens Imperativas",
    "section": "12.7 Casos Especiais: TypeScript, Go e Swift",
    "text": "12.7 Casos Especiais: TypeScript, Go e Swift\n\n12.7.1 TypeScript: Tipagem Estrutural Gradual\nTypeScript combina tipagem estrutural com inferência flow-sensitive:\nfunction processar(valor: string | number): number {\n    if (typeof valor === \"string\") {\n        // Aqui, TypeScript sabe que valor: string\n        return valor.length;\n    } else {\n        // Aqui, TypeScript sabe que valor: number\n        return valor * 2;\n    }\n}\nRegra de Type Narrowing:\n\\[\\frac{\n  \\Gamma \\vdash e : T_1 | T_2 \\quad\n  \\Gamma, e : T_1 \\vdash b_1 : \\tau \\quad\n  \\Gamma, e : T_2 \\vdash b_2 : \\tau\n}{\n  \\Gamma \\vdash (\\text{if typeof}(e) \\text{ then } b_1 \\text{ else } b_2) : \\tau\n}\\]\n\n\n12.7.2 Go: Simplicidade Nominal\nGo usa um sistema simples e nominal, mas com interfaces estruturais:\ntype Desenhavel interface {\n    Desenhar() string\n}\n\ntype Circulo struct{}\n\nfunc (c Circulo) Desenhar() string {\n    return \"○\"\n}\n\n// Circulo implementa Desenhavel implicitamente\nRegra de Interface Implícita:\n\\[\\frac{\n  \\forall m \\in \\text{methods}(I), \\quad T \\text{ implementa } m\n}{\n  T &lt;: I\n}\\]\n\n\n12.7.3 Swift: Sistema Híbrido\nSwift combina inferência local forte com type classes (protocols) e tipos opcionais:\nfunc processar&lt;T: Numeric&gt;(valor: T?) -&gt; T {\n    return valor ?? 0\n}",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Julgamento de Tipos em Linguagens Imperativas</span>"
    ]
  },
  {
    "objectID": "10b-semantico.html#comparação-complexidade-dos-sistemas",
    "href": "10b-semantico.html#comparação-complexidade-dos-sistemas",
    "title": "12  Julgamento de Tipos em Linguagens Imperativas",
    "section": "12.8 Comparação: Complexidade dos Sistemas",
    "text": "12.8 Comparação: Complexidade dos Sistemas\nA tabela a seguir resume as características dos sistemas de tipos discutidos:\n\n\n\n\n\n\n\n\n\n\nLinguagem\nVerificação\nEquivalência\nInferência\nComplexidade\n\n\n\n\nPython\nGradual\nEstrutural\nNenhuma (puro) / Flow-sensitive (hints)\nO(n) (hints)\n\n\nC\nEstática fraca\nNominal\nNenhuma\nO(n)\n\n\nC++\nEstática fraca\nNominal\nLocal (auto) / Instanciação (templates)\nO(n) (base) / Exponencial (templates)\n\n\nRust\nEstática forte\nNominal\nGlobal (quase HM)\nO(n³) (pior caso)\n\n\nTypeScript\nGradual\nEstrutural\nFlow-sensitive\nO(n²)\n\n\nGo\nEstática forte\nNominal (estrutural p/ interfaces)\nNenhuma\nO(n)\n\n\nJava\nEstática forte\nNominal\nLocal (var)\nO(n)",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Julgamento de Tipos em Linguagens Imperativas</span>"
    ]
  },
  {
    "objectID": "10b-semantico.html#trade-offs-fundamentais",
    "href": "10b-semantico.html#trade-offs-fundamentais",
    "title": "12  Julgamento de Tipos em Linguagens Imperativas",
    "section": "12.9 Trade-offs Fundamentais",
    "text": "12.9 Trade-offs Fundamentais\nA evolução dos sistemas de tipos em linguagens imperativas revela três trade-offs fundamentais:\n\n12.9.1 1. Expressividade vs. Decidibilidade\n\nC: Sistema simples e sempre decidível, mas expressividade limitada\nC++: Templates Turing-completos, decidibilidade sacrificada\nRust: Máxima expressividade mantendo decidibilidade (com restrições)\n\n\n\n12.9.2 2. Segurança vs. Performance\n\nC: Segurança mínima, performance máxima\nJava/C#: Segurança com verificações runtime quando necessário\nRust: Segurança sem custo em runtime (zero-cost abstractions)\n\n\n\n12.9.3 3. Inferência vs. Clareza\n\nML/Haskell: Inferência máxima, pode ser confuso\nC/Go: Tipos explícitos, sempre claro mas verboso\nRust/C++: Inferência local, balanceando ambos",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Julgamento de Tipos em Linguagens Imperativas</span>"
    ]
  },
  {
    "objectID": "10b-semantico.html#conclusão-o-pragmatismo-vence-a-elegância",
    "href": "10b-semantico.html#conclusão-o-pragmatismo-vence-a-elegância",
    "title": "12  Julgamento de Tipos em Linguagens Imperativas",
    "section": "12.10 Conclusão: O Pragmatismo Vence a Elegância",
    "text": "12.10 Conclusão: O Pragmatismo Vence a Elegância\nA comparação entre o Algoritmo W e os sistemas de tipos imperativos revela uma verdade fundamental sobre design de linguagens: não existe solução universal. O Algoritmo W é uma maravilha matemática para linguagens funcionais puras, mas as necessidades do mundo imperativo exigem abordagens diferentes.\nAs linguagens imperativas modernas convergem para um modelo híbrido:\n\nVerificação estática rigorosa onde possível\nInferência local previsível para conveniência\nEscape hatches controlados para flexibilidade\nSistemas de restrições (traits, concepts, protocols) para polimorfismo\n\nO futuro provavelmente verá mais convergência: linguagens funcionais adotando características imperativas (efeitos, mutabilidade controlada) e linguagens imperativas adotando ideias funcionais (inferência, imutabilidade por padrão, pattern matching). O sistema de tipos de Rust pode representar um vislumbre desse futuro: a ponte entre a elegância teórica e as necessidades práticas da programação de sistemas.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Julgamento de Tipos em Linguagens Imperativas</span>"
    ]
  },
  {
    "objectID": "10c-semantico.html",
    "href": "10c-semantico.html",
    "title": "13  O Sistema de Tipos Hindley-Milner",
    "section": "",
    "text": "13.1 Introdução: A Joia da Teoria dos Tipos\nO sistema de tipos Hindley-Milner (HM), também conhecido como Damas-Milner ou Damas-Hindley-Milner, representa uma das conquistas mais elegantes da ciência da computação teórica cuja história é marcada por uma descoberta independente e pelo reconhecimento tardio de seu verdadeiro potencial. Em 1969, o lógico britânico Roger Hindley publicou um artigo sobre combinadores, no qual descreveu um sistema de tipos polimórfico e um algoritmo de inferência para o cálculo lambda simplesmente tipado com polimorfismo. Contudo, seu trabalho permaneceu relativamente obscuro na comunidade de ciência da computação, circulando principalmente entre lógicos matemáticos. Hindley estava interessado em questões puramente teóricas sobre combinadores e tipos, sem uma aplicação prática imediata em mente. Seu algoritmo, embora correto e completo, não foi implementado nem testado em linguagens de programação reais na época.\nA história ganha impulso em 1978, quando Robin Milner, trabalhando no desenvolvimento da linguagem ML (Meta Language) na Universidade de Edinburgh, redescobriu independentemente o mesmo sistema de tipos e desenvolveu o que ficaria conhecido como Algoritmo W. Milner estava enfrentando um problema prático: como criar uma linguagem de programação funcional que combinasse a segurança de tipos com a conveniência de não exigir anotações de tipo explícitas em cada função. Sua motivação era permitir que programadores escrevessem código conciso sem sacrificar as garantias de correção. O trabalho de Milner foi revolucionário porque demonstrou que inferência completa de tipos era não apenas teoricamente possível, mas praticamente viável, ML foi a primeira linguagem de programação a implementar inferência automática de tipos polimórficos com sucesso. A implementação provou que o algoritmo terminava em tempo razoável para programas reais, dissipando ceticismos sobre sua aplicabilidade.\nO reconhecimento da contribuição dupla veio mais tarde, quando Luis Damas, orientando de doutorado de Milner, formalizou rigorosamente o sistema em sua tese de 1985, provando suas propriedades fundamentais de soundness, completude e existência de tipos principais. Damas descobriu o trabalho anterior de Hindley e estabeleceu a conexão formal entre ambas as abordagens, levando à denominação Hindley-Milner (ou Damas-Milner, ou ainda Damas-Hindley-Milner). A tese de Damas forneceu as provas matemáticas rigorosas que transformaram o sistema de uma técnica engenhosa em uma teoria sólida. Desde então, HM tornou-se a base para os sistemas de tipos de Haskell, OCaml, F#, Elm, e influenciou profundamente Rust, Swift, Scala, e até extensões de TypeScript. A história de HM ilustra um padrão comum na ciência: descobertas teóricas aguardando aplicações práticas, e como a implementação bem-sucedida pode transformar uma ideia matemática obscura em fundação de toda uma família de linguagens de programação.\nO ponto importante que a curiosa leitora deve ter em mente é que este sistema resolve um problema aparentemente impossível: como inferir tipos automaticamente em uma linguagem com polimorfismo paramétrico mantendo decidibilidade e completude.\nA importância do sistema HM transcende sua elegância matemática. Ele fundamenta os sistemas de tipos de linguagens como ML, Haskell, OCaml, F#, e influencia fortemente Scala, Rust, Swift e TypeScript. Sua combinação única de propriedades o torna um marco na teoria das linguagens de programação:\nA atenta leitora deve distinguir claramente: Hindley-Milner é um sistema de tipos, enquanto o Algoritmo W é uma das formas de implementá-lo. Esta distinção, frequentemente negligenciada, é fundamental para compreender tanto a teoria quanto as implementações práticas.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>O Sistema de Tipos Hindley-Milner</span>"
    ]
  },
  {
    "objectID": "10c-semantico.html#introdução-a-joia-da-teoria-dos-tipos",
    "href": "10c-semantico.html#introdução-a-joia-da-teoria-dos-tipos",
    "title": "13  O Sistema de Tipos Hindley-Milner",
    "section": "",
    "text": "Note\n\n\n\nPolimorfismo Paramétrico e Suas Alternativas\nO polimorfismo paramétrico (também chamado de polimorfismo universal) é o tipo de polimorfismo implementado por Hindley-Milner. Ele permite que uma única função opere sobre qualquer tipo de forma uniforme, sem conhecimento da estrutura interna dos valores. A função identidade id : ∀α. α → α é o exemplo clássico: ela funciona para inteiros, strings, listas - qualquer tipo - porque não inspeciona nem modifica seus argumentos.\nCaracterísticas fundamentais:\n\nUniformidade: o mesmo código é executado independente do tipo concreto;\nType erasure: em muitas implementações, tipos podem ser apagados após verificação;\nPredicabilidade: o comportamento é determinado pela estrutura da função, não pelos tipos;\n\nAlternativas ao Polimorfismo Paramétrico:\n1. Polimorfismo ad-hoc (Sobrecarga): múltiplas implementações com o mesmo nome, selecionadas pelo tipo dos argumentos:\nint soma(int a, int b) { return a + b; }\nfloat soma(float a, float b) { return a + b; }\nstring soma(string a, string b) { return a + b; }\nO operador + em muitas linguagens é polimórfico ad-hoc: comporta-se diferentemente para números (adição) e strings (concatenação). Cada tipo tem sua própria implementação.\nType classes em Haskell são polimorfismo ad-hoc estruturado:\nclass Eq a where\n  (==) :: a -&gt; a -&gt; Bool\n\n-- Implementações diferentes para cada tipo\ninstance Eq Int where ...\ninstance Eq String where ...\n2. Polimorfismo de Subtipo (Herança): baseado em hierarquias de tipos, comum em linguagens orientadas a objetos:\nclass Animal {\n    void emitirSom() { }\n}\n\nclass Cachorro extends Animal {\n    void emitirSom() { System.out.println(\"Au au\"); }\n}\n\nvoid fazer_barulho(Animal a) {\n    a.emitirSom();  // Funciona para qualquer subtipo de Animal\n}\nUma função que aceita Animal também aceita Cachorro, Gato, etc. O comportamento varia por subtipo (dynamic dispatch).\nComparação:\n\n\n\n\n\n\n\n\n\nTipo\nQuando selecionar\nUniformidade\nExemplo\n\n\n\n\nParamétrico\nCompilação (tipo conhecido)\nSim - mesmo código\nreverse :: [a] -&gt; [a]\n\n\nad-hoc\nCompilação (sobrecarga) ou Runtime (type classes)\nNão - código específico\n(+) em Java/C++\n\n\nSubtipo\nRuntime (dynamic dispatch)\nNão - métodos virtuais\nHierarquias OOP\n\n\n\n\n\n\n\nInferência completa: o sistema pode deduzir o tipo mais geral (principal) de qualquer expressão sem anotações;\nDecidibilidade: existe um algoritmo que sempre termina e determina se um programa é bem-tipado;\nPolimorfismo paramétrico: funções podem operar sobre tipos arbitrários mantendo segurança de tipos;\nsoundness: programas bem-tipados não produzem erros de tipo em runtime;\nCompletude: se um programa tem um tipo, o algoritmo o encontrará.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>O Sistema de Tipos Hindley-Milner</span>"
    ]
  },
  {
    "objectID": "10c-semantico.html#fundamentos-matemáticos-a-estrutura-do-sistema",
    "href": "10c-semantico.html#fundamentos-matemáticos-a-estrutura-do-sistema",
    "title": "13  O Sistema de Tipos Hindley-Milner",
    "section": "13.2 Fundamentos Matemáticos: A Estrutura do Sistema",
    "text": "13.2 Fundamentos Matemáticos: A Estrutura do Sistema\n\n13.2.1 Sintaxe dos Tipos\nO sistema HM define tipos através de uma gramática formal. Seja \\(\\tau\\) uma metavariável representando tipos:\n\\[\\tau ::= \\alpha \\mid T \\mid \\tau_1 \\rightarrow \\tau_2\\]\nOnde: - \\(\\alpha\\) é uma variável de tipo (type variable), representando tipos desconhecidos ou polimórficos - \\(T\\) é um tipo base (como int, bool, string) - \\(\\tau_1 \\rightarrow \\tau_2\\) é um tipo de função que recebe \\(\\tau_1\\) e retorna \\(\\tau_2\\)\nTipos polimórficos (também chamados de esquemas de tipo ou type schemes) são introduzidos através de quantificação universal:\n\\[\\sigma ::= \\tau \\mid \\forall \\alpha. \\sigma\\]\nExemplo: o tipo da função identidade:\n\\[\\text{id} : \\forall \\alpha. \\alpha \\rightarrow \\alpha\\]\nQue lê-se: “para qualquer tipo α, id é uma função que recebe α e retorna α”.\n\n\n13.2.2 Contexto de Tipagem\nO contexto de tipagem (typing context) \\(\\Gamma\\) é um mapeamento de variáveis para esquemas de tipo:\n\\[\\Gamma = \\{x_1 : \\sigma_1, x_2 : \\sigma_2, \\ldots, x_n : \\sigma_n\\}\\]\nOperações sobre contextos:\n\nExtensão: \\(\\Gamma, x : \\sigma\\) adiciona uma nova ligação\nConsulta: \\(x : \\sigma \\in \\Gamma\\) verifica se x está no contexto\nRemoção: \\(\\Gamma \\setminus \\{x\\}\\) remove x do contexto\n\n\n\n13.2.3 Variáveis de Tipo Livres\nO conjunto de variáveis de tipo livres em um tipo \\(\\tau\\), denotado \\(\\text{ftv}(\\tau)\\), é definido recursivamente:\n\\[\\begin{align*}\n\\text{ftv}(\\alpha) &= \\{\\alpha\\} \\\\\n\\text{ftv}(T) &= \\emptyset \\\\\n\\text{ftv}(\\tau_1 \\rightarrow \\tau_2) &= \\text{ftv}(\\tau_1) \\cup \\text{ftv}(\\tau_2) \\\\\n\\text{ftv}(\\forall \\alpha. \\sigma) &= \\text{ftv}(\\sigma) \\setminus \\{\\alpha\\}\n\\end{align*}\\]\nPara contextos:\n\\[\\text{ftv}(\\Gamma) = \\bigcup_{x : \\sigma \\in \\Gamma} \\text{ftv}(\\sigma)\\]\nEste conceito é crucial para a operação de generalização.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>O Sistema de Tipos Hindley-Milner</span>"
    ]
  },
  {
    "objectID": "10c-semantico.html#regras-de-julgamento-de-tipo",
    "href": "10c-semantico.html#regras-de-julgamento-de-tipo",
    "title": "13  O Sistema de Tipos Hindley-Milner",
    "section": "13.3 Regras de Julgamento de Tipo",
    "text": "13.3 Regras de Julgamento de Tipo\nO sistema HM é definido por um conjunto de regras de inferência que formam um sistema dedutivo. Um julgamento de tipo tem a forma:\n\\[\\Gamma \\vdash e : \\sigma\\]\nLê-se: “no contexto \\(\\Gamma\\), a expressão \\(e\\) tem o tipo (ou esquema de tipo) \\(\\sigma\\)”.\n\n13.3.1 Regras Fundamentais\nRegra [Var] - Variável:\n\\[\\frac{x : \\sigma \\in \\Gamma}{\\Gamma \\vdash x : \\sigma}\\]\nSe x está no contexto com tipo \\(\\sigma\\), então x tem tipo \\(\\sigma\\).\nRegra [Abs] - Abstração (função):\n\\[\\frac{\\Gamma, x : \\tau_1 \\vdash e : \\tau_2}{\\Gamma \\vdash \\lambda x. e : \\tau_1 \\rightarrow \\tau_2}\\]\nPara descobrir o tipo de uma função, assumimos um tipo para o parâmetro e derivamos o tipo do corpo.\nRegra [App] - Aplicação:\n\\[\\frac{\\Gamma \\vdash e_1 : \\tau_1 \\rightarrow \\tau_2 \\quad \\Gamma \\vdash e_2 : \\tau_1}{\\Gamma \\vdash e_1 \\, e_2 : \\tau_2}\\]\nSe \\(e_1\\) é uma função de \\(\\tau_1\\) para \\(\\tau_2\\) e \\(e_2\\) tem tipo \\(\\tau_1\\), então a aplicação tem tipo \\(\\tau_2\\).\nRegra [Let] - Let Polimórfico:\n\\[\\frac{\\Gamma \\vdash e_1 : \\sigma \\quad \\Gamma, x : \\sigma \\vdash e_2 : \\tau}{\\Gamma \\vdash \\text{let } x = e_1 \\text{ in } e_2 : \\tau}\\]\nEsta é a regra crucial que introduz polimorfismo. O tipo de \\(e_1\\) pode ser generalizado antes de ser usado em \\(e_2\\).\nRegra [Inst] - Instanciação:\n\\[\\frac{\\Gamma \\vdash e : \\forall \\alpha. \\sigma}{\\Gamma \\vdash e : \\sigma[\\alpha := \\tau]}\\]\nUm tipo polimórfico pode ser instanciado substituindo variáveis quantificadas por tipos concretos.\nRegra [Gen] - Generalização:\n\\[\\frac{\\Gamma \\vdash e : \\tau \\quad \\alpha \\notin \\text{ftv}(\\Gamma)}{\\Gamma \\vdash e : \\forall \\alpha. \\tau}\\]\nUm tipo pode ser generalizado quantificando sobre variáveis que não aparecem livres no contexto.\n\n\n13.3.2 Exemplo de Derivação: Função Identidade\nVamos derivar o tipo da função identidade λx. x:\n\\[\\frac{\n  \\frac{\n    x : \\alpha \\in \\{x : \\alpha\\}\n  }{\n    \\{x : \\alpha\\} \\vdash x : \\alpha\n  } \\text{[Var]}\n}{\n  \\emptyset \\vdash \\lambda x. x : \\alpha \\rightarrow \\alpha\n} \\text{[Abs]}\\]\nComo \\(\\alpha \\notin \\text{ftv}(\\emptyset)\\), podemos generalizar:\n\\[\\frac{\n  \\emptyset \\vdash \\lambda x. x : \\alpha \\rightarrow \\alpha \\quad \\alpha \\notin \\text{ftv}(\\emptyset)\n}{\n  \\emptyset \\vdash \\lambda x. x : \\forall \\alpha. \\alpha \\rightarrow \\alpha\n} \\text{[Gen]}\\]\n\n\n13.3.3 Exemplo de Derivação: Aplicação Polimórfica\nPara let id = λx. x in id 42, precisamos usar [Let]:\n\\[\\frac{\n  \\begin{gathered}\n  \\emptyset \\vdash \\lambda x. x : \\forall \\alpha. \\alpha \\rightarrow \\alpha \\\\\n  \\{id : \\forall \\alpha. \\alpha \\rightarrow \\alpha\\} \\vdash id \\, 42 : \\text{int}\n  \\end{gathered}\n}{\n  \\emptyset \\vdash \\text{let } id = \\lambda x. x \\text{ in } id \\, 42 : \\text{int}\n} \\text{[Let]}\\]\nA segunda premissa se expande:\n\\[\\frac{\n  \\begin{gathered}\n  \\frac{\n    id : \\forall \\alpha. \\alpha \\rightarrow \\alpha \\in \\Gamma\n  }{\n    \\Gamma \\vdash id : \\text{int} \\rightarrow \\text{int}\n  } \\text{[Inst]} \\quad\n  \\frac{}{\n    \\Gamma \\vdash 42 : \\text{int}\n  } \\text{[Const]}\n  \\end{gathered}\n}{\n  \\Gamma \\vdash id \\, 42 : \\text{int}\n} \\text{[App]}\\]\nOnde \\(\\Gamma = \\{id : \\forall \\alpha. \\alpha \\rightarrow \\alpha\\}\\) e instanciamos \\(\\alpha\\) como int.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>O Sistema de Tipos Hindley-Milner</span>"
    ]
  },
  {
    "objectID": "10c-semantico.html#unificação-o-coração-do-sistema",
    "href": "10c-semantico.html#unificação-o-coração-do-sistema",
    "title": "13  O Sistema de Tipos Hindley-Milner",
    "section": "13.4 Unificação: O Coração do Sistema",
    "text": "13.4 Unificação: O Coração do Sistema\nA unificação é o algoritmo fundamental que permite resolver equações de tipos. Dado dois tipos \\(\\tau_1\\) e \\(\\tau_2\\), o algoritmo de unificação tenta encontrar uma substituição \\(S\\) tal que:\n\\[S(\\tau_1) = S(\\tau_2)\\]\n\n13.4.1 Substituições\nUma substituição é um mapeamento finito de variáveis de tipo para tipos:\n\\[S = [\\alpha_1 := \\tau_1, \\alpha_2 := \\tau_2, \\ldots, \\alpha_n := \\tau_n]\\]\nAplicação de substituição:\n\\[\\begin{align*}\nS(\\alpha) &=\n\\begin{cases}\n\\tau & \\text{se } \\alpha := \\tau \\in S \\\\\n\\alpha & \\text{caso contrário}\n\\end{cases} \\\\\nS(T) &= T \\\\\nS(\\tau_1 \\rightarrow \\tau_2) &= S(\\tau_1) \\rightarrow S(\\tau_2) \\\\\nS(\\forall \\alpha. \\sigma) &= \\forall \\alpha. S(\\sigma) \\quad (\\alpha \\notin \\text{dom}(S))\n\\end{align*}\\]\nComposição de substituições:\n\\[S_2 \\circ S_1 = S_1 \\cup \\{(\\alpha := S_2(\\tau)) \\mid \\alpha := \\tau \\in S_2, \\alpha \\notin \\text{dom}(S_1)\\}\\]\n\n\n13.4.2 Algoritmo de Unificação\nO algoritmo de unificação \\(\\mathcal{U}(\\tau_1, \\tau_2)\\) retorna a substituição mais geral (mgu - most general unifier):\nfunção unify(τ₁, τ₂):\n  caso (τ₁, τ₂) de:\n    // Regra: tipos idênticos\n    (T, T) → ∅\n    \n    // Regra: variável de tipo\n    (α, τ) → \n      se α ∈ ftv(τ) e α ≠ τ então\n        erro \"Occur check: ciclo infinito\"\n      senão\n        [α := τ]\n    \n    (τ, α) → unify(α, τ)\n    \n    // Regra: funções\n    (τ₁ → τ₂, τ₃ → τ₄) →\n      S₁ ← unify(τ₁, τ₃)\n      S₂ ← unify(S₁(τ₂), S₁(τ₄))\n      retornar S₂ ∘ S₁\n    \n    // Incompatível\n    caso contrário → erro \"Tipos incompatíveis\"\nExemplo de unificação:\nUnificar \\((\\alpha \\rightarrow \\beta)\\) com \\((\\text{int} \\rightarrow \\gamma)\\):\n\nUnificar \\(\\alpha\\) com int: \\(S_1 = [\\alpha := \\text{int}]\\)\nAplicar \\(S_1\\): \\(S_1(\\beta)\\) = \\(\\beta\\), \\(S_1(\\gamma)\\) = \\(\\gamma\\)\nUnificar \\(\\beta\\) com \\(\\gamma\\): \\(S_2 = [\\beta := \\gamma]\\)\nCompor: \\(S_2 \\circ S_1 = [\\alpha := \\text{int}, \\beta := \\gamma]\\)\n\nOccur Check: a verificação crucial que previne tipos infinitos:\nunify(α, α → β)  // ERRO: α aparece em α → β\nSem o occur check, teríamos \\(\\alpha = \\alpha \\rightarrow \\beta\\), um tipo infinito.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>O Sistema de Tipos Hindley-Milner</span>"
    ]
  },
  {
    "objectID": "10c-semantico.html#o-algoritmo-w-inferência-sintaxe-dirigida",
    "href": "10c-semantico.html#o-algoritmo-w-inferência-sintaxe-dirigida",
    "title": "13  O Sistema de Tipos Hindley-Milner",
    "section": "13.5 O Algoritmo W: Inferência Sintaxe-Dirigida",
    "text": "13.5 O Algoritmo W: Inferência Sintaxe-Dirigida\nO Algoritmo W (de Robin Milner, 1978) é o algoritmo clássico de inferência de tipos para HM. Ele é sintaxe-dirigido: processa a estrutura da expressão recursivamente.\n\n13.5.1 Estrutura do Algoritmo\nO algoritmo W tem a assinatura:\n\\[\\mathcal{W} : \\text{Context} \\times \\text{Expr} \\rightarrow \\text{Substitution} \\times \\text{Type}\\]\nRetorna uma substituição acumulada e o tipo inferido.\n\n\n13.5.2 Pseudocódigo Completo\nfunção W(Γ, e):\n  caso e de:\n    // Variável\n    x →\n      se x : σ ∈ Γ então\n        τ ← instanciar(σ)  // Remove quantificadores\n        retornar (∅, τ)\n      senão\n        erro \"Variável não ligada\"\n    \n    // Constante\n    c →\n      retornar (∅, tipo_de(c))\n    \n    // Abstração λx.e₁\n    λx.e₁ →\n      α ← nova_variavel_tipo()\n      (S₁, τ₁) ← W(Γ ∪ {x : α}, e₁)\n      retornar (S₁, S₁(α) → τ₁)\n    \n    // Aplicação e₁ e₂\n    e₁ e₂ →\n      (S₁, τ₁) ← W(Γ, e₁)\n      (S₂, τ₂) ← W(S₁(Γ), e₂)\n      α ← nova_variavel_tipo()\n      S₃ ← unify(S₂(τ₁), τ₂ → α)\n      retornar (S₃ ∘ S₂ ∘ S₁, S₃(α))\n    \n    // Let polimórfico\n    let x = e₁ in e₂ →\n      (S₁, τ₁) ← W(Γ, e₁)\n      σ ← generalizar(S₁(Γ), τ₁)\n      (S₂, τ₂) ← W(S₁(Γ) ∪ {x : σ}, e₂)\n      retornar (S₂ ∘ S₁, τ₂)\n\nfunção generalizar(Γ, τ):\n  // Quantifica sobre variáveis livres em τ que não estão em Γ\n  vars ← ftv(τ) \\ ftv(Γ)\n  retornar ∀vars. τ\n\nfunção instanciar(∀α₁...αₙ. τ):\n  // Substitui cada αᵢ por uma nova variável fresca\n  βᵢ ← novas_variaveis_tipo(n)\n  retornar [α₁ := β₁, ..., αₙ := βₙ](τ)\n\n\n13.5.3 Traço de Execução: Exemplo Completo\nVamos executar W para let id = λx. x in id 5:\nPasso 1: Processar let id = λx. x in id 5\nPasso 2: Avaliar λx. x: - Criar variável fresca: \\(\\alpha_1\\) - Avaliar corpo x no contexto \\(\\{x : \\alpha_1\\}\\) - Resultado: \\((\\emptyset, \\alpha_1)\\) - Retornar: \\((\\emptyset, \\alpha_1 \\rightarrow \\alpha_1)\\)\nPasso 3: Generalizar: - \\(\\text{ftv}(\\alpha_1 \\rightarrow \\alpha_1) = \\{\\alpha_1\\}\\) - \\(\\text{ftv}(\\emptyset) = \\emptyset\\) - Generalizar: \\(\\sigma = \\forall \\alpha_1. \\alpha_1 \\rightarrow \\alpha_1\\)\nPasso 4: Avaliar id 5 no contexto \\(\\{id : \\forall \\alpha_1. \\alpha_1 \\rightarrow \\alpha_1\\}\\):\nPasso 4a: Avaliar id: - Instanciar: \\(\\forall \\alpha_1. \\alpha_1 \\rightarrow \\alpha_1\\) → \\(\\alpha_2 \\rightarrow \\alpha_2\\) (variável fresca) - Resultado: \\((\\emptyset, \\alpha_2 \\rightarrow \\alpha_2)\\)\nPasso 4b: Avaliar 5: - Resultado: \\((\\emptyset, \\text{int})\\)\nPasso 4c: Unificar para aplicação: - Criar variável fresca \\(\\alpha_3\\) - Unificar \\(\\alpha_2 \\rightarrow \\alpha_2\\) com \\(\\text{int} \\rightarrow \\alpha_3\\) - \\(S_1 = \\text{unify}(\\alpha_2, \\text{int}) = [\\alpha_2 := \\text{int}]\\) - \\(S_2 = \\text{unify}(S_1(\\alpha_2), S_1(\\alpha_3)) = \\text{unify}(\\text{int}, \\alpha_3) = [\\alpha_3 := \\text{int}]\\) - Composição: \\(S = [\\alpha_2 := \\text{int}, \\alpha_3 := \\text{int}]\\)\nResultado final: \\((S, \\text{int})\\)\n\n\n13.5.4 Propriedades do Algoritmo W\nTeorema (Correção): Se \\(\\mathcal{W}(\\Gamma, e) = (S, \\tau)\\), então \\(S(\\Gamma) \\vdash e : \\tau\\).\nTeorema (Completude): Se \\(\\Gamma \\vdash e : \\tau\\), então existe \\((S, \\tau')\\) tal que \\(\\mathcal{W}(\\Gamma, e) = (S, \\tau')\\) e \\(\\tau'\\) é mais geral que \\(\\tau\\).\nTeorema (Tipo Principal): Se \\(e\\) é tipável, então existe um tipo \\(\\tau\\) (o tipo principal) tal que qualquer outro tipo válido para \\(e\\) é uma instância de \\(\\tau\\).",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>O Sistema de Tipos Hindley-Milner</span>"
    ]
  },
  {
    "objectID": "10c-semantico.html#complexidade-computacional",
    "href": "10c-semantico.html#complexidade-computacional",
    "title": "13  O Sistema de Tipos Hindley-Milner",
    "section": "13.6 Complexidade Computacional",
    "text": "13.6 Complexidade Computacional\n\n13.6.1 Análise do Algoritmo W\nA complexidade do Algoritmo W é surpreendentemente alta:\nPior caso: \\(O(2^n)\\) exponencial, onde \\(n\\) é o tamanho da expressão.\nExemplo patológico:\nlet f1 = λx. (x, x) in\nlet f2 = λx. f1(f1(x)) in\nlet f3 = λx. f2(f2(x)) in\nlet f4 = λx. f3(f3(x)) in\nf4\nO tipo de f4 tem tamanho exponencial em relação ao código:\n\nf1 : α → (α × α)\nf2 : α → ((α × α) × (α × α))\nf3 : tipo com 16 componentes\nf4 : tipo com 256 componentes\n\n\n\n13.6.2 Prática vs. Teoria\nNa prática, o Algoritmo W se comporta quase linearmente porque:\n\nProgramas reais raramente exibem o pior caso\nOtimizações modernas (como compartilhamento de estrutura)\nHeurísticas de unificação eficientes\n\nComplexidade prática: \\(O(n \\log n)\\) a \\(O(n^2)\\)\nContudo, a leitora perspicaz deve lembrar: a complexidade teórica permanece exponencial, e casos patológicos existem.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>O Sistema de Tipos Hindley-Milner</span>"
    ]
  },
  {
    "objectID": "10c-semantico.html#algoritmo-m-uma-alternativa",
    "href": "10c-semantico.html#algoritmo-m-uma-alternativa",
    "title": "13  O Sistema de Tipos Hindley-Milner",
    "section": "13.7 Algoritmo M: Uma Alternativa",
    "text": "13.7 Algoritmo M: Uma Alternativa\nO Algoritmo M (de Clement, 1987) é uma reformulação do Algoritmo W que separa as fases:\n\nGeração de restrições: produz um conjunto de equações de tipo\nResolução: resolve as equações via unificação\n\n\n13.7.1 Pseudocódigo do Algoritmo M\nfunção M(Γ, e):\n  (τ, C) ← gerar_restrições(Γ, e)\n  S ← resolver(C)\n  retornar (S, S(τ))\n\nfunção gerar_restrições(Γ, e):\n  caso e de:\n    x →\n      se x : σ ∈ Γ então\n        τ ← instanciar(σ)\n        retornar (τ, ∅)\n      senão erro\n    \n    λx.e₁ →\n      α ← nova_variavel()\n      (τ₁, C₁) ← gerar_restrições(Γ ∪ {x : α}, e₁)\n      retornar (α → τ₁, C₁)\n    \n    e₁ e₂ →\n      (τ₁, C₁) ← gerar_restrições(Γ, e₁)\n      (τ₂, C₂) ← gerar_restrições(Γ, e₂)\n      α ← nova_variavel()\n      retornar (α, C₁ ∪ C₂ ∪ {τ₁ = τ₂ → α})\n    \n    let x = e₁ in e₂ →\n      (τ₁, C₁) ← gerar_restrições(Γ, e₁)\n      S₁ ← resolver(C₁)\n      σ ← generalizar(S₁(Γ), S₁(τ₁))\n      (τ₂, C₂) ← gerar_restrições(S₁(Γ) ∪ {x : σ}, e₂)\n      retornar (τ₂, C₁ ∪ C₂)\n\nfunção resolver(C):\n  S ← ∅\n  para cada (τ₁ = τ₂) ∈ C:\n    S' ← unify(S(τ₁), S(τ₂))\n    S ← S' ∘ S\n  retornar S\nVantagem: separação de concerns facilita otimizações e extensões.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>O Sistema de Tipos Hindley-Milner</span>"
    ]
  },
  {
    "objectID": "10c-semantico.html#algoritmo-j-baseado-em-constraint-solving",
    "href": "10c-semantico.html#algoritmo-j-baseado-em-constraint-solving",
    "title": "13  O Sistema de Tipos Hindley-Milner",
    "section": "13.8 Algoritmo J: Baseado em Constraint Solving",
    "text": "13.8 Algoritmo J: Baseado em Constraint Solving\nO Algoritmo J usa um framework moderno de resolução de restrições:\nfunção J(Γ, e):\n  (τ, C) ← collect(Γ, e, ∅)\n  S ← solve(C)\n  retornar S(τ)\n\nfunção collect(Γ, e, C):\n  caso e de:\n    x → (lookup(Γ, x), C)\n    \n    λx.e₁ →\n      α, β ← novas_variaveis()\n      (τ, C') ← collect(Γ ∪ {x : α}, e₁, C ∪ {β = α → τ})\n      retornar (β, C')\n    \n    e₁ e₂ →\n      α ← nova_variavel()\n      (τ₁, C₁) ← collect(Γ, e₁, C)\n      (τ₂, C₂) ← collect(Γ, e₂, C₁)\n      retornar (α, C₂ ∪ {τ₁ = τ₂ → α})\n\nfunção solve(C):\n  // Usa técnicas avançadas de constraint solving\n  retornar unify_all(C)",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>O Sistema de Tipos Hindley-Milner</span>"
    ]
  },
  {
    "objectID": "10c-semantico.html#implementação-prática-em-python",
    "href": "10c-semantico.html#implementação-prática-em-python",
    "title": "13  O Sistema de Tipos Hindley-Milner",
    "section": "13.9 Implementação Prática em Python",
    "text": "13.9 Implementação Prática em Python\nVamos implementar um interpretador completo com inferência HM:\nfrom dataclasses import dataclass\nfrom typing import Dict, Set, Optional, Tuple\n\n# Tipos\n@dataclass\nclass TypeVar:\n    name: str\n    \n@dataclass\nclass TypeInt:\n    pass\n\n@dataclass\nclass TypeBool:\n    pass\n\n@dataclass\nclass TypeFun:\n    arg: 'Type'\n    ret: 'Type'\n\n@dataclass  \nclass TypeScheme:\n    vars: Set[str]\n    type: 'Type'\n\nType = TypeVar | TypeInt | TypeBool | TypeFun\n\n# Substituição\nclass Substitution:\n    def __init__(self, mapping: Dict[str, Type] = None):\n        self.mapping = mapping or {}\n    \n    def apply_type(self, t: Type) -&gt; Type:\n        if isinstance(t, TypeVar):\n            return self.mapping.get(t.name, t)\n        elif isinstance(t, TypeFun):\n            return TypeFun(\n                self.apply_type(t.arg),\n                self.apply_type(t.ret)\n            )\n        else:\n            return t\n    \n    def compose(self, other: 'Substitution') -&gt; 'Substitution':\n        new_mapping = {\n            k: other.apply_type(v) \n            for k, v in self.mapping.items()\n        }\n        new_mapping.update(other.mapping)\n        return Substitution(new_mapping)\n\n# Funções auxiliares\ndef ftv_type(t: Type) -&gt; Set[str]:\n    if isinstance(t, TypeVar):\n        return {t.name}\n    elif isinstance(t, TypeFun):\n        return ftv_type(t.arg) | ftv_type(t.ret)\n    else:\n        return set()\n\ndef ftv_scheme(s: TypeScheme) -&gt; Set[str]:\n    return ftv_type(s.type) - s.vars\n\ndef ftv_env(env: Dict[str, TypeScheme]) -&gt; Set[str]:\n    return set().union(*[ftv_scheme(s) for s in env.values()])\n\n# Unificação\nclass UnificationError(Exception):\n    pass\n\ndef occurs_check(var: str, t: Type) -&gt; bool:\n    return var in ftv_type(t)\n\ndef unify(t1: Type, t2: Type) -&gt; Substitution:\n    if isinstance(t1, TypeInt) and isinstance(t2, TypeInt):\n        return Substitution()\n    elif isinstance(t1, TypeBool) and isinstance(t2, TypeBool):\n        return Substitution()\n    elif isinstance(t1, TypeVar):\n        if t1.name == getattr(t2, 'name', None):\n            return Substitution()\n        if occurs_check(t1.name, t2):\n            raise UnificationError(f\"Occur check: {t1.name} in {t2}\")\n        return Substitution({t1.name: t2})\n    elif isinstance(t2, TypeVar):\n        return unify(t2, t1)\n    elif isinstance(t1, TypeFun) and isinstance(t2, TypeFun):\n        s1 = unify(t1.arg, t2.arg)\n        s2 = unify(s1.apply_type(t1.ret), s1.apply_type(t2.ret))\n        return s2.compose(s1)\n    else:\n        raise UnificationError(f\"Cannot unify {t1} with {t2}\")\n\n# Generalização e Instanciação\n_type_var_counter = 0\n\ndef fresh_var() -&gt; TypeVar:\n    global _type_var_counter\n    name = f\"t{_type_var_counter}\"\n    _type_var_counter += 1\n    return TypeVar(name)\n\ndef generalize(env: Dict[str, TypeScheme], t: Type) -&gt; TypeScheme:\n    vars = ftv_type(t) - ftv_env(env)\n    return TypeScheme(vars, t)\n\ndef instantiate(scheme: TypeScheme) -&gt; Type:\n    subst = Substitution({\n        var: fresh_var() \n        for var in scheme.vars\n    })\n    return subst.apply_type(scheme.type)\n\n# Algoritmo W\ndef infer(env: Dict[str, TypeScheme], expr) -&gt; Tuple[Substitution, Type]:\n    if isinstance(expr, Var):\n        if expr.name not in env:\n            raise TypeError(f\"Unbound variable: {expr.name}\")\n        return Substitution(), instantiate(env[expr.name])\n    \n    elif isinstance(expr, Lit):\n        if isinstance(expr.value, int):\n            return Substitution(), TypeInt()\n        elif isinstance(expr.value, bool):\n            return Substitution(), TypeBool()\n    \n    elif isinstance(expr, Lam):\n        tv = fresh_var()\n        new_env = env.copy()\n        new_env[expr.var] = TypeScheme(set(), tv)\n        s1, t1 = infer(new_env, expr.body)\n        return s1, TypeFun(s1.apply_type(tv), t1)\n    \n    elif isinstance(expr, App):\n        tv = fresh_var()\n        s1, t1 = infer(env, expr.func)\n        s2, t2 = infer(apply_subst_env(s1, env), expr.arg)\n        s3 = unify(apply_subst(s2, t1), TypeFun(t2, tv))\n        return s3.compose(s2).compose(s1), apply_subst(s3, tv)\n    \n    elif isinstance(expr, Let):\n        s1, t1 = infer(env, expr.value)\n        new_env = apply_subst_env(s1, env)\n        scheme = generalize(new_env, t1)\n        new_env[expr.var] = scheme\n        s2, t2 = infer(new_env, expr.body)\n        return s2.compose(s1), t2\n\ndef apply_subst(s: Substitution, t: Type) -&gt; Type:\n    return s.apply_type(t)\n\ndef apply_subst_env(s: Substitution, env: Dict[str, TypeScheme]) -&gt; Dict[str, TypeScheme]:\n    return {\n        k: TypeScheme(v.vars, s.apply_type(v.type))\n        for k, v in env.items()\n    }\n\n# AST\n@dataclass\nclass Var:\n    name: str\n\n@dataclass\nclass Lit:\n    value: int | bool\n\n@dataclass\nclass Lam:\n    var: str\n    body: 'Expr'\n\n@dataclass\nclass App:\n    func: 'Expr'\n    arg: 'Expr'\n\n@dataclass\nclass Let:\n    var: str\n    value: 'Expr'\n    body: 'Expr'\n\nExpr = Var | Lit | Lam | App | Let\n\n# Teste\ndef test():\n    # let id = λx. x in id 5\n    expr = Let(\n        \"id\",\n        Lam(\"x\", Var(\"x\")),\n        App(Var(\"id\"), Lit(5))\n    )\n    \n    s, t = infer({}, expr)\n    print(f\"Tipo inferido: {t}\")  # TypeInt()\n\nif __name__ == \"__main__\":\n    test()",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>O Sistema de Tipos Hindley-Milner</span>"
    ]
  },
  {
    "objectID": "10c-semantico.html#extensões-modernas-do-sistema-hm",
    "href": "10c-semantico.html#extensões-modernas-do-sistema-hm",
    "title": "13  O Sistema de Tipos Hindley-Milner",
    "section": "13.10 Extensões Modernas do Sistema HM",
    "text": "13.10 Extensões Modernas do Sistema HM\n\n13.10.1 Type Classes (Haskell)\nHaskell estende HM com type classes, permitindo polimorfismo ad-hoc:\nclass Eq a where\n  (==) :: a -&gt; a -&gt; Bool\n\ninstance Eq Int where\n  x == y = primEqInt x y\nO sistema de tipos se torna:\n\\[\\tau ::= \\alpha \\mid T \\mid \\tau_1 \\rightarrow \\tau_2\\] \\[\\sigma ::= \\tau \\mid \\forall \\alpha. C \\Rightarrow \\sigma\\]\nOnde \\(C\\) são restrições de classe (ex: Eq a).\n\n\n13.10.2 Higher-Rank Types\nHM básico só permite polimorfismo rank-1 (quantificação apenas no topo). System F permite ranks arbitrários:\n-- Rank-1 (HM)\nid :: forall a. a -&gt; a\n\n-- Rank-2\nrunST :: (forall s. ST s a) -&gt; a\n\n-- Rank-n\napplyToFive :: (forall a. a -&gt; a) -&gt; Int\napplyToFive f = f 5\nInferência para rank &gt; 1 é indecidível, requerendo anotações.\n\n\n13.10.3 GADTs (Generalized Algebraic Data Types)\nGADTs permitem refinar tipos em pattern matching:\ndata Expr a where\n  LitInt  :: Int -&gt; Expr Int\n  LitBool :: Bool -&gt; Expr Bool\n  Add     :: Expr Int -&gt; Expr Int -&gt; Expr Int\n  If      :: Expr Bool -&gt; Expr a -&gt; Expr a -&gt; Expr a\n\neval :: Expr a -&gt; a\neval (LitInt n)  = n\neval (LitBool b) = b\neval (Add e1 e2) = eval e1 + eval e2\neval (If c t e)  = if eval c then eval t else eval e\nInferência com GADTs é indecidível no caso geral.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>O Sistema de Tipos Hindley-Milner</span>"
    ]
  },
  {
    "objectID": "10c-semantico.html#limitações-e-o-teorema-da-indecidibilidade",
    "href": "10c-semantico.html#limitações-e-o-teorema-da-indecidibilidade",
    "title": "13  O Sistema de Tipos Hindley-Milner",
    "section": "13.11 Limitações e o Teorema da Indecidibilidade",
    "text": "13.11 Limitações e o Teorema da Indecidibilidade\n\n13.11.1 Limitações Fundamentais do HM\nTeorema (Wells, 1999): Type checking para System F (lambda calculus polimórfico de segunda ordem) é indecidível.\nCorolário: Qualquer extensão de HM que permita polimorfismo de ordem superior ou GADTs gerais torna a inferência indecidível.\nO que isso significa: para sistemas mais expressivos, devemos escolher: 1. Exigir anotações de tipo 2. Aceitar incompletude (o algoritmo pode falhar mesmo para programas tipáveis) 3. Restringir a linguagem\n\n\n13.11.2 O Trade-off Fundamental\nExiste um trade-off irreconciliável entre três propriedades desejáveis:\n\nExpressividade: poder expressar muitos padrões de programação\nInferência completa: deduzir tipos sem anotações\nDecidibilidade: garantir que o algoritmo sempre termina\n\nTeorema: Não é possível ter as três simultaneamente.\nHM escolhe inferência + decidibilidade, sacrificando expressividade (sem rank-n types, sem GADTs gerais).",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>O Sistema de Tipos Hindley-Milner</span>"
    ]
  },
  {
    "objectID": "10c-semantico.html#comparação-hm-vs.-outros-sistemas",
    "href": "10c-semantico.html#comparação-hm-vs.-outros-sistemas",
    "title": "13  O Sistema de Tipos Hindley-Milner",
    "section": "13.12 Comparação: HM vs. Outros Sistemas",
    "text": "13.12 Comparação: HM vs. Outros Sistemas\n\n\n\n\n\n\n\n\n\n\nSistema\nPolimorfismo\nInferência\nDecidibilidade\nExemplo\n\n\n\n\nSimply Typed λ\nNenhum\nCompleta\nSim\nC simples\n\n\nHindley-Milner\nParamétrico (rank-1)\nCompleta\nSim\nML, Haskell base\n\n\nSystem F\nParamétrico (rank-n)\nIncompleta\nIndecidível\nHaskell + extensões\n\n\nF&lt;:\nParamétrico + Subtyping\nIncompleta\nPSPACE-completo\nJava generics\n\n\nDependent Types\nDependente\nManual\nSim (mas difícil)\nCoq, Agda",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>O Sistema de Tipos Hindley-Milner</span>"
    ]
  },
  {
    "objectID": "10c-semantico.html#conclusão-o-legado-de-hindley-milner",
    "href": "10c-semantico.html#conclusão-o-legado-de-hindley-milner",
    "title": "13  O Sistema de Tipos Hindley-Milner",
    "section": "13.13 Conclusão: O Legado de Hindley-Milner",
    "text": "13.13 Conclusão: O Legado de Hindley-Milner\nO sistema Hindley-Milner representa um dos pontos ótimos no espaço de design de sistemas de tipos. Sua combinação única de inferência completa, decidibilidade e polimorfismo paramétrico o torna extremamente útil na prática, enquanto sua elegância matemática o estabelece como um objeto de estudo teórico fundamental.\nContudo, a atenta leitora deve reconhecer suas limitações. Linguagens modernas frequentemente precisam de recursos além do HM básico:\n\nType classes para polimorfismo ad-hoc\nHigher-rank types para abstrações mais expressivas\n\nGADTs para tipos dependentes limitados\nSubtyping para OOP\n\nEstas extensões quebram as garantias de HM, exigindo anotações ou aceitando incompletude. O Algoritmo W, em sua forma pura, já não é suficiente. Implementações modernas (GHC, OCaml, Rust) usam variantes sofisticadas: bidirectional typing, constraint-based inference, unificação de ordem superior.\nAinda assim, HM permanece a fundação. Todo sistema de tipos moderno para linguagens funcionais é, essencialmente, “Hindley-Milner + extensões”. Compreender HM profundamente é compreender tanto suas possibilidades quanto suas limitações fundamentais - e porque os compiladores modernos fazem as escolhas que fazem.\nO verdadeiro legado de Hindley-Milner não é ter resolvido todos os problemas de tipos, mas ter mostrado que inferência automática e soundness podem coexistir - um resultado que parecia impossível antes de 1969.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>O Sistema de Tipos Hindley-Milner</span>"
    ]
  },
  {
    "objectID": "11-tabelaSimbolos.html",
    "href": "11-tabelaSimbolos.html",
    "title": "14  Tabela de Símbolos em Compiladores Modernos",
    "section": "",
    "text": "14.1 Um pouco de História: A Evolução da Tabela de Símbolos\nA Tabela de Símbolos, embora conceitualmente simples, passou por uma notável evolução ao longo da história da computação. Sua trajetória reflete diretamente o avanço na complexidade das linguagens de programação, as mudanças nas arquiteturas de hardware e a busca contínua por compiladores mais rápidos e eficientes. A Figura Figure 14.2 sintetiza a história evolutiva das Tabelas de Símbolos. Sem medo de errar muito, podemos dizer que tudo começou com o Fortran.\nO primeiro compilador FORTRAN (1957) marcou o nascimento da compilação como a conhecemos hoje. As necessidades do compilador Fortran eram relativamente simples: mapear identificadores, nomes de variáveis, sub-rotinas, para seus atributos, tipo, endereço de memória. Para isso, foram utilizadas estruturas de dados lineares, como arrays ou listas. A busca por um símbolo era feita sequencialmente, varrendo a estrutura do início ao fim. Para os programas da época, essa abordagem de complexidade linear, \\(O(n)\\), era suficiente. Sob o olhar do desenvolvedor moderno pode parecer ineficiente e demasiado inocente, mas estabeleceu o princípio fundamental: associar um nome a um conjunto de metadados.\nO próximo passo importante acontece com a introdução do ALGOL 60 (1960). O ALGOL 60 introduziu um conceito transformador: a estrutura de blocos (begin…end), que permitia o aninhamento de escopos. Uma variável declarada dentro de um bloco só era visível ali dentro e em seus sub-blocos. Uma Tabela de Símbolos linear e global mostrou-se inadequada. É muito difícil e pouco eficiente criar estruturas aninhadas em uma tabela linear. A solução foi o desenvolvimento de Tabelas de Símbolos hierárquicas ou aninháveis. Ao percorrer um novo escopo, o compilador cria uma nova tabela, ou se preferir, um novo nível, de forma aninhada. Deste ponto em diante, a busca por um símbolo passou a percorrer uma pilha, do escopo mais interno para o mais externo, modelando perfeitamente as regras de visibilidade de símbolos do ALGOL. Para programas pequenos com escopos rasos, essa abordagem ainda era viável, mas a busca continuava sendo linear em cada nível.\nÀ medida que os programas se tornaram maiores e mais complexos, o desempenho da busca linear nas tabelas de símbolos transformou-se em um gargalo de performance consumindo tempo significativo. A solução veio com a adoção massiva das tabelas de dispersão ou hash tables. Oferecendo um tempo de busca médio de complexidade \\(O(1)\\), estas estruturas de dados representaram um degrau evolutivo em eficiência criando um novo patamar de expectativas. Muito rapidamente a capacidade de encontrar as informações de um identificador com complexidade \\(O(1)\\), independentemente do tamanho do programa, tornou-se a abordagem padrão. Neste ponto, éramos felizes e sabíamos.\nNa década de 1970 linguagens como Pascal e C popularizaram a compilação em múltiplas passagens, na qual o código-fonte é processado em várias etapas (análise léxica, sintática, semântica, otimização, geração de código). Isso exigiu que a Tabela de Símbolos persistisse entre as passagens. Neste ponto da história a Tabela de Símbolos deixou de ser uma estrutura de dados temporária da análise sintática para se tornar um repositório central de informações semânticas, consultado e enriquecido ao longo de todo o processo de compilação. Isso foi fundamental para habilitar recursos como declarações antecipadas forward declarations e otimizações mais complexas.\nPraticamente 20 anos se passaram até a década de 1990 quando a popularização de linguagens orientadas a objetos como C++ e Java introduziu uma nova camada de complexidade. As tabelas de símbolos precisaram evoluir para gerenciar:\nNa mesma proporção em que o tempo passa, a evolução tecnológica não para. Chegamos aos anos 2000, a era moderna dos compiladores. Esta era é marcada pela explosão na escala dos projetos de software e pela onipresença de processadores multi-core. O foco se voltou para otimizações de baixo nível e gerenciamento de memória, resultando em inovações como:",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Tabela de Símbolos em Compiladores Modernos</span>"
    ]
  },
  {
    "objectID": "11-tabelaSimbolos.html#um-pouco-de-história-a-evolução-da-tabela-de-símbolos",
    "href": "11-tabelaSimbolos.html#um-pouco-de-história-a-evolução-da-tabela-de-símbolos",
    "title": "14  Tabela de Símbolos em Compiladores Modernos",
    "section": "",
    "text": "Evolução Histórica das Tabelas de Símbolos\n\n\n\n\nFigure 14.2\n\n\n\n\n\n\n\n\n\nHierarquias de herança: mapear a relação entre classes base e derivadas;\n\nSobrecarga de métodos: Distinguir entre múltiplos métodos com o mesmo nome, mas assinaturas diferentes;\n\nResolução de métodos virtuais: lidar com a vinculação dinâmica (late binding) em tempo de execução; As tabelas de símbolos se tornaram mais estruturadas, frequentemente integradas a outras representações, como árvores de classes, para modelar essas relações complexas.\n\n\n\nAlocação em arena: em vez de alocar memória para cada símbolo individualmente, grandes blocos de memória, as arenas, são alocados de uma vez. Símbolos são então “esculpidos” nesses blocos com um simples incremento de ponteiro. Isso acelera drasticamente a alocação e permite que toda a memória da compilação seja liberada de uma só vez, eliminando a sobrecarga de desalocações individuais.\n\nInternalização de strings, String Interning: garante que cada string de identificador idêntico exista apenas uma vez na memória. Todas as ocorrências de minhaVariavel, por exemplo, apontam para a mesma instância de string. Isso reduz o consumo de memória e permite comparações de strings ultrarrápidas através da simples comparação de ponteiros com complexidade \\(O(1)\\).\n\nDesign Consciente de Cache, Cache-Conscious Design: os engenheiros de compiladores começaram a projetar estruturas de dados que minimizam falhas de cache, os cache misses. Técnicas como o armazenamento co-localizado de strings, como pode ser visto no LLVM, em que o nome do símbolo é armazenado junto com seus metadados na mesma alocação de memória, melhoram a localidade de dados e, consequentemente, o desempenho.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Tabela de Símbolos em Compiladores Modernos</span>"
    ]
  },
  {
    "objectID": "11-tabelaSimbolos.html#interação-com-as-fases-do-compilador-uma-perspetiva-de-ciclo-de-vida",
    "href": "11-tabelaSimbolos.html#interação-com-as-fases-do-compilador-uma-perspetiva-de-ciclo-de-vida",
    "title": "14  Tabela de Símbolos em Compiladores Modernos",
    "section": "14.2 Interação com as Fases do Compilador: Uma Perspetiva de Ciclo de Vida",
    "text": "14.2 Interação com as Fases do Compilador: Uma Perspetiva de Ciclo de Vida\nA Tabela de Símbolos é uma estrutura de dados dinâmica que evolui ao longo do processo de compilação, servindo como interface entre o frontend, módulos de análise, e o backend, módulos de síntese, do processo de tradução. Seja este processo a compilação, a interpretação ou qualquer outro método misto. O estado de completude da Tabela de Símbolos reflete o progresso do processo de tradução em questão. Assim, teremos:\n\nAnálise Léxica: esta é a primeira fase do processo de tradução. É a fase que encontra os identificadores. Pode realizar a inserção inicial na Tabela de Símbolos, muitas vezes armazenando apenas o lexema. Palavras-chave, operadores e identificadores predefinidos são tipicamente pré-carregados na tabela durante a inicialização do compilador.\nAnálise Sintática e Semântica: nestas duas fases, a Tabela de Símbolos é mais intensamente utilizada e enriquecida. A análise sintática fornece o contexto, por exemplo, uma declaração de variável, para que o analisador semântico adicione atributos detalhados como tipo, escopo e requisitos de memória. A análise semântica depende da Tabela de Símbolos para realizar verificações essenciais, como a compatibilidade de tipos, a resolução de escopo e a detecção de variáveis não declaradas ou declaradas múltiplas vezes no mesmo escopo. Historicamente será o analisador semântico o módulo de tradução responsável pelo preenchimento da maior parte da Tabela de Símbolos. Apenas durante a análise semântica é possível conhecer informação suficiente para descrever um nome de forma correta e completa.\nGeração de Código Intermédio e Final: as fases de backend consomem a informação armazenada na Tabela de Símbolos para gerar código de máquina eficiente. Utilizam especificamente os endereços de memória, os deslocamentos, offsets, dentro dos registos de ativação e os tamanhos dos tipos para alocar armazenamento e gerar as diretivas de montagem apropriadas.\n\nO design da estrutura de dados da Tabela de Símbolos deve, portanto, acomodar este preenchimento incremental e os padrões de acesso variados de cada fase. Uma operação de inserção ineficiente retardaria o frontend do compilador (analisadores léxico, sintático e semântico), enquanto uma operação de pesquisa, lookup, lenta constituiria um estrangulamento tanto para o frontend como para o backend.\nA Figura Figure 14.3 mostra a relação típica de uma Tabela de Símbolos ao longo das várias fases do processo de tradução.\n\n\n\n\n\n\nRelação entre Tabela de Símbolos e os módulos do Processo de Tradução\n\n\n\n\nFigure 14.3",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Tabela de Símbolos em Compiladores Modernos</span>"
    ]
  },
  {
    "objectID": "11-tabelaSimbolos.html#informação-armazenada-os-atributos-de-um-identificador",
    "href": "11-tabelaSimbolos.html#informação-armazenada-os-atributos-de-um-identificador",
    "title": "14  Tabela de Símbolos em Compiladores Modernos",
    "section": "14.3 Informação Armazenada: Os Atributos de um Identificador",
    "text": "14.3 Informação Armazenada: Os Atributos de um Identificador\nUma entrada na Tabela de Símbolos é uma coleção de atributos associados a um símbolo. Os atributos exatos dependem da linguagem, mas tipicamente incluem os seguintes campos:\n\nNome: o lexema ou a representação em cadeia de caracteres do identificador (ex: x, for, 1).\n\nTipo: o tipo de dados do símbolo (por exemplo, int, float, struct, ponteiro para função). Esta informação é utilizada para a verificação de tipos.\n\nClasse/Tipo (Kind): a natureza do símbolo, que pode ser uma variável, constante, procedimento, parâmetro, rótulo, tipo, campo de registo, entre outros.\n\nEscopo (Scope): o bloco lexical ou o espaço de nomes, namespace no qual o símbolo é válido. A gestão do escopo é frequentemente uma função da própria estrutura da Tabela de Símbolos.\n\nInformação de Memória: o endereço de tempo de execução, o deslocamento dentro de um registo de ativação ou a classe de armazenamento (por exemplo, estático, automático).\n\nPara Funções/Procedimentos: o número e os tipos dos parâmetros, bem como o tipo de retorno.\n\nPara Matrizes (Arrays): as dimensões e os limites de cada dimensão.\n\nOutros Atributos: informação adicional, como visibilidade (público, privado), se uma variável é constante, ou referências cruzadas para as linhas nas quais o símbolo é declarado e utilizado.",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Tabela de Símbolos em Compiladores Modernos</span>"
    ]
  },
  {
    "objectID": "11-tabelaSimbolos.html#estruturas-de-dados-fundamentais-para-a-implementação-da-tabela-de-símbolos",
    "href": "11-tabelaSimbolos.html#estruturas-de-dados-fundamentais-para-a-implementação-da-tabela-de-símbolos",
    "title": "14  Tabela de Símbolos em Compiladores Modernos",
    "section": "14.4 Estruturas de Dados Fundamentais para a Implementação da Tabela de Símbolos",
    "text": "14.4 Estruturas de Dados Fundamentais para a Implementação da Tabela de Símbolos\nNesta secção a curiosa leitora encontrará um estudo das principais estruturas de dados utilizadas para construção e gestão da Tabela de Símbolos, estabelecendo por que certas escolhas são dominantes na prática moderna. Originalmente, as Tabelas de Símbolos foram implementadas usando listas ligadas simples ou vetores não ordenados. Estas abordagens são fáceis de entender e implementar, mas não escalam bem com o aumento do número de símbolos.\nAs listas ligadas e os vetores não ordenados representam as implementações mais simples para uma Tabela de Símbolos. Contudo, sua simplicidade vem atrelada a um alto custo computacional. Ambas as estruturas exigem uma pesquisa linear para localizar um símbolo, o que resulta em uma complexidade de tempo de \\(O(n)\\) no pior caso e no caso médio. Para o grande número de símbolos encontrados em programas modernos, nos quais as operações de busca são extremamente frequentes, essa complexidade as torna ineficientes.\nO uso de vetores ordenados pode representar uma melhoria considerável na complexidade da pesquisa. Manter os símbolos em ordem alfabética, por exemplo, permite a utilização de algoritmos de pesquisa binária, que possuem uma complexidade de tempo típica de \\(O(\\log n)\\). Isso representa um ganho de desempenho substancial para as operações de busca em comparação com a abordagem linear. Contudo, neste caso, o problema é transferido para as operações de inserção e exclusão. Para manter a ordem do vetor, a inserção de um novo símbolo pode exigir o deslocamento de uma grande porção dos elementos existentes para abrir espaço, uma operação com complexidade \\(O(n)\\). Da mesma forma, a remoção de um símbolo pode necessitar que os elementos subsequentes sejam deslocados para preencher a lacuna. Portanto, embora a pesquisa seja otimizada, as operações de inserção e exclusão tornam-se um gargalo significativo, especialmente em cenários onde a Tabela de Símbolos é frequentemente modificada.\n\n14.4.1 hash tables (Hash Tables): O Padrão De Fato\nA esmagadora preferência por hash tables na implementação de tabelas de símbolos é uma consequência direta da carga de trabalho típica de um processo de tradução. As pesquisas por símbolos são muito mais frequentes do que as suas inserções. Um símbolo é declarado apenas uma vez, o que corresponde a uma única inserção, mas pode ser referenciado centenas ou milhares de vezes, resultando em múltiplas operações de pesquisa. Portanto, otimizar o tempo médio de pesquisa, que possui complexidade \\(O(1)\\) para hash tables, proporciona o maior benefício de desempenho geral. Esta assimetria na frequência das operações torna a pesquisa de caso médio \\(O(1)\\) de uma hash table o fator decisivo para sua escolha, mesmo que seu desempenho no pior caso seja \\(O(n)\\), o que ainda é linear, e que a estrutura não possua as características de ordenação inerentes a uma árvore de pesquisa binária.\nO conceito central de uma hash table é mapear chaves, neste caso os nomes dos símbolos, para índices em um vetor. Esses índices, chamados de buckets ou slots, são determinados por meio de uma função de dispersão, conhecida como hash function.\nO design da função de dispersão é o aspecto fundamental para o projeto de uma hash table com bom desempenho. O objetivo principal é distribuir os símbolos de maneira uniforme pelos buckets para minimizar a ocorrência de colisões, que podem acontecer quando duas chaves diferentes geram o mesmo índice. Uma função de dispersão, hash, simples poderia envolver a soma dos valores ASCII dos caracteres de uma string e, em seguida, a aplicação do operador módulo com o tamanho da tabela para obter um índice. No entanto, na prática, são utilizadas funções mais sofisticadas para garantir uma distribuição mais homogênea e evitar agrupamentos. Mesmo com uma boa função de dispersão, as colisões são inevitáveis. Por isso, uma estratégia eficaz de resolução de colisões é essencial.\nExistem duas abordagens comuns para lidar com o problema das colisões em hash tables usadas no processo de tradução. A primeira, conhecida como encadeamento, chaining ou closed addressing, consiste em fazer com que cada bucket da tabela aponte para uma lista ligada contendo todos os símbolos que foram mapeados para aquele mesmo índice. Esta é uma abordagem comum e robusta. Porém a atenta leitora deve observar que nesta solução, a performance degrada de forma gradual com o aumento do número de elementos em colisão devido a complexidade de busca na lista ligada em cada bucket, uma lista ligada tem complexidade \\(O(n)\\) nos casos médio e pior e \\(O(1)\\) no melhor caso.\nA segunda abordagem é o endereçamento aberto, open addressing ou open hashing. Nesta técnica, se um bucket já estiver ocupado, o algoritmo procura o próximo espaço disponível dentro da própria tabela para inserir o novo elemento. Existem métodos diferentes para encontrar o próximo bucket livre, incluindo a sondagem linear, que verifica sequencialmente os índices (i + 1, i + 2, …), e a sondagem quadrática, que verifica os índices (i + 1², i + 2², …), ajudando a mitigar problemas de agrupamento primário que podem ocorrer com a sondagem linear.\nO problema de agrupamento primário, ou primary clustering, é um fenômeno negativo que ocorre especificamente com o método de sondagem linear em hash tables que usam endereçamento aberto. Em termos simples, ele pode ser descrito da seguinte forma:\n\nCausa: na sondagem linear, quando uma colisão ocorre em um índice i, o algoritmo procura o próximo bucket livre verificando sequencialmente os índices i + 1, i + 2, i + 3, e assim por diante.\nEfeito: essa abordagem sequencial faz com que chaves que colidem na mesma região da tabela comecem a formar aglomerados ou clusters, longas sequências de buckets ocupados um após o outro.\nProblema: uma vez que um aglomerado se forma, ele tende a crescer cada vez mais rápido. Qualquer nova chave que seja mapeada para qualquer posição dentro desse aglomerado terá que percorrer o aglomerado inteiro até encontrar um espaço vazio no final, aumentando ainda mais o seu tamanho.\n\nO resultado é que o desempenho da hash table se degrada significativamente. Em vez de se aproximar da complexidade de busca de \\(O(1)\\) (tempo constante), o tempo de busca começa a se parecer com o de uma busca linear (\\(O(n)\\)), pois o algoritmo gasta muito tempo percorrendo esses longos aglomerados.\nA sondagem quadrática ajuda a mitigar esse problema porque, em vez de verificar o próximo espaço sequencial (+1, +2, …), essa solução cria saltos pela tabela de forma não sequencial (+1², +2², …), o que ajuda a espalhar os elementos e a evitar a formação desses aglomerados contíguos.\n\n\n14.4.2 Estruturas Baseadas em Árvores: A Alternativa Ordenada\nAs estruturas baseadas em árvores representam a principal alternativa ordenada às hash tables na implementação das Tabelas de Símbolos. O conceito central por trás desta abordagem é o uso dos algoritmos de árvores de pesquisa binária, de Binary Search Tree, BST autobalanceadas, como as árvores AVL ou as árvores Vermelho-Preto, Red-Black Trees, que mantêm os símbolos em uma ordem lexicográfica constante. Esta propriedade de ordenação é a sua vantagem mais significativa em relação às hash tables.\nA propriedade chave que define uma BST garante que, para qualquer vértice, todas as chaves em sua subárvore esquerda são menores que a chave do próprio vértice, e todas as chaves em sua subárvore direita são maiores. Em uma BST simples, inserções sequenciais poderiam levar à degeneração da árvore, fazendo-a se comportar como uma lista ligada com desempenho \\(O(n)\\). Para evitar isso, os algoritmos de autobalanceamento, como os das árvores AVL e Vermelho-Preto, realizam rotações para assegurar que a altura da árvore permaneça logarítmica (\\(O(\\log n)\\)) em relação ao número de vértices, garantindo assim a eficiência das operações.\nEssa ordenação inerente aos algoritmos das árvores AVL e Vermelho-Preto, permite a implementação eficiente de funcionalidades com as quais as hash tables têm grande dificuldade. Por exemplo, é possível realizar um percurso ordenado para listar todos os símbolos em ordem alfabética de forma trivial. Além disso, operações como encontrar o símbolo sucessor ou predecessor de um dado símbolo e realizar consultas de intervalo, como encontrar todos os símbolos entre inicio e fim, por exemplo, podem ser executadas de maneira computacionalmente eficiente, capacidades que não são suportadas nativamente por uma hash table padrão.\n\n\n14.4.3 Uma Análise Comparativa de Desempenho, Memória e Funcionalidade\nAo comparar as duas estruturas aparentemente viáveis que vimos até o momento, hash tables e árvores de pesquisa binária, é essencial que a atenta leitora analise as contrapartidas relacionadas ao uso de memória e ao desempenho do hardware, como a localidade de cache. Em termos de sobrecarga de memória, memory overhead, as hash tables exigem a pré-alocação de um vetor de tamanho fixo. Se o número de símbolos for muito inferior a esse tamanho, resultando em um baixo fator de carga, uma quantidade significativa de memória pode ser desperdiçada. Além disso, a estratégia de encadeamento para resolução de colisões adiciona a sobrecarga de um ponteiro para cada entrada na tabela. Em contrapartida, as BSTs autobalanceadas tipicamente possuem uma sobrecarga por elemento maior, pois cada vértice precisa armazenar, no mínimo, dois ponteiros para os filhos e, frequentemente, informações adicionais para o balanceamento, como a cor em árvores Vermelho-Preto. No entanto, sua alocação de memória é dinâmica, vértice a vértice, o que evita o desperdício de um grande bloco de memória pré-alocado e não utilizado.\nA localidade de cache é outro diferenciador de desempenho importante. As hash tables que utilizam endereçamento aberto, especialmente com sondagem linear, podem exibir uma excelente localidade de cache. Isso ocorre porque as sondagens para resolver colisões são sequenciais na memória, aumentando a probabilidade de que os dados necessários já estejam na mesma linha de cache. Por outro lado, estruturas baseadas em ponteiros, como hash tables com encadeamento e todas as BSTs, frequentemente sofrem de má localidade de cache. Percorrer uma lista ligada ou os ramos de uma árvore envolve seguir ponteiros que podem apontar para endereços de memória distantes e dispersos. Esse processo, conhecido como pointer chasing, leva a uma maior taxa de falhas de cache, cache misses, o que pode impactar negativamente o desempenho.\nDo ponto de vista funcional, existem diferenças claras. As BSTs suportam naturalmente o percurso ordenado dos símbolos; por exemplo, um percurso in-order percorre os vértices em ordem alfabética, o que é útil para gerar listagens ordenadas em saídas de depuração ou para implementar certas características de linguagem. Essa não é uma operação natural ou eficiente para hash tables. Da mesma forma, as BSTs se destacam em consultas de intervalo e proximidade, como encontrar a chave mais próxima de um valor ou todas as chaves dentro de um determinado intervalo, funcionalidades que não são suportadas de forma eficiente por hash tables.\nFinalmente, a complexidade de implementação é um fator prático a ser considerado. Embora implementações de bibliotecas padrão para ambas as estruturas estejam prontamente disponíveis, construir uma hash table personalizada e de alta qualidade — o que envolve projetar uma boa função de dispersão e uma estratégia de redimensionamento eficiente — pode ser uma tarefa complexa. No entanto, as BSTs autobalanceadas são notoriamente mais difíceis de implementar corretamente do zero, devido à complexidade dos algoritmos de rotação e manutenção das propriedades de balanceamento. A Tabela ?tbl-comp1 resume as principais características comparativas dessas estruturas de dados.\n\nTabela 1: Comparação das principais estruturas de dados para Tabelas de Símbolos.{#tbl-comp1}\n\n\n\n\n\n\n\n\n\nCaracterística\nhash tables (Encadeamento)\nhash tables (End. Aberto)\nBST Balanceada\nVetor Ordenado\n\n\n\n\nPesquisa (Média)\n\\(O(1)\\)\n\\(O(1)\\)\n\\(O(logn)\\)\n\\(O(logn)\\)\n\n\nPesquisa (Pior Caso)\n\\(O(n)\\)\n\\(O(n)\\)\n\\(O(logn)\\)\n\\(O(logn)\\)\n\n\nInserção (Média)\n\\(O(1)\\)\n\\(O(1)\\)\n\\(O(logn)\\)\n\\(O(n)\\)\n\n\nInserção (Pior Caso)\n\\(O(n)\\)\n\\(O(n)\\)\n\\(O(logn)\\)\n\\(O(n)\\)\n\n\nSobrecarga de Memória\nModerada (ponteiros + vetor)\nBaixa a Alta (depende do fator de carga)\nAlta (ponteiros por vértice)\nBaixa\n\n\nLocalidade de Cache\nFraca\nBoa (sondagem linear)\nFraca\nExcelente\n\n\nSuporte a Percurso Ordenado\nNão\nNão\nSim (eficiente)\nSim (trivial)\n\n\nComplexidade de Implementação\nModerada\nModerada a Alta\nAlta\nBaixa",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Tabela de Símbolos em Compiladores Modernos</span>"
    ]
  },
  {
    "objectID": "11-tabelaSimbolos.html#implementação-avançada-o-desafio-dos-escopos-lexicais",
    "href": "11-tabelaSimbolos.html#implementação-avançada-o-desafio-dos-escopos-lexicais",
    "title": "14  Tabela de Símbolos em Compiladores Modernos",
    "section": "14.5 Implementação Avançada: O Desafio dos escopos Lexicais",
    "text": "14.5 Implementação Avançada: O Desafio dos escopos Lexicais\nDesde o ALGOL 60, com seus blocos de código, que as linguagens de programação modernas suportam escopos lexicais aninhados. Isto significa que uma variável declarada dentro de um bloco só é visível dentro desse bloco e em seus sub-blocos. A gestão eficaz destes escopos aninhados é um desafio fundamental na implementação de uma Tabela de Símbolos. A estrutura de dados selecionada para a Tabela de Símbolos deve não apenas armazenar os símbolos, mas também refletir a hierarquia dos escopos para garantir que as regras de visibilidade sejam corretamente aplicadas durante o processo de tradução.\n\n14.5.1 O Modelo de Pilha de Tabelas\nEsta abordagem, amplamente utilizada por sua simplicidade e intuição, gerencia escopos mantendo uma pilha de Tabelas de Símbolos, que são tipicamente implementadas como hash tables. O mecanismo espelha diretamente a estrutura de escopos aninhados de uma linguagem de programação. Quando um novo escopo é introduzido no código-fonte, como o corpo de uma função, um laço ou um bloco condicional, uma nova hash table vazia é empurrada (pushed) para o topo da pilha, representando o escopo atual. Ao final desse escopo, a tabela correspondente é simplesmente retirada (popped) da pilha e descartada, restaurando o escopo anterior.\nAs operações são diretas e eficientes. A inserção de um novo símbolo ocorre sempre na tabela que está no topo da pilha, ou seja, no escopo local. A pesquisa por um símbolo começa na tabela do topo e, caso o símbolo não seja encontrado, prossegue para as tabelas abaixo na pilha, uma a uma, em direção à base. A primeira correspondência encontrada é retornada. Este método de busca implementa naturalmente a regra de ocultação (shadowing), na qual as declarações em escopos internos têm precedência sobre as de escopos externos. Como pode ser visto na Figura Figure 14.4.\n\n\n\n\n\n\nBusca de Símbolos em Escopos Aninhados\n\n\n\n\nFigure 14.4\n\n\n\n\n\n14.5.2 Algoritmo: Implementando Escopos com uma Pilha de Dicionários\nA lógica para gerenciar uma Tabela de Símbolos com escopo pode ser formalizada por meio de um pseudocódigo simples. Assumimos que nossa estrutura é uma pilha de tabelas, na qual cada tabela mapeia nomes a atributos.\n// Estrutura de dados global\npilha_de_escopos = []  // Inicia com uma tabela para o escopo global\n\nPROCEDIMENTO enter_scope():\n    nova_tabela = criar_tabela_vazia()\n    pilha_de_escopos.push(nova_tabela)\n\nPROCEDIMENTO exit_scope():\n    SE tamanho(pilha_de_escopos) &gt; 1 ENTÃO\n        pilha_de_escopos.pop()\n    SENÃO\n        erro(\"Não é possível sair do escopo global\")\n\nPROCEDIMENTO declare(nome, atributos):\n    tabela_atual = pilha_de_escopos.top()\n    SE nome JÁ EXISTE em tabela_atual ENTÃO\n        erro(\"Símbolo já declarado neste escopo\")\n    SENÃO\n        inserir(nome, atributos) em tabela_atual\n\nFUNÇÃO lookup(nome):\n    PARA cada tabela em pilha_de_escopos (do topo para a base):\n        SE nome EXISTE em tabela ENTÃO\n            RETORNAR atributos_de(nome) em tabela\n    // Se o loop terminar, o nome não foi encontrado\n    RETORNAR nulo  // Ou lançar erro de \"símbolo não declarado\"\n\n\n14.5.3 Implementação em Python: Uma Tabela de Símbolos Funcional\nComo exemplo simples, podemos traduzir diretamente o pseudocódigo para uma classe Python funcional. Usaremos uma lista de dicionários para representar a pilha de escopos.\nimport pprint\n\nclass Symbol:\n    def __init__(self, name, category, type):\n        self.name = name\n        self.category = category\n        self.type = type\n\n    def __repr__(self):\n        return f\"&lt;Symbol(name='{self.name}', category='{self.category}', type='{self.type}')&gt;\"\n\nclass SymbolTable:\n    \"\"\"\n    Uma **Tabela de Símbolos** que gerencia escopos aninhados usando uma pilha.\n    \"\"\"\n    def __init__(self):\n        # A pilha de escopos, onde cada escopo é um dicionário (tabela de _hash_).\n        # Começa com o escopo global.\n        self.scoped_tables = [{}]\n\n    def enter_scope(self):\n        \"\"\" Inicia um novo escopo empilhando um novo dicionário. \"\"\"\n        print(\"--- Entrando em um novo escopo ---\")\n        self.scoped_tables.append({})\n\n    def exit_scope(self):\n        \"\"\" Finaliza o escopo atual desempilhando seu dicionário. \"\"\"\n        print(\"--- Saindo do escopo atual ---\")\n        if len(self.scoped_tables) &gt; 1:\n            self.scoped_tables.pop()\n        else:\n            # Idealmente, isso seria um erro do compilador\n            print(\"Aviso: Tentativa de sair do escopo global.\")\n\n    def declare(self, symbol):\n        \"\"\" Declara um símbolo no escopo atual. \"\"\"\n        current_scope = self.scoped_tables[-1]\n        if symbol.name in current_scope:\n            raise NameError(f\"Erro: Símbolo '{symbol.name}' já declarado no escopo atual.\")\n        current_scope[symbol.name] = symbol\n        print(f\"Declarado: {symbol}\")\n\n    def lookup(self, name):\n        \"\"\" Procura por um símbolo do escopo mais interno para o mais externo. \"\"\"\n        print(f\"Buscando por '{name}'...\")\n        # Iterar de trás para frente na lista (do topo para a base da pilha)\n        for scope in reversed(self.scoped_tables):\n            if name in scope:\n                symbol = scope[name]\n                print(f\"Encontrado: {symbol}\")\n                return symbol\n        raise NameError(f\"Erro: Símbolo '{name}' não foi declarado.\")\n\n    def display(self):\n        \"\"\" Exibe o estado atual da **Tabela de Símbolos**. \"\"\"\n        print(\"\\n=== Estado da **Tabela de Símbolos** ===\")\n        for i, scope in reversed(list(enumerate(self.scoped_tables))):\n            print(f\"--- Escopo Nível {i} ---\")\n            pprint.pprint(scope)\n        print(\"==================================\\n\")\n\n# Exemplo de uso\nif __name__ == \"__main__\":\n    st = SymbolTable()\n    st.display()\n\n    # Declarando no escopo global\n    st.declare(Symbol(\"g\", \"variavel\", \"int\"))\n    st.declare(Symbol(\"PI\", \"constante\", \"float\"))\n    st.display()\n\n    # Entrando em um novo escopo (ex.: uma função)\n    st.enter_scope()\n    st.declare(Symbol(\"x\", \"parametro\", \"int\"))\n    st.declare(Symbol(\"g\", \"variavel\", \"float\"))  # Ocultando (shadowing) o 'g' global\n    st.display()\n\n    # Buscando símbolos\n    st.lookup(\"x\")   # Encontra 'x' local\n    st.lookup(\"g\")   # Encontra 'g' local (float), não o global\n    st.lookup(\"PI\")  # Não encontra local, busca no global e encontra\n\n    # Entrando em um escopo aninhado (ex.: um bloco if)\n    st.enter_scope()\n    st.declare(Symbol(\"y\", \"variavel\", \"bool\"))\n    st.display()\n    st.lookup(\"y\")\n    st.lookup(\"x\")  # Encontra no escopo da função\n\n    # Saindo dos escopos\n    st.exit_scope()\n    st.display()\n    try:\n        st.lookup(\"y\")  # Deve falhar, 'y' estava no escopo interno\n    except NameError as e:\n        print(e)\n    \n    st.exit_scope()\n    st.display()\n    st.lookup(\"g\")  # Agora encontra o 'g' global (int)\nUma implementação deste mesmo modelo, em C++23 pode ser vista no bloco de código a seguir:\n/**\n * @file SymbolTable.h\n * @author Gemini\n * @date 26 de Setembro de 2025\n * @brief Implementação de uma **Tabela de Símbolos** com escopo em **C++**23.\n *\n * @copyright Copyright (c) 2025\n *\n * Este arquivo contém uma implementação didática de uma **Tabela de Símbolos**\n * com escopo, baseada no modelo clássico \"pilha de tabelas\" popularizado\n * em cursos de compiladores como o de Stanford (CS143). O objetivo é\n * demonstrar de forma clara como o aninhamento de escopos e o sombreamento\n * de variáveis (variable shadowing) podem ser gerenciados.\n */\n\n#include &lt;iostream&gt;\n#include &lt;stack&gt;\n#include &lt;string&gt;\n#include &lt;unordered_map&gt;\n#include &lt;optional&gt;\n#include &lt;string_view&gt;\n\n/**\n * @class SymbolTable\n * @brief Gerencia símbolos e seus tipos em escopos aninhados.\n *\n * A classe SymbolTable implementa o modelo de \"pilha de tabelas de símbolos\".\n * Cada escopo léxico (ex: corpo de uma função, um bloco 'if', um laço 'for')\n * é representado por uma tabela de hash (std::unordered_map) separada.\n * Uma pilha (std::stack) é usada para gerenciar essas tabelas, refletindo\n * a natureza aninhada dos escopos no código-fonte.\n *\n * A tabela no topo da pilha sempre representa o escopo mais interno, ou seja,\n * o escopo atual.\n */\nclass SymbolTable {\npublic:\n    /**\n     * @brief Construtor da **Tabela de Símbolos**.\n     *\n     * Ao ser instanciada, a **Tabela de Símbolos** cria automaticamente o primeiro\n     * escopo, que representa o escopo global do programa. Todas as declarações\n     * feitas antes de qualquer outro escopo aninhado residirão aqui.\n     */\n    SymbolTable() {\n        // Empurra a primeira tabela na pilha para representar o escopo global.\n        enterScope();\n    }\n\n    /**\n     * @brief Inicia um novo escopo aninhado.\n     *\n     * Esta função deve ser chamada sempre que o analisador do compilador\n     * encontra o início de um novo bloco léxico (ex: o caractere '{').\n     * Ela empurra uma nova tabela de hash vazia para o topo da pilha, que\n     * se torna o escopo ativo para novas declarações de símbolos.\n     */\n    void enterScope() {\n        std::cout &lt;&lt; \"[SYSTEM] Entrando em um novo escopo...\\n\";\n        m_scopes.push({}); // Empurra um map (hash table) vazio na pilha.\n    }\n\n    /**\n     * @brief Finaliza o escopo atual.\n     *\n     * Esta função deve ser chamada quando o analisador encontra o fim de um\n     * bloco léxico (ex: o caractere '}'). Ela remove (dá 'pop') a tabela de\n     * hash do topo da pilha. Com isso, todos os símbolos declarados nesse\n     * escopo são efetivamente destruídos e se tornam inacessíveis,\n     * implementando corretamente as regras de escopo da maioria das linguagens.\n     * Protege contra a remoção do escopo global.\n     */\n    void exitScope() {\n        if (m_scopes.size() &gt; 1) {\n            std::cout &lt;&lt; \"[SYSTEM] Saindo do escopo atual...\\n\";\n            m_scopes.pop();\n        } else {\n            std::cout &lt;&lt; \"[SYSTEM] Aviso: Tentativa de sair do escopo global. Ação ignorada.\\n\";\n        }\n    }\n\n    /**\n     * @brief Adiciona um novo símbolo ao escopo atual.\n     * @param name O nome do símbolo (identificador).\n     * @param type O tipo associado ao símbolo (ex: \"int\", \"string\").\n     * @return true se o símbolo foi adicionado com sucesso.\n     * @return false se um símbolo com o mesmo nome já existe no escopo ATUAL.\n     *\n     * A inserção ocorre exclusivamente na tabela do topo da pilha, ou seja,\n     * no escopo mais interno. A função verifica se já existe uma declaração\n     * com o mesmo nome NESTE escopo para evitar redeclarações.\n     */\n    bool addSymbol(const std::string& name, const std::string& type) {\n        // Acessa a tabela do topo (escopo atual) sem removê-la.\n        auto& current_scope = m_scopes.top();\n\n        // O método 'contains' (**C++**20) é uma forma eficiente de verificar a existência da chave.\n        if (current_scope.contains(name)) {\n            std::cout &lt;&lt; \"[ERROR] Símbolo '\" &lt;&lt; name &lt;&lt; \"' já declarado neste escopo.\\n\";\n            return false;\n        }\n\n        current_scope[name] = type;\n        std::cout &lt;&lt; \"[ADD] Adicionado símbolo '\" &lt;&lt; name &lt;&lt; \"' com tipo '\" &lt;&lt; type &lt;&lt; \"' ao escopo atual.\\n\";\n        return true;\n    }\n\n    /**\n     * @brief Procura por um símbolo a partir do escopo atual para o mais externo.\n     * @param name O nome do símbolo a ser procurado.\n     * @return std::optional&lt;std::string&gt; contendo o tipo do símbolo se encontrado.\n     * @return std::nullopt se o símbolo não for encontrado em nenhum escopo visível.\n     *\n     * Esta é a operação mais importante para a análise semântica. A busca\n     * começa na tabela do topo da pilha (escopo mais interno) e, se o símbolo\n     * não for encontrado, a busca continua na tabela imediatamente abaixo, e\n     * assim por diante, até a base da pilha (escopo global).\n     *\n     * Este mecanismo de busca implementa naturalmente o **sombreamento de variáveis**\n     * (variable shadowing): a primeira ocorrência do símbolo encontrada (a mais interna)\n     * \"esconde\" quaisquer outras com o mesmo nome em escopos mais externos.\n     *\n     * O uso de `std::optional` (**C++**17) é a forma moderna e segura de retornar um\n     * valor que pode ou não existir, evitando o uso de ponteiros nulos ou\n     * valores mágicos (como uma string vazia).\n     */\n    std::optional&lt;std::string&gt; lookupSymbol(const std::string& name) const {\n        // Para percorrer a pilha sem destruí-la (pois o método é const),\n        // criamos uma cópia. Para uma implementação de compilador real,\n        // uma estrutura de dados com iteradores (como std::vector) seria\n        // mais performática, mas std::stack é conceitualmente mais clara\n        // para este exemplo didático.\n        std::stack&lt;std::unordered_map&lt;std::string, std::string&gt;&gt; temp_stack = m_scopes;\n\n        int scope_level = temp_stack.size();\n        while (!temp_stack.empty()) {\n            auto& scope = temp_stack.top();\n            auto it = scope.find(name); // Procura na hash table do escopo atual\n\n            if (it != scope.end()) {\n                // Símbolo encontrado!\n                std::cout &lt;&lt; \"[LOOKUP] Símbolo '\" &lt;&lt; name &lt;&lt; \"' encontrado no nível de escopo \" &lt;&lt; scope_level &lt;&lt; \".\\n\";\n                return it-&gt;second; // Retorna o tipo (o valor no map)\n            }\n\n            temp_stack.pop(); // Não encontrou, vá para o escopo pai (abaixo na pilha)\n            scope_level--;\n        }\n\n        // Se o loop terminar, o símbolo não foi encontrado em nenhum escopo.\n        std::cout &lt;&lt; \"[LOOKUP] Símbolo '\" &lt;&lt; name &lt;&lt; \"' não encontrado em nenhum escopo visível.\\n\";\n        return std::nullopt;\n    }\n\nprivate:\n    /// @brief A pilha de tabelas de hash que gerencia os escopos.\n    std::stack&lt;std::unordered_map&lt;std::string, std::string&gt;&gt; m_scopes;\n};\n\n/**\n * @brief Função principal para demonstrar o uso da SymbolTable.\n *\n * O código a seguir simula o processo de um compilador analisando\n * um código com escopos aninhados.\n */\nint main() {\n    SymbolTable table; // Escopo global é criado automaticamente.\n\n    std::cout &lt;&lt; \"\\n--- DECLARAÇÕES NO ESCOPO GLOBAL ---\\n\";\n    table.addSymbol(\"g_var\", \"int\");\n    table.addSymbol(\"PI\", \"const double\");\n\n    // { // Entrando no escopo de uma função, por exemplo.\n    table.enterScope();\n    std::cout &lt;&lt; \"\\n--- DECLARAÇÕES NO ESCOPO DA FUNÇÃO 'main' ---\\n\";\n    table.addSymbol(\"x\", \"string\");\n    // Tenta sombrear a variável global 'g_var'\n    table.addSymbol(\"g_var\", \"char*\");\n\n    std::cout &lt;&lt; \"\\n--- BUSCAS DENTRO DA FUNÇÃO 'main' ---\\n\";\n    // Deve encontrar a versão 'char*' de 'g_var' (sombreamento)\n    if (auto type = table.lookupSymbol(\"g_var\")) {\n        std::cout &lt;&lt; \"  -&gt; Tipo de 'g_var' é: \" &lt;&lt; *type &lt;&lt; \"\\n\";\n    }\n    // Deve encontrar 'x' como 'string'\n    if (auto type = table.lookupSymbol(\"x\")) {\n        std::cout &lt;&lt; \"  -&gt; Tipo de 'x' é: \" &lt;&lt; *type &lt;&lt; \"\\n\";\n    }\n    // Deve encontrar a variável global 'PI'\n    if (auto type = table.lookupSymbol(\"PI\")) {\n        std::cout &lt;&lt; \"  -&gt; Tipo de 'PI' é: \" &lt;&lt; *type &lt;&lt; \"\\n\";\n    }\n\n    //   { // Entrando em um bloco 'if' interno\n    table.enterScope();\n    std::cout &lt;&lt; \"\\n--- DECLARAÇÕES NO BLOCO 'IF' INTERNO ---\\n\";\n    table.addSymbol(\"y\", \"bool\");\n    // Tenta sombrear 'x' do escopo da função\n    table.addSymbol(\"x\", \"float\");\n\n    std::cout &lt;&lt; \"\\n--- BUSCAS DENTRO DO BLOCO 'IF' INTERNO ---\\n\";\n    // Deve encontrar 'x' como 'float' (sombreamento mais interno)\n    if (auto type = table.lookupSymbol(\"x\")) {\n        std::cout &lt;&lt; \"  -&gt; Tipo de 'x' é: \" &lt;&lt; *type &lt;&lt; \"\\n\";\n    }\n    // Deve encontrar 'y'\n    if (auto type = table.lookupSymbol(\"y\")) {\n        std::cout &lt;&lt; \"  -&gt; Tipo de 'y' é: \" &lt;&lt; *type &lt;&lt; \"\\n\";\n    }\n    // Tentando buscar um símbolo que não existe\n    table.lookupSymbol(\"z\");\n\n    //   } // Saindo do bloco 'if'\n    table.exitScope();\n    std::cout &lt;&lt; \"\\n--- APÓS SAIR DO BLOCO 'IF', DE VOLTA À FUNÇÃO 'main' ---\\n\";\n\n    // Agora 'x' deve voltar a ser 'string'\n    if (auto type = table.lookupSymbol(\"x\")) {\n        std::cout &lt;&lt; \"  -&gt; Tipo de 'x' agora é: \" &lt;&lt; *type &lt;&lt; \"\\n\";\n    }\n    // 'y' não deve mais existir\n    std::cout &lt;&lt; \"  -&gt; Buscando por 'y' que não deve mais existir...\\n\";\n    table.lookupSymbol(\"y\");\n\n    // } // Saindo da função 'main'\n    table.exitScope();\n    std::cout &lt;&lt; \"\\n--- APÓS SAIR DA FUNÇÃO 'main', DE VOLTA AO ESCOPO GLOBAL ---\\n\";\n\n    // 'g_var' deve voltar a ser 'int'\n    if (auto type = table.lookupSymbol(\"g_var\")) {\n        std::cout &lt;&lt; \"  -&gt; Tipo de 'g_var' agora é: \" &lt;&lt; *type &lt;&lt; \"\\n\";\n    }\n    // 'x' não deve mais existir\n    std::cout &lt;&lt; \"  -&gt; Buscando por 'x' que não deve mais existir...\\n\";\n    table.lookupSymbol(\"x\");\n\n    return 0;\n}\nNo código C++23 acima, a classe SymbolTable encapsula toda a funcionalidade necessária para gerenciar escopos aninhados usando uma pilha de tabelas de símbolos. Cada tabela é implementada como um std::unordered_map, que oferece acesso eficiente aos símbolos por meio de hashing. O uso de std::optional para o método lookupSymbol permite um tratamento elegante dos casos em que um símbolo não é encontrado.\n\n\n14.5.4 O Modelo de Tabela Única com Encadeamento de Escopo\nEste modelo adota uma abordagem diferente, utilizando uma única hash table global para armazenar todos os símbolos de todos os escopos. Para gerenciar os diferentes escopos, as entradas para um mesmo identificador são encadeadas em uma lista ligada dentro do bucket correspondente. Geralmente, essa lista é ordenada do escopo mais interno para o mais externo, de modo que a declaração mais recente esteja no início da lista.\nAs operações neste modelo são mais complexas. Para entrar em um escopo, um marcador especial, que não é um símbolo válido da linguagem, é empurrado para uma pilha de escopos auxiliar, simplesmente para registrar a fronteira do novo escopo. A inserção de um símbolo envolve adicionar uma nova entrada ao início da lista ligada no bucket apropriado, ligando-a a quaisquer entradas pré-existentes para esse mesmo nome. A pesquisa começa com a função de dispersão, que localiza o início da lista ligada para o identificador. Em seguida, essa lista é percorrida para encontrar a primeira entrada válida que corresponda ao escopo atual ou a um escopo envolvente. A principal desvantagem surge ao sair de um escopo. É necessário identificar e remover da hash table todos os símbolos que foram inseridos desde que o último marcador de escopo foi empurrado. Este processo de limpeza é consideravelmente mais complexo e menos eficiente do que a simples operação de pop do modelo de pilha.\n\n\n14.5.5 Representações de Escopo Hierárquicas e Baseadas em Árvores\nNesta abordagem, a estrutura aninhada dos escopos de um programa é representada explicitamente como uma árvore, frequentemente chamada de “árvore de escopos”. O escopo global atua como o vértice raiz da árvore, e os blocos de código aninhados (funções, classes, laços, etc.) são representados como vértices filhos. Cada vértice nesta árvore contém sua própria Tabela de Símbolos local, que pode ser uma hash table, uma árvore de pesquisa binária ou outra estrutura de dados.\nAs operações de busca refletem a estrutura da árvore. A pesquisa por um identificador começa na Tabela de Símbolos do vértice correspondente ao escopo atual, que pode ser um vértice folha ou um vértice interno. Se o símbolo não for encontrado localmente, a busca continua na Tabela de Símbolos do vértice pai, depois no avô, e assim por diante, subindo pela árvore até a raiz (o escopo global). Esse percurso ascendente do vértice atual até a raiz espelha funcionalmente a busca descendente no modelo de pilha de tabelas, mas o faz dentro de uma estrutura de dados explícita e persistente, que pode ser mais flexível para certas análises do compilador. A Tabela ?tbl-comp2 resume as principais características comparativas dessas estratégias de gestão de escopos.\n\nTabela 2: Comparação das principais estratégias para gestão de escopos em Tabelas de Símbolos.{#tbl-comp2}\n\n\n\n\n\n\n\n\n\n\nEstratégia\nComplexidade Enter Scope\nComplexidade Exit Scope\nComplexidade Lookup\nGestão de Memória\nComplexidade de Implementação\n\n\n\n\nPilha de Tabelas\nO(1)\nO(1)\nO(D) (D = profundidade do escopo)\nSimples (tabelas inteiras são alocadas/desalocadas)\nBaixa a Moderada\n\n\nTabela Única com Encadeamento\nO(1)\nO(k) (k = símbolos no escopo)\nO(L) (L = comprimento da cadeia)\nComplexa (requer limpeza seletiva)\nModerada a Alta\n\n\nÁrvore de escopos Explícita\nO(logS) ou O(1) (S = n.º de escopos)\nO(1)\nO(D)\nModerada (vértices da árvore + tabelas)\nAlta",
    "crumbs": [
      "Analisadores Semânticos",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Tabela de Símbolos em Compiladores Modernos</span>"
    ]
  },
  {
    "objectID": "14-GeraInter.html",
    "href": "14-GeraInter.html",
    "title": "15  Geração de Código Intermediário: A Linguagem Universal do Compilador",
    "section": "",
    "text": "15.1 Perspectiva Histórica: A Evolução das Representações Intermediárias\nA concepção e o uso de representações intermediárias não são uma invenção recente; sua evolução espelha as prioridades cambiantes da engenharia de software ao longo das décadas.\nNos anos 1970, o cenário de hardware era extremamente fragmentado. Muitas empresas produziam máquinas completamente independentes. A portabilidade de software era um desafio colossal. Foi nesse contexto que Niklaus Wirth, o criador da linguagem Pascal, popularizou uma abordagem revolucionária com seu compilador. Em vez de gerar código nativo para uma máquina específica, o compilador Pascal da ETH Zurich produzia um código intermediário simples para uma máquina de pilha virtual, conhecido como P-code.\nA influência do P-code estendeu-se muito além de sua época. Décadas depois, nos anos 1990, a Sun Microsystems adotou uma arquitetura conceitual similar ao projetar a Java Virtual Machine (JVM). Assim como o P-code, o bytecode da JVM é um código intermediário para uma máquina de pilha abstrata, permitindo que programas Java sejam compilados uma vez e executados em qualquer plataforma que possua uma JVM. O famoso slogan “Write Once, Run Anywhere” de Java é, em essência, uma reformulação do princípio de portabilidade que Wirth pioneirou com o P-code. Esta linhagem conceitual demonstra como soluções elegantes para problemas fundamentais de engenharia transcendem gerações de tecnologia.\nPara executar um programa Pascal em uma nova arquitetura, não era mais necessário reescrever o compilador inteiro. Bastava implementar um pequeno e simples interpretador de P-code para a nova máquina. Essa estratégia reduziu drasticamente o esforço de portabilidade e foi um fator chave para a disseminação do Pascal, especialmente em plataformas emergentes como os computadores pessoais Apple II com o sistema UCSD Pascal. No caso do Pascal, a prioridade era clara: portabilidade, mesmo que ao custo de alguma performance, já que o código era interpretado em vez de executado nativamente.\nAvançando para as décadas de 1980 e 1990, a complexidade do software cresceu graças aos sistemas operacionais e as grandes aplicações sendo escritas em C. O modelo de compilação do C, embora não centrado em uma RI para otimização, introduziu um conceito fundamental: a compilação separada. O compilador C traduz cada arquivo-fonte (.c) em um arquivo-objeto (.o ou .obj). Um arquivo-objeto é, em si, uma forma de código intermediário. Ele contém código de máquina relocável. Ou seja, as instruções já estão em formato binário, mas os endereços de memória ainda não são finais, e uma tabela de símbolos que lista as funções e variáveis que ele define e as que ele precisa de outros módulos. Um programa final é criado por um aplicativo de linker, que combina múltiplos arquivos-objeto e bibliotecas, resolvendo essas referências cruzadas. Este modelo de modularidade permitiu gerenciar a complexidade de grandes projetos de software, mantendo a separação de preocupações. O modelo criado pelo C também solidificou a ideia de traduzir o código-fonte para uma forma linear, autocontida e próxima da máquina, que serviu de inspiração para o design de RIs lineares como o Código de Três Endereços, que veremos neste capítulo.\nA partir dos anos 2000, a prioridade da indústria mudou outra vez. O hardware, embora mais padronizado, como as máquinas x86 e ARM, tornou-se imensamente complexo internamente, pipelines profundos, caches multinível, paralelismo em nível de instrução. O desafio principal passou a ser extrair o máximo de performance dessas arquiteturas. É nesse contexto que surge o LLVM. Originalmente um projeto de pesquisa na Universidade de Illinois, originalmente chamado de Low Level Virtual Machine, o LLVM foi concebido como uma infraestrutura de compilação, modular e reutilizável.\nO LLVM representou uma mudança de paradigma. Sua representação intermediária, a LLVM IR, não foi projetada apenas como um formato de passagem de dados, mas como o coração de uma infraestrutura de compilação completa, modular e reutilizável. O LLVM fornece um conjunto de bibliotecas, e aplicativos. para a análise, transformação e otimização que operam diretamente sobre a LLVM IR. Neste ponto da história, o código intermediário deixou de ser um mero produto do front-end para se tornar uma linguagem de programação por si só, na qual otimizadores de propósito geral são escritos e compartilhados por múltiplos front-ends, tais como Clang para C/C++ e rustc para Rust) e back-ends. Esta evolução, da portabilidade, P-code, à modularidade, arquivos-objeto e, finalmente, à performance com o LLVM IR, demonstra como as RIs são artefatos capazes de encapsular e permitir a solução dos desafios de engenharia mais prementes de sua época.",
    "crumbs": [
      "Geração de Código Intermediário",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Geração de Código Intermediário: A Linguagem Universal do Compilador</span>"
    ]
  },
  {
    "objectID": "14-GeraInter.html#fundamentos-teóricos-propriedades-e-tipos-de-representações-intermediárias",
    "href": "14-GeraInter.html#fundamentos-teóricos-propriedades-e-tipos-de-representações-intermediárias",
    "title": "15  Geração de Código Intermediário: A Linguagem Universal do Compilador",
    "section": "15.2 Fundamentos Teóricos: Propriedades e Tipos de Representações Intermediárias",
    "text": "15.2 Fundamentos Teóricos: Propriedades e Tipos de Representações Intermediárias\nPara que uma Representação Intermediária seja eficaz, ela deve possuir um conjunto de propriedades bem definidas. Além disso, as RIs podem ser classificadas em diferentes categorias, cada uma com suas próprias forças e fraquezas.\n\n15.2.1 Propriedades de uma Boa RI\nUma RI robusta e útil deve exibir as seguintes características:\n\nIndependência do código-fonte: a RI deve ser genérica o suficiente para representar as construções semânticas de diversas linguagens de programação de alto nível, desde C++ e Rust até Python e Swift.\nIndependência da arquitetura-alvo: a RI deve abstrair detalhes específicos da arquitetura de hardware, como o número de registradores, o conjunto de instruções e as convenções de chamada de procedimento. Isso é importante para a portabilidade do front-end e dos otimizadores.\nFacilidade de Geração: a tradução da ASTA para a RI deve ser um processo relativamente direto e algorítmico, tipicamente implementado através de um percurso na árvore.\nAdequação para Otimizações: esta é, talvez, a propriedade mais crítica em compiladores modernos. A estrutura da RI deve facilitar a análise e a transformação do código. Por exemplo, a identificação de operandos e resultados deve ser explícita, e a estrutura de controle do programa deve ser claramente representável.\n\n\n\n15.2.2 Categorização dos Tipos de CI\nAs representações intermediárias são geralmente classificadas em três categorias principais:\n\nRepresentações em Grafos: estas RIs mantêm a estrutura hierárquica, ou de grafo, do programa. Os exemplos mais comuns são a própria Árvore de Sintaxe Abstrata (AST) e os Grafos Acíclicos Dirigidos (DAGs). Um DAG é uma otimização da ASTA na qual subexpressões comuns são representadas por um único vértice, eliminando a redundância e tornando explícita a reutilização de cálculos. RIs gráficas são excelentes para capturar a estrutura do código, mas podem ser complexas de manipular para otimizações que dependem do fluxo de execução.\nRepresentações Lineares: estas RIs se assemelham a uma linguagem de montagem, Assembly, para uma máquina abstrata. O código é uma sequência linear de instruções simples. Os principais exemplos são o Código de Pilha, ou Código de Um Endereço, na qual os operandos são implicitamente colocados e retirados de uma pilha, e, mais proeminentemente, o Código de Três Endereços (TAC), que será o foco principal de nossa análise.\nRepresentações Híbridas: otimizadores modernos quase sempre utilizam uma abordagem híbrida. Eles combinam uma representação gráfica de alto nível para o fluxo de controle com uma representação linear para as computações. A estrutura mais comum é o Grafo de Fluxo de Controle (CFG), no qual cada vértice do grafo é um Bloco Básico. Um bloco básico é uma sequência de instruções lineares, tipicamente em TAC, que possui um único ponto de entrada, a primeira instrução, ou líder, e um único ponto de saída, a última instrução, sem saltos para dentro ou para fora do meio do bloco. Esta combinação oferece o melhor dos dois mundos: o grafo de fluxo de controle expõe a estrutura de laços e desvios para otimizações de fluxo de controle, enquanto o TAC dentro de cada bloco facilita a otimização de expressões locais.\n\nA tabela a #tbl-categorias-ri resume e compara essas categorias.\n\nComparação das Categorias de Representações Intermediárias.\n\n\n\n\n\n\n\n\n\n\nTipo de RI\nEstrutura\nNível de Abstração\nPrós\nContras\nExemplo de Uso\n\n\n\n\nAST\nGráfica (Árvore)\nAlto\nFiel à sintaxe da fonte, bom para análise semântica.\nDifícil para otimizações de fluxo de dados.\nSaída do analisador semântico.\n\n\nDAG\nGráfica (Grafo)\nAlto/Médio\nCompacto, elimina subexpressões comuns.\nMais complexo de construir que a ASTA.\nOtimizações locais.\n\n\nCódigo de Pilha\nLinear\nBaixo\nGeração simples, interpretador compacto.\nAcesso implícito a operandos, difícil de otimizar.\nJVM, P-code.\n\n\nTAC\nLinear\nMédio\nEndereçamento explícito, fácil de otimizar e rearranjar.\nMais verboso que código de pilha.\nBloco básico em CFGs.\n\n\nCFG + TAC****\nHíbrida\nMédio\nModela explicitamente o fluxo de controle e as computações.\nEstrutura de dados mais complexa.\nPadrão em compiladores otimizadores (GCC, LLVM).",
    "crumbs": [
      "Geração de Código Intermediário",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Geração de Código Intermediário: A Linguagem Universal do Compilador</span>"
    ]
  },
  {
    "objectID": "14-GeraInter.html#aprofundamento-em-código-de-três-endereços-tac",
    "href": "14-GeraInter.html#aprofundamento-em-código-de-três-endereços-tac",
    "title": "15  Geração de Código Intermediário: A Linguagem Universal do Compilador",
    "section": "15.3 Aprofundamento em Código de Três Endereços (TAC)",
    "text": "15.3 Aprofundamento em Código de Três Endereços (TAC)\nO Código de Três Endereços é uma das formas mais populares e influentes de representação intermediária linear. Sua simplicidade e estrutura o tornam um excelente candidato para análise e transformação.\n\n15.3.1 Definição Formal e Instruções\nO TAC consiste em uma sequência de instruções que se assemelham a uma linguagem Assembly. A forma geral de uma instrução é:\n\\[resultado := argumento_1 \\text{ operador } argumento_2\\]\nA característica definidora do TAC é a restrição de que há no máximo um operador no lado direito de cada instrução de atribuição. Isso significa que expressões complexas do código-fonte devem ser decompostas em múltiplas instruções TAC, utilizando variáveis temporárias geradas pelo compilador para armazenar resultados intermediários. Por exemplo, a instrução w = (a + b) * c; seria traduzida para:\nt1:= a + b\nt2:= t1 * c\nw:= t2\nUm código TAC deve ter máximo um operador no lado direito de cada instrução de atribuição. Mas, isso quer dizer que podemos ter um conjunto diverso de instruções. As mais comuns são:\n\nAtribuições:\n\n\nBinária: x := y op z;\nUnária: x := op y;\nCópia: x := y.\n\n\nSaltos:\n\n\nIncondicional: goto L;\nCondicional: if x relop y goto L.\n\n\nChamadas de Procedimento:\n\n\nparam x (para passar um parâmetro);\ncall p, n (para chamar o procedimento p com n parâmetros);\nreturn y (para retornar de um procedimento).\n\n\nAcesso a Arrays e Ponteiros:\n\n\nx := y[i] (leitura de array);\nx[i] := y (escrita em array);\nx := &y (endereço de y);\nx := *y (leitura via ponteiro);\n*x := y (escrita via ponteiro).\n\n\n\n15.3.2 Implementações do TAC\nExistem três formas clássicas de implementar o TAC em uma estrutura de dados:\n\nQuádruplas: cada instrução é um registro, ou struct, com quatro campos: (operador, arg1, arg2, resultado). O resultado de cada operação é armazenado explicitamente. Esta é a forma mais fácil de manipular, pois as instruções são autocontidas. Mover uma instrução durante a otimização é trivial, pois não há referências implícitas a ela.\nTriplas: para economizar espaço, podemos omitir o campo do resultado. Cada instrução é um registro com três campos: (operador, arg1, arg2). Os resultados intermediários não recebem nomes explícitos; em vez disso, eles são referenciados pela posição, ou índice, da instrução que os calcula. A principal desvantagem é que mover uma instrução durante a otimização se torna uma operação cara, pois todas as outras instruções que se referem a ela por seu índice precisariam ser atualizadas.\nTriplas Indiretas: esta abordagem combina os benefícios das duas anteriores. As triplas são armazenadas em um array, como antes. No entanto, a execução do programa é ditada por um segundo array, que contém ponteiros para as triplas. Para reordenar o código, o compilador simplesmente reorganiza os ponteiros neste segundo array, sem mover as triplas em si. Isso torna a otimização eficiente, mantendo a vantagem de espaço das triplas.\n\nA tabela #tbl-implementacoes-tac resume as características dessas implementações, incluindo considerações sobre consumo de memória.\n\nComparação das Implementações de Código de Três Endereços.\n\n\n\n\n\n\n\n\n\n\nImplementação\nEstrutura\nReferência ao Resultado\nConsumo de Memória\nVantagens\nDesvantagens\n\n\n\n\nQuádruplas\n(op, arg1, arg2, res)\nExplícita (campo res)\nBaseline (100%)\nFácil de mover/reordenar instruções.\nUsa mais espaço; temporários precisam ser gerenciados.\n\n\nTriplas\n(op, arg1, arg2)\nImplícita (índice da instrução)\n~75% (25% mais compacto)\nMais compacta em espaço.\nReordenar instruções é caro (exige atualização de referências).\n\n\nTriplas Indiretas\nLista de ponteiros para Triplas\nImplícita (via ponteiro)\n~80-85% (overhead de array de ponteiros)\nCompacta como Triplas, mas fácil de reordenar (só move ponteiros).\nNível extra de indireção pode adicionar pequena sobrecarga.\n\n\n\nPara ilustrar concretamente, considere uma instrução t1 := a + b. Uma quádrupla requer quatro campos (operador +, argumentos a e b, resultado t1), tipicamente entre \\(16\\) e \\(32 bytes\\) dependendo da implementação. Uma tripla armazena apenas três campos (operador +, argumentos a e b), resultando em um espaço entre \\(12\\) e \\(24 bytes\\). A tripla indireta adiciona o custo de um ponteiro, de \\(4\\) a \\(8 bytes\\) no array de indireção, mas esse custo extra é amortizado ao longo de muitas instruções e é amplamente compensado pela eficiência nas operações de reordenamento durante a otimização.\nA escolha entre essas representações envolve uma análise de balanceamento clássica entre espaço e flexibilidade. Compiladores modernos frequentemente optam por quádruplas ou uma variante delas durante as fases de otimização intensiva, na qual a facilidade de manipulação supera a preocupação com memória, especialmente considerando que o código intermediário é, na maior parte das vezes, transitório e existe apenas durante a compilação.\n\n\n15.3.3 Exemplos Práticos de Tradução\nA geração de Código de Três Endereços é formalmente especificada usando Esquemas de Tradução Dirigida por Sintaxe, ou em inglês Syntax Directed Translation, SDT. Um SDT anexa ações semânticas às produções de uma gramática, que geram o código intermediário à medida que a árvore de sintaxe é percorrida.\n\n15.3.3.1 Tradução de Expressões Aritméticas\nConsideremos a produção gramatical para uma adição: \\(E \\rightarrow E_1 + E_2\\). A ação semântica associada gera o código para \\(E_1\\), depois o código para \\(E_2\\), e finalmente emite uma nova instrução de adição.\nProdução: \\(S \\rightarrow \\textbf{id}:= E\\)\nAção Semântica: \\[\n\\begin{array}{l}\n\\text{S.code}:= \\text{E.code} \\ \\vert  \\\\\n\\qquad \\qquad \\text{gen}(\\text{id.place} \\, := \\, \\text{E.place})\n\\end{array}\n\\]\nProdução: \\(E \\rightarrow E_1 + E_2\\)\nAção Semântica: \\[\n\\begin{array}{l}\n\\text{E.place} \\ := \\text{newtemp}() \\\\\n\\text{E.code} \\ := \\text{E}_1\\text{.code} \\ \\vert  \\\\\n\\qquad \\qquad \\text{E}_2\\text{.code} \\ \\vert  \\\\\n\\qquad \\qquad \\text{gen}(\\text{E.place} \\, := \\, \\text{E}_1\\text{.place} \\, + \\, \\text{E}_2\\text{.place})\n\\end{array}\n\\]\nAqui, E.place é um atributo que armazena o nome do temporário ou variável que contém o valor de \\(E\\), e E.code armazena a sequência de instruções TAC geradas. A função newtemp() cria um novo nome de variável temporária.\n\n\n15.3.3.2 Tradução de Estruturas de Controle de Fluxo\nPara estruturas de controle, a tradução envolve a geração de rótulos (labels) e instruções de salto.\n\n15.3.3.2.1 Tradução de if-then-else:\nPara uma instrução if (B) then S1 else S2, precisamos de rótulos para o início do bloco then (B.true), o início do bloco else (B.false) e o final da instrução (S.next).\nProdução: \\(S \\rightarrow \\textbf{if} \\, (B) \\, S_1 \\, \\textbf{else} \\, S_2\\)\nAção Semântica: \\[\n\\begin{array}{l}\n\\text{B.true}:= \\text{newlabel}() \\\\\n\\text{B.false}:= \\text{newlabel}() \\\\\n\\text{S.next}:= \\text{newlabel}() \\\\\n\\text{S.code}:= \\text{B.code} \\ \\vert  \\\\\n\\qquad \\qquad \\text{gen}(\\text{B.true} \\ `:`) \\ \\vert  \\\\\n\\qquad \\qquad S_1\\text{.code} \\ \\vert  \\\\\n\\qquad \\qquad \\text{gen}(\\text{goto} \\ \\text{S.next}) \\ \\vert  \\\\\n\\qquad \\qquad \\text{gen}(\\text{B.false} \\ `:`) \\ \\vert  \\\\\n\\qquad \\qquad S_2\\text{.code} \\ \\vert  \\\\\n\\qquad \\qquad \\text{gen}(\\text{S.next} \\ `:`)\n\\end{array}\n\\]\nO código para a expressão booleana B.code é projetado para saltar para B.true se a condição for verdadeira e para B.false se for falsa.\n\n\n15.3.3.2.2 Tradução de while:\nPara uma instrução while (B) do S1, precisamos de um rótulo para o início do laço (S.begin), um para o corpo do laço (B.true) e um para o ponto de saída (B.false ou S.next).\nProdução: \\(S \\rightarrow \\textbf{while} \\, (B) \\, \\textbf{do} \\, S_1\\)\nAção Semântica: \\[\n\\begin{array}{l}\n\\text{S.begin}:= \\text{newlabel}() \\\\\n\\text{B.true}:= \\text{newlabel}() \\\\\n\\text{S.next}:= \\text{newlabel}() \\\\\n\\text{S.code}:= \\text{gen}(\\text{S.begin} \\ `:`) \\ \\vert  \\\\\n\\qquad \\qquad \\text{B.code} \\ \\vert  \\\\\n\\qquad \\qquad \\text{gen}(\\text{B.true} \\ `:`) \\ \\vert  \\\\\n\\qquad \\qquad S_1\\text{.code} \\ \\vert  \\\\\n\\qquad \\qquad \\text{gen}(`\\text{goto}` \\ \\text{S.begin}) \\ \\vert  \\\\\n\\qquad \\qquad \\text{gen}(\\text{S.next} \\ `:`)\n\\end{array}\n\\]\nNovamente, B.code é gerado para saltar para B.true ou S.next (que é B.false). Após a execução do corpo S1.code, um salto incondicional goto S.begin nos leva de volta ao teste da condição, fechando o laço.\n\n\n\n\n15.3.4 Exemplo Completo: Tradução de Função para TAC\nPara que a esforçada leitora possa solidificar seu entendimento sobre o Código de Três Endereços, vamos traduzir uma função completa de uma linguagem de alto nível para TAC, incluindo declaração, parâmetros, corpo e retorno. Este exemplo ilustrará como todas as construções se integram em uma tradução real.\n\n15.3.4.1 Função em Linguagem de Alto Nível\nConsidere a seguinte função em C que calcula o máximo entre dois números e retorna o dobro desse valor:\nint double_max(int a, int b) {\n  int max;\n  if (a &gt; b) {\n    max = a;\n  } else {\n    max = b;\n  }\n  int result = max * 2;\n  return result;\n}\nEsta função apresenta diversos elementos interessantes para tradução:\n\nDois parâmetros formais (a e b);\nDeclaração de variáveis locais (max e result);\nEstrutura de controle condicional (if-then-else);\nExpressões aritméticas (comparação e multiplicação);\nRetorno de valor.\n\n\n\n15.3.4.2 Tradução para Código de Três Endereços\nA tradução completa para TAC segue abaixo. Utilizamos rótulos para delimitar blocos, o padrão de comentários em linha do C e instruções específicas para gerenciar parâmetros e retorno:\n// Prólogo da função\nbegin_function double_max\n// Declaração de parâmetros (recebidos na pilha ou registradores)\nparam a\nparam b\n// Avaliação da condição: a &gt; b\nL1:\nt1 := a &gt; b\nif t1 goto L2\ngoto L3\n// Ramo THEN: a é maior\nL2:\nmax := a\ngoto L4\n// Ramo ELSE: b é maior ou igual\nL3:\nmax := b\ngoto L4\n// Ponto de convergência após o if-then-else\nL4:\nt2 := max * 2\nresult := t2\n// Retorno do valor\nreturn result\n// Epílogo da função\nend_function\nVamos examinar cada seção da tradução:\n1. Prólogo e Parâmetros:\nbegin_function double_max\nparam a\nparam b\nO prólogo marca o início da função. As instruções param indicam que a e b são parâmetros recebidos. Em uma implementação real, o compilador usaria estas marcações para gerar código que recupera os valores da pilha ou de registradores conforme a convenção de chamada da arquitetura-alvo.\n2. Estrutura Condicional\nL1:\nt1 := a &gt; b\nif t1 goto L2\ngoto L3\nA condição a &gt; b é avaliada e armazenada no temporário t1. A instrução if t1 goto L2 implementa o salto condicional: se verdadeiro, executa o ramo then (L2); caso contrário, o goto L3 explícito direciona para o ramo else.\n3. Blocos Then e Else\nL2:\nmax := a\ngoto L4\nL3:\nmax := b\ngoto L4\nCada ramo realiza a atribuição apropriada a max. Note que ambos terminam com goto L4, convergindo para o ponto de continuação comum. Este padrão é característico da tradução de estruturas condicionais.\n4. Continuação e Retorno\nL4:\nt2 := max * 2\nresult := t2\nreturn result\nend_function\nApós o if-then-else, o código calcula o dobro de max. A multiplicação é armazenada em t2 e depois copiada para result. A instrução return especifica o valor de retorno da função.\n\n\n15.3.4.3 Implementação em Quádruplas\nPara tornar a tradução ainda mais concreta, vejamos como estas instruções seriam representadas, na tabela #tbl-implementacoes-tac2, usando a implementação de quádruplas:\n\nRepresentação em Quádruplas do Código de Três Endereços para double_max.\n\n\nÍndice\nOperador\nArg1\nArg2\nResultado\n\n\n\n\n0\nbegin_func\ndouble_max\n-\n-\n\n\n1\nparam\na\n-\n-\n\n\n2\nparam\nb\n-\n-\n\n\n3\n&gt;\na\nb\nt1\n\n\n4\nif_true\nt1\nL2\n-\n\n\n5\ngoto\nL3\n-\n-\n\n\n6\nlabel\nL2\n-\n-\n\n\n7\n:=\na\n-\nmax\n\n\n8\ngoto\nL4\n-\n-\n\n\n9\nlabel\nL3\n-\n-\n\n\n10\n:=\nb\n-\nmax\n\n\n11\ngoto\nL4\n-\n-\n\n\n12\nlabel\nL4\n-\n-\n\n\n13\n*\nmax\n2\nt2\n\n\n14\n:=\nt2\n-\nresult\n\n\n15\nreturn\nresult\n-\n-\n\n\n16\nend_func\n-\n-\n-\n\n\n\nEsta representação tabular torna explícito como cada campo da quádrupla é preenchido. Note que operações unárias e de controle de fluxo deixam alguns campos vazios, marcados com -.\n\n\n15.3.4.4 Implementação em Triplas\nNa representação por triplas, cada instrução é composta por três campos: operador, argumento 1 e argumento 2. O resultado de uma operação é implícito e representado pelo índice da própria tripla. Variáveis temporárias e locais apontam para esses índices. Operações de controle de fluxo (como goto e if) usam índices de rótulos como alvos. A tabela abaixo adapta a tradução anterior para triplas, mantendo a mesma sequência lógica das quádruplas originais:\n\n\n\nTable 15.1: Representação em Triplas do Código de Três Endereços para double_max. Aqui, max aponta para o índice 7 ou 10 (dependendo do ramo), t1 para 3, t2 para 13 e result para 14. Os alvos de saltos (como (6) para L2) referenciam os índices das triplas de rótulos. Campos vazios são marcados com -.\n\n\n\n\n\nÍndice\nOperador\nArg1\nArg2\n\n\n\n\n0\nbegin_func\ndouble_max\n-\n\n\n1\nparam\na\n-\n\n\n2\nparam\nb\n-\n\n\n3\n&gt;\na\nb\n\n\n4\nif_true\n(3)\n(6)\n\n\n5\ngoto\n(9)\n-\n\n\n6\nlabel\nL2\n-\n\n\n7\n:=\na\n-\n\n\n8\ngoto\n(12)\n-\n\n\n9\nlabel\nL3\n-\n\n\n10\n:=\nb\n-\n\n\n11\ngoto\n(12)\n-\n\n\n12\nlabel\nL4\n-\n\n\n13\n*\nmax\n2\n\n\n14\n:=\n(13)\n-\n\n\n15\nreturn\n(14)\n-\n\n\n16\nend_func\n-\n-\n\n\n\n\n\n\nEsta estrutura economiza espaço em comparação às quádruplas, pois o resultado é o índice da tripla, mas requer uma tabela de símbolos para mapear variáveis aos índices apropriados.\n\n\n15.3.4.5 Implementação em Triplas Indiretas\nAs triplas indiretas adicionam uma camada de indireção: uma tabela principal de ponteiros (índices) aponta para entradas em uma tabela de triplas reais (onde ficam o operador, arg1 e arg2). Isso facilita otimizações, como a remoção de instruções mortas, ao permitir a substituição de ponteiros sem alterar as triplas subjacentes. A tabela abaixo mostra a estrutura para o mesmo exemplo:\nTabela de Ponteiros (Índices Indiretos):\n\n\n\nÍndice\nPonteiro para Tripla\n\n\n\n\n0\n0\n\n\n1\n1\n\n\n2\n2\n\n\n3\n3\n\n\n4\n4\n\n\n5\n5\n\n\n6\n6\n\n\n7\n7\n\n\n8\n8\n\n\n9\n9\n\n\n10\n10\n\n\n11\n11\n\n\n12\n12\n\n\n13\n13\n\n\n14\n14\n\n\n15\n15\n\n\n16\n16\n\n\n\nTabela de Triplas Reais (acessadas via ponteiros):\n\n\n\nTable 15.2: Representação em Triplas Indiretas do Código de Três Endereços para double_max. A tabela de ponteiros mapeia os índices lógicos para as triplas reais, permitindo flexibilidade em otimizações. As referências (como (3)) apontam para índices na tabela de ponteiros, que por sua vez levam às triplas.\n\n\n\n\n\nÍndice da Tripla\nOperador\nArg1\nArg2\n\n\n\n\n0\nbegin_func\ndouble_max\n-\n\n\n1\nparam\na\n-\n\n\n2\nparam\nb\n-\n\n\n3\n&gt;\na\nb\n\n\n4\nif_true\n(3)\n(6)\n\n\n5\ngoto\n(9)\n-\n\n\n6\nlabel\nL2\n-\n\n\n7\n:=\na\n-\n\n\n8\ngoto\n(12)\n-\n\n\n9\nlabel\nL3\n-\n\n\n10\n:=\nb\n-\n\n\n11\ngoto\n(12)\n-\n\n\n12\nlabel\nL4\n-\n\n\n13\n*\nmax\n2\n\n\n14\n:=\n(13)\n-\n\n\n15\nreturn\n(14)\n-\n\n\n16\nend_func\n-\n-\n\n\n\n\n\n\nEssa representação é particularmente útil em compiladores que realizam análises de fluxo de dados, pois modificar um ponteiro pode “desconectar” uma tripla sem afetar outras partes do código.\n\n\n15.3.4.6 Variações e Otimizações\nVale mencionar que esta tradução é deliberadamente não otimizada, preservando a estrutura do código original. Um otimizador poderia aplicar várias transformações:\n\nEliminação de Cópia: a atribuição result := t2 é redundante; poderíamos retornar t2 diretamente.\nPropagação de Constantes: se os valores de a e b fossem conhecidos em tempo de compilação, a comparação e toda a estrutura condicional poderiam ser resolvidas estaticamente.\nDobramento de Constantes: a multiplicação por 2 poderia ser substituída por uma operação de deslocamento à esquerda (max &lt;&lt; 1), que é mais eficiente em muitas arquiteturas.\n\nA versão otimizada poderia se reduzir a algo como:\nbegin_function double_max\nparam a\nparam b\nt1 := a &gt; b\nif t1 goto L2\nt2 := b\ngoto L3\nL2:\nt2 := a\nL3:\nt3 := t2 &lt;&lt; 1\nreturn t3\nend_function\nEste exemplo completo demonstra como o TAC serve como uma ponte entre a abstração da linguagem de alto nível e a concretude do código de máquina, mantendo uma estrutura simples e regular que facilita tanto a geração quanto a otimização.\n\n\n\n15.3.5 Exemplo Completo: Tradução de Função com Laço While para TAC\nPara ilustrar a tradução de uma estrutura de controle com laço while, vamos criar e resolver um exemplo simples em C. A função calcula a soma dos números de 1 até n (soma triangular), usando um laço while. Este exemplo integra:\n\nParâmetro de entrada (n).\nDeclarações de variáveis locais (sum e i).\nInicialização de variáveis.\nCondição booleana no while.\nOperações aritméticas dentro do laço (adição e incremento).\nRetorno do resultado.\n\nSeguiremos o mesmo padrão do exemplo anterior (double_max): - Código C original. - Tradução para TAC em formato textual (com rótulos, temporários e saltos). - Análise seção por seção. - Representação em Quádruplas (tabela). - Representação em Triplas (tabela). - Representação em Triplas Indiretas (tabelas). - Uma versão otimizada breve para comparação.\n\n15.3.5.1 Função em Linguagem de Alto Nível (C)\nint sum_up_to(int n) {\n    int sum = 0;\n    int i = 1;\n    while (i &lt;= n) {\n        sum = sum + i;\n        i = i + 1;\n    }\n    return sum;\n}\n\nDestaques: Parâmetro n, inicializações, condição i &lt;= n, corpo do laço com adição e incremento, retorno.\n\n\n\n15.3.5.2 Tradução para Código de Três Endereços (TAC)\nA tradução completa para TAC segue abaixo. Utilizamos rótulos para delimitar o início do laço (L2), o corpo (L3) e a saída (L4). O código é gerado via SDT, como nos exemplos anteriores (percorrendo a AST bottom-up).\n// Prólogo da função\nbegin_function sum_up_to\n// Declaração de parâmetro\nparam n\n// Inicializações\nL1:\nsum := 0\ni := 1\n// Início do while: teste da condição\nL2:\nt1 := i &lt;= n\nif t1 goto L3\ngoto L4\n// Corpo do while\nL3:\nt2 := sum + i\nsum := t2\nt3 := i + 1\ni := t3\n// Volta ao teste da condição\ngoto L2\n// Saída do while e retorno\nL4:\nreturn sum\n// Epílogo da função\nend_function\n\n\n15.3.5.3 Análise da Tradução TAC\nVamos examinar cada seção da tradução, destacando como o SDT gera o código para o while (similar ao exemplo de SDT para while (B) do S1):\n1. Prólogo e Parâmetro:\nbegin_function sum_up_to\nparam n\n\nMarca o início da função. param n indica o parâmetro recebido (da pilha ou registradores).\n\n2. Inicializações:\nL1:\nsum := 0\ni := 1\n\nAtribuições simples antes do laço. L1 é o ponto de entrada após as inicializações.\n\n3. Estrutura do While:\nL2:\nt1 := i &lt;= n\nif t1 goto L3\ngoto L4\n\nL2: Rótulo do teste da condição (B.code no SDT: avalia i &lt;= n em t1).\nSalto condicional: Se verdadeiro (t1), vai para o corpo (L3 = B.true); senão, sai (L4 = S.next = B.false).\n\n4. Corpo do While:\nL3:\nt2 := sum + i\nsum := t2\nt3 := i + 1\ni := t3\ngoto L2\n\nL3: Rótulo do corpo (B.true no SDT).\nAdição: sum + i em temporário t2, depois atribui a sum.\nIncremento: i + 1 em t3, depois atribui a i.\ngoto L2: Volta ao teste da condição (como no SDT: gen(goto S.begin)).\n\n5. Saída e Retorno:\nL4:\nreturn sum\nend_function\n\nL4: Ponto de convergência após o laço.\nreturn sum: Retorna o valor acumulado.\n\nEste TAC é não otimizado, preservando a estrutura original para clareza. O laço forma um ciclo via saltos: L2 → L3 → L2 (se condição verdadeira).\n\n\n15.3.5.4 Implementação em Quádruplas\nRepresentação tabular em quádruplas (cada linha: (operador, arg1, arg2, resultado)). Campos vazios: -.\n\n\n\nÍndice\nOperador\nArg1\nArg2\nResultado\n\n\n\n\n0\nbegin_func\nsum_up_to\n-\n-\n\n\n1\nparam\nn\n-\n-\n\n\n2\nlabel\nL1\n-\n-\n\n\n3\n:=\n0\n-\nsum\n\n\n4\n:=\n1\n-\ni\n\n\n5\nlabel\nL2\n-\n-\n\n\n6\n&lt;=\ni\nn\nt1\n\n\n7\nif_true\nt1\nL3\n-\n\n\n8\ngoto\nL4\n-\n-\n\n\n9\nlabel\nL3\n-\n-\n\n\n10\n+\nsum\ni\nt2\n\n\n11\n:=\nt2\n-\nsum\n\n\n12\n+\ni\n1\nt3\n\n\n13\n:=\nt3\n-\ni\n\n\n14\ngoto\nL2\n-\n-\n\n\n15\nlabel\nL4\n-\n-\n\n\n16\nreturn\nsum\n-\n-\n\n\n17\nend_func\n-\n-\n-\n\n\n\n\nVantagem: Resultados explícitos (t1, t2, etc.); fácil reordenar durante otimizações.\n\n\n\n15.3.5.5 Implementação em Triplas\nEm triplas ((operador, arg1, arg2)), o resultado é o índice da tripla. Variáveis mapeiam para índices (ex.: sum aponta para 3 ou 11; t1 = 6).\n\n\n\nÍndice\nOperador\nArg1\nArg2\n\n\n\n\n0\nbegin_func\nsum_up_to\n-\n\n\n1\nparam\nn\n-\n\n\n2\nlabel\nL1\n-\n\n\n3\n:=\n0\n-\n\n\n4\n:=\n1\n-\n\n\n5\nlabel\nL2\n-\n\n\n6\n&lt;=\ni\nn\n\n\n7\nif_true\n(6)\n(9)\n\n\n8\ngoto\n(15)\n-\n\n\n9\nlabel\nL3\n-\n\n\n10\n+\nsum\ni\n\n\n11\n:=\n(10)\n-\n\n\n12\n+\ni\n1\n\n\n13\n:=\n(12)\n-\n\n\n14\ngoto\n(5)\n-\n\n\n15\nlabel\nL4\n-\n\n\n16\nreturn\nsum\n-\n\n\n17\nend_func\n-\n-\n\n\n\n\nObservação: Referências como (6) apontam para o índice da tripla que define o valor. Reordenar exige atualizar todos os ponteiros.\n\n\n\n15.3.5.6 Implementação em Triplas Indiretas\nAdiciona uma tabela de ponteiros (índices lógicos → triplas reais) para facilitar otimizações (mover ponteiros sem alterar triplas).\nTabela de Ponteiros (Índices Indiretos):\n\n\n\nÍndice\nPonteiro para Tripla\n\n\n\n\n0\n0\n\n\n1\n1\n\n\n2\n2\n\n\n3\n3\n\n\n4\n4\n\n\n5\n5\n\n\n6\n6\n\n\n7\n7\n\n\n8\n8\n\n\n9\n9\n\n\n10\n10\n\n\n11\n11\n\n\n12\n12\n\n\n13\n13\n\n\n14\n14\n\n\n15\n15\n\n\n16\n16\n\n\n17\n17\n\n\n\nTabela de Triplas Reais (acessadas via ponteiros):\n\n\n\nÍndice da Tripla\nOperador\nArg1\nArg2\n\n\n\n\n0\nbegin_func\nsum_up_to\n-\n\n\n1\nparam\nn\n-\n\n\n2\nlabel\nL1\n-\n\n\n3\n:=\n0\n-\n\n\n4\n:=\n1\n-\n\n\n5\nlabel\nL2\n-\n\n\n6\n&lt;=\ni\nn\n\n\n7\nif_true\n(6)\n(9)\n\n\n8\ngoto\n(15)\n-\n\n\n9\nlabel\nL3\n-\n\n\n10\n+\nsum\ni\n\n\n11\n:=\n(10)\n-\n\n\n12\n+\ni\n1\n\n\n13\n:=\n(12)\n-\n\n\n14\ngoto\n(5)\n-\n\n\n15\nlabel\nL4\n-\n\n\n16\nreturn\nsum\n-\n\n\n17\nend_func\n-\n-\n\n\n\n\nVantagem: Otimizações como remoção de código morto só reorganizam ponteiros.\n\n\n\n15.3.5.7 Variações e Otimizações\nEsta tradução é deliberadamente não otimizada. Um otimizador poderia aplicar:\n\nEliminação de Cópias: Remover sum := t2 e i := t3 (usar resultados diretamente).\nIndução de Laço: Reconhecer o incremento de i como padrão e transformar em soma fechada (fórmula: n*(n+1)/2).\nDobrar Constantes: i + 1 poderia ser otimizado em registradores.\n\nVersão Otimizada (exemplo simplificado, após eliminação de cópias e força de laço):\nbegin_function sum_up_to\nparam n\nt1 := n * (n + 1)\nt2 := t1 / 2  // Soma triangular fechada\nreturn t2\nend_function\n\nImpacto: De ~18 instruções para 4 – Muito mais eficiente para execução!\n\nEste exemplo reforça como o TAC captura laços via rótulos e saltos, facilitando análises de fluxo em otimizadores. Se quiser variações (ex.: com arrays ou mais complexidade), é só pedir!",
    "crumbs": [
      "Geração de Código Intermediário",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Geração de Código Intermediário: A Linguagem Universal do Compilador</span>"
    ]
  },
  {
    "objectID": "14-GeraInter.html#representações-intermediárias-modernas-o-ecossistema-llvm",
    "href": "14-GeraInter.html#representações-intermediárias-modernas-o-ecossistema-llvm",
    "title": "15  Geração de Código Intermediário: A Linguagem Universal do Compilador",
    "section": "15.4 Representações Intermediárias Modernas: O Ecossistema LLVM",
    "text": "15.4 Representações Intermediárias Modernas: O Ecossistema LLVM\nEnquanto o Código de Três Endereços representa os fundamentos clássicos das RIs, o cenário moderno é dominado pela infraestrutura LLVM. É crucial que a leitora compreenda o LLVM não apenas como um formato de RI, mas como um ecossistema completo que redefiniu a construção de compiladores.\n\n15.4.1 LLVM como Infraestrutura\nO LLVM é uma coleção de bibliotecas modulares e reutilizáveis escritas em C++ que fornecem todas as fases de um back-end de compilador: otimização, geração de código, montagem, etc.. A LLVM Intermediate Representation (LLVM IR) é a linguagem que une todas essas componentes. Qualquer linguagem que possa ser traduzida para LLVM IR pode, instantaneamente, se beneficiar de seu otimizador de classe mundial e de seus back-ends que suportam dezenas de arquiteturas de hardware.\n\n\n15.4.2 Características da LLVM IR\nA LLVM IR possui várias características que a tornam extremamente poderosa. Cada uma dessas propriedades foi cuidadosamente projetada para atender aos requisitos de um compilador moderno de alto desempenho.\n\n15.4.2.1 Representação Textual Legível\nUma das decisões de design mais impactantes do LLVM foi tornar sua representação intermediária legível por humanos. A LLVM IR pode ser serializada em três formatos equivalentes:\n\nFormato Textual (.ll): Uma representação em ASCII que se assemelha a uma linguagem Assembly de alto nível, com sintaxe clara e tipagem explícita.\nFormato Bitcode (.bc): Uma representação binária compacta, otimizada para armazenamento e transmissão eficiente.\nRepresentação em Memória: Estruturas de dados C++ usadas internamente pelo compilador durante a otimização.\n\nO formato textual é particularmente valioso para o desenvolvimento e depuração de compiladores. Considere este exemplo de uma função simples:\ndefine i32 @factorial(i32 %n) {\nentry:\n  %cmp = icmp sle i32 %n, 1\n  br i1 %cmp, label %base_case, label %recursive_case\n\nbase_case:\n  ret i32 1\n\nrecursive_case:\n  %n_minus_1 = sub i32 %n, 1\n  %rec_result = call i32 @factorial(i32 %n_minus_1)\n  %result = mul i32 %n, %rec_result\n  ret i32 %result\n}\nMesmo sem conhecimento prévio de LLVM IR, um programador experiente pode compreender rapidamente a estrutura desta função: ela compara %n com 1, ramifica para um caso base ou recursivo, e realiza uma multiplicação. Esta legibilidade facilita:\n\nDepuração de Transformações: Desenvolvedores de otimizações podem inspecionar visualmente o código antes e depois de cada passe de otimização, usando ferramentas como opt -S -pass-name.\nEducação e Experimentação: Estudantes e pesquisadores podem escrever LLVM IR manualmente para testar hipóteses sobre otimizações sem precisar construir um front-end completo.\nReprodução de Bugs: Quando um bug é encontrado, o código LLVM IR que o expõe pode ser extraído e compartilhado como um caso de teste mínimo, independentemente do código-fonte original.\n\nA conversão entre formatos é trivial: llvm-as converte .ll para .bc, e llvm-dis faz o caminho inverso. Esta flexibilidade é um diferencial significativo em relação a sistemas como a JVM, onde o bytecode é primariamente binário e ferramentas de desmontagem são necessárias para inspeção humana.\n\n\n15.4.2.2 Sistema de Tipos Forte e Verificação Estática\nDiferentemente de linguagens Assembly tradicionais, onde operações operam sobre registradores não tipados ou fracamente tipados, a LLVM IR implementa um sistema de tipos robusto e expressivo. Cada valor na LLVM IR possui um tipo explícito, e operações inválidas são rejeitadas estaticamente pelo verificador.\n\n15.4.2.2.1 Tipos Primitivos e Compostos\nA LLVM IR suporta uma hierarquia rica de tipos:\n\nTipos Inteiros: i1 (booleano), i8, i16, i32, i64, ou qualquer largura arbitrária como i128 ou i7.\nTipos de Ponto Flutuante: half (16 bits), float (32 bits), double (64 bits), fp128 (128 bits).\nTipos Ponteiro: i32* (ponteiro para inteiro de 32 bits), i8** (ponteiro para ponteiro).\nTipos Agregados: [10 x i32] (array de 10 inteiros), {i32, i8, double} (estrutura heterogênea).\nTipos Vetoriais: &lt;4 x float&gt; (vetor SIMD de 4 floats), essencial para otimizações de paralelismo de dados.\n\n\n\n15.4.2.2.2 Verificação em Tempo de Compilação\nO verificador de LLVM IR (opt -verify) executa uma série de verificações que garantem a consistência do código:\n; Exemplo VÁLIDO\n%ptr = alloca i32\nstore i32 42, i32* %ptr\n%value = load i32, i32* %ptr\n\n; Exemplo INVÁLIDO - erro de tipo detectado\n%ptr = alloca i32\nstore i64 42, i32* %ptr  ; ERRO: tentando armazenar i64 em ponteiro i32*\nSe tentarmos compilar o código inválido, o verificador rejeitará com uma mensagem clara:\nerror: stored value and pointer type do not match\nstore i64 42, i32* %ptr\nEsta verificação previne uma categoria inteira de bugs que, em Assembly tradicional, apenas se manifestariam em tempo de execução como corrupção de memória ou comportamento indefinido.\n\n\n15.4.2.2.3 Benefícios para Otimização\nO sistema de tipos não é apenas uma ferramenta de correção; ele habilita otimizações mais agressivas. Por exemplo:\n\nAnálise de Aliasing Baseada em Tipos (TBAA): O otimizador pode inferir que um ponteiro i32* e um ponteiro float* nunca apontam para a mesma localização de memória, permitindo reordenamento de acessos.\nVetorização Automática: Tipos vetoriais explícitos como &lt;4 x float&gt; permitem que o otimizador identifique oportunidades de usar instruções SIMD (SSE, AVX, NEON).\nPropagação de Informação de Tamanho: Saber que um tipo é i8 (1 byte) versus i64 (8 bytes) permite otimizações de layout de memória e eliminação de extensões de sinal redundantes.\n\nA tabela a #tbl-contrast1 ilustra o contraste entre Assembly tradicional x86-64 e LLVM IR tipada:\n\nContraste entre Assembly Tradicional e LLVM IR Tipada.\n\n\n\n\n\n\n\nAspecto\nAssembly x86-64\nLLVM IR\n\n\n\n\nDeclaração de Variável\nmov eax, 42 (tipo implícito pelo tamanho do registrador)\n%x = alloca i32 seguido de store i32 42, i32* %x\n\n\nDetecção de Erro de Tipo\nImpossível estaticamente; mov eax, qword [ptr] compilará mesmo que inconsistente\nVerificador rejeita store i64 42, i32* %ptr\n\n\nInformação para Otimização\nLimitada; otimizador infere tipos de padrões de uso\nExplícita; cada instrução carrega informação semântica completa\n\n\nPortabilidade\nEspecífico de x86-64\nIndependente de arquitetura; tipos mapeados para registradores/memória no back-end\n\n\n\nEm resumo, o sistema de tipos da LLVM IR eleva a representação intermediária de uma simples linguagem de instruções para uma linguagem com semântica bem definida e verificável, servindo simultaneamente como proteção contra erros e como fonte de informação para otimizações sofisticadas.\n\n\n\n\n15.4.3 Forma de Atribuição Única Estática (SSA)\nA Forma de Atribuição Única Estática (SSA) é a característica mais distintiva e poderosa da LLVM IR. Para compreender sua essência, vamos primeiro observar como o código tradicional difere do código em forma SSA.\n\n15.4.3.1 Transformação Básica: Código Não-SSA para SSA\nConsidere o seguinte fragmento de código em uma representação intermediária tradicional (não-SSA):\nx := 5\ny := x + 3\nx := y * 2\nz := x + y\nNeste código, a variável x recebe dois valores diferentes: primeiro 5, depois o resultado de y * 2. Em uma análise de fluxo de dados, para determinar qual valor de x é usado no cálculo de z, seria necessário rastrear todas as atribuições anteriores e determinar qual delas “alcança” o uso de x na última linha.\nAgora, vejamos a transformação deste mesmo código para a forma SSA:\nx₁ := 5\ny₁ := x₁ + 3\nx₂ := y₁ * 2\nz₁ := x₂ + y₁\nNa forma SSA, cada atribuição cria uma nova versão da variável, identificada por um subscrito único. Agora, a relação entre definições e usos é cristalina: z₁ usa inequivocamente x₂ e y₁. Não há ambiguidade, não há necessidade de análise complexa de fluxo de dados. O grafo de dependências de dados é explícito nos próprios nomes das variáveis.\n\n\n15.4.3.2 Lidando com Fluxo de Controle: A Função-Φ\nA transformação acima funciona perfeitamente para código com fluxo linear. Mas o que acontece quando há desvios condicionais? Considere este exemplo:\n// Código original (não-SSA)\nx := 1\nif (condição) then\n    x := 10\nelse\n    x := 20\nendif\ny := x + 5\nAqui, a variável x pode ter dois valores diferentes quando alcança a instrução y := x + 5, dependendo de qual ramo do if foi executado. Como representar isso em SSA, onde cada variável pode ser atribuída apenas uma vez?\nA resposta é a função-Φ (phi). A transformação para SSA introduz versões distintas de x em cada ramo e usa uma instrução especial phi para “mesclar” os valores:\n// Código em SSA\nx₁ := 1\nif (condição) goto L1 else goto L2\n\nL1:\n    x₂ := 10\n    goto L3\n\nL2:\n    x₃ := 20\n    goto L3\n\nL3:\n    x₄ := φ(x₂, x₃)\n    y₁ := x₄ + 5\nA instrução x₄ := φ(x₂, x₃) é colocada no ponto de convergência dos dois caminhos de controle (o bloco L3). Ela seleciona um valor dentre seus argumentos: se o controle veio de L1, x₄ recebe o valor de x₂; se veio de L2, x₄ recebe o valor de x₃. Formalmente, uma função-φ tem a forma:\n\\[x_i := \\phi(x_j \\text{ de } B_1, x_k \\text{ de } B_2, \\ldots, x_m \\text{ de } B_n)\\]\nna qual \\(B_1, B_2, \\ldots, B_n\\) são os blocos predecessores do bloco atual no grafo de fluxo de controle.\n\n\n15.4.3.3 Por Que SSA Simplifica as Otimizações\nA forma SSA transforma problemas complexos de análise de fluxo de dados em simples travessias de grafos de dependência. Por exemplo:\n\nPropagação de Constantes: Se x₁ := 5 e todas as referências usam x₁, sabemos imediatamente que o valor é constante. Não há necessidade de verificar se alguma atribuição posterior poderia modificar x.\nEliminação de Código Morto: Se uma variável SSA x₇ é definida mas nunca usada, podemos removê-la com segurança. A análise de vivacidade torna-se trivial.\nDetecção de Invariantes de Laço: Se uma variável dentro de um laço é definida apenas por uma função-φ que escolhe entre um valor externo e ela mesma, isso indica um possível invariante.\n\nEm resumo, a SSA eleva a representação intermediária a um nível onde a semântica do fluxo de dados está codificada estruturalmente nos nomes das variáveis, eliminando uma camada inteira de análise que seria necessária em representações tradicionais. Esta é a razão pela qual a SSA é a base de praticamente todos os compiladores otimizadores modernos, incluindo GCC, LLVM, e JVMs de alto desempenho.\n\n\n\n15.4.4 Exemplo Prático: C++ para LLVM IR\nVamos observar um exemplo simples. Considere a seguinte função em C++:\nint add(int a, int b) {\n int result = a + b;\n return result;\n}\nPodemos usar o Clang (o front-end C++ para LLVM) para gerar a LLVM IR correspondente. O comando clang++ -S -emit-llvm -O0 exemplo.cpp produzirá um arquivo exemplo.ll. O -O0 desabilita as otimizações para que possamos ver o código não otimizado, que é mais próximo da tradução direta. O conteúdo será semelhante a este:\n;ModuleID = 'exemplo.cpp'\ntarget triple = \"x86_64-pc-linux-gnu\"\n\n; Function Attrs: noinline nounwind optnone uwtable\ndefine dso_local i32 @_Z3addii(i32 %a, i32 %b) #0 {\n entry:\n %a.addr = alloca i32, align 4\n %b.addr = alloca i32, align 4\n %result = alloca i32, align 4\n store i32 %a, i32* %a.addr, align 4\n store i32 %b, i32* %b.addr, align 4\n %0 = load i32, i32* %a.addr, align 4\n %1 = load i32, i32* %b.addr, align 4\n %add = add nsw i32 %0, %1\n store i32 %add, i32* %result, align 4\n %2 = load i32, i32* %result, align 4\n ret i32 %2\n}\nAnalisando o código não otimizado:\n\ndefine... @_Z3addii(i32 %a, i32 %b): Define a função add, que recebe dois inteiros de 32 bits (%a, %b) e retorna um i32. O nome @_Z3addii é a versão “mangled” do nome da função C++.\nalloca i32: Aloca espaço na pilha para as variáveis locais a, b e result.\nstore i32 %a, i32* %a.addr: Armazena o valor do parâmetro de entrada %a no espaço alocado na pilha %a.addr.\n%0 = load i32, i32* %a.addr: Carrega o valor de a da pilha para um novo registrador virtual, %0. Note a forma SSA: %0 é um novo “nome”.\n%add = add nsw i32 %0, %1: Soma os valores dos registradores %0 e %1 e armazena o resultado no registrador %add.\nret i32 %2: Retorna o valor final.\n\nAgora, vejamos o que acontece quando habilitamos as otimizações com o comando clang++ -S -emit-llvm -O2 exemplo.cpp:\n; ModuleID = 'exemplo.cpp'\ntarget triple = \"x86_64-pc-linux-gnu\"\n\n; Function Attrs: mustprogress nofree norecurse nosync nounwind readnone willreturn uwtable\ndefine dso_local i32 @_Z3addii(i32 %a, i32 %b) local_unnamed_addr #0 {\n entry:\n %add = add nsw i32 %b, %a\n ret i32 %add\n}\nA transformação é impressionante. O otimizador LLVM aplicou múltiplas otimizações:\n\nEliminação de Alocações de Memória: As três instruções alloca foram completamente removidas. O otimizador detectou que as variáveis nunca precisavam residir na memória e poderiam permanecer em registradores virtuais.\nEliminação de Cargas e Armazenamentos Redundantes: Todas as operações store e load foram eliminadas. Os valores dos parâmetros %a e %b já estão disponíveis como registradores virtuais e podem ser usados diretamente.\nPropagação de Cópias: A variável intermediária result foi reconhecida como uma cópia desnecessária do resultado da adição e foi eliminada.\nPromoção de Memória para Registrador: Esta otimização, chamada mem2reg, é uma das mais fundamentais no LLVM. Ela transforma acessos à memória local em operações puramente sobre registradores virtuais, que são muito mais eficientes.\n\nO código otimizado contém apenas duas instruções: a adição e o retorno. Passamos de 12 linhas de código intermediário para apenas 2, sem perder nenhuma funcionalidade. Esta redução não é apenas cosmética: cada instrução eliminada representa ciclos de CPU economizados, menos pressão sobre o cache e, potencialmente, menos registradores físicos necessários na arquitetura-alvo. Os atributos de função adicionados (nofree, norecurse, nosync, nounwind, readnone, willreturn) são metadados que informam otimizações posteriores sobre propriedades garantidas da função, permitindo transformações ainda mais agressivas em pontos de chamada.\nEste exemplo, embora simples, ilustra o poder da infraestrutura de otimização do LLVM. Mesmo funções triviais se beneficiam significativamente, e em código real com estruturas de controle complexas e múltiplas funções, o impacto acumulativo dessas otimizações pode resultar em melhorias de desempenho de 2x a 10x ou mais.",
    "crumbs": [
      "Geração de Código Intermediário",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Geração de Código Intermediário: A Linguagem Universal do Compilador</span>"
    ]
  },
  {
    "objectID": "14-GeraInter.html#conclusão-a-ponte-para-a-otimização-de-código",
    "href": "14-GeraInter.html#conclusão-a-ponte-para-a-otimização-de-código",
    "title": "15  Geração de Código Intermediário: A Linguagem Universal do Compilador",
    "section": "15.5 Conclusão: A Ponte para a Otimização de Código",
    "text": "15.5 Conclusão: A Ponte para a Otimização de Código\nNeste capítulo, realizamos a transição crucial da fase de análise para a fase de síntese do compilador. Partimos de uma ASTA semanticamente rica e a traduzimos para uma Representação Intermediária — uma linguagem universal projetada não apenas para desacoplar o front-end do back-end, mas, fundamentalmente, para servir de alicerce para a melhoria do código.\nA geração de CI, seja na forma clássica do Código de Três Endereços ou na moderna LLVM IR, não é um fim em si mesma. É a preparação do terreno, a transformação do programa em uma forma estruturada, linear e explícita que é ideal para análise e manipulação algorítmica. A estrutura limpa do CI é a base sobre a qual as fases de otimização de código irão operar para tornar os programas mais rápidos, menores e mais eficientes.\nCom o nosso programa agora representado nesta forma poderosa, estamos prontos para dar o próximo passo. O próximo capítulo, “Otimização de Código”, explorará as diversas técnicas que podem ser aplicadas diretamente sobre o código intermediário que aprendemos a gerar, transformando um programa correto em um programa excelente.",
    "crumbs": [
      "Geração de Código Intermediário",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Geração de Código Intermediário: A Linguagem Universal do Compilador</span>"
    ]
  },
  {
    "objectID": "14-GeraInter.html#exercícios-propostos",
    "href": "14-GeraInter.html#exercícios-propostos",
    "title": "15  Geração de Código Intermediário: A Linguagem Universal do Compilador",
    "section": "15.6 Exercícios Propostos",
    "text": "15.6 Exercícios Propostos\n\n(Prático) Dado o seguinte trecho de código, traduza-o para Código de Três Endereços usando a representação de Quádruplas. Mostre explicitamente os rótulos e os saltos gerados.\n\nint total = 0;\nfor (int i = 0; i &lt; 10; i = i + 1) {\nif (i % 2 == 0) {\ntotal = total + i;\n} else {\ntotal = total - 1;\n}\n}\n\n(Teórico/Analítico) Explique, com um exemplo de código, por que a Forma SSA, utilizada no LLVM IR, simplifica a implementação da otimização de propagação de constantes em comparação com uma RI que permite múltiplas atribuições à mesma variável. Discuta o papel da função-\\(\\Phi\\) no seu exemplo.\n(Comparativo) Compare as vantagens e desvantagens de usar Triplas em vez de Quádruplas para representar o TAC, especificamente no contexto de um compilador que realiza muitas otimizações de reordenamento de código (como code motion). Por que as Triplas Indiretas são consideradas um bom meio-termo?\n(Interpretação) Analise o trecho de LLVM IR abaixo. Descreva, em alto nível, o que a função correspondente em C/C++ provavelmente faz. Identifique as instruções de alocação de memória na pilha, as operações aritméticas e o controle de fluxo.\n\ndefine i32 @mystery(i32 %n) {\nentry:\n%cmp = icmp sgt i32 %n, 1\nbr i1 %cmp, label %if.then, label %if.else\n\nif.then:\n%sub = sub nsw i32 %n, 1\n%call = call i32 @mystery(i32 %sub)\n%mul = mul nsw i32 %n, %call\nret i32 %mul\n\nif.else:\nret i32 1\n}\n\n(Design) Proponha um conjunto de regras de Tradução Dirigida por Sintaxe (SDT) para traduzir uma construção switch-case (com break) para Código de Três Endereços. Sua solução deve ser eficiente e, se possível, evitar uma cadeia de testes if-then-else sequenciais (sugestão: pense em como uma tabela de saltos poderia ser usada).",
    "crumbs": [
      "Geração de Código Intermediário",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Geração de Código Intermediário: A Linguagem Universal do Compilador</span>"
    ]
  },
  {
    "objectID": "15-llvmIR.html",
    "href": "15-llvmIR.html",
    "title": "16  Introdução à Representação Intermediária (IR) do LLVM com C++",
    "section": "",
    "text": "16.1 Configuração do Ambiente (Ubuntu, VS Code, C++ e LLVM)\nNesta seção, vamos ser um pouco mais detalhados, quase um tutorial de instalação, para garantir que a esforçada leitora tenha o ambiente correto para a criação deste conhecimento.\nOs passos a seguir foram testados em uma instalação limpa do Ubuntu 24.04 LTS, limpa, atualizada e rodando em uma máquina virtual VirtualBox.",
    "crumbs": [
      "Geração de Código Intermediário",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introdução à Representação Intermediária (IR) do LLVM com C++</span>"
    ]
  },
  {
    "objectID": "15-llvmIR.html#configuração-do-ambiente-ubuntu-vs-code-c-e-llvm",
    "href": "15-llvmIR.html#configuração-do-ambiente-ubuntu-vs-code-c-e-llvm",
    "title": "16  Introdução à Representação Intermediária (IR) do LLVM com C++",
    "section": "",
    "text": "16.1.1 Pré-requisitos\nAntes de começar, certifique-se de ter:\n\nUbuntu 24.04 LTS ou superior;\nConexão com internet;\nPrivilégios de superusuário (sudo);\nPelo menos 5GB de espaço livre em disco.\n\n\n\n16.1.2 Instalação do LLVM/Clang\nO repositório oficial do LLVM fornece as versões mais atualizadas das ferramentas.\nPasso 1: Baixar e preparar o script de instalação:\nwget https://apt.llvm.org/llvm.sh\nchmod +x llvm.sh\nPasso 2: Instalar a versão mais recente do LLVM:\n# Para instalar a versão 20 (substitua pelo número da versão mais recente)\nsudo ./llvm.sh 21\n\n# Instalar ferramentas adicionais úteis\nsudo apt install -y llvm-21-tools llvm-21-dev\nPasso 3: Configurar alternativas do sistema:\nCrie links simbólicos para usar os comandos sem especificar a versão:\n# Os comandos a seguir configuram os links simbólicos do sistema\n# para que os comandos gerais (clang, opt, etc.) apontem para \n# a versão 21 do LLVM/Clang com prioridade 150.\n\n# 1. Clang (Compilador C)\nsudo update-alternatives --install /usr/bin/clang `clang` /usr/bin/clang-21 150\n\n# 2. Clang++ (Compilador C++)\nsudo update-alternatives --install /usr/bin/clang++ clang++ /usr/bin/clang++-21 150\n\n# 3. Opt (Otimizador IR do LLVM)\nsudo update-alternatives --install /usr/bin/opt opt /usr/bin/opt-21 150\n\n# 4. llvm-dis (Desassemblador de Bitcode para IR de texto)\nsudo update-alternatives --install /usr/bin/llvm-dis llvm-dis /usr/bin/llvm-dis-21 150\n\n# 5. llvm-as (Assembler de IR de texto para Bitcode)\nsudo update-alternatives --install /usr/bin/llvm-as llvm-as /usr/bin/llvm-as-21 150\n\n# 6. llc (Compilador de Linguagem de Baixo Nível - Backend)\nsudo update-alternatives --install /usr/bin/llc llc /usr/bin/llc-21 150\n\n# 7. llvm-link (Ferramenta para unir módulos LLVM)\nsudo update-alternatives --install /usr/bin/llvm-link llvm-link /usr/bin/llvm-link-21 150\nPasso 4: Verificar a instalação:\nclang --version\nopt --version\nllvm-dis --version\nVocê deve ver algo como:\nclang version 20.x.x\nTarget: x86_64-pc-linux-gnu\n...\nPasso 5: Testar o opt com um exemplo rápido:\nVamos criar um teste direto na linha de comando para garantir que opt está funcionando:\n# Criar um arquivo C++ de teste mais complexo\ncat &gt; teste.cpp &lt;&lt; 'EOF'\nint calcular(int x) {\n    int a = x + 5;\n    int b = a * 2;\n    int c = b + 10;\n    int d = c * 3;\n    return d;\n}\n\nint main() {\n    int resultado = calcular(10);\n    return 0;\n}\nEOF\n\n# IMPORTANTE: Use -Xclang -disable-O0-optnone para evitar o atributo optnone, este atributo impede otimizações posteriores\n# Gerar IR não otimizado mas sem o atributo optnone\nclang++ -S -emit-llvm -O0 -Xclang -disable-O0-optnone teste.cpp -o teste.ll\n\n# Testar otimização\nopt -passes='default&lt;O3&gt;' -S teste.ll -o teste_opt.ll\n\n# Verificar se houve otimização\necho \"=== Antes da otimização ===\"\nwc -l teste.ll\necho \"=== Depois da otimização ===\"\nwc -l teste_opt.ll\n\n# Ver uma comparação lado a lado\necho -e \"\\n=== Função calcular ANTES da otimização ===\"\nsed -n '/@.*calcular/,/^}/p' teste.ll | head -20\necho -e \"\\n=== Função calcular DEPOIS da otimização ===\"\nsed -n '/@.*calcular/,/^}/p' teste_opt.ll\nSe tudo funcionou:\nA atenta leitora verá algo como:\n=== Função calcular ANTES da otimização ===\ndefine dso_local noundef i32 @_Z8calculari(i32 noundef %0) #0 {\n  %2 = alloca i32, align 4\n  %3 = alloca i32, align 4\n  %4 = alloca i32, align 4\n  %5 = alloca i32, align 4\n  %6 = alloca i32, align 4\n  store i32 %0, ptr %2, align 4\n  %7 = load i32, ptr %2, align 4\n  %8 = add nsw i32 %7, 5\n  store i32 %8, ptr %3, align 4\n  %9 = load i32, ptr %3, align 4\n  %10 = mul nsw i32 %9, 2\n  store i32 %10, ptr %4, align 4\n  %11 = load i32, ptr %4, align 4\n  %12 = add nsw i32 %11, 10\n  store i32 %12, ptr %5, align 4\n  %13 = load i32, ptr %5, align 4\n  %14 = mul nsw i32 %13, 3\n  store i32 %14, ptr %6, align 4\n  %15 = load i32, ptr %6, align 4\n\n=== Função calcular DEPOIS da otimização ===\ndefine dso_local noundef i32 @_Z8calculari(i32 noundef %0) local_unnamed_addr #0 {\n  %2 = mul i32 %0, 6\n  %3 = add i32 %2, 60\n  ret i32 %3\n}\n\nO arquivo otimizado deve ter significativamente menos linhas;\nAs operações intermediárias devem ter sido eliminadas, constant folding;\nAs instruções alloca, store e load devem ter desaparecido.\n\n\n\n\n\n\n\nNote\n\n\n\nProblema Comum: Atributo optnone\nNão é raro encontrar exemplos de compilação com a opção -O0 sem -Xclang -disable-O0-optnone. Neste caso, o clang adiciona o atributo optnone às funções. Este atributo instrui o opt a pular completamente a otimização dessas funções, mesmo que você tente otimizá-las depois!\nVerifique se suas funções têm este atributo:\ngrep \"optnone\" teste.ll\n\n\nSe você ver optnone, então o opt vai ignorar suas funções. Para corrigir, recompile com -Xclang -disable-O0-optnone.\n\n\n16.1.3 Instalação do VS Code e Extensões\nPasso 1: Instalar o VS Code:\n# Adicionar repositório da Microsoft\nwget -qO- https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor &gt; packages.microsoft.gpg\nsudo install -D -o root -g root -m 644 packages.microsoft.gpg /etc/apt/keyrings/packages.microsoft.gpg\nsudo sh -c 'echo \"deb [arch=amd64,arm64,armhf signed-by=/etc/apt/keyrings/packages.microsoft.gpg] https://packages.microsoft.com/repos/code stable main\" &gt; /etc/apt/sources.list.d/vscode.list'\n\n# Instalar\nsudo apt update\nsudo apt install code\nPasso 2: Instalar extensões essenciais:\nAbra o VS Code e instale as seguintes extensões (Ctrl+Shift+X):\n\nC/C++ Extension Pack (Microsoft)\nLLVM (LLVM Team) - Syntax highlighting para arquivos .ll\nCode Runner (Jun Han) - Execução rápida de código\nGraphviz Preview (Para visualizar CFGs)\n\nPasso 3: Instalar Graphviz (para visualização de CFG):\nsudo apt install graphviz\n\n\n16.1.4 Criar Diretório de Trabalho\nmkdir -p ~/llvm-aula\ncd ~/llvm-aula",
    "crumbs": [
      "Geração de Código Intermediário",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introdução à Representação Intermediária (IR) do LLVM com C++</span>"
    ]
  },
  {
    "objectID": "15-llvmIR.html#fundamentos-teóricos-do-llvm-ir",
    "href": "15-llvmIR.html#fundamentos-teóricos-do-llvm-ir",
    "title": "16  Introdução à Representação Intermediária (IR) do LLVM com C++",
    "section": "16.2 Fundamentos Teóricos do LLVM IR",
    "text": "16.2 Fundamentos Teóricos do LLVM IR\n\n16.2.1 O que é LLVM?\nLLVM (Low Level Virtual Machine) é uma infraestrutura de compilador modular e reutilizável. Apesar do nome, não é exatamente uma máquina virtual tradicional, mas sim um framework de compilação.\nComponentes principais: - Frontend: Converte código fonte (C++, C, etc.) em IR - Middle-end: Otimiza o IR independente de arquitetura - Backend: Converte IR em código de máquina específico\n\n\n16.2.2 O que é LLVM IR?\nA Representação Intermediária do LLVM é uma linguagem de programação de baixo nível que serve como ponte entre código de alto nível e código de máquina.\nCaracterísticas principais:\n\nFortemente Tipada: Cada valor tem um tipo explícito\nFormulário SSA (Static Single Assignment): Cada variável é atribuída exatamente uma vez\nAbstração de Plataforma: Independente de arquitetura específica\nFormato Triplo: Texto (.ll), Bitcode (.bc), e representação em memória\n\n\n\n16.2.3 Fluxo de Compilação Completo\nC++ Source Code (.cpp)\n        |\n        | clang++ -S -emit-llvm\n        v\nLLVM IR Text (.ll)\n        |\n        | opt -O2/-O3\n        v\nLLVM IR Optimized (.ll)\n        |\n        | llc\n        v\nAssembly (.s)\n        |\n        | clang++\n        v\nExecutable\n\n\n16.2.4 Conceitos Fundamentais do SSA\nNo formato SSA:\n\ncada variável tem apenas uma atribuição;\nusa-se notação com % para valores temporários;\nnós Phi (φ) para mesclar valores de diferentes caminhos de execução.\n\nConsidere o código a seguir:\nint x = 5;\nx = x + 3;\nx = x * 2;\nEm SSA (conceitual):\n%x1 = 5\n%x2 = %x1 + 3\n%x3 = %x2 * 2",
    "crumbs": [
      "Geração de Código Intermediário",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introdução à Representação Intermediária (IR) do LLVM com C++</span>"
    ]
  },
  {
    "objectID": "15-llvmIR.html#primeiro-exemplo-geração-e-análise-de-ir",
    "href": "15-llvmIR.html#primeiro-exemplo-geração-e-análise-de-ir",
    "title": "16  Introdução à Representação Intermediária (IR) do LLVM com C++",
    "section": "16.3 Primeiro Exemplo: Geração e Análise de IR",
    "text": "16.3 Primeiro Exemplo: Geração e Análise de IR\n\n16.3.1 Exemplo Mais Completo: Múltiplas Operações\nPara ver otimizações mais claramente, vamos usar um exemplo mais elaborado.\nCrie o arquivo exemplo01_completo.cpp:\n// exemplo01_completo.cpp\nint calcular(int x) {\n    int temp1 = x + 5;\n    int temp2 = temp1 * 2;\n    int temp3 = temp2 + 10;\n    int temp4 = temp3 * 3;\n    return temp4;\n}\n\nint main() {\n    int x = 10;\n    int resultado = calcular(x);\n    return 0;\n}\nEste exemplo é melhor porque:\n\ntem múltiplas operações que podem ser simplificadas;\nusa variáveis intermediárias que podem ser eliminadas;\nO resultado final pode ser calculado em tempo de compilação, constant folding.\n\n\n\n16.3.2 Gerar o LLVM IR\n\n\n\n\n\n\nNote\n\n\n\nIMPORTANTE sobre o atributo optnone\nQuando você compila com -O0, o clang adiciona o atributo optnone às funções criadas em representação intermediária. Este atributo instrui o LLVM a não otimizar essas funções, mesmo se você usar opt depois.\n\n\nPara gerar IR não otimizado mas que possa ser otimizado posteriormente, use uma destas abordagens:\nOpção 1: Desabilitar o atributo optnone (recomendada para estudo):\nclang++ -S -emit-llvm -O0 -Xclang -disable-O0-optnone exemplo01_completo.cpp -o exemplo01_completo.ll\nOpção 2: Usar -O1 e desabilitar passes do LLVM (alternativa):\nclang++ -S -emit-llvm -O1 -Xclang -disable-llvm-passes exemplo01_soma.cpp -o exemplo01_soma.ll\nOpção 3: Gerar IR otimizado diretamente (para comparação):\nclang++ -S -emit-llvm -O3 exemplo01_soma.cpp -o exemplo01_soma.ll\nNas três opções acima, a atenta leitora evitará os problemas causados pelo optnone. Além disso, nas três opções o arquivo gerado será um arquivo .ll legível. Nas três opções usamos os seguintes parâmetros:\n\n-S: gera assembly, ou IR neste caso;\n-emit-llvm: especifica que a saída deve ser IR do LLVM;\n-O0: sem otimização, mas adiciona optnone por padrão;\n-Xclang -disable-O0-optnone: remove o atributo optnone;\n-Xclang -disable-llvm-passes: desabilita passes de otimização do LLVM;\n-o: nome do arquivo de saída.\n\nPara verificar se uma função tem o atributo optnone:\nCaso a leitora ainda tenha dúvidas se o atributo optnone está presente, pode usar o comando:\ngrep \"optnone\" exemplo01_soma.ll\nSe você vir optnone na lista de atributos da função, o opt vai ignorar essa função completamente! E a desapontada leitora terá dois arquivos idênticos antes e depois da otimização.\n\n\n16.3.3 Análise Detalhada do IR Gerado\nSe a curiosa leitora abrir o arquivo exemplo01_completo.ll no VS Code. Você verá algo como:\n; ModuleID = 'exemplo01_soma.cpp'\nsource_filename = \"exemplo01_soma.cpp\"\ntarget datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128\"\ntarget triple = \"x86_64-pc-linux-gnu\"\n\n; Função soma\ndefine dso_local i32 @_Z4somaii(i32 noundef %a, i32 noundef %b) #0 {\nentry:\n  %add = add nsw i32 %a, %b\n  ret i32 %add\n}\n\n; Função main\ndefine dso_local i32 @main() #0 {\nentry:\n  %x = alloca i32, align 4\n  %y = alloca i32, align 4\n  %resultado = alloca i32, align 4\n  store i32 5, ptr %x, align 4\n  store i32 7, ptr %y, align 4\n  %0 = load i32, ptr %x, align 4\n  %1 = load i32, ptr %y, align 4\n  %call = call i32 @_Z4somaii(i32 noundef %0, i32 noundef %1)\n  store i32 %call, ptr %resultado, align 4\n  ret i32 0\n}\nNeste ponto, a leitora deve se familiarizar com os principais elementos que compõem o LLVM IR:\n\nMetadados do Módulo\n\ntarget datalayout: layout de dados da arquitetura alvo;\ntarget triple: especificação da plataforma (OS, arquitetura);\n\nTipos de Dados\n\ni32: inteiro de 32 bits;\nptr: ponteiro genérico;\nvoid: sem valor de retorno;\n\nNome Mangling\n\n@_Z4somaii: nome decorado da função soma;\nC++ usa name mangling para suportar sobrecarga de funções;\n\n:::{.callout-note} Name Mangling, do inglês decoração de nomes, é uma técnica que o compilador C++ usa para transformar nomes de funções em identificadores únicos. Esta técnica é necessária porque o C++ permite sobrecarga de funções (múltiplas funções com o mesmo nome mas parâmetros diferentes):\nint calcular(int x);\nint calcular(int x, int y);\nfloat calcular(float x);\nNo código de máquina (ou IR), cada função precisa de um nome único. O compilador “decora” o nome incluindo informações sobre os parâmetros. Por exemplo:\nint calcular(int x);\nNo LLVM IR teremos:\n@_Z8calculari\nNo qual temos:\n\n_Z = prefixo padrão;\n8 = tamanho do nome “calcular” (8 caracteres);\ncalcular = nome original;\ni = parâmetro do tipoint`; :::\n\nInstruções de Memória:\n\nalloca: aloca memória na stack;\nstore: armazena valor em endereço de memória;\nload: carrega valor de endereço de memória;\n\nInstruções Aritméticas:\n\nadd nsw: adição com verificação de overflow, no signed wrap_;\nmul, sub, sdiv, etc.\n\n:::{.callout-note} Adição com Verificação de Overflow (nsw - No Signed Wrap)\nQuando você soma dois inteiros com sinal, existe o risco de overflow: o resultado pode ser grande demais para caber no tipo de dado.\nExemplo de overflow:\nint a = 2000000000;\nint b = 2000000000;\nint c = a + b;  // Overflow! Resultado indefinido\nNo LLVM IR, a instrução add nsw significa:\n%resultado = add nsw i32 %a, %b\n\nnsw = “no signed wrap” (sem estouro de sinal);\nÉ uma garantia para o otimizador: este cálculo nunca vai causar overflow;\nSe houver overflow, o comportamento é indefinido (não é detectado, apenas assumido que não acontece);\n\nPor que isso importa?\nO flag nsw permite otimizações mais agressivas:\n; Com nsw, o otimizador pode fazer:\nx = (a + 1) - 1    →    x = a\n\n; Sem nsw, essa simplificação pode estar errada se houver _overflow_\nOutros flags similares:\n\nnuw = “no unsigned wrap” (para inteiros sem sinal);\nnnan = “no NaN” (para ponto flutuante);\nexact = divisão exata (sem resto).\n\nPor padrão, o clang adiciona nsw quando compila código C++ normal, assumindo que o programador evita overflow intencionalmente por ser este um comportamento indefinido em C++. :::\nControle de Fluxo:\n\nret: retorna da função;\ncall: chama outra função;\nbr: Branch (desvio condicional ou incondicional)\n\n\n\n\n16.3.4 Otimização do IR\n\n\n\n\n\n\nWarning\n\n\n\nIMPORTANTE: A partir do LLVM 14, o novo Pass Manager é o padrão. A sintaxe antiga -O3 não funciona mais com opt. Use a nova sintaxe com -passes:\n# Sintaxe CORRETA para LLVM 14+\nopt -passes='default&lt;O3&gt;' -S exemplo01_completo.ll -o exemplo01_completo_opt.ll\nSintaxe Alternativa, se a versão for anterior ao LLVM 14:\n# Sintaxe antiga (LLVM 13 e anteriores)\nopt -O3 -S exemplo01_completo.ll -o exemplo01_completo_opt.ll\nPara verificar qual versão do LLVM você está usando:\nopt --version\n\n\nCaso a esforçada leitora abra exemplo01_completo_opt.ll e compare com o original, verá algo dramático:\n; Função calcular OTIMIZADA\ndefine dso_local i32 @_Z8calculari(i32 noundef %x) {\nentry:\n  %mul = mul nsw i32 %x, 6\n  %add = add nsw i32 %mul, 60\n  ret i32 %add\n}\n\n; Ou ainda mais otimizado (dependendo do compilador)\ndefine dso_local i32 @_Z8calculari(i32 noundef %x) {\nentry:\n  %0 = mul nsw i32 %x, 6\n  %1 = add nsw i32 %0, 60\n  ret i32 %1\n}\nMudanças a serem observadas com atenção:\n\nAs 5 variáveis locais (temp1, temp2, etc.) foram completamente eliminadas;\nAs 4 operações foram simplificadas para 2: uma multiplicação e uma adição;\nTodas as operações alloca, store e load foram removidas;\nA matemática foi simplificada: ((x + 5) * 2 + 10) * 3 = (x * 6) + 60;\n\nFunção main otimizada:\ndefine dso_local i32 @main() {\nentry:\n  ret i32 0\n}\nA função main foi completamente simplificada! O otimizador:\n\nDetectou que calcular(10) pode ser calculado em tempo de compilação;\nCalculou: (10 * 6) + 60 = 120;\nNotou que o resultado não é usado;\nEliminou todo o código morto;\nManteve apenas o return 0.\n\nComparar tamanhos:\necho \"Linhas antes da otimização:\"\nwc -l exemplo01_completo.ll\necho \"Linhas depois da otimização:\"\nwc -l exemplo01_completo_opt.ll\nA leitora deve ver uma redução de aproximadamente 60-70% no número de linhas!\n\n\n16.3.5 Gerar IR com Diferentes Níveis de Otimização\nExistem duas formas de gerar IR otimizado:\nMétodo 1: Usar o clang diretamente:\n# Sem otimização (mas sem optnone!)\nclang++ -S -emit-llvm -O0 -Xclang -disable-O0-optnone exemplo01_completo.cpp -o exemplo01_O0.ll\n\n# Otimização nível 1\nclang++ -S -emit-llvm -O1 exemplo01_completo.cpp -o exemplo01_O1.ll\n\n# Otimização nível 2\nclang++ -S -emit-llvm -O2 exemplo01_completo.cpp -o exemplo01_O2.ll\n\n# Otimização nível 3\nclang++ -S -emit-llvm -O3 exemplo01_completo.cpp -o exemplo01_O3.ll\nMétodo 2: Gerar IR não-otimizado e depois otimizar com opt (LLVM 14+):\n# Primeiro gerar o IR sem otimização (SEM optnone!)\nclang++ -S -emit-llvm -O0 -Xclang -disable-O0-optnone exemplo01_completo.cpp -o exemplo01_base.ll\n\n# Depois otimizar com opt usando o novo Pass Manager\nopt -passes='default&lt;O1&gt;' -S exemplo01_base.ll -o exemplo01_O1_opt.ll\nopt -passes='default&lt;O2&gt;' -S exemplo01_base.ll -o exemplo01_O2_opt.ll\nopt -passes='default&lt;O3&gt;' -S exemplo01_base.ll -o exemplo01_O3_opt.ll\nComparando os resultados:\n# Ver número de linhas de cada versão\nwc -l exemplo01_O*.ll\n\n# Ver apenas a função calcular de cada versão\nfor arquivo in exemplo01_O*.ll; do\n    echo \"=== $arquivo ===\"\n    sed -n '/@.*calcular/,/^}/p' \"$arquivo\"\n    echo \"\"\ndone\nDemonstração: O Problema do optnone:\nVamos ver a diferença entre gerar IR com e sem optnone:\n# COM optnone (ERRADO para otimização posterior)\nclang++ -S -emit-llvm -O0 exemplo01_completo.cpp -o com_optnone.ll\n\n# SEM optnone (CORRETO)\nclang++ -S -emit-llvm -O0 -Xclang -disable-O0-optnone exemplo01_completo.cpp -o sem_optnone.ll\n\n# Verificar a presença do atributo\necho \"=== Arquivo COM optnone ===\"\ngrep \"attributes #0\" com_optnone.ll\n\necho -e \"\\n=== Arquivo SEM optnone ===\"\ngrep \"attributes #0\" sem_optnone.ll\n\n# Tentar otimizar ambos\nopt -passes='default&lt;O3&gt;' -S com_optnone.ll -o com_optnone_opt.ll\nopt -passes='default&lt;O3&gt;' -S sem_optnone.ll -o sem_optnone_opt.ll\n\n# Comparar resultados\necho -e \"\\n=== Resultados da otimização ===\"\necho \"COM optnone - linhas antes: $(wc -l &lt; com_optnone.ll), depois: $(wc -l &lt; com_optnone_opt.ll)\"\necho \"SEM optnone - linhas antes: $(wc -l &lt; sem_optnone.ll), depois: $(wc -l &lt; sem_optnone_opt.ll)\"\nVocê verá que o arquivo COM optnone não será otimizado!\n\n\n16.3.6 Entendendo o Novo Pass Manager (LLVM 14+)\nO LLVM mudou para um novo sistema de gerenciamento de passes a partir da versão 14. Aqui estão as principais diferenças:\nSintaxe Antiga (Legacy Pass Manager - até LLVM 13):\nopt -O3 -S arquivo.ll -o arquivo_opt.ll\nopt -mem2reg -instcombine -S arquivo.ll -o arquivo_opt.ll\nSintaxe Nova (New Pass Manager - LLVM 14+):\n# Pipelines predefinidos\nopt -passes='default&lt;O3&gt;' -S arquivo.ll -o arquivo_opt.ll\n\n# Passes individuais separados por vírgula\nopt -passes='mem2reg,instcombine' -S arquivo.ll -o arquivo_opt.ll\n\n# Pipeline completo com múltiplos passes\nopt -passes='function(mem2reg,instcombine),module(inline)' -S arquivo.ll -o arquivo_opt.ll\nVerificar qual Pass Manager está ativo:\n# Ver informações sobre passes\nopt -passes='default&lt;O3&gt;' -debug-pass-manager -S arquivo.ll -o /dev/null 2&gt;&1 | head -20\nForçar o uso do Legacy PM (se necessário, apenas LLVM 13-15):\nopt -enable-new-pm=0 -O3 -S arquivo.ll -o arquivo_opt.ll",
    "crumbs": [
      "Geração de Código Intermediário",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introdução à Representação Intermediária (IR) do LLVM com C++</span>"
    ]
  },
  {
    "objectID": "15-llvmIR.html#exemplo-com-controle-de-fluxo",
    "href": "15-llvmIR.html#exemplo-com-controle-de-fluxo",
    "title": "16  Introdução à Representação Intermediária (IR) do LLVM com C++",
    "section": "16.4 Exemplo com Controle de Fluxo",
    "text": "16.4 Exemplo com Controle de Fluxo\n\n16.4.1 Código com Condicionais\nA criativa leitora deve criar o exemplo02_condicional.cpp:\n// exemplo02_condicional.cpp\n// teste_branch.cpp\n#include &lt;cstdio&gt; // Necessário para a função printf\n\n// Uma função externa simples para garantir o efeito colateral\nvoid log_escolha(const char* mensagem) {\n    printf(\"%s\\n\", mensagem);\n}\n\nint max_com_efeito_colateral(int a, int b) {\n    int resultado;\n    if (a &gt; b) {\n        log_escolha(\"O caminho 'a' foi escolhido.\");\n        resultado = a;\n    } else {\n        log_escolha(\"O caminho 'b' foi escolhido.\");\n        resultado = b;\n    }\n    return resultado;\n}\n\nint main() {\n    max_com_efeito_colateral(10, 20);\n    return 0;\n}\n\n\n16.4.2 Gerar e Analisar o IR\nIMPORTANTE: Para ver os blocos básicos do if, compile SEM otimização e SEM optnone:\nclang++ -S -emit-llvm -O0 -Xclang -disable-O0-optnone exemplo02_condicional.cpp -o exemplo02_condicional.ll\nExamine a função max:\ndefine dso_local i32 @_Z3maxii(i32 noundef %a, i32 noundef %b) {\nentry:\n  %a.addr = alloca i32, align 4\n  %b.addr = alloca i32, align 4\n  store i32 %a, ptr %a.addr, align 4\n  store i32 %b, ptr %b.addr, align 4\n  %0 = load i32, ptr %a.addr, align 4\n  %1 = load i32, ptr %b.addr, align 4\n  %cmp = icmp sgt i32 %0, %1\n  br i1 %cmp, label %if.then, label %if.else\n\nif.then:                                          ; preds = %entry\n  %2 = load i32, ptr %a.addr, align 4\n  ret i32 %2\n\nif.else:                                          ; preds = %entry\n  %3 = load i32, ptr %b.addr, align 4\n  ret i32 %3\n}\nNovos Conceitos:\n\nBlocos Básicos: entry, if.then, if.else\n\nUm bloco básico é uma sequência de instruções sem desvios internos\nTermina com uma instrução de controle de fluxo\n\nComparação: icmp sgt\n\nicmp: Integer comparison\nsgt: Signed greater than\nOutros: eq (igual), ne (diferente), slt (menor que), etc.\n\nBranch Condicional: br i1 %cmp, label %if.then, label %if.else\n\nSe %cmp for verdadeiro, vai para if.then\nCaso contrário, vai para if.else\n\n\n\n\n16.4.3 4.2.1. Comparação: Sem Otimização vs Com Otimização\nVamos ver a diferença dramática quando compilamos com otimização:\n# Sem otimização (mostra os blocos do if)\nclang++ -S -emit-llvm -O0 -Xclang -disable-O0-optnone exemplo02_condicional.cpp -o exemplo02_O0.ll\n\n# Com otimização (pode eliminar os blocos!)\nclang++ -S -emit-llvm -O3 exemplo02_condicional.cpp -o exemplo02_O3.ll\n\n# Ver a diferença\necho \"=== SEM otimização ===\"\ngrep -A 15 \"define.*max\" exemplo02_O0.ll\n\necho -e \"\\n=== COM otimização ===\"\ngrep -A 10 \"define.*max\" exemplo02_O3.ll\nResultado com -O3:\ndefine dso_local i32 @_Z3maxii(i32 noundef %a, i32 noundef %b) {\nentry:\n  %cmp = icmp sgt i32 %a, %b\n  %retval.0 = select i1 %cmp, i32 %a, i32 %b\n  ret i32 %retval.0\n}\nO que aconteceu? - Os blocos if.then e if.else foram eliminados! - O condicional foi transformado em uma instrução select (equivalente ao operador ternário ?:) - Código mais eficiente: sem branches, sem jumps - Isso é chamado de predication (execução predicada)\nInstrução Select:\n%resultado = select i1 %condicao, i32 %valor_se_true, i32 %valor_se_false\nPor isso é crucial compilar com -O0 -Xclang -disable-O0-optnone quando você quer estudar a estrutura do código!\n\n\n16.4.4 4.3. Visualizar o Grafo de Fluxo de Controle (CFG)\nPara visualizar o CFG, você precisa do Graphviz instalado. Se ainda não instalou:\nsudo apt install graphviz\nAgora gere o CFG:\n# Gerar arquivo DOT do CFG\nopt -passes='dot-cfg' -disable-output exemplo02_condicional.ll\n\n# Isso cria arquivos .dot. Liste-os:\nls -la *.dot\n\n# Converter um arquivo .dot específico para PNG\n# O arquivo será algo como .max.dot ou ._Z3maxii.dot\ndot -Tpng .*.dot -o max_cfg.png\n\n# Ou converter todos os arquivos .dot\nfor file in .*.dot; do\n    dot -Tpng \"$file\" -o \"${file%.dot}.png\"\ndone\nAbra o arquivo PNG gerado para ver uma representação visual dos blocos básicos e do fluxo de controle.\nAlternativa: Ver o CFG no formato texto\nopt -passes='print&lt;cfg&gt;' -disable-output exemplo02_condicional.ll 2&gt;&1 | less",
    "crumbs": [
      "Geração de Código Intermediário",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introdução à Representação Intermediária (IR) do LLVM com C++</span>"
    ]
  },
  {
    "objectID": "15-llvmIR.html#exemplo-com-loops",
    "href": "15-llvmIR.html#exemplo-com-loops",
    "title": "16  Introdução à Representação Intermediária (IR) do LLVM com C++",
    "section": "16.5 5. Exemplo com Loops",
    "text": "16.5 5. Exemplo com Loops\n\n16.5.1 5.1. Código com Loop\nCrie exemplo03_loop.cpp:\n// exemplo03_loop.cpp\nint soma_array(int* arr, int size) {\n    int soma = 0;\n    for (int i = 0; i &lt; size; i++) {\n        soma += arr[i];\n    }\n    return soma;\n}\n\nint main() {\n    int numeros[] = {1, 2, 3, 4, 5};\n    int resultado = soma_array(numeros, 5);\n    return 0;\n}\n\n\n16.5.2 5.2. Gerar o IR\nclang++ -S -emit-llvm exemplo03_loop.cpp -o exemplo03_loop.ll\nA função soma_array terá múltiplos blocos básicos:\ndefine dso_local i32 @_Z10soma_arrayPii(ptr noundef %arr, i32 noundef %size) #0 {\nentry:\n  br label %for.cond\n\nfor.cond:\n  %soma.0 = phi i32 [ 0, %entry ], [ %add, %for.body ]\n  %i.0 = phi i32 [ 0, %entry ], [ %inc, %for.body ]\n  %cmp = icmp slt i32 %i.0, %size\n  br i1 %cmp, label %for.body, label %for.end\n\nfor.body:\n  %idxprom = sext i32 %i.0 to i64\n  %arrayidx = getelementptr inbounds i32, ptr %arr, i64 %idxprom\n  %0 = load i32, ptr %arrayidx, align 4\n  %add = add nsw i32 %soma.0, %0\n  %inc = add nsw i32 %i.0, 1\n  br label %for.cond\n\nfor.end:\n  ret i32 %soma.0\n}\nNovos Conceitos:\n\nNó Phi (φ): phi i32 [ 0, %entry ], [ %add, %for.body ]\n\nMescla valores de diferentes blocos básicos\nEssencial para SSA em loops\nFormato: phi tipo [valor1, bloco1], [valor2, bloco2], ...\n\nGEP (GetElementPtr): getelementptr inbounds i32, ptr %arr, i64 %idxprom\n\nCalcula endereços de elementos em arrays/estruturas\nNão acessa memória, apenas calcula o ponteiro\n\nEstrutura do Loop:\n\nentry: Inicialização\nfor.cond: Condição do loop\nfor.body: Corpo do loop\nfor.end: Saída do loop\n\n\n\n\n16.5.3 5.3. Otimização de Loops\nopt -passes='default&lt;O3&gt;' -S exemplo03_loop.ll -o exemplo03_loop_opt.ll\nO otimizador pode aplicar: - Loop Unrolling: Desenrolar o loop - Vectorization: Usar instruções SIMD - Loop Invariant Code Motion: Mover código constante para fora do loop\nPara ver quais passes foram aplicados:\nopt -passes='default&lt;O3&gt;' -debug-pass-manager -S exemplo03_loop.ll -o exemplo03_loop_opt.ll 2&gt;&1 | grep -i loop",
    "crumbs": [
      "Geração de Código Intermediário",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introdução à Representação Intermediária (IR) do LLVM com C++</span>"
    ]
  },
  {
    "objectID": "15-llvmIR.html#trabalhando-com-bitcode",
    "href": "15-llvmIR.html#trabalhando-com-bitcode",
    "title": "16  Introdução à Representação Intermediária (IR) do LLVM com C++",
    "section": "16.6 6. Trabalhando com Bitcode",
    "text": "16.6 6. Trabalhando com Bitcode\n\n16.6.1 6.1. Converter para Bitcode\nO Bitcode é a representação binária do LLVM IR, mais compacta e eficiente.\n# Gerar bitcode diretamente\nclang++ -c -emit-llvm exemplo01_soma.cpp -o exemplo01_soma.bc\n\n# Ou converter de .ll para .bc\nllvm-as exemplo01_soma.ll -o exemplo01_soma.bc\n\n\n16.6.2 6.2. Converter Bitcode de volta para Texto\nllvm-dis exemplo01_soma.bc -o exemplo01_soma_reverso.ll\n\n\n16.6.3 6.3. Linkar Múltiplos Arquivos Bitcode\nCrie dois arquivos:\nmodulo_a.cpp:\nint funcao_a(int x) {\n    return x * 2;\n}\nmodulo_b.cpp:\nint funcao_a(int x);\n\nint funcao_b(int y) {\n    return funcao_a(y) + 10;\n}\nCompile e linke:\nclang++ -c -emit-llvm modulo_a.cpp -o modulo_a.bc\nclang++ -c -emit-llvm modulo_b.cpp -o modulo_b.bc\nllvm-link modulo_a.bc modulo_b.bc -o modulos_linkados.bc\nllvm-dis modulos_linkados.bc -o modulos_linkados.ll",
    "crumbs": [
      "Geração de Código Intermediário",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introdução à Representação Intermediária (IR) do LLVM com C++</span>"
    ]
  },
  {
    "objectID": "15-llvmIR.html#análise-de-otimizações-comuns",
    "href": "15-llvmIR.html#análise-de-otimizações-comuns",
    "title": "16  Introdução à Representação Intermediária (IR) do LLVM com C++",
    "section": "16.7 7. Análise de Otimizações Comuns",
    "text": "16.7 7. Análise de Otimizações Comuns\n\n16.7.1 7.1. Constant Folding\nCódigo:\nint calcular() {\n    return 5 + 3 * 2;\n}\nIR Otimizado:\ndefine i32 @calcular() {\n  ret i32 11\n}\n\n\n16.7.2 7.2. Dead Code Elimination\nCódigo:\nint funcao() {\n    int x = 10;\n    int y = 20;  // não usado\n    return x;\n}\nA variável y será eliminada no IR otimizado.\n\n\n16.7.3 7.3. Function Inlining\nPequenas funções são automaticamente incorporadas no ponto de chamada.\n\n\n16.7.4 7.4. Common Subexpression Elimination\nCódigo:\nint x = a * b + c;\nint y = a * b + d;\nO cálculo a * b será feito apenas uma vez.",
    "crumbs": [
      "Geração de Código Intermediário",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introdução à Representação Intermediária (IR) do LLVM com C++</span>"
    ]
  },
  {
    "objectID": "15-llvmIR.html#exercícios-práticos",
    "href": "15-llvmIR.html#exercícios-práticos",
    "title": "16  Introdução à Representação Intermediária (IR) do LLVM com C++",
    "section": "16.8 8. Exercícios Práticos",
    "text": "16.8 8. Exercícios Práticos\n\n16.8.1 Exercício 1: Análise Básica\nCrie uma função que calcula o fatorial iterativo:\nint fatorial(int n) {\n    int resultado = 1;\n    for(int i = 1; i &lt;= n; i++) {\n        resultado *= i;\n    }\n    return resultado;\n}\nTarefas: 1. Gere o IR sem otimização (-O0) 2. Identifique os blocos básicos no IR 3. Localize os nós phi (φ-nodes) 4. Otimize com -passes='default&lt;O3&gt;' e compare 5. Conte quantas instruções foram eliminadas\n\n\n16.8.2 Exercício 2: Testando Passes Individuais\nUse o mesmo código do fatorial e aplique passes individuais em sequência:\n# Gerar IR base\nclang++ -S -emit-llvm -O0 fatorial.cpp -o fatorial.ll\n\n# Aplicar mem2reg\nopt -passes='mem2reg' -S fatorial.ll -o fatorial_mem2reg.ll\n\n# Aplicar instcombine após mem2reg\nopt -passes='instcombine' -S fatorial_mem2reg.ll -o fatorial_inst.ll\n\n# Aplicar loop-simplify\nopt -passes='loop-simplify' -S fatorial_inst.ll -o fatorial_loop.ll\n\n# Comparar com otimização completa\nopt -passes='default&lt;O3&gt;' -S fatorial.ll -o fatorial_O3.ll\nPerguntas: - Qual pass teve maior impacto na redução de código? - O resultado final é idêntico ao -passes='default&lt;O3&gt;'?\n\n\n16.8.3 Exercício 3: Estruturas de Dados\nImplemente uma struct simples: 1. Gere o IR sem otimização 2. Identifique os blocos básicos 3. Localize os nós phi 4. Otimize com -O3 e compare\n\n\n16.8.4 Exercício 2: Estruturas de Dados\nImplemente uma struct simples:\nstruct Ponto {\n    int x;\n    int y;\n};\n\nint distancia_manhattan(Ponto a, Ponto b) {\n    return abs(a.x - b.x) + abs(a.y - b.y);\n}\nAnalise como structs são representadas no IR.\n\n\n16.8.5 Exercício 3: Ponteiros e Referências\nCompare o IR gerado para:\nvoid func_valor(int x) { x++; }\nvoid func_ref(int& x) { x++; }\nvoid func_ptr(int* x) { (*x)++; }\n\n\n16.8.6 Exercício 4: Otimização de Loop\nImplemente e compare:\n// Versão 1\nint soma_v1(int n) {\n    int s = 0;\n    for(int i = 0; i &lt; n; i++) {\n        s += i;\n    }\n    return s;\n}\n\n// Versão 2 (fórmula direta)\nint soma_v2(int n) {\n    return n * (n - 1) / 2;\n}",
    "crumbs": [
      "Geração de Código Intermediário",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introdução à Representação Intermediária (IR) do LLVM com C++</span>"
    ]
  },
  {
    "objectID": "15-llvmIR.html#comandos-de-referência-rápida",
    "href": "15-llvmIR.html#comandos-de-referência-rápida",
    "title": "16  Introdução à Representação Intermediária (IR) do LLVM com C++",
    "section": "16.9 9. Comandos de Referência Rápida",
    "text": "16.9 9. Comandos de Referência Rápida\n\n16.9.1 Geração de IR\n\n\n\n\n\n\n\nComando\nDescrição\n\n\n\n\nclang++ -S -emit-llvm arquivo.cpp -o arquivo.ll\nGera IR em texto\n\n\nclang++ -c -emit-llvm arquivo.cpp -o arquivo.bc\nGera Bitcode\n\n\nclang++ -S -emit-llvm -O2 arquivo.cpp -o arquivo.ll\nGera IR otimizado\n\n\n\n\n\n16.9.2 Conversão\n\n\n\nComando\nDescrição\n\n\n\n\nllvm-as arquivo.ll -o arquivo.bc\nTexto para Bitcode\n\n\nllvm-dis arquivo.bc -o arquivo.ll\nBitcode para texto\n\n\n\n\n\n16.9.3 Otimização (LLVM 14+)\n\n\n\n\n\n\n\nComando\nDescrição\n\n\n\n\nopt -passes='default&lt;O1&gt;' -S arquivo.ll -o saida.ll\nOtimização nível 1\n\n\nopt -passes='default&lt;O2&gt;' -S arquivo.ll -o saida.ll\nOtimização nível 2\n\n\nopt -passes='default&lt;O3&gt;' -S arquivo.ll -o saida.ll\nOtimização nível 3\n\n\nopt -passes='mem2reg' -S arquivo.ll -o saida.ll\nPass específico (mem2reg)\n\n\nopt -passes='mem2reg,instcombine' -S arquivo.ll -o saida.ll\nMúltiplos passes\n\n\nopt -passes='default&lt;O3&gt;' -debug-pass-manager -S arquivo.ll -o saida.ll\nVer passes executados\n\n\n\n\n\n16.9.4 Análise e Visualização\n\n\n\nComando\nDescrição\n\n\n\n\nopt -passes=dot-cfg arquivo.ll -disable-output\nGera CFG em DOT\n\n\ndot -Tpng arquivo.dot -o grafo.png\nConverte DOT para PNG\n\n\n\n\n\n16.9.5 Compilação Final\n\n\n\nComando\nDescrição\n\n\n\n\nllc arquivo.ll -o arquivo.s\nIR para Assembly\n\n\nclang++ arquivo.ll -o executavel\nIR para executável\n\n\nclang++ arquivo.bc -o executavel\nBitcode para executável",
    "crumbs": [
      "Geração de Código Intermediário",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introdução à Representação Intermediária (IR) do LLVM com C++</span>"
    ]
  },
  {
    "objectID": "15-llvmIR.html#troubleshooting-comum",
    "href": "15-llvmIR.html#troubleshooting-comum",
    "title": "16  Introdução à Representação Intermediária (IR) do LLVM com C++",
    "section": "16.10 10. Troubleshooting Comum",
    "text": "16.10 10. Troubleshooting Comum\n\n16.10.1 Problema: opt não otimiza nada, arquivos têm mesmo tamanho\nSintoma: Você executa opt -passes='default&lt;O3&gt;' mas o arquivo de saída é idêntico ao de entrada.\nCausas Comuns:\n1. Atributo optnone (causa mais comum)\nVerifique se suas funções têm o atributo optnone:\ngrep \"optnone\" arquivo.ll\nSe você ver algo como attributes #0 = { ... optnone ... }, esse é o problema. O atributo optnone instrui o LLVM a pular completamente essa função.\nSoluções:\n# Solução 1: Recompilar sem optnone\nclang++ -S -emit-llvm -O0 -Xclang -disable-O0-optnone arquivo.cpp -o arquivo.ll\n\n# Solução 2: Usar -O1 e desabilitar passes\nclang++ -S -emit-llvm -O1 -Xclang -disable-llvm-passes arquivo.cpp -o arquivo.ll\n\n# Solução 3: Compilar direto com otimização\nclang++ -S -emit-llvm -O3 arquivo.cpp -o arquivo.ll\n2. Código muito simples\nSe o código é extremamente simples (como int soma(int a, int b) { return a + b; }), pode não haver muito o que otimizar. Use um exemplo mais complexo:\n// exemplo_complexo.cpp\nint calcular(int x) {\n    int a = x + 5;\n    int b = a * 2;\n    int c = b + 10;\n    return c * 3;\n}\nDiagnóstico completo:\n# 1. Verificar atributo optnone\necho \"=== Verificando optnone ===\"\ngrep -n \"optnone\" arquivo.ll\n\n# 2. Verificar se há funções para otimizar\necho -e \"\\n=== Funções no arquivo ===\"\ngrep \"^define\" arquivo.ll\n\n# 3. Tentar otimizar com debug\necho -e \"\\n=== Tentando otimizar com debug ===\"\nopt -passes='default&lt;O3&gt;' -debug-pass-manager -S arquivo.ll -o arquivo_opt.ll 2&gt;&1 | head -30\n\n# 4. Comparar arquivos\necho -e \"\\n=== Comparação de tamanhos ===\"\nwc -l arquivo.ll arquivo_opt.ll\n\n# 5. Ver diferenças\necho -e \"\\n=== Diferenças (primeiras 20 linhas) ===\"\ndiff arquivo.ll arquivo_opt.ll | head -20\n3. Sintaxe incorreta do opt\nCertifique-se de usar a sintaxe correta para sua versão:\n# Verificar versão\nopt --version\n\n# LLVM 14+: usar -passes\nopt -passes='default&lt;O3&gt;' -S arquivo.ll -o arquivo_opt.ll\n\n# LLVM 13-: sintaxe antiga pode funcionar\nopt -O3 -S arquivo.ll -o arquivo_opt.ll\n4. Script de teste completo\nSalve este script como testar_opt.sh:\n#!/bin/bash\n\necho \"=== Teste de Otimização do LLVM opt ===\"\necho \"\"\n\n# Criar código de teste\ncat &gt; teste_opt.cpp &lt;&lt; 'EOF'\nint calcular(int x) {\n    int a = x + 5;\n    int b = a * 2;\n    int c = b + 10;\n    int d = c * 3;\n    return d;\n}\n\nint main() {\n    int resultado = calcular(10);\n    return 0;\n}\nEOF\n\necho \"1. Código criado: teste_opt.cpp\"\n\n# Gerar IR sem optnone\nclang++ -S -emit-llvm -O0 -Xclang -disable-O0-optnone teste_opt.cpp -o teste_opt.ll\necho \"2. IR gerado: teste_opt.ll\"\n\n# Verificar optnone\nif grep -q \"optnone\" teste_opt.ll; then\n    echo \"   ⚠️  AVISO: Atributo optnone encontrado! Otimização pode não funcionar.\"\nelse\n    echo \"   ✓ OK: Sem atributo optnone\"\nfi\n\n# Contar linhas antes\nlinhas_antes=$(wc -l &lt; teste_opt.ll)\necho \"3. Linhas antes da otimização: $linhas_antes\"\n\n# Otimizar\nopt -passes='default&lt;O3&gt;' -S teste_opt.ll -o teste_opt_O3.ll 2&gt;&1 | head -5\necho \"4. Otimização aplicada\"\n\n# Contar linhas depois\nlinhas_depois=$(wc -l &lt; teste_opt_O3.ll)\necho \"5. Linhas depois da otimização: $linhas_depois\"\n\n# Calcular redução\nreducao=$((100 * (linhas_antes - linhas_depois) / linhas_antes))\necho \"\"\necho \"=== RESULTADO ===\"\nif [ $linhas_depois -lt $linhas_antes ]; then\n    echo \"✓ SUCESSO! Código foi otimizado!\"\n    echo \"  Redução: $reducao%\"\n    echo \"\"\n    echo \"Função calcular ANTES (primeiras 15 linhas):\"\n    sed -n '/@.*calcular/,/^}/p' teste_opt.ll | head -15\n    echo \"\"\n    echo \"Função calcular DEPOIS:\"\n    sed -n '/@.*calcular/,/^}/p' teste_opt_O3.ll\nelse\n    echo \"✗ FALHA! Otimização não funcionou.\"\n    echo \"  Possíveis causas:\"\n    echo \"  1. Atributo optnone presente\"\n    echo \"  2. Sintaxe incorreta do opt\"\n    echo \"  3. Problema com o LLVM\"\nfi\n\necho \"\"\necho \"=== Comparação Visual ===\"\necho \"Função calcular ANTES:\"\ngrep -A 10 \"define.*calcular\" teste_opt.ll\necho \"\"\necho \"Função calcular DEPOIS:\"\ngrep -A 10 \"define.*calcular\" teste_opt_O3.ll\n\n# Limpar\nrm -f teste_opt.cpp teste_opt.ll teste_opt_O3.ll\nExecute:\nchmod +x testar_opt.sh\n./testar_opt.sh\n\n\n16.10.2 Problema: Comando não encontrado\nSolução:\n# Verificar se o LLVM está instalado\nwhich clang++\nwhich opt\n\n# Se não estiver, reinstalar\nsudo ./llvm.sh 20\n\n\n16.10.3 Problema: Versão incorreta sendo usada\nSolução:\n# Atualizar alternativas\nsudo update-alternatives --config clang++\n\n\n16.10.4 Problema: Arquivo .ll corrompido ou ilegível\nSolução:\n# Regenerar o arquivo\nrm arquivo.ll\nclang++ -S -emit-llvm arquivo.cpp -o arquivo.ll\n\n\n16.10.5 Problema: Otimização não funciona como esperado\nSolução: Use -Rpass=.* para ver quais otimizações foram aplicadas:\nclang++ -O3 -Rpass=.* arquivo.cpp",
    "crumbs": [
      "Geração de Código Intermediário",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introdução à Representação Intermediária (IR) do LLVM com C++</span>"
    ]
  },
  {
    "objectID": "15-llvmIR.html#recursos-adicionais",
    "href": "15-llvmIR.html#recursos-adicionais",
    "title": "16  Introdução à Representação Intermediária (IR) do LLVM com C++",
    "section": "16.11 11. Recursos Adicionais",
    "text": "16.11 11. Recursos Adicionais\n\n16.11.1 Documentação Oficial\n\nLLVM Language Reference: https://llvm.org/docs/LangRef.html\nLLVM Passes: https://llvm.org/docs/Passes.html\nGetting Started: https://llvm.org/docs/GettingStarted.html\n\n\n\n16.11.2 Ferramentas Úteis\n\nCompiler Explorer (https://godbolt.org): Visualize IR online\nLLVM Tutorial: https://llvm.org/docs/tutorial/\n\n\n\n16.11.3 Leitura Recomendada\n\n“Getting Started with LLVM Core Libraries” - Bruno Cardoso Lopes\nLLVM Blog: https://blog.llvm.org/",
    "crumbs": [
      "Geração de Código Intermediário",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introdução à Representação Intermediária (IR) do LLVM com C++</span>"
    ]
  },
  {
    "objectID": "15-llvmIR.html#apêndice-tipos-de-dados-no-llvm-ir",
    "href": "15-llvmIR.html#apêndice-tipos-de-dados-no-llvm-ir",
    "title": "16  Introdução à Representação Intermediária (IR) do LLVM com C++",
    "section": "16.12 12. Apêndice: Tipos de Dados no LLVM IR",
    "text": "16.12 12. Apêndice: Tipos de Dados no LLVM IR\n\n16.12.1 Tipos Primitivos\n\ni1, i8, i16, i32, i64: Inteiros de 1, 8, 16, 32, 64 bits\nfloat: Ponto flutuante 32 bits\ndouble: Ponto flutuante 64 bits\nptr: Ponteiro genérico\nvoid: Sem valor\n\n\n\n16.12.2 Tipos Derivados\n\n[N x tipo]: Array de N elementos\n{tipo1, tipo2, ...}: Struct (estrutura)\n&lt;N x tipo&gt;: Vetor (para SIMD)\n\n\n\n16.12.3 Exemplos\n%array = alloca [10 x i32]              ; array de 10 inteiros\n%struct = alloca {i32, float}           ; struct com int e float\n%vector = alloca &lt;4 x float&gt;            ; vetor SIMD de 4 floats",
    "crumbs": [
      "Geração de Código Intermediário",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introdução à Representação Intermediária (IR) do LLVM com C++</span>"
    ]
  },
  {
    "objectID": "15-llvmIR.html#apêndice-migrando-da-sintaxe-antiga-para-nova-legacy-pm-new-pm",
    "href": "15-llvmIR.html#apêndice-migrando-da-sintaxe-antiga-para-nova-legacy-pm-new-pm",
    "title": "16  Introdução à Representação Intermediária (IR) do LLVM com C++",
    "section": "16.13 13. Apêndice: Migrando da Sintaxe Antiga para Nova (Legacy PM → New PM)",
    "text": "16.13 13. Apêndice: Migrando da Sintaxe Antiga para Nova (Legacy PM → New PM)\nSe você encontrar tutoriais antigos ou código que usa a sintaxe antiga do opt, aqui está um guia de conversão:\n\n16.13.1 Comandos Básicos de Otimização\n\n\n\nSintaxe Antiga (até LLVM 13)\nSintaxe Nova (LLVM 14+)\n\n\n\n\nopt -O0 arquivo.ll\nopt -passes='default&lt;O0&gt;' arquivo.ll\n\n\nopt -O1 arquivo.ll\nopt -passes='default&lt;O1&gt;' arquivo.ll\n\n\nopt -O2 arquivo.ll\nopt -passes='default&lt;O2&gt;' arquivo.ll\n\n\nopt -O3 arquivo.ll\nopt -passes='default&lt;O3&gt;' arquivo.ll\n\n\n\n\n\n16.13.2 Passes Individuais\n\n\n\n\n\n\n\nSintaxe Antiga\nSintaxe Nova\n\n\n\n\nopt -mem2reg arquivo.ll\nopt -passes='mem2reg' arquivo.ll\n\n\nopt -instcombine arquivo.ll\nopt -passes='instcombine' arquivo.ll\n\n\nopt -inline arquivo.ll\nopt -passes='inline' arquivo.ll\n\n\nopt -dce arquivo.ll\nopt -passes='dce' arquivo.ll\n\n\nopt -simplifycfg arquivo.ll\nopt -passes='simplifycfg' arquivo.ll\n\n\n\n\n\n16.13.3 Múltiplos Passes em Sequência\n\n\n\n\n\n\n\nSintaxe Antiga\nSintaxe Nova\n\n\n\n\nopt -mem2reg -instcombine arquivo.ll\nopt -passes='mem2reg,instcombine' arquivo.ll\n\n\nopt -inline -mem2reg -dce arquivo.ll\nopt -passes='inline,mem2reg,dce' arquivo.ll\n\n\n\n\n\n16.13.4 Debug e Análise\n\n\n\n\n\n\n\nSintaxe Antiga\nSintaxe Nova\n\n\n\n\nopt -debug-pass=Arguments\nopt -debug-pass-manager\n\n\nopt -debug-pass=Structure\nopt -debug-pass-manager\n\n\nopt -view-cfg arquivo.ll\nopt -passes='dot-cfg' arquivo.ll seguido de visualização\n\n\nopt -print-callgraph arquivo.ll\nopt -passes='print&lt;callgraph&gt;' arquivo.ll\n\n\n\n\n\n16.13.5 Passes de Análise (Print)\n\n\n\n\n\n\n\nSintaxe Antiga\nSintaxe Nova\n\n\n\n\nopt -print-cfg arquivo.ll\nopt -passes='print&lt;cfg&gt;' arquivo.ll\n\n\nopt -print-dom-info arquivo.ll\nopt -passes='print&lt;domtree&gt;' arquivo.ll\n\n\nopt -print-loops arquivo.ll\nopt -passes='print&lt;loops&gt;' arquivo.ll\n\n\n\n\n\n16.13.6 Forçar o Legacy PM (Versões de Transição LLVM 13-15)\nSe você realmente precisa usar a sintaxe antiga em versões de transição:\n# Forçar legacy PM (não recomendado, pode não estar disponível)\nopt -enable-new-pm=0 -O3 arquivo.ll\n\n\n16.13.7 Dicas para Conversão\n\nPasses separados por vírgula: Na nova sintaxe, use vírgula , para separar passes\nAspas são importantes: Use ' para evitar problemas com o shell\nPipeline default: Para otimizações padrão, use default&lt;ON&gt;\nHierarquia de passes: Alguns passes agora precisam especificar o escopo:\n\nfunction(...) para passes de função\nmodule(...) para passes de módulo\n\n\nExemplo complexo:\n# Antiga\nopt -inline -mem2reg -instcombine -simplifycfg -dce arquivo.ll\n\n# Nova\nopt -passes='inline,function(mem2reg,instcombine,simplifycfg,dce)' arquivo.ll\n\n\n16.13.8 Como Encontrar o Nome Correto de um Pass\n# Listar todos os passes disponíveis no novo PM\nopt --print-passes\n\n# Buscar um pass específico\nopt --print-passes | grep -i \"nome_do_pass\"\n\n\n16.13.9 Script de Conversão Rápida\nSe você tem muitos scripts usando a sintaxe antiga, aqui está um exemplo de como converter:\n#!/bin/bash\n# converter_opt.sh - Converte comandos opt antigos para novos\n\n# Exemplo de uso: ./converter_opt.sh \"opt -O3 -inline arquivo.ll\"\n\ncomando=$1\n\n# Substituir -O0, -O1, -O2, -O3\ncomando=$(echo \"$comando\" | sed \"s/-O\\([0-3]\\)/-passes='default&lt;O\\1&gt;'/g\")\n\n# Avisar sobre outras flags que precisam conversão manual\nif echo \"$comando\" | grep -qE ' -[a-z-]+ ' | grep -v \"passes=\"; then\n    echo \"AVISO: Comando contém flags que precisam ser convertidas manualmente\"\n    echo \"Comando original: $1\"\n    echo \"Comando parcialmente convertido: $comando\"\n    echo \"Consulte a tabela de conversão para flags individuais\"\nelse\n    echo \"Comando convertido: $comando\"\nfi",
    "crumbs": [
      "Geração de Código Intermediário",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introdução à Representação Intermediária (IR) do LLVM com C++</span>"
    ]
  },
  {
    "objectID": "fase1.html",
    "href": "fase1.html",
    "title": "17  Fase 1 - Projeto Prático",
    "section": "",
    "text": "17.1 Objetivo\nPesquisar e praticar conceitos de analisador léxico para desenvolver um programa em Python, C, ou ****C++**** que processe expressões aritméticas em notação polonesa reversa (RPN), conforme definida neste texto, a partir de um arquivo de texto, utilizando máquinas de estado finito (FSMs) implementadas obrigatoriamente com funções. O programa deve executar as expressões em um ambiente de teste (ex.: o notebook do aluno) e em um Arduino Uno, ou Mega. O seu trabalho será gerar o código Assembly, compatível com a arquitetura do Arduino Uno, ou Mega. Um guia de como compilar e executar o código Assembly está incluído na seção Chapter 18 no final deste documento.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#descrição-do-trabalho",
    "href": "fase1.html#descrição-do-trabalho",
    "title": "17  Fase 1 - Projeto Prático",
    "section": "17.2 Descrição do Trabalho",
    "text": "17.2 Descrição do Trabalho\nO objetivo é desenvolver um programa capaz de:\n\nLer um arquivo de texto contendo expressões aritméticas em Escritas RPN, segundo o formato especificado neste documento, com uma expressão por linha. Este arquivo contém o código do programa que será analisado pelo analisador léxico.\nAnalisar as expressões usando um analisador léxico baseado em Autômatos Finitos Determinísticos, com estados implementados por funções.\nTransformar as expressões em um texto contendo o código Assembly para o Arduino (uno|mega), utilizando as operações aritméticas e comandos especiais especificados. As operações serão realizadas no Arduino.\nExecutar as expressões em um Arduino (Uno|Mega), de forma que os resultados possam ser acompanhados (display, leds ou serial).\nHospedar o código, arquivos de teste e documentação em um repositório público no GitHub.\n\n\n17.2.1 Características Especiais\nAs expressões devem ser escritas em notação RPN, no formato (A B op), no qual A e B são números reais, os operandos, e op é um operador aritmético entre os listados neste documento. O programa deve suportar operações aritméticas básicas listadas neste documento, comandos especiais para manipulação de memória, e deve ser capaz de lidar com expressões aninhadas sem limites de aninhamento. Segundo a seguinte sintaxe:\n\nConsiderando que A e B são números reais, e usando o ponto como separador decimal, (ex.: 3.14), teremos:\nOperadores suportados na Fase 1:\n\nAdição: + (ex.: (A B +));\nSubtração: - (ex.: (A B -));\nMultiplicação: * (ex.: (A B *));\nDivisão real: / (ex.: (A B /));\nDivisão inteira: / (ex.: (A B /) para inteiros);\nResto da divisão inteira: % (ex.: (A B %));\nPotenciação: ^ (ex.: (A B ^), onde B é um inteiro positivo);\n\nTodas as operações (exceto divisão inteira e resto) usam números reais em formato de meia precisão (16 bits, IEEE 754), com duas casas decimais. A página Os desafios da norma IEEE 754 contém informações relevantes sobre a norma IEEE 754.\nExpressões podem ser aninhadas sem limite, por exemplo:\n\n(A (C D *) +): Soma A ao produto de C e D;\n((A B *) (D E *) /): Divide o produto de A e B pelo produto de D e E.\n((A B +) (C D *) /): Divide a soma de A e B pelo produto de C e D.\n\nA ordem de precedência das operações segue a ordem de precedência usual em matemática.\n\n\n\n17.2.2 Comandos Especiais\nA linguagem que estamos criando inclui comandos especiais para manipulação de memória e resultados:\n\n(N RES): Retorna o resultado da expressão N linhas anteriores (N é um inteiro não negativo).\n(V MEM): Armazena o valor real V em uma memória chamada MEM.\n(MEM): Retorna o valor armazenado em MEM. Se a memória não foi inicializada, retorna \\(0.0\\).\nCada arquivo de texto, código fonte da linguagem que estamos criando, representa um escopo independente de memória.\nMEM pode ser qualquer conjunto de letras maiúsculas, tal como MEM, VAR, X, etc.\nRES é uma keyword da linguagem que estamos criando. A única keyword da linguagem nesta fase.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#analisador-léxico-com-autômatos-finitos-determinístico",
    "href": "fase1.html#analisador-léxico-com-autômatos-finitos-determinístico",
    "title": "17  Fase 1 - Projeto Prático",
    "section": "17.3 Analisador Léxico com Autômatos Finitos Determinístico",
    "text": "17.3 Analisador Léxico com Autômatos Finitos Determinístico\n\nO analisador léxico deve ser implementado usando Autômatos Finitos Determinísticos, com cada estado representado por uma função. Qualquer forma diferente de implementação provocará o zeramento do trabalho.\nO Autômato Finito Determinístico deve reconhecer tokens válidos: números reais (duas casas decimais), operadores (+, -, *, /, %, ^), comandos especiais (RES, MEM), e parênteses.\nFunções de teste específicas devem ser criadas para validar o analisador léxico, cobrindo:\n\n\nEntradas válidas (ex.: (3.14 2.0 +), (5 RES), (10.5 CONTADOr));\nEntradas inválidas (ex.: (3.14 2.0 &), números malformados como 3.14.5, 3,45 ou parênteses desbalanceados).",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#arquivos-de-teste",
    "href": "fase1.html#arquivos-de-teste",
    "title": "17  Fase 1 - Projeto Prático",
    "section": "17.4 Arquivos de Teste",
    "text": "17.4 Arquivos de Teste\n\nFornecer mínimo de 3 arquivos de texto, cada um com pelo menos 10 linhas de expressões segundo as especificações deste documento.\nCada arquivo deve incluir todas as operações (+, -, *, /, %, ^) e comandos especiais ((N RES), (V MEM), (MEM)).\nOs arquivos devem estar no mesmo diretório do código-fonte e ser processados via argumento de linha de comando (ex.: ./NomeDoSeuPrograma teste1.txt).\nO programa não deve incluir menu ou qualquer seleção interativa de arquivos.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#hospedagem-no-github",
    "href": "fase1.html#hospedagem-no-github",
    "title": "17  Fase 1 - Projeto Prático",
    "section": "17.5 Hospedagem no GitHub",
    "text": "17.5 Hospedagem no GitHub\n\nO projeto deve ser hospedado em um repositório público no GitHub. Se o professor não indicar qual repositório deve ser usado, o repositório deve ser criado por um dos alunos do grupo.\nO repositório deve conter:\n\nCódigo-fonte do programa;\nArquivos de teste (mínimo 3);\nFunções de teste para o analisador léxico;\nÚltima versão do Código Assembly para Arduino gerado pelo programa;\nDocumentação (ex.: README.md) explicando como compilar, executar e testar o programa. Contendo o nome da instituição, disciplina, professor e nome dos alunos do grupo, em ordem alfabética seguido do usuário deste aluno no github.\n\nO repositório deve ser organizado com commits claros, as contribuições de cada um dos alunos devem estar registradas na forma de pull requests.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#requisitos-do-código",
    "href": "fase1.html#requisitos-do-código",
    "title": "17  Fase 1 - Projeto Prático",
    "section": "17.6 Requisitos do Código",
    "text": "17.6 Requisitos do Código\n\nAs primeiras linhas do código devem conter:\n\n\nNomes dos integrantes do grupo, em ordem alfabética seguidos do usuário deste aluno no github.\nNome do grupo no ambiente virtual de aprendizagem (Canvas).\n\n\nO programa deve receber o nome do arquivo de teste como argumento na linha de comando.\nO código deve ser escrito em Python, C, ou ****C++****. Com as funções nomeadas como está explicitado na Section 17.7.\nA última versão do código Assembly gerado para o Arduino deve ser funcional e incluído no repositório.\nA versão em Assembly deve ser exatamente o mesmo algoritmo explicitado em cada texto de teste de forma que o resultados das expressões seja obtido pelo cálculo realizado no Arduino.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#sec-divisaoTarefas",
    "href": "fase1.html#sec-divisaoTarefas",
    "title": "17  Fase 1 - Projeto Prático",
    "section": "17.7 Divisão de Tarefas para a Fase 1",
    "text": "17.7 Divisão de Tarefas para a Fase 1\nPara resolver o problema de processamento da expressões definidas nos arquivos de teste da fase 1, o trabalho será dividido entre até quatro alunos, trabalhando independentemente, na mesma sala, ou de forma remota. Cada aluno será responsável por uma parte específica do sistema, com interfaces claras para facilitar a integração. Abaixo está uma sugestão da divisão das tarefas, considerando as funções solicitadas: parseExpressao, executarExpressao, gerarAssembly, e exibirResultados.\n\n\n\n\n\n\nWarning\n\n\n\nNota: As tarefas podem ser divididas da forma que cada grupo achar mais conveniente, desde que as funções e interfaces sejam respeitadas.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNota: O vetor de tokens gerado pelo Analisador Léxico deve ser salvo em txt para uso nas próximas fases do projeto. Cabe ao grupo decidir o formato que será usado para salvar os tokens. Apenas os tokens referentes a última execução do código do analisador léxico devem ser salvos.\n\n\n\n17.7.1 Aluno 1: Função parseExpressao e Analisador Léxico com Autômato Finito Determinístico\nResponsabilidades:\n\nImplementar parseExpressao(std::string linha, std::vector&lt;std::string&gt;& _tokens_) (ou equivalente em Python/C) para analisar uma linha de expressão RPN e extrair tokens.\nImplementar o analisador léxico usando Autômatos Finitos Determinísticos (AFDs), com cada estado como uma função (ex.: estadoNumero, estadoOperador, estadoParenteses).\nValidar tokens:\n\nNúmeros reais (ex.: 3.14) usando ponto como separador decimal;\nOperadores (+, -, *, /, %, ^);\nComandos especiais (RES, MEM) e parênteses;\nDetectar erros como números malformados (ex.: 3.14.5), parênteses desbalanceados ou operadores inválidos;\n\nCriar funções de teste para o analisador léxico, cobrindo entradas válidas e inválidas.\n\nTarefas Específicas:\n\nEscrever parseExpressao para dividir a linha em tokens usando um Autômato Finito Determinístico;\nValidar tokens;\nTestar o Autômato Finito Determinístico com entradas diversificadas (3.14 2.0 +), (5 RES), (3.14.5 2.0 +) (inválido);\nCriar um método, antes do Autômato Finito Determinístico, para lidar com parênteses aninhados.\n\nInterface:\n\nRecebe uma linha de texto e retorna um vetor de tokens;\nFornece tokens válidos para executarExpressao.\n\n\n\n17.7.2 Aluno 2: Função executarExpressao e Gerenciamento de Memória\nResponsabilidades:\n\nImplementar executarExpressao(const std::vector&lt;std::string&gt;& _tokens_, std::vector&lt;float&gt;& resultados, float& memoria) para executar uma expressão RPN;\nGerenciar a memória MEM para comandos (V MEM) e (MEM);\nManter um histórico de resultados para suportar (N RES);\nCriar funções de teste para validar a execução de expressões e comandos especiais.\n\nTarefas Específicas:\n\nUsar uma pilha para avaliar expressões RPN (ex.: em C++: std::stack&lt;float&gt;);\nImplementar operações (+, -, *, /, %, ^) com precisão de 16 bits (IEEE 754);\nTratar divisão inteira e resto separadamente;\nTestar com expressões como (3.14 2.0 +), ((1.5 2.0 *) (3.0 4.0 *) /), (5.0 MEM), (2 RES);\nVerificar erros como divisão por zero ou N inválido em (N RES).\n\nInterface:\n\nRecebe tokens de parseExpressao e atualiza resultados e memoria;\nFornece resultados para exibirResultados e Assembly.\n\n\n\n17.7.3 Aluno 3: Função gerarAssembly e Leitura de Arquivo\nResponsabilidades:\n\nImplementar gerarAssembly(const std::vector&lt;std::string&gt;& _tokens_, std::string& codigoAssembly) para gerar código Assembly para Arduino;\nImplementar lerArquivo(std::string nomeArquivo, std::vector&lt;std::string&gt;& linhas) para ler o arquivo de entrada;\nCriar funções de teste para validar a leitura de arquivos e a geração de Assembly; Lembre-se o Assembly deve conter todas as operações do texto de teste.\nAlertar se o arquivo tiver linhas malformadas ou exceder limites.\n\nTarefas Específicas:\n\nLer o arquivo de tokens linha por linha, ignorando linhas vazias;\nGerar Assembly AVR para operações RPN e comandos especiais, usando registradores e instruções do Arduíno considerando os modelos adequados a esta fase;\nTestar com arquivos contendo 10 linhas, ou mais, incluindo expressões aninhadas e comandos especiais;\nVerificar erros de abertura de arquivo e exibir mensagens claras.\n\nInterface:\n\nlerArquivo fornece linhas para parseExpressao;\ngerarAssembly produz código Assembly para Arduino.\n\n\n\n17.7.4 Aluno 4: Função exibirResultados, Interface do Usuário e Testes\nResponsabilidades:\n\nImplementar exibirResultados(const std::vector&lt;float&gt;& resultados) para exibir os resultados das expressões;\nImplementar exibirMenu() e gerenciar a interface no main, incluindo leitura do argumento de linha de comando;\nCorrigir problemas de entrada (ex.: em C++: std::cin.ignore(std::numeric_limits&lt;std::streamsize&gt;::max(), '\\n'));\nCriar funções de teste para validar a saída e o comportamento do programa completo.\n\nTarefas Específicas:\n\nExibir resultados com formato claro (ex.: uma casa decimal para números reais);\nImplementar o main para chamar lerArquivo, parseExpressao, executarExpressao, e exibirResultados;\nTestar com arquivos de teste fornecidos, verificando saídas para expressões simples e complexas;\nTestar o FSM com comandos especiais como (V MEM) e (MEM);\n\nInterface:\n\nUsa resultados de executarExpressao para exibir saídas;\nGerencia a execução do programa via argumento de linha de comando.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#considerações-para-integração",
    "href": "fase1.html#considerações-para-integração",
    "title": "17  Fase 1 - Projeto Prático",
    "section": "17.8 Considerações para Integração",
    "text": "17.8 Considerações para Integração\n\nInterfaces: concordar com assinaturas das funções e formatos de dados (ex.: vetor de tokens, resultados em float);\nDepuração: testar cada parte isoladamente, simulando entradas/saídas;\nPassos de Integração:\n\nCopiar main e exibirMenu do Aluno 4;\nInserir lerArquivo e gerarAssembly do Aluno 3;\nAdicionar executarExpressao do Aluno 2;\nIncluir parseExpressao do Aluno 1;\n\nResolução de Conflitos: discutir problemas imediatamente na sala, ou de forma remota.\nDepuração Final: Testar o programa com os 3 arquivos de teste, verificando expressões, comandos especiais, e saída Assembly.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#avaliação",
    "href": "fase1.html#avaliação",
    "title": "17  Fase 1 - Projeto Prático",
    "section": "17.9 Avaliação",
    "text": "17.9 Avaliação\nO trabalho será avaliado antes da prova de autoria, com os seguintes critérios:\n\nCálculos e Funcionalidades (70%):\n\nImplementação completa de todas as operações RPN e comandos especiais.\n\nCada operação não implementada reduz 10% dos 70%.\nFalha na divisão inteira reduz 50% dos 70%.\n\nAnalisador léxico com Autômato Finito funcional e testado.\n\nUso de Expressões Regulares reduz a nota deste item em 90%.\n\n\nOrganização e Legibilidade do Código (15%):\n\nCódigo claro, comentado e bem estruturado.\nRepositório GitHub organizado, com README claro.\n\nRobustez (15%):\n\nTratamento de erros em expressões complexas e entradas inválidas.\nTestes do analisador léxico cobrindo todos os casos.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#prova-de-autoria",
    "href": "fase1.html#prova-de-autoria",
    "title": "17  Fase 1 - Projeto Prático",
    "section": "17.10 Prova de Autoria",
    "text": "17.10 Prova de Autoria\n\nUm aluno do grupo será sorteado, por um sistema disponível online para responder uma pergunta em uma lista de 10 perguntas;\nFalha na resposta reduz 35% da nota obtida na Avaliação do Projeto.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#entrega",
    "href": "fase1.html#entrega",
    "title": "17  Fase 1 - Projeto Prático",
    "section": "17.11 Entrega",
    "text": "17.11 Entrega\n\nO repositório GitHub deve conter:\n\nCódigo-fonte (Python, C, ou C++);\nTrês arquivos de teste com expressões RPN;\nFunções de teste para o analisador léxico;\nCódigo Assembly para Arduino da última execução do analisador léxico;\nArquivo de texto contendo os tokens gerados na última execução do analisador léxico;\nREADME com instruções de compilação, execução e testes;\n\nO programa deve ser executado com o comando ./NomeDoSeuPrograma.cpp.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#toolchain-assembly-para-arduino-linha-de-comando",
    "href": "fase1.html#toolchain-assembly-para-arduino-linha-de-comando",
    "title": "17  Fase 1 - Projeto Prático",
    "section": "18.1 Toolchain Assembly para Arduino (Linha de Comando)",
    "text": "18.1 Toolchain Assembly para Arduino (Linha de Comando)\nO conjunto de ferramentas (toolchain) padrão, gratuito e de código aberto para a arquitetura AVR (usada no Arduino Uno/Mega) é o AVR-GCC Toolchain e o AVRDUDE. Este guia mostra como compilar um arquivo Assembly (.s) e enviá-lo para a placa via linha de comando, sem usar a IDE do Arduino.\n\n\n\n\n\n\nWarning\n\n\n\nNota: O plugin platformIO do VsCode deve simplificar muito este processo. Cabe ao grupo instalar e testar este plugin na máquina que usará para fazer a prova de autoria. Não ser capaz de gerar o código Assembly por qualquer motivo pode resulta em zeramento da tarefa.\nVocê não precisará fazer nada manualmente na linha de comando. O PlatformIO gerenciará toda a cadeia de ferramentas (compilador, linker, uploader) para você, bastando clicar nos botões de Compilar e Gravar.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#ferramentas-necessárias",
    "href": "fase1.html#ferramentas-necessárias",
    "title": "17  Fase 1 - Projeto Prático",
    "section": "18.2 1. Ferramentas Necessárias",
    "text": "18.2 1. Ferramentas Necessárias\nVocê precisa instalar:\n\navr-gcc: compilador/montador para AVR.\navr-libc: biblioteca C padrão para AVR (necessária mesmo em código assembly puro porque fornece símbolos de inicialização como __do_copy_data, __do_clear_bss, etc.).\navrdude: programa de upload para o microcontrolador.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#instalação",
    "href": "fase1.html#instalação",
    "title": "17  Fase 1 - Projeto Prático",
    "section": "18.3 2. Instalação",
    "text": "18.3 2. Instalação\n\n18.3.1 Linux (Debian/Ubuntu)\nsudo apt update\nsudo apt install gcc-avr binutils-avr avr-libc avrdude\n\nObservação: Para outras distribuições, use o gerenciador de pacotes correspondente (ex: dnf para Fedora ou pacman para Arch).\n\n\n\n18.3.2 Windows\n\n18.3.2.1 Opção 1: Microchip AVR-GCC Toolchain (Recomendado)\n\nFaça o download do “AVR 8-bit Toolchain” do site da Microchip.\nInstale o .exe e adicione a pasta bin da toolchain à variável de ambiente PATH (ex: C:\\Program Files (x86)\\Atmel\\Studio\\7.0\\toolchain\\avr8\\avr8-gnu-toolchain\\bin).\nInstale avrdude via Chocolatey ou Scoop:\nchoco install avrdude\n\n\n\n18.3.2.2 Opção 2: WSL (Windows Subsystem for Linux)\n\nInstale o WSL e uma distribuição Linux (ex: Ubuntu) via Microsoft Store.\nSiga as mesmas instruções de instalação para Linux dentro do ambiente WSL.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#o-processo-de-compilação-e-upload",
    "href": "fase1.html#o-processo-de-compilação-e-upload",
    "title": "17  Fase 1 - Projeto Prático",
    "section": "18.4 3. O Processo de Compilação e Upload",
    "text": "18.4 3. O Processo de Compilação e Upload\nAssumindo que você tenha um arquivo chamado meu_codigo.s.\n\n18.4.1 Passos Principais\n\nMontar e Linkar: Converter o código Assembly (.s) em um arquivo executável no formato ELF (.elf).\nExtrair o HEX: Converter o arquivo .elf para o formato Intel HEX (.hex).\nFazer o Upload: Enviar o arquivo .hex para a memória flash do Arduino usando avrdude.\n\n\n\n18.4.2 Exemplo Completo para Arduino UNO\n\n18.4.2.1 Informações do Hardware\n\nMCU: atmega328p\nProgramador: arduino\nBaud Rate: 115200\nPorta Serial: /dev/ttyACM0 (Linux) ou COM3 (Windows)\n\n# Nome do arquivo de entrada (sem extensão)\nFILENAME=meu_codigo\n\n# MCU e parâmetros do avrdude\nMCU=atmega328p\nAVRDUDE_PARTNO=m328p\nAVRDUDE_PROGRAMMER=arduino\nAVRDUDE_PORT=COM3 # Mude para sua porta. Ex: /dev/ttyACM0 no Linux\nAVRDUDE_BAUDRATE=115200\n\n# 1. Montar e Linkar (Assembly -&gt; ELF)\n# Usamos avr-gcc como front-end. Ele chamará o montador (avr-as) e o linker (avr-ld)\navr-gcc -mmcu=$MCU -o $FILENAME.elf $FILENAME.s\n\n# 2. Extrair o arquivo .hex (ELF -&gt; HEX)\n# -O ihex: formato de saída Intel HEX\n# -R .eeprom: remove a seção de dados da EEPROM do arquivo de saída\navr-objcopy -O ihex -R .eeprom $FILENAME.elf $FILENAME.hex\n\n# 3. Fazer o Upload para o Arduino UNO\navrdude -c $AVRDUDE_PROGRAMMER -p $AVRDUDE_PARTNO -P $AVRDUDE_PORT -b $AVRDUDE_BAUDRATE -U flash:w:$FILENAME.hex\n\n\n18.4.2.2 Exemplo Completo para Arduino MEGA\n\n\n\n18.4.3 Informações do Hardware:\n\nMCU: atmega2560\nProgramador: wiring\nBaud Rate: 115200\nPorta Serial: /dev/ttyACM0 (Linux) ou COM4 (Windows)\n\n# Nome do arquivo de entrada (sem extensão)\nFILENAME=meu_codigo_mega\n\n# MCU e parâmetros do avrdude\nMCU=atmega2560\nAVRDUDE_PARTNO=m2560\nAVRDUDE_PROGRAMMER=wiring\nAVRDUDE_PORT=COM4 # Mude para sua porta. Ex: /dev/ttyACM0 no Linux\nAVRDUDE_BAUDRATE=115200\n\n# 1. Montar e Linkar (Assembly -&gt; ELF)\navr-gcc -mmcu=$MCU -o $FILENAME.elf $FILENAME.s\n\n# 2. Extrair o arquivo .hex (ELF -&gt; HEX)\navr-objcopy -O ihex -R .eeprom $FILENAME.elf $FILENAME.hex\n\n# 3. Fazer o Upload para o Arduino MEGA\navrdude -c $AVRDUDE_PROGRAMMER -p $AVRDUDE_PARTNO -P $AVRDUDE_PORT -b $AVRDUDE_BAUDRATE -U flash:w:$FILENAME.hex",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#observações-importantes",
    "href": "fase1.html#observações-importantes",
    "title": "17  Fase 1 - Projeto Prático",
    "section": "18.5 Observações Importantes",
    "text": "18.5 Observações Importantes\n\nDiferença entre avr-gcc e avrdude:\n\navr-gcc usa -mmcu=atmega328p.\navrdude usa -p m328p.\n\nBootloader:\n\nPressione o botão Reset na placa antes de executar avrdude para garantir que o bootloader esteja ativo.\n\nExemplo Prático:\n\n// meu_codigo.s\n.device atmega328p\n.org 0x0000\nrjmp reset\n\nreset:\n    ldi r16, 0xFF\n    out 0x24, r16        ; DDRB = 0xFF (PB7..PB0 como saída)\n    ldi r16, 0x00\n    out 0x25, r16        ; PORTB = 0x00 (todos apagados)\n\nloop:\n    out 0x25, r16        ; PORTB = valor atual\n    ldi r16, 0x01\n    lsr r16              ; desloca bit para baixo\n    rjmp loop\nCompile e envie:\navr-gcc -mmcu=atmega328p -o led.elf led.s\navr-objcopy -O ihex -R .eeprom led.elf led.hex\navrdude -c arduino -p m328p -P COM3 -b 115200 -U flash:w:led.hex",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase2.html",
    "href": "fase2.html",
    "title": "18  Fase 2 - Analisador Sintático \\(LL(1)\\)",
    "section": "",
    "text": "18.1 1. Objetivo\nPesquisar e praticar os conceitos de analisadores léxico e sintático para desenvolver um programa em Python, C, ou C++. O objetivo é complementar o material de aula, aprimorando sua formação acadêmica e profissional por meio da construção de um parser \\[LL(1)\\] para a linguagem de programação simplificada que está descrita neste documento, utilizando como entrada o vetor de tokens gerado na Fase 1.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Fase 2 - Analisador Sintático $LL(1)$</span>"
    ]
  },
  {
    "objectID": "fase2.html#descrição-do-trabalho",
    "href": "fase2.html#descrição-do-trabalho",
    "title": "18  Fase 2 - Analisador Sintático \\(LL(1)\\)",
    "section": "18.2 2. Descrição do Trabalho",
    "text": "18.2 2. Descrição do Trabalho\nO objetivo é desenvolver um programa capaz de:\n\nLer um arquivo de texto contendo o código-fonte de um programa escrito na linguagem especificada neste documento (uma expressão ou declaração por linha);\nUtilizar o string de tokens (gerado por um analisador léxico, como o da Fase 1) como entrada para o analisador sintático;\nImplementar um analisador sintático descendente recursivo do tipo \\(LL(1)\\) para validar a estrutura do código;\nGerar a árvore sintática correspondente para cada expressão válida;\nImplementar estruturas de controle (tomada de decisão e laços) mantendo a notação polonesa reversa;\nDetectar e reportar erros sintáticos de forma clara e informativa.\n\nNesta fase, não será necessário gerar código Assembly.\n\n18.2.1 2.1. Características da Linguagem\nAs declarações continuam sendo escritas em notação polonesa reversa (RPN), no formato (A B op), no qual A e B são operandos (números reais ou inteiros, ou referências a memória), e op é um operador aritmético, ou keyword, ou identificador, entre os listados neste documento.\nOperadores Suportados:\n\nAdição: + (ex.: (A B +));\nSubtração: - (ex.: (A B -));\nMultiplicação: * (ex.: (A B *));\nDivisão Real: | (ex.: (A B |));\nDivisão Inteira: / (ex.: (A B /) para inteiros);\nResto da Divisão Inteira: % (ex.: (A B %));\nPotenciação: ^ (ex.: (A B ^), onde B é um inteiro positivo).\n\nPrecisão Numérica:\nA precisão dos números de ponto flutuante depende da arquitetura do processador:\n\nSe a arquitetura for de 8 bits: use meia precisão (16 bits, IEEE 754);\nSe a arquitetura for de 16 bits: use precisão simples (32 bits, IEEE 754);\nSe a arquitetura for de 32 bits: use precisão dupla (64 bits, IEEE 754);\nSe a arquitetura for de 64 bits: use precisão quádrupla (128 bits, IEEE 754).\n\nAs operações de divisão inteira e resto são realizadas exclusivamente com números inteiros.\nExpressões Aninhadas:\nExpressões podem ser aninhadas sem limite definido, por exemplo:\n\n(A (C D *) +): Soma A ao produto de C e D;\n((A B %) (D E *) /): Divide o resto de A por B pelo produto de D e E;\n((A B +) (C D *) |): Divide (usando divisão real) a soma de A e B pelo produto de C e D.\n\nNestes exemplos, A, B, C, D, e E podem ser números literais ou referências a memórias.\n\n\n18.2.2 2.2. Comandos Especiais\nA linguagem inclui os mesmos comandos especiais da fase anterior para manipulação de memória e resultados:\n\n(N RES): Retorna o resultado da expressão N linhas anteriores (N é um inteiro não negativo);\n(V MEM): Armazena o valor real V em uma memória chamada MEM;\n(MEM): Retorna o valor armazenado em MEM. Se a memória não foi inicializada, retorna 0.\n\nRegras de Escopo:\n\nCada arquivo de texto representa um escopo de memória independente;\nMEM pode ser qualquer conjunto de letras maiúsculas (ex: MEM, VAR, X, CONTADOR);\nRES é uma keyword da linguagem.\n\n\n\n18.2.3 2.3. Novas Estruturas: Tomada de Decisão e Laços\nVocê deverá criar e documentar a sintaxe para estruturas de tomada de decisão e laços de repetição. A única restrição é que estas estruturas mantenham o padrão da linguagem: devem estar contidas entre parênteses e seguir a lógica de operadores pós-fixados.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Fase 2 - Analisador Sintático $LL(1)$</span>"
    ]
  },
  {
    "objectID": "fase2.html#analisador-sintático-com-parser-ll1",
    "href": "fase2.html#analisador-sintático-com-parser-ll1",
    "title": "18  Fase 2 - Analisador Sintático \\(LL(1)\\)",
    "section": "18.3 3. Analisador Sintático com parser \\(LL(1)\\)",
    "text": "18.3 3. Analisador Sintático com parser \\(LL(1)\\)\nO analisador sintático deve ser implementado como um parser descendente recursivo do tipo \\(LL(1)\\). A implementação deve incluir:\n\n18.3.1 3.1. Gramática \\(LL(1)\\)\nDefinir o conjunto completo de regras de produção para a linguagem, incluindo:\n\nExpressões aritméticas em RPN;\nComandos especiais (RES, MEM);\nEstruturas de controle (decisão e laços);\nTratamento de aninhamento.\n\n\n\n18.3.2 3.2. Conjuntos FIRST e FOLLOW\nCalcular e documentar os conjuntos FIRST e FOLLOW para cada não-terminal da gramática. Estes conjuntos devem ser incluídos na documentação do projeto.\n\n\n18.3.3 3.3. Tabela de Análise (parsing Table)\nConstruir a tabela de análise \\(LL(1)\\) baseada nos conjuntos FIRST e FOLLOW. A tabela deve mapear pares (não-terminal, terminal) para regras de produção.\n\n\n18.3.4 3.4. parser Descendente Recursivo\nImplementar o parser utilizando:\n\nBuffer de entrada para os tokens;\nPilha de análise para controle do parsing;\nFunções recursivas para cada não-terminal;\nGeração da árvore sintática durante o parsing.\n\n\n\n18.3.5 3.5. Funções de Teste\nCriar funções de teste específicas para validar o analisador sintático, cobrindo:\n\nExpressões válidas simples e aninhadas;\nEstruturas de controle válidas;\nEntradas inválidas (erros sintáticos);\nCasos extremos (aninhamento profundo, expressões vazias);",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Fase 2 - Analisador Sintático $LL(1)$</span>"
    ]
  },
  {
    "objectID": "fase2.html#arquivos-de-teste",
    "href": "fase2.html#arquivos-de-teste",
    "title": "18  Fase 2 - Analisador Sintático \\(LL(1)\\)",
    "section": "18.4 4. Arquivos de Teste",
    "text": "18.4 4. Arquivos de Teste\nFornecer um mínimo de 3 arquivos de texto, cada um com pelo menos 10 linhas de expressões segundo as especificações deste documento.\nRequisitos dos Arquivos de Teste:\n\nCada arquivo deve incluir todas as operações (+, -, *, |, /, %, ^);\nCada arquivo deve incluir comandos especiais ((N RES), (V MEM), (MEM));\nCada arquivo deve incluir pelo menos um laço de repetição;\nCada arquivo deve incluir pelo menos uma tomada de decisão;\nOs testes devem incluir literais inteiros, reais e o uso de memórias (variáveis);\nIncluir casos de teste com erros sintáticos para validar o tratamento de erros.\n\nOs arquivos devem estar no mesmo diretório do código-fonte e ser processados via argumento de linha de comando (ex.: ./AnalisadorSintatico teste1.txt).",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Fase 2 - Analisador Sintático $LL(1)$</span>"
    ]
  },
  {
    "objectID": "fase2.html#hospedagem-no-github",
    "href": "fase2.html#hospedagem-no-github",
    "title": "18  Fase 2 - Analisador Sintático \\(LL(1)\\)",
    "section": "18.5 5. Hospedagem no GitHub",
    "text": "18.5 5. Hospedagem no GitHub\nO projeto deve ser hospedado em um repositório público no GitHub criado exclusivamente para este projeto. O repositório deve ser criado por um dos alunos do grupo.\nO repositório deve conter:\n\nCódigo-fonte do programa (Python, C, ou C++);\nArquivos de teste (mínimo 3);\nFunções de teste para o analisador sintático;\nDocumentação completa (README.md);\nArquivo markdown com a gramática, conjuntos FIRST/FOLLOW, tabela \\(LL(1)\\) e a árvore sintática da última execução.\n\nO repositório deve ser organizado com commits claros, as contribuições de cada um dos alunos devem estar registradas na forma de pull requests.\n\n\n\n\n\n\nImportant\n\n\n\n\nO nome do repositório deve ser o nome do grupo no Canvas (ex.: RA2_1);\nÉ fundamental que cada aluno documente suas contribuições de forma clara e detalhada;\nO aluno não pode trocar o seu usuário no GitHub durante o desenvolvimento do projeto;\nO aluno não pode alterar o repositório para privado;\nO uso de issues no GitHub é encorajado para discutir tarefas e bugs;",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Fase 2 - Analisador Sintático $LL(1)$</span>"
    ]
  },
  {
    "objectID": "fase2.html#requisitos-do-código",
    "href": "fase2.html#requisitos-do-código",
    "title": "18  Fase 2 - Analisador Sintático \\(LL(1)\\)",
    "section": "18.6 6. Requisitos do Código",
    "text": "18.6 6. Requisitos do Código\nAs primeiras linhas do código devem conter:\n// Integrantes do grupo (ordem alfabética):\n// Nome Completo 1 - username1\n// Nome Completo 2 - username2\n// Nome Completo 3 - username3\n// Nome Completo 4 - username4\n//\n// Nome do grupo no Canvas: [Nome do Grupo]\nO programa deve receber o nome do arquivo de teste como argumento na linha de comando.\nO código deve ser escrito em Python, C, ou C++, com as funções nomeadas como está explicitado na Seção 7 Section 19.7.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Fase 2 - Analisador Sintático $LL(1)$</span>"
    ]
  },
  {
    "objectID": "fase2.html#sec-divisao-tarefas",
    "href": "fase2.html#sec-divisao-tarefas",
    "title": "18  Fase 2 - Analisador Sintático \\(LL(1)\\)",
    "section": "18.7 7. Divisão de Tarefas para a Fase 2",
    "text": "18.7 7. Divisão de Tarefas para a Fase 2\nPara resolver o problema de análise sintática da Fase 2, o trabalho será dividido entre até quatro alunos, trabalhando independentemente, na mesma sala, ou de forma remota. Cada aluno será responsável por uma parte específica do sistema, com interfaces claras para facilitar a integração. Abaixo está uma sugestão de divisão das tarefas, considerando as funções solicitadas: lerTokens, construirGramatica, parsear, e gerarArvore.\n\n\n\n\n\n\nNote\n\n\n\nNota: As tarefas podem ser divididas da forma que cada grupo achar mais conveniente, desde que as funções e interfaces sejam respeitadas.\nNota: A árvore sintática gerada pelo parser além de estar na documentação em markdown, deve ser salva em formato texto ou JSON para uso nas próximas fases do projeto. Cabe ao grupo decidir o formato que será usado para salvar a árvore.\n\n\n\n18.7.1 7.1. Aluno 1: Função construirGramatica e Análise \\(LL(1)\\)\nResponsabilidades:\n\nImplementar construirGramatica() para definir as regras de produção da linguagem;\nCalcular os conjuntos FIRST e FOLLOW para cada não-terminal;\nConstruir a tabela de análise \\(LL(1)\\);\nValidar que a gramática é \\(LL(1)\\) (sem conflitos na tabela);\nDocumentar a gramática completa em formato EBNF (use letras maiúsculas para não-terminais e minusculas para terminais).\n\nTarefas Específicas:\n\nEscrever as regras de produção para expressões RPN, comandos especiais e estruturas de controle;\nImplementar algoritmos para calcular FIRST e FOLLOW;\nDetectar e resolver conflitos na gramática (se houver);\nCriar funções auxiliares: calcularFirst(), calcularFollow(), construirTabelaLL1();\nTestar a tabela com entradas diversas para garantir determinismo.\n\nInterface:\n\nEntrada: Nenhuma (a gramática é fixa);\nSaída: Estrutura de dados contendo gramática, FIRST, FOLLOW e tabela \\(LL(1)\\);\nFornece tabela \\(LL(1)\\) para a função parsear().\n\n\n\n18.7.2 7.2. Aluno 2: Função parsear e parser Descendente Recursivo\nResponsabilidades:\n\nImplementar parsear(_tokens_, tabela_ll1) para análise sintática descendente recursiva;\nUsar a tabela \\(LL(1)\\) para guiar o processo de parsing;\nImplementar a pilha de análise e controle de derivação;\nDetectar e reportar erros sintáticos com mensagens claras;\nCriar funções de teste para validar o parser.\n\nTarefas Específicas:\n\nImplementar o algoritmo de parsing \\(LL(1)\\) com pilha;\nCriar função para cada não-terminal (parsing descendente recursivo);\nGerenciar o buffer de entrada de tokens;\nImplementar recuperação de erros básica;\nTestar com expressões válidas e inválidas: (3.14 2.0 +), ((A B +) (C D *) /), (A B + C) (erro);\n\nInterface:\n\nEntrada: Vetor de tokens e tabela \\(LL(1)\\);\nSaída: Estrutura de derivação ou erro sintático;\nFornece estrutura de derivação para gerarArvore().\n\n\n\n18.7.3 7.3. Aluno 3: Função lerTokens e Estruturas de Controle\nResponsabilidades:\n\nImplementar lerTokens(arquivo) para ler tokens salvos da Fase 1;\nDefinir e implementar a sintaxe para a estrutura de decisão e repetição em notação pós-fixada;\nCriar tokens especiais para estruturas de controle;\nDocumentar a sintaxe das novas estruturas;\nCriar arquivos de teste com estruturas de controle.\n\nTarefas Específicas:\n\nLer arquivo de tokens no formato definido pelo grupo;\nAdicionar tokens para as estruturas de decisão e repetição e operadores relacionais (&gt;, &lt;, ==, etc.);\nImplementar validação básica de tokens;\nCriar exemplos;\nTestar integração com o analisador léxico da Fase 1.\n\nInterface:\n\nEntrada: Nome do arquivo de tokens;\nSaída: Vetor de tokens estruturado;\nFornece tokens para a função parsear().\n\n\n\n18.7.4 7.4. Aluno 4: Função gerarArvore, Interface e Integração\nResponsabilidades:\n\nImplementar gerarArvore(derivacao) para construir a árvore sintática;\nImplementar a função main() e gerenciar a interface;\nCriar visualização da árvore sintática (texto ou gráfica);\nCoordenar a integração de todos os módulos;\nCriar funções de teste end-to-end.\n\nTarefas Específicas:\n\nTransformar a derivação em estrutura de árvore;\nImplementar impressão da árvore em formato legível;\nSalvar árvore em arquivo (JSON ou formato customizado);\nImplementar o main() para chamar lerTokens(), construirGramatica(), parsear(), e gerarArvore();\nTestar o sistema completo com os 3 arquivos de teste.\n\nInterface:\n\nEntrada: Estrutura de derivação do parser;\nSaída: Árvore sintática em formato estruturado;\nGerencia a execução completa via linha de comando.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Fase 2 - Analisador Sintático $LL(1)$</span>"
    ]
  },
  {
    "objectID": "fase2.html#considerações-para-integração",
    "href": "fase2.html#considerações-para-integração",
    "title": "18  Fase 2 - Analisador Sintático \\(LL(1)\\)",
    "section": "18.8 8. Considerações para Integração",
    "text": "18.8 8. Considerações para Integração\nInterfaces: Concordar com assinaturas das funções e formatos de dados (ex.: estrutura de tokens, formato da árvore).\nDepuração: Testar cada parte isoladamente, simulando entradas/saídas.\nPassos de Integração:\n\nCopiar main() e interface do Aluno 4;\nInserir lerTokens() e definições de estruturas de controle do Aluno 3;\nAdicionar construirGramatica() e tabela \\(LL(1)\\) do Aluno 1;\nIncluir parsear() do Aluno 2;\nIntegrar gerarArvore() do Aluno 4.\n\nResolução de Conflitos: Discutir problemas imediatamente na sala ou de forma remota.\nDepuração Final: Testar o programa com os 3 arquivos de teste, verificando expressões, estruturas de controle, e árvore sintática.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Fase 2 - Analisador Sintático $LL(1)$</span>"
    ]
  },
  {
    "objectID": "fase2.html#avaliação",
    "href": "fase2.html#avaliação",
    "title": "18  Fase 2 - Analisador Sintático \\(LL(1)\\)",
    "section": "18.9 9. Avaliação",
    "text": "18.9 9. Avaliação\nO trabalho será pré-avaliado de forma automática e novamente durante a prova de autoria, com os seguintes critérios:\n\n18.9.1 9.1. Funcionalidades do Analisador (70%)\n\nEntrega e validação do string de tokens: Base da nota;\nCada operação aritmética não identificada corretamente: -10%;\nFalhas na definição/implementação dos laços de repetição: -20%;\nFalhas na definição/implementação da tomada de decisão: -20%;\nSe o analisador só processar números inteiros: -50%;\nFalha na geração da árvore sintática: -30%;\nGramática não-\\(LL(1)\\) ou com conflitos: -20%.\n\n\n\n18.9.2 9.2. Organização e Legibilidade do Código (15%)\n\nCódigo claro, comentado e bem estruturado\nREADME bem escrito contendo:\n\nNome da instituição de ensino, ano, disciplina, professor;\nIntegrantes do grupo em ordem alfabética;\nInstruções para compilar, executar e depurar;\nDocumentação da sintaxe das estruturas de controle;\n\nRepositório GitHub organizado com commits claros.\n\n\n\n18.9.3 9.3. Robustez (15%)\n\nTratamento de erros em expressões complexas e entradas inválidas;\nMensagens de erro claras indicando linha e tipo de erro;\nTestes cobrindo todos os casos (válidos e inválidos);\nRecuperação básica de erros.\n\n\nAviso: Trabalhos identificados como cópias terão a nota zerada.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Fase 2 - Analisador Sintático $LL(1)$</span>"
    ]
  },
  {
    "objectID": "fase2.html#sec-prova-autoria",
    "href": "fase2.html#sec-prova-autoria",
    "title": "18  Fase 2 - Analisador Sintático \\(LL(1)\\)",
    "section": "18.10 10. Prova de Autoria",
    "text": "18.10 10. Prova de Autoria\n\nUm aluno do grupo será sorteado usando um aplicativo online;\nDepois de explicar o projeto, o aluno sorteado escolherá um número de \\(1\\) a \\(10\\), que corresponderá a uma pergunta sobre o projeto;\nA falha na resposta, ou na explicação do projeto, implicará na redução de 35% da nota automática do projeto para todo o grupo.\n\n\n\n\n\n\n\nImportant\n\n\n\nApesar da sugestão de divisão de tarefas, todos os alunos devem entender o funcionamento completo do projeto. O aluno sorteado para a prova de autoria deve ser capaz de responder qualquer pergunta sobre qualquer parte do projeto.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Fase 2 - Analisador Sintático $LL(1)$</span>"
    ]
  },
  {
    "objectID": "fase2.html#entrega",
    "href": "fase2.html#entrega",
    "title": "18  Fase 2 - Analisador Sintático \\(LL(1)\\)",
    "section": "18.11 11. Entrega",
    "text": "18.11 11. Entrega\nO repositório no GitHub deve conter:\n\n18.11.1 11.1. Código-fonte\n\nPrograma completo em Python, C, ou C++;\nTodas as funções especificadas implementadas.\n\n\n\n18.11.2 11.2. Arquivos de Teste\n\nMínimo de \\(3\\) arquivos com \\(10\\) linhas, ou mais, cada;\nCasos de teste válidos e inválidos;\nExemplos com estruturas de controle.\n\n\n\n18.11.3 11.3. Documentação\n\nREADME.md bem formatado com:\n\nInformações institucionais;\nInstruções de compilação e execução;\nSintaxe das estruturas de controle;\nExemplos de uso.\n\n\n\n\n18.11.4 11.4. Arquivo de Saída\nUm arquivo markdown contendo:\n\nO conjunto de regras de produção da gramática;\nOs conjuntos FIRST e FOLLOW;\nA Tabela de Análise \\(LL(1)\\);\nA representação da árvore sintática de um arquivo de teste.\n\n\n\n18.11.5 11.5. Formato de Execução\nO programa deve ser executado com: ./AnalisadorSintatico teste1.txt.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Fase 2 - Analisador Sintático $LL(1)$</span>"
    ]
  },
  {
    "objectID": "fase3.html",
    "href": "fase3.html",
    "title": "19  Fase 3 - Analisador Semântico",
    "section": "",
    "text": "19.1 1. Objetivo\nPesquisar e praticar os conceitos de analisadores léxico, sintático e semântico desenvolvendo um programa em Python, C, ou C++. O objetivo é complementar o material de aula, aprimorando sua formação acadêmica e profissional por meio da construção de um analisador semântico sobre um parser \\[LL(1)\\] para a linguagem de programação simplificada que está descrita neste documento, utilizando como entrada a árvore sintática abstrata gerada na Fase 2.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Fase 3 - Analisador Semântico</span>"
    ]
  },
  {
    "objectID": "fase3.html#descrição-do-trabalho",
    "href": "fase3.html#descrição-do-trabalho",
    "title": "19  Fase 3 - Analisador Semântico",
    "section": "19.2 2. Descrição do Trabalho",
    "text": "19.2 2. Descrição do Trabalho\nO objetivo é desenvolver um programa capaz de:\n\nLer um arquivo de texto contendo o código-fonte de um programa escrito na linguagem especificada neste documento (uma expressão ou declaração por linha). Este arquivo deve estar em formato texto simples (.txt) usando apenas caracteres ASCII;\nUtilizar a árvore sintática abstrata gerada pelo analisador sintático da Fase 2 como entrada;\nCriar a Gramática de Atributos da linguagem descrita neste documento;\nImplementar um analisador semântico para criar a árvore sintática abstrata atribuída.\nGerar um documento de análise semântica para o julgamento de tipos e outras verificações semânticas;\nDetectar e reportar erros léxicos, sintáticos e semânticos de forma clara e informativa.\n\nObservação MUITO IMPORTANTE: A partir dessa fase, todos os três analisadores serão utilizados em conjunto. Ou seja, o arquivo de teste irá passar pelo analisador léxico (Fase 1), depois pelo analisador sintático (Fase 2) e finalmente pelo analisador semântico (Fase 3). Portanto, é fundamental que o grupo utilize o mesmo formato de tokens definido na Fase 1 e a mesma gramática definida na Fase 2. Qualquer divergência entre as fases resultará em erros de integração e perda de pontos na avaliação.\nNesta fase, não será necessário gerar código Assembly.\n\n19.2.1 2.1. Características da Linguagem\nAs declarações continuam sendo escritas em notação polonesa reversa (RPN), no formato (A B op), no qual A e B são operandos (números reais ou inteiros, ou referências a memória), e op é um operador aritmético, ou keyword, ou identificador, entre os listados neste documento.\nOperadores Suportados:\n\nAdição: + (ex.: (A B +));\nSubtração: - (ex.: (A B -));\nMultiplicação: * (ex.: (A B *));\nDivisão Real: | (ex.: (A B |));\nDivisão Inteira: / (ex.: (A B /) para inteiros);\nResto da Divisão Inteira: % (ex.: (A B %));\nPotenciação: ^ (ex.: (A B ^), onde B é um inteiro positivo).\n\nPrecisão Numérica:\nA precisão dos números de ponto flutuante depende da arquitetura do processador onde o programa será executado para avaliação:\n\nSe a arquitetura for de 8 bits: use meia precisão (16 bits, IEEE 754);\nSe a arquitetura for de 16 bits: use precisão simples (32 bits, IEEE 754);\nSe a arquitetura for de 32 bits: use precisão dupla (64 bits, IEEE 754);\nSe a arquitetura for de 64 bits: use precisão quádrupla (128 bits, IEEE 754).\n\nAs operações de divisão inteira e resto são realizadas exclusivamente com números inteiros. Além disso, lembre-se, para esta avaliação você deve considerar apenas a arquitetura do Arduino Uno R3, Arduino Mega, ou qualquer outro processador de 8 bits.\nExpressões Aninhadas:\nExpressões podem ser aninhadas sem limite definido, por exemplo:\n\n(A (C D *) +): Soma A ao produto de C e D;\n((A B %) (D E *) /): Divide o resto de A por B pelo produto de D e E;\n((A B +) (C D *) |): Divide (usando divisão real) a soma de A e B pelo produto de C e D.\n\nNestes exemplos, A, B, C, D, e E podem ser números literais ou referências a memórias.\nTipos de Dados:\nA linguagem suporta três tipos de dados:\n\nint: Números inteiros\nreal (ou float): Números de ponto flutuante\nbooleano: Resultado de operações relacionais (usado internamente)\n\nObservação: O tipo booleano não pode ser armazenado em memórias (MEM), sendo usado apenas como resultado de expressões relacionais em estruturas de controle.\nAgora que temos laços e estruturas de decisão, nossa gramática deve ser capaz de lidar com expressões que retornam valores booleanos. Portanto, você deve implementar os seguintes operadores relacionais, tanto na gramática quanto no analisador semântico:\nOperadores Relacionais (retornam tipo booleano): - &gt; : maior que - &lt; : menor que\n- &gt;= : maior ou igual - &lt;= : menor ou igual - == : igual - != : diferente\nTodos aceitam operandos int ou real e retornam booleano.\nObservação: a inclusão dos operadores relacionais implica na necessidade de atualizar a gramática, o parser e o analisador semântico para suportar expressões que envolvem esses operadores, garantindo a correta verificação de tipos e a geração da árvore sintática abstrata atribuída.\n\n\n19.2.2 2.2. Comandos Especiais\nA linguagem inclui os mesmos comandos especiais da fase anterior para manipulação de memória e resultados:\n\n(N RES): Retorna o resultado da expressão N linhas anteriores (N é um inteiro não negativo);\n(V MEM): Armazena o valor real V em uma memória chamada MEM;\n(MEM): Retorna o valor armazenado em MEM. Se a memória não foi inicializada, apresenta um erro semântico.\n\nObservação: O tratamento de MEM, mudou desde a FASE 2. Agora, MEM retorna um erro semântico se não foi inicializada e deve ser verificada no analisador semântico.\nRegras de Escopo:\n\nCada arquivo de texto representa um escopo de memória independente;\nMEM pode ser qualquer conjunto de letras maiúsculas (ex: MEM, VAR, X, CONTADOR);\nRES é uma keyword da linguagem.\n\n\n\n19.2.3 2.3. Tomada de Decisão e Laços\nA sua gramática agora deve incluir as estruturas de tomada de decisão e laços de repetição que foram criados na Fase 2. A sintaxe dessas estruturas deve ser definida em notação pós-fixada (RPN), e você deve criar os tokens necessários para representá-las.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Fase 3 - Analisador Semântico</span>"
    ]
  },
  {
    "objectID": "fase3.html#analisador-semântico",
    "href": "fase3.html#analisador-semântico",
    "title": "19  Fase 3 - Analisador Semântico",
    "section": "19.3 3. Analisador Semântico",
    "text": "19.3 3. Analisador Semântico\nO analisador semântico que você vai criar deve ser capaz de criar a árvore sintática abstrata atribuída, aplicando as regras da Gramática de Atributos que você vai definir. Além disso, ele deve realizar as seguintes verificações semânticas:\n\nJulgamento de Tipos: Verificar se as operações são realizadas entre tipos compatíveis (inteiros e reais). Por exemplo, a operação de potência ^ deve ter um expoente inteiro;\nVerificação de Memória: Garantir que as memórias são inicializadas antes de serem usadas;\nVerificação de Comandos Especiais: Validar o uso correto dos comandos especiais (N RES), (V MEM), e (MEM);\nVerificação de Estruturas de Controle: Garantir que as estruturas de tomada de decisão e laços de repetição estão corretamente formadas e que suas condições são válidas.\nDetecção de Erros Semânticos: Reportar erros semânticos com mensagens claras, indicando a linha do arquivo onde o erro ocorreu e a natureza do erro.\nGeração de Relatório: Criar um documento de análise semântica que detalhe todas as verificações realizadas, os erros encontrados, e a estrutura da árvore sintática abstrata atribuída.\n\n\n19.3.1 3.1 Gramática de Atributos\nVocê deve definir a Gramática de Atributos para a linguagem descrita neste documento. A gramática deve incluir regras para todas as operações, comandos especiais, e estruturas de controle mencionadas. Cada regra de produção deve ser acompanhada por atributos que descrevem o tipo e o valor dos elementos envolvidos.\nEsta gramática de atributos será composta de atributos e regras de produção e deve estar documentada em um arquivo markdown no repositório do GitHub.\nTipos de Atributos:\n\nAtributos Sintetizados: Calculados a partir dos atributos dos vértices filhos (propagam informação de baixo para cima na árvore).\nAtributos Herdados: Calculados a partir dos atributos do vértice pai ou irmãos (propagam informação de cima para baixo na árvore).\n\nAtributos Principais:\nPara cada não-terminal e terminal da gramática, você deve definir atributos como:\n\ntipo: O tipo da expressão (inteiro, real, booleano)\nvalor: O valor calculado da expressão (quando aplicável)\ninicializada: Para memórias, indica se foram inicializadas\nescopo: Nível de escopo da variável\n\nExemplos de Regras de Produção com Atributos:\nA seguir, apresentamos exemplos de como as regras de produção devem ser documentadas com seus atributos e regras semânticas:\n\n\n19.3.2 Regras Aderentes ao Documento\n\n19.3.2.1 1. Adição de Inteiros\n\\[\\frac{\\Gamma \\vdash e_1 : \\text{int} \\quad \\Gamma \\vdash e_2 : \\text{int}}{\\Gamma \\vdash e_1 + e_2 : \\text{int}}\\]\n\n\n19.3.2.2 2. Adição com Promoção de Tipo\n\\[\\frac{\\Gamma \\vdash e_1 : \\text{int} \\quad \\Gamma \\vdash e_2 : \\text{float}}{\\Gamma \\vdash e_1 + e_2 : \\text{float}}\\]\nOu usando a função promover_tipo do documento: \\[\\frac{\\Gamma \\vdash e_1 : T_1 \\quad \\Gamma \\vdash e_2 : T_2}{\\Gamma \\vdash e_1 + e_2 : \\text{promover\\_tipo}(T_1, T_2)}\\]\n\n\n19.3.2.3 3. Estrutura Condicional (Simplificada)\n\\[\\frac{\\Gamma \\vdash e_1 : \\text{booleano} \\quad \\Gamma \\vdash e_2 : T \\quad \\Gamma \\vdash e_3 : T}{\\Gamma \\vdash \\text{if } e_1 \\text{ then } e_2 \\text{ else } e_3 : T}\\]\n\n\n19.3.2.4 4. Declaração de Variável\n\\[\\frac{\\Gamma \\vdash e : T' \\quad T' \\leq T \\quad \\Gamma[x \\mapsto T] \\vdash e_{corpo} : T_{corpo}}{\\Gamma \\vdash (x : T \\leftarrow e; e_{corpo}) : T_{corpo}}\\]\n\n\n19.3.2.5 5. Chamada de Função\n\\[\\frac{\\text{tabela}(f) = (T_1, \\ldots, T_n) \\rightarrow T_{ret} \\quad \\Gamma \\vdash e_i : T_i' \\quad T_i' \\leq T_i \\text{ para } i=1..n}{\\Gamma \\vdash f(e_1, \\ldots, e_n) : T_{ret}}\\]\nObservações Importantes:\n\nOs exemplos de regras acima usam notação infixa padrão para facilitar o entendimento. Ao implementar o analisador semântico, você deve adaptar essas regras para a notação RPN da linguagem. Por exemplo:\n\n\nNotação infixa (nas regras): if e₁ then e₂ else e₃\nNotação RPN (no código): (e₁ e₂ e₃ IF)\n\nA semântica de tipagem permanece a mesma, apenas a sintaxe de superfície muda.\n\nTodas as regras devem verificar a compatibilidade de tipos antes de realizar operações.\nA Tabela de Símbolos deve ser atualizada sempre que uma memória for declarada ou modificada.\nOperações que misturam inteiros e reais devem promover o resultado para real.\nTodas as verificações de erro devem incluir o número da linha onde o erro ocorreu.\nA gramática completa deve cobrir todas as construções da linguagem descritas neste documento.\n\n\n\n19.3.2.6 Exemplo de Aplicação em RPN\nPara a expressão: (5 3 +)\nAplicação das regras: 1. Γ ⊢ 5 : int (literal inteiro) 2. Γ ⊢ 3 : int (literal inteiro) 3. Pela regra de adição: Γ ⊢ 5 + 3 : int\nPara a expressão: (5.0 3 +)\nAplicação das regras: 1. Γ ⊢ 5.0 : float (literal real) 2. Γ ⊢ 3 : int (literal inteiro) 3. Pela regra de promoção: Γ ⊢ 5.0 + 3 : float",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Fase 3 - Analisador Semântico</span>"
    ]
  },
  {
    "objectID": "fase3.html#arquivos-de-teste",
    "href": "fase3.html#arquivos-de-teste",
    "title": "19  Fase 3 - Analisador Semântico",
    "section": "19.4 4. Arquivos de Teste",
    "text": "19.4 4. Arquivos de Teste\nFornecer um mínimo de 3 arquivos de texto, cada um com pelo menos 10 linhas de expressões segundo as especificações deste documento.\nRequisitos dos Arquivos de Teste:\n\nCada arquivo deve incluir todas as operações (+, -, *, |, /, %, ^);\nCada arquivo deve incluir comandos especiais ((N RES), (V MEM), (MEM));\nCada arquivo deve incluir pelo menos um laço de repetição;\nCada arquivo deve incluir pelo menos uma tomada de decisão;\nOs testes devem incluir literais inteiros, reais e o uso de memórias (variáveis);\nIncluir casos de teste com erros semânticos para validar o tratamento de erros.\n\nOs arquivos devem estar no mesmo diretório do código-fonte e ser processados via argumento de linha de comando (ex.: ./compilar teste1.txt).\nAtenção: não é preciso criar sistemas de testes automatizados. Contudo, é necessário incluir no readme as rotinas de testes que foram usadas para validar o funcionamento do analisador semântico.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Fase 3 - Analisador Semântico</span>"
    ]
  },
  {
    "objectID": "fase3.html#hospedagem-no-github",
    "href": "fase3.html#hospedagem-no-github",
    "title": "19  Fase 3 - Analisador Semântico",
    "section": "19.5 5. Hospedagem no GitHub",
    "text": "19.5 5. Hospedagem no GitHub\nO projeto deve ser hospedado em um repositório público no GitHub criado exclusivamente para este projeto. O repositório deve ser criado por um dos alunos, que será o responsável por revisar e aprovar todos os pull requests antes do merge. Os demais alunos do grupo devem contribuir por meio de pull requests.\nAtenção: a colaboração entre os membros do grupo deve ser feita de forma equitativa. Todos os alunos devem contribuir com o projeto e entender o funcionamento completo do código.\nO repositório deve conter:\n\nCódigo-fonte do programa (Python, C, ou C++);\nArquivos de teste (mínimo 3);\nDocumentação completa (README.md), constando:\n\nNome da instituição de ensino, ano, disciplina, professor;\nIntegrantes do grupo em ordem alfabética;\nInstruções para compilar, executar e depurar o programa;\nExemplos de uso do programa.\n\nArquivo markdown com a gramática de atributos\nArquivo markdown com árvore sintática da última execução do código.\nArquivo markdown com os erros semânticos detectados.\nArquivo markdown com o julgamento de tipos descrevendo as regras de dedução criadas e onde foram aplicadas na última execução do código.\n\nO repositório deve ser organizado com commits claros, as contribuições de cada um dos alunos devem estar registradas na forma de pull requests.\n\n\n\n\n\n\nImportant\n\n\n\n\nO nome do repositório deve ser o nome do grupo no Canvas (ex.: RA3_1);\nÉ fundamental que cada aluno documente suas contribuições de forma clara e detalhada;\nO aluno não pode trocar o seu usuário no GitHub durante o desenvolvimento do projeto;\nO aluno não pode alterar o repositório para privado;\nO uso de issues no GitHub é encorajado para discutir tarefas e bugs;",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Fase 3 - Analisador Semântico</span>"
    ]
  },
  {
    "objectID": "fase3.html#requisitos-do-código",
    "href": "fase3.html#requisitos-do-código",
    "title": "19  Fase 3 - Analisador Semântico",
    "section": "19.6 6. Requisitos do Código",
    "text": "19.6 6. Requisitos do Código\nAs primeiras linhas do código devem conter:\n// Integrantes do grupo (ordem alfabética):\n// Nome Completo 1 - username1\n// Nome Completo 2 - username2\n// Nome Completo 3 - username3\n// Nome Completo 4 - username4\n//\n// Nome do grupo no Canvas: [Nome do Grupo]\nObservação: os comentários acima são um exemplo para C/C++/Rust. Em Python, use # no lugar de //.\nO programa deve receber o nome do arquivo de teste como argumento na linha de comando e rodar os analisadores léxico, sintático e semântico em sequência, gravando a árvore sintática abstrata atribuída e os demais arquivos de documentação em markdown. Os erros semânticos (se houver) devem ser apresentados no console.\nO código deve ser escrito em Python, C, ou C++, com as funções nomeadas como está explicitado na Seção a seguir:",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Fase 3 - Analisador Semântico</span>"
    ]
  },
  {
    "objectID": "fase3.html#sec-divisao-tarefas",
    "href": "fase3.html#sec-divisao-tarefas",
    "title": "19  Fase 3 - Analisador Semântico",
    "section": "19.7 7. Divisão de Tarefas para a Fase 3",
    "text": "19.7 7. Divisão de Tarefas para a Fase 3\nPara resolver o problema de análise semântica da Fase 3, o trabalho será dividido entre até quatro alunos. Cada aluno será responsável por uma parte específica do sistema, com interfaces claras para facilitar a integração. Porém, todos os alunos devem entender o funcionamento completo do projeto.\n\n\n\n\n\n\nNote\n\n\n\nNota: As tarefas podem ser divididas da forma que cada grupo achar mais conveniente, desde que as funções e interfaces sejam respeitadas. Nota: A árvore sintática abstrata atribuída, além de estar na documentação, deve ser salva em JSON para facilitar a interoperabilidade com a próxima fase da disciplina. O formato deve incluir pelo menos: tipo do vértice, tipo inferido, filhos, e número da linha.\n\n\n\n19.7.1 7.1. Aluno 1: Função definirGramaticaAtributos e Regras Semânticas\nResponsabilidades:\n\nImplementar definirGramaticaAtributos() para especificar as regras semânticas da linguagem.\nDefinir os atributos (sintetizados e herdados) para cada símbolo da gramática.\nDocumentar a gramática de atributos completa em formato EBNF, detalhando as regras de produção e as ações semânticas associadas.\nCriar a estrutura de dados para a Tabela de Símbolos (ex: identificadores, tipos, escopo).\n\nTarefas Específicas:\n\nEscrever as regras de verificação de tipos para operadores aritméticos.\nDefinir as regras para validação de escopo e uso de memórias (MEM).\nEspecificar as ações semânticas para as estruturas de controle (decisão e laços).\nCriar funções auxiliares: inicializarTabelaSimbolos(), adicionarSimbolo(), buscarSimbolo().\n\nInterface:\n\nEntrada: Nenhuma (a gramática de atributos é fixa).\nSaída: Estrutura de dados contendo a gramática de atributos e uma Tabela de Símbolos inicializada.\nFornece as regras semânticas e a Tabela de Símbolos para a função analisarSemantica().\n\n\n\n19.7.2 7.2. Aluno 2: Função analisarSemantica e Verificação de Tipos\nResponsabilidades:\n\nImplementar analisarSemantica(arvoreSintatica, gramaticaAtributos, tabelaSimbolos) para percorrer a árvore sintática abstrata.\nAplicar as regras semânticas definidas na gramática de atributos para realizar a verificação de tipos.\nDetectar e reportar erros de tipo com mensagens claras.\nImplementar a coerção de tipos quando aplicável (ex: inteiro para real em operações mistas).\n\nFormato de Mensagens de Erro:\nERRO SEMÂNTICO [Linha X]: &lt;descrição&gt;\nContexto: &lt;trecho relevante do código&gt;\n``\nExemplo:\n```shell\nERRO SEMÂNTICO [Linha 5]: Memória 'CONTADOR' utilizada sem inicialização\nContexto: (CONTADOR)\nTarefas Específicas:\n\nImplementar um algoritmo de percurso da árvore (ex: pós-ordem).\nValidar a compatibilidade de tipos em expressões aritméticas.\nVerificar se o expoente da potenciação é um inteiro.\nGarantir que operandos de / e % são inteiros.\nCriar funções de teste específicas para validação de tipos.\n\nInterface:\n\nEntrada: Árvore sintática abstrata (da Fase 2), gramática de atributos e Tabela de Símbolos.\nSaída: Árvore sintática com anotações de tipo ou um erro semântico.\nFornece a árvore anotada para a função gerarArvoreAtribuida().\n\n\n\n19.7.3 7.3. Aluno 3: Funções analisarSemanticaMemoria e analisarSemanticaControle\nResponsabilidades:\n\nImplementar analisarSemanticaMemoria(arvoreSintatica, tabelaSimbolos) para validar o uso de memórias.\nImplementar analisarSemanticaControle(arvoreSintatica, tabelaSimbolos) para validar estruturas de controle.\nGarantir que as memórias (MEM) são inicializadas antes de serem lidas.\nValidar o uso correto dos comandos (N RES) e (V MEM).\nVerificar se as condições nas estruturas de controle resultam em um valor booleano (ou equivalente).\nImplementar a lógica de escopo para as variáveis.\n\nTarefas Específicas:\n\nPopular a Tabela de Símbolos com informações sobre a inicialização de memórias.\nImplementar verificações para garantir que (MEM) não seja usado antes de (V MEM).\nValidar se o N em (N RES) é um inteiro não negativo e aponta para uma expressão válida.\nVerificar se as condições em estruturas de decisão e laços são expressões válidas.\nValidar o escopo correto de variáveis dentro de estruturas de controle aninhadas.\nCriar arquivos de teste com erros semânticos relacionados a memórias e controle de fluxo.\n\nInterface:\n\nEntrada: Árvore sintática, Tabela de Símbolos.\nSaída: Lista de erros semânticos relacionados a memórias e estruturas de controle, além da Tabela de Símbolos atualizada.\nAs funções analisarSemanticaMemoria e analisarSemanticaControle devem ser chamadas sequencialmente após analisarSemantica do Aluno 2.\nColabora diretamente com o Aluno 2, recebendo a árvore já anotada com tipos e complementando com validações de memória e controle de fluxo.\n\n\n\n19.7.4 7.4. Aluno 4: Função gerarArvoreAtribuida, Interface e Integração\nResponsabilidades:\n\nImplementar gerarArvoreAtribuida(arvoreAnotada) para construir a árvore sintática abstrata atribuída final.\nImplementar a função main() que gerencia a execução sequencial dos analisadores (léxico, sintático e semântico).\nGerar os relatórios em markdown: árvore atribuída, julgamento de tipos e erros semânticos.\nCoordenar a integração de todos os módulos.\n\nTarefas Específicas:\n\nTransformar a árvore sintática anotada em uma estrutura final (a árvore atribuída).\nImplementar a impressão da árvore em formato de texto ou JSON.\nSalvar a árvore atribuída e os relatórios nos arquivos de saída especificados.\nImplementar o main() para chamar as funções das Fases 1, 2 e 3 em sequência.\nTestar o sistema completo com os 3 arquivos de teste.\n\nInterface:\n\nEntrada: Árvore sintática anotada pela análise semântica.\nSaída: Árvore sintática abstrata atribuída e arquivos de relatório.\nGerencia a execução completa do compilador via linha de comando.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Fase 3 - Analisador Semântico</span>"
    ]
  },
  {
    "objectID": "fase3.html#considerações-para-integração",
    "href": "fase3.html#considerações-para-integração",
    "title": "19  Fase 3 - Analisador Semântico",
    "section": "19.8 8. Considerações para Integração",
    "text": "19.8 8. Considerações para Integração\nInterfaces: Concordar com os formatos de dados (ex.: estrutura da árvore sintática, formato da Tabela de Símbolos, árvore atribuída). Depuração: Testar cada módulo isoladamente antes da integração final.\nPassos de Integração:\n\nUtilizar o sistema integrado das Fases 1 e 2 como base.\nInserir definirGramaticaAtributos() e a Tabela de Símbolos do Aluno 1.\nIntegrar a função analisarSemantica() (desenvolvida pelos Alunos 2 e 3).\nIntegrar gerarArvoreAtribuida() e atualizar o main() (do Aluno 4) para orquestrar todo o processo.\nRealizar testes completos com os arquivos de teste, verificando a cadeia de execução.\n\nResolução de Conflitos: Discutir problemas imediatamente na sala ou de forma remota. Depuração Final: Testar o programa completo, validando a detecção de erros léxicos, sintáticos e semânticos, e a geração correta dos artefatos de saída.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Fase 3 - Analisador Semântico</span>"
    ]
  },
  {
    "objectID": "fase3.html#avaliação",
    "href": "fase3.html#avaliação",
    "title": "19  Fase 3 - Analisador Semântico",
    "section": "19.9 9. Avaliação",
    "text": "19.9 9. Avaliação\nO trabalho será pré-avaliado de forma automática e novamente durante a prova de autoria, com os seguintes critérios:\n\n19.9.1 9.1. Funcionalidades do Analisador (70%)\n\nImplementação correta da verificação de tipos: Base da nota;\nFalha na verificação de inicialização de memória: -20%;\nFalha na validação das estruturas de controle: -20%;\nCada regra de tipo não verificada corretamente (ex: expoente inteiro): -10%;\nFalha na geração da árvore sintática abstrata atribuída: -30%;\nGramática de atributos incompleta ou mal documentada: -20%.\n\n\n\n19.9.2 9.2. Organização e Legibilidade do Código (15%)\n\nCódigo claro, adequadamente comentado (funções principais e lógica complexa) e bem estruturado. Na dúvida, use ferramentas de IA para melhorar a qualidade dos comentários usando como referência o Google C++ Style Guide ou o PEP 8 para Python.\nREADME bem escrito contendo, no mínimo:\n\nNome da instituição de ensino, ano, disciplina, professor;\nIntegrantes do grupo em ordem alfabética;\nInstruções para compilar, executar e depurar;\nDocumentação da sintaxe das estruturas de controle.\n\nRepositório GitHub organizado com commits claros e pull requests.\n\nAtenção: a participação dos integrantes do grupo é fundamental para o sucesso do projeto. Esta participação será avaliada automaticamente, o desbalanceamento na participação dos integrantes do grupo resultará na redução da nota final do trabalho.\n\n\n19.9.3 9.3. Robustez (15%)\n\nTratamento de erros semânticos com mensagens claras, indicando linha e tipo de erro.\nTestes cobrindo todos os casos semânticos (válidos e inválidos).\nGeração correta de todos os arquivos de saída em markdown.\n\n\nAviso: Trabalhos identificados como cópias terão a nota zerada.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Fase 3 - Analisador Semântico</span>"
    ]
  },
  {
    "objectID": "fase3.html#sec-prova-autoria",
    "href": "fase3.html#sec-prova-autoria",
    "title": "19  Fase 3 - Analisador Semântico",
    "section": "19.10 10. Prova de Autoria",
    "text": "19.10 10. Prova de Autoria\n\nUm aluno do grupo será sorteado usando um aplicativo online disponível em https://frankalcantara.com/sorteio.html.\nDepois de explicar o projeto e responder as dúvidas do professor, o aluno sorteado escolherá um número de \\(1\\) a \\(10\\), que corresponderá a uma pergunta sobre o projeto;\nA falha na resposta, ou na explicação do projeto, implicará na redução de 35% da nota provisória que tenha sido atribuída ao projeto. Esta redução será aplicada para todo o grupo.\n\n\n\n\n\n\n\nImportant\n\n\n\nApesar da sugestão de divisão de tarefas, todos os alunos devem entender o funcionamento completo do projeto. O aluno sorteado para a prova de autoria deve ser capaz de responder qualquer pergunta sobre qualquer parte do projeto.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Fase 3 - Analisador Semântico</span>"
    ]
  },
  {
    "objectID": "fase3.html#entrega",
    "href": "fase3.html#entrega",
    "title": "19  Fase 3 - Analisador Semântico",
    "section": "19.11 11. Entrega",
    "text": "19.11 11. Entrega\nA entrega será um link para um repositório do GitHub contendo:\n\n19.11.1 11.1. Código-fonte\n\nPrograma completo em Python, C, ou C++;\nTodas as funções especificadas implementadas.\n\n\n\n19.11.2 11.2. Arquivos de Teste\n\nMínimo de \\(3\\) arquivos com \\(10\\) linhas, ou mais, cada;\nCasos de teste válidos e inválidos (léxicos, sintáticos e semânticos);\nExemplos com estruturas de controle.\n\n\n\n19.11.3 11.3. Documentação\n\nREADME.md bem formatado com:\n\nInformações institucionais;\nInstruções de compilação e execução;\nSintaxe das estruturas de controle;\nExemplos de uso.\n\n\n\n\n19.11.4 11.4. Arquivos de Saída\nArquivos markdown gerados pela execução do programa:\n\nA gramática de atributos da linguagem.\nO relatório de julgamento de tipos.\nO relatório de erros semânticos encontrados.\nA representação da árvore sintática abstrata atribuída de um arquivo de teste.\n\n\n\n19.11.5 11.5. Formato de Execução\nO programa deve ser executado com: ./compilar teste1.txt.",
    "crumbs": [
      "Projetos da Disciplina - 2025-2",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Fase 3 - Analisador Semântico</span>"
    ]
  },
  {
    "objectID": "apend1.html",
    "href": "apend1.html",
    "title": "20  Apêndice 1: A Relação de Myhill-Nerode",
    "section": "",
    "text": "20.1 Definição Formal da Relação\nSeja \\(L\\) uma linguagem sobre um alfabeto \\(\\Sigma\\). A relação de Myhill-Nerode para \\(L\\) é uma relação de equivalência \\(\\sim_L\\) definida sobre \\(\\Sigma^*\\) (o conjunto de todas as strings possíveis sobre \\(\\Sigma\\)).\nDefinição: Para duas strings \\(x, y \\in \\Sigma^*\\), dizemos que \\(x \\sim_L y\\) (lê-se “\\(x\\) é equivalente a \\(y\\) módulo \\(L\\)”) se, e somente se:\n\\[\\forall z \\in \\Sigma^*: (xz \\in L \\iff yz \\in L)\\]\nEm outras palavras, duas strings são equivalentes segundo Myhill-Nerode se, ao concatenarmos qualquer string \\(z\\) a cada uma delas, ambas as strings resultantes têm o mesmo comportamento em relação à linguagem \\(L\\): ou ambas pertencem a \\(L\\), ou ambas não pertencem.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Apêndice 1: A Relação de Myhill-Nerode</span>"
    ]
  },
  {
    "objectID": "apend1.html#definição-formal-da-relação",
    "href": "apend1.html#definição-formal-da-relação",
    "title": "20  Apêndice 1: A Relação de Myhill-Nerode",
    "section": "",
    "text": "20.1.1 Intuição por Trás da Definição\nA intuição fundamental é que strings equivalentes são indistinguíveis do ponto de vista de \\(L\\). Se um autômato finito está processando uma entrada e chega a um estado após ler \\(x\\), ele deveria estar no mesmo estado após ler \\(y\\) (se \\(x \\sim_L y\\)), porque qualquer continuação da entrada (\\(z\\)) levará ao mesmo resultado.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Apêndice 1: A Relação de Myhill-Nerode</span>"
    ]
  },
  {
    "objectID": "apend1.html#classes-de-equivalência",
    "href": "apend1.html#classes-de-equivalência",
    "title": "20  Apêndice 1: A Relação de Myhill-Nerode",
    "section": "20.2 Classes de Equivalência",
    "text": "20.2 Classes de Equivalência\nA relação \\(\\sim_L\\) particiona \\(\\Sigma^*\\) em classes de equivalência. Cada classe agrupa todas as strings que são mutuamente equivalentes segundo a relação.\nNotação: A classe de equivalência de uma string \\(x\\) é denotada por \\([x]_L\\) ou simplesmente \\([x]\\) quando \\(L\\) está claro no contexto:\n\\[[x]_L = \\{y \\in \\Sigma^* \\mid x \\sim_L y\\}\\]\n\n20.2.1 Exemplo Fundamental\nConsidere a linguagem \\(L = \\{w \\in \\{0,1\\}^* \\mid w \\text{ termina em } 01\\}\\).\nAnálise das classes:\n\nStrings que terminam em \\(0\\): \\([0] = \\{0, 10, 00, 110, 010, \\ldots\\}\\)\n\nPara qualquer \\(z\\), temos \\(xz \\in L\\) se e somente se \\(z\\) começar com \\(1\\)\n\nStrings que terminam em \\(1\\) (exceto \\(01\\)): \\([1] = \\{1, 11, 001, 101, \\ldots\\}\\)\n\nPara qualquer \\(z\\), temos \\(xz \\in L\\) se e somente se \\(z = \\epsilon\\) ou \\(z\\) começar com \\(0\\) seguido de algo que termine em \\(1\\)\n\nStrings que terminam em \\(01\\): \\([01] = \\{01, 101, 001, 1001, \\ldots\\}\\)\n\nTemos \\(x\\epsilon = x \\in L\\), então \\(z = \\epsilon\\) sempre leva à aceitação\n\nString vazia e outras: \\([\\epsilon] = \\{\\epsilon, 11, 0011, \\ldots\\}\\)\n\nApós análise cuidadosa, esta linguagem possui exatamente 3 classes de equivalência.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Apêndice 1: A Relação de Myhill-Nerode</span>"
    ]
  },
  {
    "objectID": "apend1.html#o-teorema-de-myhill-nerode",
    "href": "apend1.html#o-teorema-de-myhill-nerode",
    "title": "20  Apêndice 1: A Relação de Myhill-Nerode",
    "section": "20.3 O Teorema de Myhill-Nerode",
    "text": "20.3 O Teorema de Myhill-Nerode\nO teorema principal estabelece a conexão fundamental entre regularidade e finitude das classes de equivalência.\nTeorema (Myhill-Nerode): Uma linguagem \\(L\\) é regular se, e somente se, a relação \\(\\sim_L\\) possui um número finito de classes de equivalência.\n\n20.3.1 Demonstração (Esboço)\n(\\(\\Rightarrow\\)) Se \\(L\\) é regular, então \\(\\sim_L\\) tem finitas classes:\nSe \\(L\\) é regular, existe um autômato finito determinístico (AFD) \\(M\\) com \\(n\\) estados que reconhece \\(L\\). Definimos uma função \\(f: \\Sigma^* \\to Q\\) (onde \\(Q\\) é o conjunto de estados) que mapeia cada string para o estado em que \\(M\\) para após processá-la.\nSe \\(f(x) = f(y)\\), então para qualquer \\(z\\), o autômato, partindo do mesmo estado, chegará ao mesmo estado final ao processar \\(z\\). Logo, \\(xz \\in L \\iff yz \\in L\\), provando que \\(x \\sim_L y\\).\nComo há no máximo \\(n\\) estados distintos, há no máximo \\(n\\) classes de equivalência.\n(\\(\\Leftarrow\\)) Se \\(\\sim_L\\) tem finitas classes, então \\(L\\) é regular:\nSuponha que existam \\(k\\) classes de equivalência: \\(C_1, C_2, \\ldots, C_k\\). Construímos um AFD onde:\n\nEstados: \\(Q = \\{C_1, C_2, \\ldots, C_k\\}\\)\nEstado inicial: \\(C_i\\) tal que \\(\\epsilon \\in C_i\\)\n\nEstados finais: \\(F = \\{C_i \\mid \\exists x \\in C_i: x \\in L\\}\\)\nFunção de transição: \\(\\delta(C_i, a) = C_j\\) onde \\(j\\) é tal que se \\(x \\in C_i\\), então \\(xa \\in C_j\\)\n\nEste autômato reconhece exatamente \\(L\\).",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Apêndice 1: A Relação de Myhill-Nerode</span>"
    ]
  },
  {
    "objectID": "apend1.html#aplicações-práticas",
    "href": "apend1.html#aplicações-práticas",
    "title": "20  Apêndice 1: A Relação de Myhill-Nerode",
    "section": "20.4 Aplicações Práticas",
    "text": "20.4 Aplicações Práticas\n\n20.4.1 1. Determinação de Regularidade\nA relação de Myhill-Nerode fornece um método sistemático para provar que uma linguagem não é regular:\nExemplo: \\(L = \\{a^nb^n \\mid n \\geq 0\\}\\)\nPara cada \\(i \\neq j\\), as strings \\(a^i\\) e \\(a^j\\) estão em classes diferentes, de forma que:\n\n\\(a^i b^i \\in L\\) mas \\(a^j b^i \\notin L\\) (quando \\(i \\neq j\\))\n\nComo há infinitas strings \\(a^i\\) duas-a-duas não equivalentes, \\(L\\) não é regular.\n\n\n20.4.2 2. Minimização de Autômatos\nO teorema de Myhill-Nerode também caracteriza o autômato mínimo para uma linguagem regular:\nTeorema: Se \\(L\\) é uma linguagem regular com \\(k\\) classes de equivalência segundo Myhill-Nerode, então: 1. Todo AFD que reconhece \\(L\\) tem pelo menos \\(k\\) estados 2. Existe um único AFD (a menos de isomorfismo) com exatamente \\(k\\) estados que reconhece \\(L\\)\nEste AFD é chamado de autômato canônico ou autômato mínimo para \\(L\\).\n\n\n20.4.3 3. Algoritmo de Construção do Autômato Mínimo\nEntrada: Uma linguagem regular \\(L\\)\nSaída: O autômato mínimo para \\(L\\)\nPassos: 1. Identificar representantes: Encontre strings representativas de cada classe de equivalência 2. Construir estados: Cada classe torna-se um estado 3. Definir transições: Para cada classe \\([x]\\) e símbolo \\(a\\), determine \\([xa]\\) 4. Marcar estados finais: Classes contendo strings de \\(L\\)",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Apêndice 1: A Relação de Myhill-Nerode</span>"
    ]
  },
  {
    "objectID": "apend1.html#exemplo-detalhado-construção-passo-a-passo",
    "href": "apend1.html#exemplo-detalhado-construção-passo-a-passo",
    "title": "20  Apêndice 1: A Relação de Myhill-Nerode",
    "section": "20.5 Exemplo Detalhado: Construção Passo a Passo",
    "text": "20.5 Exemplo Detalhado: Construção Passo a Passo\nLinguagem: \\(L = \\{w \\in \\{a,b\\}^* \\mid |w|_a \\text{ é par}\\}\\) (número par de \\(a\\)’s)\nPasso 1 - Identificação das classes:\nPrecisamos rastrear apenas a paridade do número de \\(a\\)’s lidos:\n\nClasse \\(C_0\\) (par de \\(a\\)’s): \\([\\epsilon] = \\{b^*ab^*ab^*...\\}\\) onde há um número par de \\(a\\)’s\nClasse \\(C_1\\) (ímpar de \\(a\\)’s): \\([a] = \\{b^*ab^*...\\}\\) onde há um número ímpar de \\(a\\)’s\n\nPasso 2 - Verificação da equivalência:\nPara \\(x, y \\in C_0\\): qualquer \\(z\\) mudará ambos da mesma forma (mantendo ou alternando paridade)\nPara \\(x \\in C_0, y \\in C_1\\): existe \\(z = \\epsilon\\) tal que \\(x \\in L\\) mas \\(y \\notin L\\)\nPasso 3 - Construção do autômato:\nEstados: {C_0, C_1}\nEstado inicial: C_0\nEstados finais: {C_0}\n\nTransições:\nC_0 --a--&gt; C_1\nC_0 --b--&gt; C_0  \nC_1 --a--&gt; C_0\nC_1 --b--&gt; C_1\nResultado: Autômato mínimo com 2 estados.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Apêndice 1: A Relação de Myhill-Nerode</span>"
    ]
  },
  {
    "objectID": "apend1.html#propriedades-importantes",
    "href": "apend1.html#propriedades-importantes",
    "title": "20  Apêndice 1: A Relação de Myhill-Nerode",
    "section": "20.6 Propriedades Importantes",
    "text": "20.6 Propriedades Importantes\n\n20.6.1 1. Refinamento de Relações\nSe \\(\\approx\\) é qualquer relação de equivalência sobre \\(\\Sigma^*\\) que respeita \\(L\\) (isto é, se \\(x \\approx y\\) então \\(\\forall z: xz \\in L \\iff yz \\in L\\)), então \\(\\sim_L\\) é um refinamento de \\(\\approx\\). Em outras palavras, Myhill-Nerode é a relação mais fina possível que mantém as propriedades da linguagem.\n\n\n20.6.2 2. Invariância por Operações\nA relação de Myhill-Nerode se comporta bem com operações regulares:\n\nUnião: \\(\\sim_{L_1 \\cup L_2}\\) tem no máximo \\(|\\sim_{L_1}| \\times |\\sim_{L_2}|\\) classes\nConcatenação: Comportamento mais complexo, mas ainda finito para linguagens regulares\nFechamento de Kleene: Preserve a finitude das classes\n\n\n\n20.6.3 3. Conexão com Congruências\nA relação \\(\\sim_L\\) é não apenas uma equivalência, mas também uma congruência à direita: se \\(x \\sim_L y\\), então \\(xz \\sim_L yz\\) para qualquer \\(z \\in \\Sigma^*\\).",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Apêndice 1: A Relação de Myhill-Nerode</span>"
    ]
  },
  {
    "objectID": "apend1.html#limitações-e-extensões",
    "href": "apend1.html#limitações-e-extensões",
    "title": "20  Apêndice 1: A Relação de Myhill-Nerode",
    "section": "20.7 Limitações e Extensões",
    "text": "20.7 Limitações e Extensões\n\n20.7.1 Limitações\n\nComputabilidade: Para linguagens arbitrárias, determinar as classes pode ser indecidível\nComplexidade: Mesmo para linguagens regulares, o número de classes pode ser exponencial no tamanho da descrição\nAplicabilidade: Limitado a linguagens regulares por definição\n\n\n\n20.7.2 Extensões Modernas\n\nRelações de Nerode Generalizadas: Para linguagens livres de contexto\nMyhill-Nerode Probabilístico: Para autômatos probabilísticos\n\nVersões Categóricas: Em teoria das categorias e sistemas de tipos",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Apêndice 1: A Relação de Myhill-Nerode</span>"
    ]
  },
  {
    "objectID": "apend1.html#conclusão-a-elegância-da-teoria",
    "href": "apend1.html#conclusão-a-elegância-da-teoria",
    "title": "20  Apêndice 1: A Relação de Myhill-Nerode",
    "section": "20.8 Conclusão: A Elegância da Teoria",
    "text": "20.8 Conclusão: A Elegância da Teoria\nA relação de Myhill-Nerode exemplifica a beleza da matemática teórica aplicada à ciência da computação. Com uma definição simples e elegante, ela resolve problemas fundamentais:\n\nCaracteriza completamente as linguagens regulares\nFornece algoritmos para minimização de autômatos\nEstabelece limites teóricos para a representação de linguagens\nConecta aspectos algébricos e combinatórios da teoria\n\nPara a engenhosa leitora, esta teoria representa não apenas uma ferramenta técnica, mas um exemplo paradigmático de como abstração matemática pode revelar estruturas profundas em sistemas computacionais. A relação de Myhill-Nerode continua sendo uma das joias conceituais da ciência da computação teórica, influenciando desenvolvimentos em áreas que vão desde compiladores até aprendizado de máquina.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Apêndice 1: A Relação de Myhill-Nerode</span>"
    ]
  },
  {
    "objectID": "sol-exercicios.html",
    "href": "sol-exercicios.html",
    "title": "21  Solução dos Exercícios",
    "section": "",
    "text": "21.1 Capítulo: Chapter 3",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Solução dos Exercícios</span>"
    ]
  },
  {
    "objectID": "sol-exercicios.html#capítulo-sec-alfabeto-linguagem-string",
    "href": "sol-exercicios.html#capítulo-sec-alfabeto-linguagem-string",
    "title": "21  Solução dos Exercícios",
    "section": "",
    "text": "21.1.1 Exercícios 1: Section 3.1.3\n\nSolução:\n\n\n\\(\\Sigma_1 = \\{a, b, c, +, -, *, /, (, )\\}\\)\nContando cada símbolo: \\(a, b, c, +, -, *, /, (, )\\)\n\\[|\\Sigma_1| = 9\\]\n\\(\\Sigma_2 = \\{0, 1, 2, \\ldots, 9, A, B, C, D, E, F\\}\\) (hexadecimal)\nDígitos decimais: \\(\\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\\}\\) → 10 símbolos\nLetras hexadecimais: \\(\\{A, B, C, D, E, F\\}\\) → 6 símbolos\n\\[|\\Sigma_2| = 10 + 6 = 16\\]\n\\(\\Sigma_3 = \\{\\text{verdadeiro}, \\text{falso}, \\land, \\lor, \\neg, (, )\\}\\)\nContando cada símbolo: \\(\\text{verdadeiro}, \\text{falso}, \\land, \\lor, \\neg, (, )\\)\n\\[|\\Sigma_3| = 7\\]\n\n\nSolução:\n\n\nExpressões lógicas booleanas simples com variáveis \\(p\\), \\(q\\), \\(r\\):\n\\[\\Sigma_{\\text{bool}} = \\{p, q, r, \\land, \\lor, \\neg, (, )\\}\\]\nJustificativa: Inclui as três variáveis proposicionais, os conectivos lógicos básicos (conjunção, disjunção, negação) e parênteses para agrupamento.\nNúmeros em notação científica (ex: \\(1.23 \\times 10^{-4}\\)):\n\\[\\Sigma_{\\text{cient}} = \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, +, -, ., \\times, ^{}, 1, 0\\}\\]\nOu de forma mais concisa:\n\\[\\Sigma_{\\text{cient}} = \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, +, -, ., \\times, ^\\}\\]\nJustificativa: Inclui dígitos para o número base, sinais para número e expoente, ponto decimal, símbolo de multiplicação e símbolo de potência.\nCoordenadas cartesianas no formato \\((x, y)\\):\n\\[\\Sigma_{\\text{coord}} = \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, +, -, ., (, ), ,\\}\\]\nJustificativa: Inclui dígitos para números, sinais, ponto decimal, parênteses para delimitação e vírgula como separador.\n\n\nSolução:\n\n\n\\(A = \\emptyset\\)\nNÃO é um alfabeto válido.\nJustificativa: A definição formal exige que um alfabeto seja um conjunto finito não-vazio. Como \\(A\\) é o conjunto vazio, ele viola a condição \\(n \\geq 1\\).\n\\(B = \\{\\epsilon\\}\\)\nÉ um alfabeto válido.\nJustificativa: É um conjunto finito (\\(|B| = 1\\)) e não-vazio. O símbolo \\(\\epsilon\\) pode ser tratado como um símbolo atômico qualquer para fins de definição de alfabeto.\n\\(C = \\{1, 2, 3, \\ldots\\}\\)\nNÃO é um alfabeto válido.\nJustificativa: Este é o conjunto dos números naturais positivos, que é infinito (\\(|C| = \\infty\\)). A definição de alfabeto exige finitude.\n\\(D = \\{a, b, a\\}\\)\nÉ um alfabeto válido, mas equivalente a \\(\\{a, b\\}\\).\nJustificativa: Como conjuntos não possuem elementos repetidos, \\(D = \\{a, b\\}\\). É finito (\\(|D| = 2\\)) e não-vazio, portanto é um alfabeto válido.\n\n\nSolução:\n\nDado \\(S = \\{\\text{if}, \\text{then}, \\text{else}, \\text{fi}\\}\\), precisamos encontrar todos os símbolos que aparecem nas strings:\n\n\\(\\text{if}\\): símbolos \\(i, f\\)\n\\(\\text{then}\\): símbolos \\(t, h, e, n\\)\n\n\\(\\text{else}\\): símbolos \\(e, l, s, e\\) (note que \\(e\\) se repete)\n\\(\\text{fi}\\): símbolos \\(f, i\\)\n\nColetando todos os símbolos únicos:\n\\[\\Sigma_{\\text{mín}} = \\{e, f, h, i, l, n, s, t\\}\\]\nCardinalidade: \\(|\\Sigma_{\\text{mín}}| = 8\\)\nVerificação: Todas as strings em \\(S\\) podem ser formadas usando apenas estes símbolos, e nenhum símbolo pode ser removido sem impossibilitar a formação de pelo menos uma string.\n\nSolução:\n\nComparando \\(\\Sigma_A = \\{0, 1\\}\\) e \\(\\Sigma_B = \\{a, b, c\\}\\):\nCardinalidade: - \\(|\\Sigma_A| = 2\\) - \\(|\\Sigma_B| = 3\\)\nNúmero de strings de comprimento 3:\nPara um alfabeto de tamanho \\(n\\), o número de strings de comprimento \\(k\\) é \\(n^k\\).\n\nPara \\(\\Sigma_A\\): \\(|\\Sigma_A|^3 = 2^3 = 8\\) strings\nStrings: \\(\\{000, 001, 010, 011, 100, 101, 110, 111\\}\\)\nPara \\(\\Sigma_B\\): \\(|\\Sigma_B|^3 = 3^3 = 27\\) strings\nExemplos: \\(\\{aaa, aab, aac, aba, abb, abc, \\ldots, ccc\\}\\)\n\nAplicabilidade para representar números binários:\n\n\\(\\Sigma_A\\): Perfeitamente adequado para representação binária, porque contém exatamente os símbolos \\(0\\) e \\(1\\) necessários.\n\\(\\Sigma_B\\): Inadequado para representação binária direta. Seria necessário estabelecer uma convenção de mapeamento (por exemplo, \\(a \\rightarrow 0\\), \\(b \\rightarrow 1\\), e \\(c\\) seria um símbolo extra não utilizado).\n\nConclusão: \\(\\Sigma_A\\) é mais eficiente para representação binária, enquanto \\(\\Sigma_B\\) oferece maior capacidade expressiva para outras aplicações devido ao seu maior tamanho.\n\n\n21.1.2 Exercícios 2: Section 3.2.4\n\nSolução:\n\nDado: \\(x = ab\\) e \\(y = cd\\)\nConcatenações: - \\(xy = ab \\cdot cd = abcd\\) - \\(yx = cd \\cdot ab = cdab\\)\nObservação: Note que \\(xy \\neq yx\\), demonstrando que a concatenação não é comutativa.\nPotências: - \\(x^3 = (ab)^3 = ab \\cdot ab \\cdot ab = ababab\\) - \\(y^2 = (cd)^2 = cd \\cdot cd = cdcd\\)\nPotências de concatenações: - \\((xy)^2 = (abcd)^2 = abcd \\cdot abcd = abcdabcd\\) - \\(x^2y^2 = (ab)^2(cd)^2 = abab \\cdot cdcd = ababcdcd\\)\nObservação: \\((xy)^2 \\neq x^2y^2\\), mostrando que \\((uv)^n \\neq u^nv^n\\) em geral.\nComprimento de \\(x^n\\):\nComo \\(|x| = |ab| = 2\\), temos:\n\\[|x^n| = n \\cdot |x| = n \\cdot 2 = 2n\\]\nVerificação: \\(|x^3| = |ababab| = 6 = 2 \\cdot 3\\)\n\nSolução:\n\nAplicando a definição recursiva \\(\\epsilon^R = \\epsilon\\) e \\((wa)^R = aw^R\\):\n\n\\(w_1 = abcde\\)\nAplicando passo a passo: \\[w_1^R = (abcde)^R = e(abcd)^R = ed(abc)^R = edc(ab)^R = edcb(a)^R = edcba\\]\n\\(w_2 = palindromo\\)\n\\[w_2^R = (palindromo)^R = omordnilap\\]\n\\(w_3 = \\epsilon\\) (string vazia)\n\\[w_3^R = \\epsilon^R = \\epsilon\\]\n\nProva de que \\((\\epsilon)^R = \\epsilon\\):\nDemonstração: Pela definição recursiva, o caso base estabelece diretamente que \\(\\epsilon^R = \\epsilon\\). Isso é consistente, porque a string vazia não possui símbolos para inverter, mantendo-se inalterada.\n\nSolução:\n\nAssociatividade: \\((xy)z = x(yz)\\) para \\(x = a\\), \\(y = bc\\), \\(z = d\\)\n\\[\\text{Lado esquerdo: } (xy)z = (a \\cdot bc)d = (abc)d = abcd\\] \\[\\text{Lado direito: } x(yz) = a(bc \\cdot d) = a(bcd) = abcd\\]\n\\[\\therefore (xy)z = x(yz) = abcd\\]\nElemento neutro: \\(w\\epsilon = \\epsilon w = w\\) para \\(w = abc\\)\n\\[w\\epsilon = abc \\cdot \\epsilon = abc\\] \\[\\epsilon w = \\epsilon \\cdot abc = abc\\]\n\\[\\therefore w\\epsilon = \\epsilon w = w = abc\\]\nNão-comutatividade: Encontrar \\(x\\) e \\(y\\) tais que \\(xy \\neq yx\\)\nExemplo: \\(x = a\\) e \\(y = b\\)\n\\[xy = a \\cdot b = ab\\] \\[yx = b \\cdot a = ba\\]\nComo \\(ab \\neq ba\\), a concatenação não é comutativa.\n\nSolução:\n\nDado: \\(w = compilador\\)\nPrefixos próprios (todos os prefixos exceto a própria string):\n\\[\\{\\epsilon, c, co, com, comp, compi, compil, compila, compilado\\}\\]\nTotal de prefixos próprios: 9\nSufixos próprios (todos os sufixos exceto a própria string):\n\\[\\{\\epsilon, r, or, dor, ador, lador, ilador, pilador, mpilador\\}\\]\nTotal de sufixos próprios: 9\nSubstrings de comprimento 4:\nPosições possíveis para substrings de comprimento 4 em uma string de comprimento 10:\n\\[\\{comp, ompi, mpil, pila, ilad, lado, ador\\}\\]\nTotal de substrings de comprimento 4: 7\nContagem total: - Prefixos totais: 10 (incluindo \\(\\epsilon\\) e a própria string) - Sufixos totais: 10 (incluindo \\(\\epsilon\\) e a própria string)\nFórmula geral: Para uma string de comprimento \\(n\\), existem \\(n+1\\) prefixos e \\(n+1\\) sufixos.\n\nSolução:\n\nDado: \\(w = aba\\)\nCálculo de \\(w^R\\): \\[w^R = (aba)^R = a(ba)^R = ab(a)^R = aba\\]\nObservação: \\(w = aba\\) é um palíndromo, logo \\(w^R = w\\).\n\\((w^R)^2\\): \\[w^R = aba\\] \\[(w^R)^2 = (aba)^2 = aba \\cdot aba = abaaba\\]\n\\((w^2)^R\\): \\[w^2 = aba \\cdot aba = abaaba\\] \\[(w^2)^R = (abaaba)^R = abaaba\\]\n\\(w^R w\\): \\[w^R w = aba \\cdot aba = abaaba\\]\nVerificação se \\((w^2)^R = (w^R)^2\\):\n\\[\\text{Lado esquerdo: } (w^2)^R = abaaba\\] \\[\\text{Lado direito: } (w^R)^2 = abaaba\\]\n\\[\\therefore (w^2)^R = (w^R)^2\\]\nExplicação: Esta igualdade vale neste caso específico porque \\(w\\) é um palíndromo (\\(w = w^R\\)). Em geral, para strings arbitrárias, \\((w^n)^R = (w^R)^n\\), que se reduz a \\(w^n = w^n\\) quando \\(w\\) é um palíndromo.\nPropriedade geral: Para qualquer string \\(u\\) e inteiro positivo \\(n\\): \\[(u^n)^R = (u^R)^n\\]\nNo nosso caso, como \\(w^R = w\\), ambos os lados se tornam \\(w^2 = abaaba\\).\n\n\n21.1.3 Exercícios 3: Section 3.3.5\n\nSolução:\n\nDado: \\(L_1 = \\{a, ab, b\\}\\) e \\(L_2 = \\{b, ba, \\epsilon\\}\\)\nUnião: \\(L_1 \\cup L_2\\) \\[L_1 \\cup L_2 = \\{a, ab, b\\} \\cup \\{b, ba, \\epsilon\\} = \\{a, ab, b, ba, \\epsilon\\}\\]\nInterseção: \\(L_1 \\cap L_2\\) \\[L_1 \\cap L_2 = \\{a, ab, b\\} \\cap \\{b, ba, \\epsilon\\} = \\{b\\}\\]\nDiferença: \\(L_1 - L_2\\) \\[L_1 - L_2 = \\{a, ab, b\\} - \\{b, ba, \\epsilon\\} = \\{a, ab\\}\\]\nCardinalidades: - \\(|L_1 \\cup L_2| = |\\{a, ab, b, ba, \\epsilon\\}| = 5\\) - \\(|L_1 \\cap L_2| = |\\{b\\}| = 1\\)\nVerificação: \\(|L_1| + |L_2| - |L_1 \\cap L_2| = 3 + 3 - 1 = 5 = |L_1 \\cup L_2|\\)\n\nSolução:\n\nDado: \\(L_1 = \\{a, bb\\}\\) e \\(L_2 = \\{c, dd\\}\\)\n\\(L_1 \\cdot L_2\\):\nAplicando a definição \\(L_1 \\cdot L_2 = \\{xy \\mid x \\in L_1 \\text{ e } y \\in L_2\\}\\):\n\\[L_1 \\cdot L_2 = \\{ac, add, bbc, bbdd\\}\\]\nCálculo detalhado: - \\(a \\cdot c = ac\\) - \\(a \\cdot dd = add\\)\n- \\(bb \\cdot c = bbc\\) - \\(bb \\cdot dd = bbdd\\)\n\\(L_2 \\cdot L_1\\):\n\\[L_2 \\cdot L_1 = \\{ca, cbb, dda, ddbb\\}\\]\nCálculo detalhado: - \\(c \\cdot a = ca\\) - \\(c \\cdot bb = cbb\\) - \\(dd \\cdot a = dda\\) - \\(dd \\cdot bb = ddbb\\)\nCardinalidade: \\[|L_1 \\cdot L_2| = |L_1| \\times |L_2| = 2 \\times 2 = 4\\] \\[|L_2 \\cdot L_1| = |L_2| \\times |L_1| = 2 \\times 2 = 4\\]\nVerificação de comutatividade: \\[L_1 \\cdot L_2 = \\{ac, add, bbc, bbdd\\}\\] \\[L_2 \\cdot L_1 = \\{ca, cbb, dda, ddbb\\}\\]\nComo os conjuntos são distintos, \\(L_1 \\cdot L_2 \\neq L_2 \\cdot L_1\\). A concatenação de linguagens não é comutativa.\n\nSolução:\n\nDado: \\(L = \\{a, b\\}\\)\n\\(L^0\\): Por definição, \\(L^0 = \\{\\epsilon\\}\\) para qualquer linguagem \\(L\\).\n\\(L^1\\): \\[L^1 = L = \\{a, b\\}\\]\n\\(L^2\\): \\[L^2 = L \\cdot L = \\{xy \\mid x \\in L \\text{ e } y \\in L\\}\\] \\[L^2 = \\{aa, ab, ba, bb\\}\\]\nCálculo detalhado de \\(L^2\\): - \\(a \\cdot a = aa\\) - \\(a \\cdot b = ab\\) - \\(b \\cdot a = ba\\) - \\(b \\cdot b = bb\\)\nFórmula para \\(|L^n|\\):\nComo \\(|L| = 2\\), temos: \\[|L^n| = |L|^n = 2^n\\]\nVerificação: - \\(|L^0| = |\\{\\epsilon\\}| = 1 = 2^0\\) - \\(|L^1| = |\\{a, b\\}| = 2 = 2^1\\)\n- \\(|L^2| = |\\{aa, ab, ba, bb\\}| = 4 = 2^2\\)\nTrês primeiras strings de \\(L^3\\) em ordem lexicográfica:\n\\(L^3\\) contém todas as strings de comprimento 3 sobre \\(\\{a, b\\}\\).\nEm ordem lexicográfica: \\(\\{aaa, aab, aba, abb, baa, bab, bba, bbb\\}\\)\nTrês primeiras: \\(aaa, aab, aba\\)\n\nSolução:\n\nDado: \\(L = \\{ab\\}\\)\nElementos de \\(L^*\\) até strings de comprimento 6:\nPela definição: \\(L^* = L^0 \\cup L^1 \\cup L^2 \\cup L^3 \\cup \\ldots\\)\n\n\\(L^0 = \\{\\epsilon\\}\\)\n\\(L^1 = \\{ab\\}\\) (comprimento 2)\n\\(L^2 = \\{ab \\cdot ab\\} = \\{abab\\}\\) (comprimento 4)\n\n\\(L^3 = \\{ab \\cdot ab \\cdot ab\\} = \\{ababab\\}\\) (comprimento 6)\n\n\\[L^* \\text{ até comprimento 6} = \\{\\epsilon, ab, abab, ababab\\}\\]\n\\(L^+\\): \\[L^+ = L^1 \\cup L^2 \\cup L^3 \\cup \\ldots = L^* - \\{\\epsilon\\}\\] \\[L^+ = \\{ab, abab, ababab, abababab, \\ldots\\}\\]\nVerificações:\n\\(\\epsilon \\in L^*\\)? Sim, porque \\(\\epsilon \\in L^0\\) e \\(L^0 \\subseteq L^*\\).\n\\(\\epsilon \\in L^+\\)? Não, porque \\(L^+ = L^1 \\cup L^2 \\cup \\ldots\\) e \\(\\epsilon \\notin L^n\\) para \\(n \\geq 1\\) quando \\(L = \\{ab\\}\\).\nPadrão geral: Como \\(L = \\{ab\\}\\) contém apenas uma string de comprimento 2, temos: \\[L^* = \\{\\epsilon, ab, (ab)^2, (ab)^3, \\ldots\\} = \\{(ab)^n \\mid n \\geq 0\\}\\] \\[L^+ = \\{ab, (ab)^2, (ab)^3, \\ldots\\} = \\{(ab)^n \\mid n \\geq 1\\}\\]\n\nSolução:\n\nDado: \\(L = \\{a\\}\\)\nPropriedade 1: \\(L^* = L^+\\)?\n\\[L^* = \\{\\epsilon, a, aa, aaa, \\ldots\\} = \\{a^n \\mid n \\geq 0\\}\\] \\[L^+ = \\{a, aa, aaa, \\ldots\\} = \\{a^n \\mid n \\geq 1\\}\\]\nComo \\(\\epsilon \\in L^*\\) mas \\(\\epsilon \\notin L^+\\), temos \\(L^* \\neq L^+\\).\nResposta: FALSO\nPropriedade 2: \\(L^* \\cup L^+ = L^*\\)?\nComo \\(L^+ \\subseteq L^*\\) (pela definição \\(L^+ = L^* - \\{\\epsilon\\}\\) quando \\(\\epsilon \\notin L\\)), temos: \\[L^* \\cup L^+ = L^*\\]\nResposta: VERDADEIRO\nPropriedade 3: \\((L^*)^* = L^*\\)?\nEsta é uma propriedade geral do fechamento de Kleene.\nDemonstração: - \\((L^*)^*\\) contém todas as concatenações possíveis de elementos de \\(L^*\\) - Como \\(L^* = \\{a^n \\mid n \\geq 0\\}\\), qualquer concatenação de elementos de \\(L^*\\) resulta em \\(a^k\\) para algum \\(k \\geq 0\\) - Portanto, \\((L^*)^* \\subseteq L^*\\) - Como \\(L^* \\subseteq (L^*)^*\\) (porque \\(L^* \\subseteq (L^*)^1 \\subseteq (L^*)^*\\)), temos a igualdade\nResposta: VERDADEIRO\nPropriedade 4: Se \\(\\epsilon \\in L\\), então \\(L^+ = L^*\\)?\nNo nosso caso, \\(L = \\{a\\}\\) e \\(\\epsilon \\notin L\\), então a premissa é falsa.\nMas vamos analisar o caso geral: se \\(\\epsilon \\in L\\), então: - \\(L^* = L^0 \\cup L^1 \\cup L^2 \\cup \\ldots\\) - \\(L^+ = L^1 \\cup L^2 \\cup L^3 \\cup \\ldots\\) - Como \\(\\epsilon \\in L = L^1\\), temos \\(\\epsilon \\in L^+\\) - Como \\(\\epsilon \\in L^0\\) também, e \\(L^+ \\supseteq L^1\\), de fato \\(L^+ = L^*\\)\nResposta: VERDADEIRO (propriedade geral válida)\nPara nosso caso específico: A propriedade não se aplica porque \\(\\epsilon \\notin \\{a\\}\\).\n\n\n21.1.4 Exercícios 4: Section 3.4.3.1\n\nSolução:\n\n\\(r_1 = a \\cup b\\)\nAplicando a regra indutiva para união: \\[L(r_1) = L(a \\cup b) = L(a) \\cup L(b) = \\{a\\} \\cup \\{b\\} = \\{a, b\\}\\]\n\\(r_2 = (a \\cup b)(a \\cup b)\\)\nAplicando as regras de concatenação e união: \\[L(r_2) = L(a \\cup b) \\cdot L(a \\cup b) = \\{a, b\\} \\cdot \\{a, b\\}\\]\nCalculando o produto cartesiano: \\[L(r_2) = \\{aa, ab, ba, bb\\}\\]\n\\(r_3 = a^*b\\)\nAplicando as regras de fechamento de Kleene e concatenação: \\[L(r_3) = L(a^*) \\cdot L(b) = L(a)^* \\cdot \\{b\\} = \\{a\\}^* \\cdot \\{b\\}\\]\nComo \\(\\{a\\}^* = \\{\\epsilon, a, aa, aaa, \\ldots\\} = \\{a^n \\mid n \\geq 0\\}\\): \\[L(r_3) = \\{a^n b \\mid n \\geq 0\\} = \\{b, ab, aab, aaab, \\ldots\\}\\]\n\\(r_4 = (ab)^*\\)\nAplicando a regra do fechamento de Kleene: \\[L(r_4) = L(ab)^* = \\{ab\\}^*\\]\nComo visto no exercício anterior: \\[L(r_4) = \\{(ab)^n \\mid n \\geq 0\\} = \\{\\epsilon, ab, abab, ababab, \\ldots\\}\\]\n\nSolução:\n\nLembrando a ordem de precedência (maior para menor): Kleene (\\(*\\)) → Concatenação (\\(\\cdot\\)) → União (\\(\\cup\\))\n\\(ab^* \\cup c\\)\n\nPrimeiro, o fechamento de Kleene: \\(b^*\\)\nDeporque, a concatenação: \\(a(b^*)\\)\n\nPor fim, a união: \\((a(b^*)) \\cup c\\)\n\nResultado: \\(((a(b^*)) \\cup c)\\)\n\\(a \\cup bc^*\\)\n\nPrimeiro, o fechamento de Kleene: \\(c^*\\)\nDeporque, a concatenação: \\(b(c^*)\\)\nPor fim, a união: \\(a \\cup (b(c^*))\\)\n\nResultado: \\((a \\cup (b(c^*)))\\)\n\\(ab \\cup cd^*e\\)\n\nPrimeiro, o fechamento de Kleene: \\(d^*\\)\nDeporque, as concatenações (da esquerda para direita): \\((ab)\\) e \\((c(d^*)e)\\)\nPor fim, a união: \\((ab) \\cup ((c(d^*))e)\\)\n\nResultado: \\(((ab) \\cup (c((d^*)e)))\\)\n\\(a^*b^* \\cup c^*\\)\n\nPrimeiro, os fechamentos de Kleene: \\(a^*\\), \\(b^*\\), \\(c^*\\)\nDeporque, a concatenação: \\((a^*)(b^*)\\)\nPor fim, a união: \\(((a^*)(b^*)) \\cup (c^*)\\)\n\nResultado: \\((((a^*)(b^*)) \\cup (c^*))\\)\n\nSolução:\n\nPara \\(r = a^*ba^*\\) e strings \\(\\{ab, ba, aba, baa, bb\\}\\):\n\\(L(a^*ba^*) = \\{a^i b a^j \\mid i, j \\geq 0\\}\\) (strings com exatamente um \\(b\\), podendo ter qualquer quantidade de \\(a\\)’s antes e deporque)\n\n\\(ab\\): \\(ab = a^1 b a^0\\) ∈ \\(L(r)\\)\n\\(ba\\): \\(ba = a^0 b a^1\\) ∈ \\(L(r)\\)\n\n\\(aba\\): \\(aba = a^1 b a^1\\) ∈ \\(L(r)\\)\n\\(baa\\): \\(baa = a^0 b a^2\\) ∈ \\(L(r)\\)\n\\(bb\\): Contém dois \\(b\\)’s, não pode ser expressa como \\(a^i b a^j\\) ∉ \\(L(r)\\)\n\nPara \\(r = (a \\cup b)^*b\\) e strings \\(\\{b, ab, ba, abb, bbb\\}\\):\n\\(L((a \\cup b)^*b) = \\{w b \\mid w \\in \\{a,b\\}^*\\}\\) (strings terminando com \\(b\\))\n\n\\(b\\): \\(b = \\epsilon \\cdot b\\) ∈ \\(L(r)\\)\n\\(ab\\): \\(ab = a \\cdot b\\) ∈ \\(L(r)\\)\n\\(ba\\): Termina com \\(a\\), não com \\(b\\) ∉ \\(L(r)\\)\n\\(abb\\): \\(abb = ab \\cdot b\\) ∈ \\(L(r)\\)\n\n\\(bbb\\): \\(bbb = bb \\cdot b\\) ∈ \\(L(r)\\)\n\n\nSolução:\n\nObjetivo: Calcular \\(L((a \\cup b)c)\\) usando a definição indutiva.\nCasos base identificados: - \\(L(a) = \\{a\\}\\) - \\(L(b) = \\{b\\}\\)\n- \\(L(c) = \\{c\\}\\)\nAplicação das regras indutivas:\nPasso 1: Calcular \\(L(a \\cup b)\\)\nUsando a regra da união: \\(L(r \\cup s) = L(r) \\cup L(s)\\) \\[L(a \\cup b) = L(a) \\cup L(b) = \\{a\\} \\cup \\{b\\} = \\{a, b\\}\\]\nPasso 2: Calcular \\(L((a \\cup b)c)\\)\nUsando a regla da concatenação: \\(L(rs) = L(r) \\cdot L(s)\\) \\[L((a \\cup b)c) = L(a \\cup b) \\cdot L(c) = \\{a, b\\} \\cdot \\{c\\}\\]\nPasso 3: Calcular o produto cartesiano \\[\\{a, b\\} \\cdot \\{c\\} = \\{ac, bc\\}\\]\nResultado: \\[L((a \\cup b)c) = \\{ac, bc\\}\\]\n\nSolução:\n\n\\(L_1 = \\{a, b, aa, bb\\}\\)\nAnalisando o padrão: strings \\(a\\) ou \\(b\\), e strings \\(aa\\) ou \\(bb\\).\nEstratégia: \\((a \\cup b) \\cup (aa \\cup bb) = (a \\cup b) \\cup (a \\cup b)(a \\cup b)\\)\nPodemos fatorar: \\((a \\cup b)(1 \\cup (a \\cup b))\\), mas isso não é uma expressão regular válida.\nSolução direta: \\[r_1 = a \\cup b \\cup aa \\cup bb\\]\nVersão mais concisa: \\[r_1 = (a \\cup b) \\cup (a \\cup b)^2\\]\n\\(L_2 = \\{\\epsilon, a, aa, aaa\\}\\)\nAnalisando: \\(\\epsilon\\), \\(a^1\\), \\(a^2\\), \\(a^3\\).\nEstratégia: União de potências específicas de \\(a\\). \\[r_2 = \\epsilon \\cup a \\cup a^2 \\cup a^3\\]\nObservação: Não há uma forma mais concisa usando apenas as três operações básicas, porque precisamos limitar a 3 repetições de \\(a\\).\n\\(L_3 = \\{w \\in \\{a,b\\}^* \\mid w \\text{ termina com } a\\}\\)\nAnálise: Qualquer sequência de \\(a\\)’s e \\(b\\)’s seguida obrigatoriamente por um \\(a\\).\nEstratégia: \\((a \\cup b)^* a\\)\nVerificação: - \\(L((a \\cup b)^* a) = \\{wa \\mid w \\in \\{a,b\\}^*\\}\\) - Isso corresponde exatamente a todas as strings sobre \\(\\{a,b\\}\\) terminando com \\(a\\)\n\\[r_3 = (a \\cup b)^*a\\]\nVerificação com exemplos: - \\(a = \\epsilon \\cdot a\\) ∈ \\(L(r_3)\\) - \\(ba = b \\cdot a\\) ∈ \\(L(r_3)\\)\n- \\(aba = ab \\cdot a\\) ∈ \\(L(r_3)\\) - \\(ab = a \\cdot b\\) ∉ \\(L(r_3)\\) (não termina com \\(a\\))\n\n\n21.1.5 Exercícios 5: Section 3.4.4.1\n\nSolução:\n\nStrings terminando em \\(10\\): \\[r_1 = (0 \\cup 1)^*10\\]\nVerificação: Qualquer sequência de 0’s e 1’s seguida obrigatoriamente por \\(10\\).\nStrings começando com \\(01\\): \\[r_2 = 01(0 \\cup 1)^*\\]\nVerificação: A string deve iniciar com \\(01\\) seguido por qualquer sequência de 0’s e 1’s.\nStrings que contêm \\(01\\) como substring: \\[r_3 = (0 \\cup 1)^*01(0 \\cup 1)^*\\]\nVerificação: Qualquer sequência antes, \\(01\\) obrigatório no meio, qualquer sequência deporque.\nStrings que NÃO contêm \\(01\\):\nAnálise: Uma string não contém \\(01\\) se nunca há um \\(0\\) seguido diretamente por um \\(1\\).\nEstratégia: A string pode ter apenas: - Sequências de \\(1\\)’s seguidas de sequências de \\(0\\)’s - Ou apenas \\(1\\)’s - Ou apenas \\(0\\)’s\n- Ou a string vazia\n\\[r_4 = 1^*0^*\\]\nJustificativa: Se há algum \\(0\\) seguido de \\(1\\), violaria o padrão \\(1^*0^*\\).\n\nSolução:\n\nStrings com número ímpar de \\(a\\)’s sobre \\(\\{a,b\\}\\):\nEstratégia: Modificar o padrão do número par. Para ter número ímpar, precisamos de um número ímpar de grupos “um \\(a\\)”.\n\\[r_1 = b^*a(b^*ab^*ab^*)^*b^*\\]\nExplicação: - \\(b^*\\): pode começar com \\(b\\)’s - \\(a\\): o primeiro \\(a\\) (garantindo número ímpar) - \\((b^*ab^*ab^*)^*\\): grupos de dois \\(a\\)’s (mantém a paridade ímpar) - \\(b^*\\): pode terminar com \\(b\\)’s\nStrings com número múltiplo de 3 de \\(a\\)’s:\nEstratégia: Agrupar os \\(a\\)’s de três em três.\n\\[r_2 = b^*(ab^*ab^*ab^*)^*b^*\\]\nExplicação: - O grupo \\((ab^*ab^*ab^*)\\) contém exatamente 3 \\(a\\)’s - A repetição \\((...)^*\\) permite 0, 3, 6, 9, … \\(a\\)’s\nStrings com pelo menos dois \\(a\\)’s:\nEstratégia: Primeiro \\(a\\), deporque segundo \\(a\\), deporque qualquer coisa.\n\\[r_3 = (a \\cup b)^*a(a \\cup b)^*a(a \\cup b)^*\\]\nSimplificação alternativa: \\[r_3 = b^*ab^*a(a \\cup b)^*\\]\n\nSolução:\n\nPermitir underscores em qualquer posição:\nAlfabeto expandido: \\(\\{a, \\ldots, z, A, \\ldots, Z, 0, \\ldots, 9, \\_\\}\\)\n\\[r_1 = [a-zA-Z\\_][a-zA-Z0-9\\_]^*\\]\nProibir dígitos na primeira e última posições:\nEstratégia: - Primeiro: apenas letra ou underscore - Meio: qualquer coisa permitida - Último: apenas letra ou underscore\nPara identificadores de 1 caractere: \\[\\text{Caso 1: } [a-zA-Z\\_]\\]\nPara identificadores de 2+ caracteres: \\[\\text{Caso 2: } [a-zA-Z\\_][a-zA-Z0-9\\_]^*[a-zA-Z\\_]\\]\nCombinando: \\[r_2 = [a-zA-Z\\_] \\cup [a-zA-Z\\_][a-zA-Z0-9\\_]^*[a-zA-Z\\_]\\]\nIdentificadores entre 3 e 8 caracteres:\n\\[r_3 = [a-zA-Z\\_][a-zA-Z0-9\\_]\\{2,7\\}\\]\nExplicação: - \\([a-zA-Z\\_]\\): primeiro caractere (1 caractere) - \\([a-zA-Z0-9\\_]\\{2,7\\}\\): de 2 a 7 caracteres adicionais - Total: 3 a 8 caracteres\n\nSolução:\n\nNúmeros decimais (com ponto decimal):\n\\[r_1 = (\\epsilon \\cup + \\cup -)[0-9]^*\\.[0-9]^+\\]\nAnálise: - Sinal opcional: \\((\\epsilon \\cup + \\cup -)\\) - Parte inteira opcional: \\([0-9]^*\\) - Ponto obrigatório: \\(\\.\\) - Parte decimal obrigatória: \\([0-9]^+\\)\nVersão mais robusta (evitando apenas “.123”): \\[r_1' = (\\epsilon \\cup + \\cup -)([0-9]^+\\.[0-9]^* \\cup [0-9]^*\\.[0-9]^+)\\]\nNotação científica simples (\\(1e5\\), \\(2e-3\\)):\n\\[r_2 = (\\epsilon \\cup + \\cup -)[0-9]^+e(\\epsilon \\cup + \\cup -)[0-9]^+\\]\nExplicação: - Sinal opcional: \\((\\epsilon \\cup + \\cup -)\\) - Mantissa: \\([0-9]^+\\) (pelo menos um dígito) - Literal ‘e’: \\(e\\) - Sinal do expoente opcional: \\((\\epsilon \\cup + \\cup -)\\) - Expoente: \\([0-9]^+\\)\nNúmeros hexadecimais com prefixo \\(0x\\):\n\\[r_3 = 0x[0-9a-fA-F]^+\\]\nExplicação: - Prefixo obrigatório: \\(0x\\) - Dígitos hex: \\([0-9a-fA-F]^+\\) (pelo menos um)\n\nSolução:\n\nVerificação para “termina em 10” (\\(r = (0 \\cup 1)^*10\\)):\n\n\\(101\\): Termina em \\(01\\), não em \\(10\\) → NÃO pertence\n\\(1010\\): Termina em \\(10\\) → SIM pertence\n\n\\(0101\\): Termina em \\(01\\), não em \\(10\\) → NÃO pertence\n\nVerificação para número ímpar de \\(a\\)’s (\\(r = b^*a(b^*ab^*ab^*)^*b^*\\)):\n\n\\(aab\\):\n\nContagem de \\(a\\)’s: 2 (par) → NÃO pertence\n\n\\(baba\\):\n\nContagem de \\(a\\)’s: 2 (par) → NÃO pertence\n\n\\(ababa\\):\n\nContagem de \\(a\\)’s: 3 (ímpar) → SIM pertence\n\n\nVerificação para identificadores com underscores (\\(r = [a-zA-Z\\_][a-zA-Z0-9\\_]^*\\)):\n\n\\(var\\_1\\):\n\nPrimeiro: \\(v\\) (letra)\nResto: \\(ar\\_1\\) (letras, underscore, dígito)\n→ SIM é válido\n\n\\(\\_temp\\):\n\nPrimeiro: \\(\\_\\) (underscore)\n\nResto: \\(temp\\) (letras)\n→ SIM é válido\n\n\\(item2\\_\\):\n\nPrimeiro: \\(i\\) (letra)\nResto: \\(tem2\\_\\) (letras, dígito, underscore)\n→ SIM é válido\n\n\nResumo dos resultados:\n\n\n\nString\nTermina em 10\nÍmpar de \\(a\\)’s\nID com underscore\n\n\n\n\n\\(101\\)\n\n-\n-\n\n\n\\(1010\\)\n\n-\n-\n\n\n\\(0101\\)\n\n-\n-\n\n\n\\(aab\\)\n-\n\n-\n\n\n\\(baba\\)\n-\n\n-\n\n\n\\(ababa\\)\n-\n\n-\n\n\n\\(var\\_1\\)\n-\n-\n\n\n\n\\(\\_temp\\)\n-\n-\n\n\n\n\\(item2\\_\\)\n-\n-\n\n\n\n\n\n\n21.1.6 Exercícios 6: {Section 3.4.5.3}\n\nSolução:\n\n\\((a \\cup \\emptyset)b\\)\nAplicando a lei do elemento neutro da união: \\(r \\cup \\emptyset \\equiv r\\) \\[(a \\cup \\emptyset)b \\equiv ab\\]\n\\(a(\\epsilon \\cup b)\\)\nAplicando a distributividade: \\(r(s \\cup t) \\equiv rs \\cup rt\\) \\[a(\\epsilon \\cup b) \\equiv a\\epsilon \\cup ab\\]\nAplicando a lei do elemento neutro da concatenação: \\(r\\epsilon \\equiv r\\) \\[a\\epsilon \\cup ab \\equiv a \\cup ab\\]\n\\((a \\cup a)^*\\)\nAplicando a idempotência da união: \\(r \\cup r \\equiv r\\) \\[(a \\cup a)^* \\equiv a^*\\]\n\\(a \\cup ab^*a\\)\nAplicando a distributividade: \\(rs \\cup rt \\equiv r(s \\cup t)\\)\nPrimeiro, fatoramos \\(a\\): \\[a \\cup ab^*a \\equiv a\\epsilon \\cup ab^*a \\equiv a(\\epsilon \\cup b^*a)\\]\nComo \\(\\epsilon \\cup b^*a\\) não pode ser simplificado usando apenas as leis básicas, a expressão mais simples é: \\[a \\cup ab^*a\\]\nNota: Esta expressão pode ser simplificada para \\(ab^*a\\) usando propriedades mais avançadas, porque \\(\\epsilon \\in b^*\\).\n\nSolução:\n\nObjetivo: Simplificar \\(((a \\cup b)a) \\cup (aa)\\)\nExpressão inicial: \\[((a \\cup b)a) \\cup (aa)\\]\nPasso 1: Aplicar distributividade em \\((a \\cup b)a\\) \\[((a \\cup b)a) = (aa \\cup ba)\\]\nPasso 2: Substituir na expressão original \\[(aa \\cup ba) \\cup (aa)\\]\nPasso 3: Aplicar associatividade da união \\[aa \\cup ba \\cup aa\\]\nPasso 4: Aplicar comutatividade para agrupar termos iguais \\[aa \\cup aa \\cup ba\\]\nPasso 5: Aplicar idempotência da união: \\(r \\cup r \\equiv r\\) \\[aa \\cup ba\\]\nPasso 6: Aplicar distributividade reversa \\[aa \\cup ba \\equiv (a \\cup b)a\\]\nResultado: \\[((a \\cup b)a) \\cup (aa) \\equiv (a \\cup b)a\\]\nVerificação com strings específicas: - Original aceita: \\(\\{aa, ba\\}\\) - Simplificada aceita: \\(\\{aa, ba\\}\\)\n\nSolução:\n\n\\(a^*a\\) e \\(aa^*\\)\nDemonstração:\nPara \\(a^*a\\): \\[L(a^*a) = L(a^*) \\cdot L(a) = \\{a^n \\mid n \\geq 0\\} \\cdot \\{a\\} = \\{a^{n+1} \\mid n \\geq 0\\} = \\{a^m \\mid m \\geq 1\\}\\]\nPara \\(aa^*\\): \\[L(aa^*) = L(a) \\cdot L(a^*) = \\{a\\} \\cdot \\{a^n \\mid n \\geq 0\\} = \\{a^{1+n} \\mid n \\geq 0\\} = \\{a^m \\mid m \\geq 1\\}\\]\nComo \\(L(a^*a) = L(aa^*) = \\{a^m \\mid m \\geq 1\\}\\), temos: \\[a^*a \\equiv aa^*\\]\n\\((a \\cup b)^*\\) e \\(\\epsilon \\cup (a \\cup b)(a \\cup b)^*\\)\nDemonstração:\nPara o lado direito, aplicando a definição recursiva do fechamento de Kleene: \\[L^* = \\epsilon \\cup LL^*\\]\nSubstituindo \\(L = (a \\cup b)\\): \\[L((a \\cup b)^*) = \\{\\epsilon\\} \\cup L(a \\cup b) \\cdot L((a \\cup b)^*)\\] \\[= \\{\\epsilon\\} \\cup L((a \\cup b)(a \\cup b)^*)\\] \\[= L(\\epsilon \\cup (a \\cup b)(a \\cup b)^*)\\]\nPortanto: \\[(a \\cup b)^* \\equiv \\epsilon \\cup (a \\cup b)(a \\cup b)^*\\]\n\\(a^*b^*\\) e \\((a \\cup b)^*\\) - esta é FALSA\nContraexemplo: - \\(ab \\in L((a \\cup b)^*)\\) (qualquer intercalação de \\(a\\)’s e \\(b\\)’s) - \\(ab \\notin L(a^*b^*)\\) (porque \\(a^*b^*\\) só aceita todos os \\(a\\)’s seguidos de todos os \\(b\\)’s)\nPortanto: \\(a^*b^* \\not\\equiv (a \\cup b)^*\\)\n\nSolução:\n\n\\((a^*)^*\\)\nAplicando a lei do fechamento do fechamento: \\((r^*)^* \\equiv r^*\\) \\[(a^*)^* \\equiv a^*\\]\n\\(\\epsilon^* \\cup a^*\\)\nAplicando a lei: \\(\\epsilon^* \\equiv \\epsilon\\) \\[\\epsilon^* \\cup a^* \\equiv \\epsilon \\cup a^*\\]\nComo \\(\\epsilon \\in L(a^*)\\) (porque \\(a^* = \\{a^n \\mid n \\geq 0\\}\\) e \\(\\epsilon = a^0\\)): \\[\\epsilon \\cup a^* \\equiv a^*\\]\n\\(\\emptyset^* \\cup a\\)\nAplicando a lei: \\(\\emptyset^* \\equiv \\epsilon\\) \\[\\emptyset^* \\cup a \\equiv \\epsilon \\cup a\\]\nEsta expressão não pode ser simplificada mais.\n\\((a \\cup \\epsilon)^*\\)\nComo \\(\\epsilon \\in L(a \\cup \\epsilon)\\), pela propriedade do fechamento de Kleene quando \\(\\epsilon\\) está presente: \\[L((a \\cup \\epsilon)^*) = L(a^*)\\]\nPortanto: \\[(a \\cup \\epsilon)^* \\equiv a^*\\]\n\nSolução:\n\nObjetivo: Simplificar \\(ab^* \\cup abb^* \\cup abbb^*\\)\nIdentificação do padrão comum:\nTodos os termos começam com \\(a\\) seguido de pelo menos um \\(b\\): - \\(ab^* = a(b^*)\\) contém \\(ab^0, ab^1, ab^2, \\ldots\\) - \\(abb^* = a(bb^*)\\) contém \\(ab^1, ab^2, ab^3, \\ldots\\)\n- \\(abbb^* = a(bbb^*)\\) contém \\(ab^2, ab^3, ab^4, \\ldots\\)\nAnálise dos conjuntos: - \\(L(ab^*) = \\{a, ab, abb, abbb, \\ldots\\}\\) - \\(L(abb^*) = \\{ab, abb, abbb, \\ldots\\}\\)\n- \\(L(abbb^*) = \\{abb, abbb, abbbb, \\ldots\\}\\)\nObservação: \\(L(abb^*) \\subseteq L(ab^*)\\) e \\(L(abbb^*) \\subseteq L(ab^*)\\)\nAplicação da distributividade:\n\\[ab^* \\cup abb^* \\cup abbb^* \\equiv a(b^* \\cup bb^* \\cup bbb^*)\\]\nSimplificação do termo entre parênteses:\nComo \\(bb^* \\subseteq b^*\\) e \\(bbb^* \\subseteq b^*\\): \\[b^* \\cup bb^* \\cup bbb^* \\equiv b^*\\]\nResultado: \\[ab^* \\cup abb^* \\cup abbb^* \\equiv ab^*\\]\nVerificação da equivalência:\nLinguagem original: \\[L(ab^* \\cup abb^* \\cup abbb^*) = L(ab^*) \\cup L(abb^*) \\cup L(abbb^*)\\] \\[= \\{a, ab, abb, \\ldots\\} \\cup \\{ab, abb, \\ldots\\} \\cup \\{abb, abbb, \\ldots\\}\\] \\[= \\{a, ab, abb, abbb, \\ldots\\}\\]\nLinguagem simplificada: \\[L(ab^*) = \\{a, ab, abb, abbb, \\ldots\\}\\]\nComo os conjuntos são idênticos, a simplificação está correta.\nBenefício da otimização: - Expressão original: 3 termos unidos - Expressão otimizada: 1 termo simples - Eficiência: Redução significativa na complexidade de avaliação\n\n\n21.1.7 Exercícios 7: {Section 3.4.6.1}\n\nSolução:\n\n\\(a^+\\)\nPor definição: \\(r^+ \\equiv rr^*\\) \\[a^+ \\equiv aa^*\\]\n\\(b?\\)\nPor definição: \\(r? \\equiv (\\epsilon \\cup r)\\) \\[b? \\equiv (\\epsilon \\cup b)\\]\n\\([abc]\\)\nClasses de caracteres são equivalentes à união: \\[[abc] \\equiv (a \\cup b \\cup c)\\]\n\\(a\\{3\\}\\)\nRepetição exata significa concatenação repetida: \\[a\\{3\\} \\equiv aaa\\]\n\\(b\\{2,4\\}\\)\nRepetição de 2 a 4 vezes: \\[b\\{2,4\\} \\equiv bb \\cup bbb \\cup bbbb\\]\nOu usando concatenação e opcionalidade: \\[b\\{2,4\\} \\equiv bb(\\epsilon \\cup b)(\\epsilon \\cup b)\\]\n\nSolução:\n\nQualquer dígito: \\([0-9]\\)\n\\[[0-9] \\equiv (0 \\cup 1 \\cup 2 \\cup 3 \\cup 4 \\cup 5 \\cup 6 \\cup 7 \\cup 8 \\cup 9)\\]\nQualquer letra minúscula: \\([a-z]\\)\n\\[[a-z] \\equiv (a \\cup b \\cup c \\cup \\ldots \\cup z)\\]\nQualquer caractere que não seja espaço: \\([^ ]\\)\nAssumindo um alfabeto básico ASCII \\(\\Sigma = \\{a, b, \\ldots, z, A, B, \\ldots, Z, 0, 1, \\ldots, 9, \\text{ }, !, @, \\#, \\ldots\\}\\):\n\\[[^ ] \\equiv (a \\cup b \\cup \\ldots \\cup z \\cup A \\cup \\ldots \\cup Z \\cup 0 \\cup \\ldots \\cup 9 \\cup ! \\cup @ \\cup \\ldots)\\]\nNota: Esta seria a união de todos os símbolos do alfabeto exceto o espaço.\nQualquer caractere alfanumérico: \\([a-zA-Z0-9]\\)\n\\[[a-zA-Z0-9] \\equiv (a \\cup b \\cup \\ldots \\cup z \\cup A \\cup B \\cup \\ldots \\cup Z \\cup 0 \\cup 1 \\cup \\ldots \\cup 9)\\]\n\nSolução:\n\nCEP brasileiro no formato \\(99999-999\\):\n\\[r_1 = [0-9]\\{5\\}-[0-9]\\{3\\}\\]\nExpandindo: \\[r_1 \\equiv [0-9][0-9][0-9][0-9][0-9]-[0-9][0-9][0-9]\\]\nPlaca de carro brasileira antiga \\(AAA-9999\\):\n\\[r_2 = [A-Z]\\{3\\}-[0-9]\\{4\\}\\]\nExpandindo: \\[r_2 \\equiv [A-Z][A-Z][A-Z]-[0-9][0-9][0-9][0-9]\\]\nSenha com exatamente 8 caracteres alfanuméricos:\n\\[r_3 = [a-zA-Z0-9]\\{8\\}\\]\nExpandindo: \\[r_3 \\equiv [a-zA-Z0-9][a-zA-Z0-9][a-zA-Z0-9][a-zA-Z0-9][a-zA-Z0-9][a-zA-Z0-9][a-zA-Z0-9][a-zA-Z0-9]\\]\nCódigo de área de telefone com 2 ou 3 dígitos:\n\\[r_4 = [0-9]\\{2,3\\}\\]\nExpandindo: \\[r_4 \\equiv [0-9][0-9] \\cup [0-9][0-9][0-9]\\]\n\nSolução:\n\nURL simples começando com \\(http\\) ou \\(https\\):\n\\[r_1 = https?://[a-zA-Z0-9.-]^+\\]\nExpandindo: \\[r_1 \\equiv (http \\cup https)://[a-zA-Z0-9.-]^+\\] \\[\\equiv http(\\epsilon \\cup s)://[a-zA-Z0-9.-]^+\\]\nData no formato \\(dd/mm/aaaa\\) (versão simples):\n\\[r_2 = [0-9]\\{2\\}/[0-9]\\{2\\}/[0-9]\\{4\\}\\]\nExpandindo: \\[r_2 \\equiv [0-9][0-9]/[0-9][0-9]/[0-9][0-9][0-9][0-9]\\]\nHorário no formato \\(hh:mm\\) (24 horas):\n\\[r_3 = ([01][0-9] \\cup 2[0-3]):[0-5][0-9]\\]\nExplicação: - \\(([01][0-9] \\cup 2[0-3])\\): horas de 00-19 ou 20-23 - \\(:[0-5][0-9]\\): minutos de 00-59\nNúmero de CPF no formato \\(999.999.999-99\\):\n\\[r_4 = [0-9]\\{3\\}\\.[0-9]\\{3\\}\\.[0-9]\\{3\\}-[0-9]\\{2\\}\\]\nExpandindo: \\[r_4 \\equiv [0-9][0-9][0-9]\\.[0-9][0-9][0-9]\\.[0-9][0-9][0-9]-[0-9][0-9]\\]\n\nSolução:\n\n\\((a \\cup b \\cup c \\cup d)(a \\cup b \\cup c \\cup d)^*\\)\nUsando classes de caracteres: \\[[abcd][abcd]^*\\]\nUsando fechamento positivo: \\[[abcd]^+\\]\nExpandindo \\([abcd]^+\\): \\[[abcd]^+ \\equiv [abcd][abcd]^* \\equiv [abcd](\\epsilon \\cup [abcd][abcd]^*)\\]\n\\(a(\\epsilon \\cup b)\\)\nUsando opcionalidade: \\[ab?\\]\nExpandindo: \\[ab? \\equiv a(\\epsilon \\cup b)\\]\n\\((0 \\cup 1 \\cup 2 \\cup 3 \\cup 4 \\cup 5 \\cup 6 \\cup 7 \\cup 8 \\cup 9)(0 \\cup 1 \\cup 2 \\cup 3 \\cup 4 \\cup 5 \\cup 6 \\cup 7 \\cup 8 \\cup 9)^*\\)\nUsando classes de caracteres: \\[[0-9][0-9]^*\\]\nUsando fechamento positivo: \\[[0-9]^+\\]\nComparação das otimizações:\n\n\n\n\n\n\n\n\nExpressão Original\nVersão Otimizada\nRedução\n\n\n\n\n\\((a \\cup b \\cup c \\cup d)(a \\cup b \\cup c \\cup d)^*\\)\n\\([abcd]^+\\)\n90%\n\n\n\\(a(\\epsilon \\cup b)\\)\n\\(ab?\\)\n70%\n\n\n\\((0 \\cup 1 \\cup \\ldots \\cup 9)(0 \\cup 1 \\cup \\ldots \\cup 9)^*\\)\n\\([0-9]^+\\)\n95%\n\n\n\nBenefícios da otimização:\n\nLegibilidade: As notações convencionais são muito mais fáceis de ler e entender\nManutenibilidade: Modificações (como adicionar novos caracteres) são mais simples\nEficiência: Implementações podem otimizar melhor as notações convencionais\nMenos propenso a erros: Reduz a chance de esquecer casos em uniões longas\n\nExemplo prático:\nPara aceitar qualquer letra maiúscula ou minúscula:\nVersão expandida (52 termos!): \\[(A \\cup B \\cup C \\cup \\ldots \\cup Z \\cup a \\cup b \\cup c \\cup \\ldots \\cup z)\\]\nVersão otimizada: \\[[a-zA-Z]\\]\nA redução é de aproximadamente 98% no tamanho da expressão!\n\n\n21.1.8 Exercícios 8: {Section 3.4.8}\n\n21.1.8.1 Solução\nPalavras-chave: \\(\\{\\text{if}, \\text{then}, \\text{else}, \\text{while}, \\text{do}\\}\\)\n\\[r_1 = \\text{if} \\cup \\text{then} \\cup \\text{else} \\cup \\text{while} \\cup \\text{do}\\]\nImplementação mais eficiente: \\[r_1 = \\text{if} \\cup \\text{then} \\cup \\text{else} \\cup \\text{while} \\cup \\text{do}\\]\nNúmeros inteiros (incluindo negativos):\n\\[r_2 = -?([1-9][0-9]^* \\cup 0)\\]\nExplicação: - \\(-?\\): sinal negativo opcional - \\(([1-9][0-9]^* \\cup 0)\\): número sem zeros à esquerda ou apenas zero\nComentários de linha iniciados por \\(//\\) até o final da linha:\n\\[r_3 = //[^\\n]^*\\]\nExplicação: - \\(//\\) literal - \\([^\\n]^*\\): qualquer caractere exceto nova linha, zero ou mais vezes\nOperadores relacionais: \\(\\{&lt;, &gt;, &lt;=, &gt;=, ==, !=\\}\\)\n\\[r_4 = &lt;= \\cup &gt;= \\cup == \\cup != \\cup &lt; \\cup &gt;\\]\nObservação: A ordem é importante - operadores de dois caracteres devem vir antes dos de um caractere para evitar conflitos durante o parsing.\n\n\n21.1.8.2 Solução\nTelefone celular: \\((11) 99999-9999\\)\n\\[r_1 = \\([0-9]\\{2\\} [0-9]\\{5\\}-[0-9]\\{4\\}\\]\nVersão mais flexível (com espaço opcional): \\[r_1' = \\([0-9]\\{2\\) ?[0-9]\\{5\\}-[0-9]\\{4\\}\\]\nRG: \\(99.999.999-9\\)\n\\[r_2 = [0-9]\\{2\\}\\.[0-9]\\{3\\}\\.[0-9]\\{3\\}-[0-9]\\]\nCNPJ: \\(99.999.999/9999-99\\)\n\\[r_3 = [0-9]\\{2\\}\\.[0-9]\\{3\\}\\.[0-9]\\{3\\}/[0-9]\\{4\\}-[0-9]\\{2\\}\\]\nCEP: \\(99999-999\\) ou \\(99.999-999\\)\n\\[r_4 = [0-9]\\{5\\}[-.]?[0-9]\\{3\\}\\]\nExplicação: - \\([0-9]\\{5\\}\\): cinco dígitos iniciais; - \\([-.]?\\): hífen ou ponto opcional, alguns CEPs não têm separador; - \\([0-9]\\{3\\}\\): três dígitos finais.\n\n\n21.1.8.3 Solução\nEndereços de email em um texto:\n\\[r_1 = [a-zA-Z0-9._%+-]^+@[a-zA-Z0-9.-]^+\\.[a-zA-Z]\\{2,\\}\\]\nExplicação: - \\([a-zA-Z0-9._%+-]^+\\): parte local (antes do @); - \\(@\\): símbolo obrigatório; - \\([a-zA-Z0-9.-]^+\\): domínio; - \\(\\.\\): ponto literal; - \\([a-zA-Z]\\{2,\\}\\): extensão com pelo menos \\(2\\) letras.\nValores monetários no formato \\(R\\$ 99,99\\):\n\\[r_2 = R\\$ ?[0-9]^+,[0-9]\\{2\\}\\]\nVersão mais robusta: \\[r_2' = R\\$ ?[0-9]\\{1,3\\}(\\.[0-9]\\{3\\})^*,[0-9]\\{2\\}\\]\nExplicação da versão robusta: - \\([0-9]\\{1,3\\}\\): 1 a 3 dígitos iniciais - \\((\\.[0-9]\\{3\\})^*\\): grupos de 3 dígitos separados por ponto (milhares) - \\(,[0-9]\\{2\\}\\): vírgula e centavos\nDatas em formatos variados: \\(dd/mm/aaaa\\), \\(dd-mm-aaaa\\), \\(dd.mm.aaaa\\)\n\\[r_3 = [0-9]\\{2\\}[/.-][0-9]\\{2\\}[/.-][0-9]\\{4\\}\\]\nVersão mais específica (garantindo consistência do separador): \\[r_3' = ([0-9]\\{2\\}/[0-9]\\{2\\}/[0-9]\\{4\\}) \\cup ([0-9]\\{2\\}-[0-9]\\{2\\}-[0-9]\\{4\\}) \\cup ([0-9]\\{2\\}\\.[0-9]\\{2\\}\\.[0-9]\\{4\\})\\]\nNúmeros de cartão de crédito (formato \\(9999-9999-9999-9999\\)):\n\\[r_4 = [0-9]\\{4\\}-[0-9]\\{4\\}-[0-9]\\{4\\}-[0-9]\\{4\\}\\]\nVersão flexível (com separadores opcionais): \\[r_4' = [0-9]\\{4\\} ?-? ?[0-9]\\{4\\} ?-? ?[0-9]\\{4\\} ?-? ?[0-9]\\{4\\}\\]\n\n\n21.1.8.4 Solução\nPara validar email: \\([a-z]+@[a-z]+.[a-z]+\\) (problema: ponto literal)\nProblema identificado: O ponto \\(.\\) é um metacaractere que corresponde a “qualquer caractere”. Para representar um ponto literal, deve ser escapado.\nCorreção: \\[[a-z]^+@[a-z]^+\\.[a-z]^+\\]\nMelhorias adicionais: - Incluir maiúsculas e números: \\([a-zA-Z0-9]^+@[a-zA-Z0-9]^+\\.[a-zA-Z]\\{2,\\}\\) - Permitir caracteres especiais válidos: \\([a-zA-Z0-9._%+-]^+@[a-zA-Z0-9.-]^+\\.[a-zA-Z]\\{2,\\}\\)\nPara números decimais: \\([0-9]*.[0-9]*\\) (problema: pontos opcionais)\nProblemas identificados: 1. Ponto não escapado (deveria ser \\(\\.\\)) 2. Ambas as partes são opcionais (aceita strings como “.” vazia)\nCorreção: \\[[0-9]^+\\.[0-9]^+ \\cup [0-9]^+\\. \\cup \\.[0-9]^+\\]\nVersão mais simples: \\[([0-9]^+\\.[0-9]^*) \\cup ([0-9]^*\\.[0-9]^+)\\]\nPara identificadores: \\([a-zA-Z][a-zA-Z0-9]?\\) (problema: comprimento mínimo)\nProblema identificado: O \\(?\\) torna o segundo caractere opcional, permitindo identificadores de apenas 1 caractere, mas limitando a 2 caracteres no máximo.\nCorreção para identificadores normais: \\[[a-zA-Z][a-zA-Z0-9]^*\\]\nSe realmente quiser 1-2 caracteres apenas: \\[[a-zA-Z][a-zA-Z0-9]?\\]\n\n\n21.1.8.5 Solução\n\\((abc|abd|abe) \\rightarrow ab(c|d|e)\\)\nAnálise: Fatoração do prefixo comum \\(ab\\).\nVerificação da equivalência: - Original: \\(\\{abc, abd, abe\\}\\) - Otimizada: \\(ab(c \\cup d \\cup e) = ab \\{c, d, e\\} = \\{abc, abd, abe\\}\\)\nBenefício: Reduz backtracking em implementações, porque \\(ab\\) só precisa ser verificado uma vez.\n\\([0-9][0-9][0-9][0-9] \\rightarrow [0-9]\\{4\\}\\)\nAnálise: Uso de quantificador para repetição.\nVerificação da equivalência: - Ambas aceitam exatamente sequências de 4 dígitos\nBenefício: Mais conciso, e muitas implementações otimizam quantificadores internamente.\n\\((a^*b^*|b^*a^*) \\rightarrow (a|b)^*\\) - VERIFICAÇÃO NECESSÁRIA\nAnálise da equivalência:\nLado esquerdo: \\((a^*b^*) \\cup (b^*a^*)\\) - \\(a^*b^*\\): zero ou mais \\(a\\)’s seguidos de zero ou mais \\(b\\)’s - \\(b^*a^*\\): zero ou mais \\(b\\)’s seguidos de zero ou mais \\(a\\)’s - União: strings da forma \\(a^i b^j\\) ou \\(b^k a^l\\)\nLado direito: \\((a \\cup b)^*\\) - Qualquer sequência de \\(a\\)’s e \\(b\\)’s em qualquer ordem\nContraexemplo: A string \\(aba\\) - \\(aba \\in L((a|b)^*)\\) - \\(aba \\notin L(a^*b^*)\\) (porque tem \\(a\\) após \\(b\\)) - \\(aba \\notin L(b^*a^*)\\) (porque tem \\(b\\) entre \\(a\\)’s) - Portanto: \\(aba \\notin L((a^*b^*|b^*a^*))\\)\nConclusão: As expressões NÃO são equivalentes.\n\n\n21.1.8.6 Solução\nDefinição do Alfabeto \\[\\sigma = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, +, -, *, /, (, ), }\\]\nSubconjuntos Úteis - Dígitos: \\(D = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\\); - Dígitos não-zero: \\(D_{nz} = {1, 2, 3, 4, 5, 6, 7, 8, 9}\\); - Operadores: \\(Op = {+, -, *, /}\\); - Delimitadores: \\(Del = {(, )}\\) - Espaço: \\(Esp = { }\\).\nExpressões Regulares por Componente\nNúmeros Inteiros (\\(r_num\\))\n\\[r_num = (+|-|ε)(0|(D_nz D^*))\\]\nOperadores (\\(r_op\\))\n\\[r_op = +|-|*|/\\]\nEspaços Opcionais (\\(r_esp\\))\n\\[r_esp = ( )^*\\]\nSem Parênteses (r_expr_simples)\n\\[r_expr_simples = r_esp r_num r_esp (r_op r_esp r_num r_esp)^*\\]\nCom Um Nível de Parênteses (r_expr_paren)\n\\[\nr_expr_paren = r_esp (r_num | (r_esp r_num r_esp r_op r_esp r_num r_esp)) r_esp\n               (r_op r_esp (r_num | (r_esp r_num r_esp r_op r_esp r_num r_esp)) r_esp)^*\n\\]\nExpressão Regular Completa, Versão Compacta\n\\[r_expr = r_esp^* Termo (Op r_esp^* Termo)^* r_esp^*\\]\nna qual:\n\\[Termo = Numero | (Esp^* Numero Esp^* Op Esp^* Numero Esp^*)\\]\n\\[Numero = (+|-)?( 0 | D_nz D^* )\\]\n\\[Op = +|-|*|/\\]\n\\[Esp = ( )^*\\]\nVersão Expandida\n\\[r_expr = ( )^* ((+|-)?( 0 | [1-9][0-9]^* ) |\n         \\(( )^* (+|-)?( 0 | [1-9][0-9]^* ) ( )^*\n         (+|-|*|/) ( )^* (+|-)?( 0 | [1-9][0-9]^* ) ( )^* \\))\n         (( )* (+|-|*|/) ( )^*\n         ((+|-)?( 0 | [1-9][0-9]^* ) |\n         \\(( )^* (+|-)?( 0 | [1-9][0-9]^* ) ( )^*\n         (+|-|*|/) ( )^* (+|-)?( 0 | [1-9][0-9]^* ) ( )^* \\)))^* ( )^*\n\\]\nExpressões Válidas - \\(42\\) - \\(3 + 5\\) - \\(-10 * 2\\) - \\((5 + 3) / 2\\) - \\(15 - 7 + 2\\) - \\((8 * 3) - (4 / 2)\\)\nExpressões Inválidas - \\(3 +\\) // operador sem segundo operando - \\(+ 5\\) // operador sem primeiro operando - \\((3 + 5\\) // parênteses não balanceados - \\(3 ++ 5\\) // operador duplo - \\(07 + 3\\) // número com zero à esquerda - \\(3.5 + 2\\) // número decimal\n\n\n21.1.8.7 Solução\na) Começa com a, termina com c, contém b\nExpressão Regular:\n\\[a(a\\cup b\\cup c)^∗b(a\\cup b\\cup c)^∗c\\]\nAnálise:\n\n\\(a\\): A expressão começa com o caractere literal \\(a\\) (concatenação).\n\n\\((a\\cup b\\cup c)^*\\): Esta parte representa qualquer sequência de caracteres do alfabeto. A união \\(a\\cup b\\cup c\\) define a escolha de um caractere, e o fechamento de Kleene (\\(*\\)) permite que essa escolha seja repetida zero ou mais vezes.\n\n\\(b\\): Garante que a cadeia contenha pelo menos um \\(b\\).\n\n\\((a\\cup b\\cup c)^*\\): Permite que qualquer outra combinação de caracteres apareça entre o \\(b\\) obrigatório e o \\(c\\) final.\n\n\\(c\\): A expressão termina com o caractere literal \\(c\\).\n\nb) Cada a é seguido por a ou b\nExpressão Regular:\n\\[(b\\cup c\\cup aa\\cup ab)^∗\\]\nAnálise:\nA estratégia é construir a expressão a partir de “blocos” válidos que podem ser repetidos. A união desses blocos, sob o fechamento de Kleene, forma a linguagem.\n\n\\(b\\) e \\(c\\): São blocos válidos.\n\n\\(aa\\) e \\(ab\\): São blocos que começam com \\(a\\) e satisfazem a regra. A concatenação é usada para formar esses blocos.\n\nA expressão \\((b\\cup c\\cup aa\\cup ab)^∗\\) permite a repetição de qualquer um desses blocos, garantindo que qualquer \\(a\\) que apareça na cadeia seja sempre seguido por \\(a\\) ou \\(b\\).\nc) Não contém a substring ac\nExpressão Regular:\n\\[(b\\cup c\\cup aa^∗b)^∗a^∗\\]\nAnálise:\nA estratégia é garantir que um \\(c\\) nunca possa ser imediatamente precedido por um \\(a\\).\n\n\\((b\\cup c\\cup aa^∗b)^∗\\): Esta é a parte principal que constrói sequências válidas. Ela permite a repetição de três tipos de blocos:\n\n\\(b\\) ou \\(c\\): Caracteres que não criam risco de formar \\(ac\\).\n\n\\(aa^∗b\\): Este bloco é fundamental. Ele representa “um ou mais \\(a\\)s, seguidos por um \\(b\\)”. O trecho \\(aa^∗\\) é a forma de representar “um ou mais \\(a\\)s” (\\(a^+\\)) usando apenas os operadores permitidos (concatenação e Kleene star). Este bloco garante que qualquer sequência de \\(a\\)s que não esteja no final da cadeia seja “terminada” por um \\(b\\).\n\n\\(a^∗\\): Este sufixo permite que a cadeia termine com qualquer número de \\(a\\)s (incluindo nenhum). Como é o final da cadeia, não há risco de um \\(c\\) aparecer depois.\n\nJuntas, essas partes garantem que um \\(c\\) nunca possa seguir imediatamente um \\(a\\).",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Solução dos Exercícios</span>"
    ]
  },
  {
    "objectID": "sol-exercicios.html#capítulo-sec-aut-finitos-deterministicos",
    "href": "sol-exercicios.html#capítulo-sec-aut-finitos-deterministicos",
    "title": "21  Solução dos Exercícios",
    "section": "21.2 Capítulo Chapter 4",
    "text": "21.2 Capítulo Chapter 4\n\n21.2.1 Exercícios 1 {Section 4.1}\n1.\n\nQ=s_0,s_1,s_2,s_3: O conjunto finito de estados.\n\nSigma=a,b: O alfabeto de entrada.\n\ndelta: A função de transição, que mapeia pares de estado e símbolo para um estado de destino (ex: delta(s_0,a)=s_1).\n\ns_0: O estado inicial único.\n\nF=s_0,s_3: O conjunto de estados de aceitação (ou finais).\n\n2. Não, o modelo não representa um Autômato Finito Determinístico válido. A justificativa é que o conjunto de estados de aceitação F=q_2 contém um estado, q_2, que não pertence ao conjunto de estados da máquina, Q=q_0,q_1. A definição formal exige que FsubseteqQ.\n3. O domínio da função de transição é QtimesSigma. O número de pares nesse domínio é o produto das cardinalidades dos conjuntos, ou seja, ∣Q∣times∣Sigma∣=3times4=12.\n4. Sim, é possível. Se F=Q, significa que todo estado do autômato é um estado de aceitação. Isso implica que, independentemente da sequência de transições, o estado final será sempre de aceitação. Portanto, o autômato reconheceria todas as strings possíveis sobre seu alfabeto, incluindo a string vazia. A linguagem reconhecida seria Sigma\\*.\n5. Sim, é perfeitamente possível. Se o estado inicial q_0 também é um estado de aceitação (q_0inF), isso significa que a computação que começa e termina em q_0 sem consumir nenhum símbolo é uma computação de aceitação. Portanto, a string vazia (epsilon) é aceita pelo autômato.\n\n\n21.2.2 Exercícios 2\n1. Não, o autômato não é completo. Faltam transições para o símbolo ‘d’ a partir de q_0 e para o símbolo ‘l’ a partir de q_1. Para completá-lo, adicionamos q_e:\n\ndelta(q_0,l)=q_1 (existente)\n\ndelta(q_0,d)=q_e (nova)\n\ndelta(q_1,d)=q_1 (existente)\n\ndelta(q_1,l)=q_e (nova)\n\ndelta(q_e,l)=q_e (nova)\n\ndelta(q_e,d)=q_e (nova)\n\n2. A propriedade determinística garante que para qualquer estado atual e qualquer símbolo de entrada, há apenas um único próximo estado possível. Isso elimina qualquer ambiguidade. Em uma implementação, isso se traduz em uma operação muito rápida e simples: uma única consulta a uma tabela (ou uma instrução switch) para determinar o próximo estado. Não há necessidade de explorar múltiplos caminhos, voltar atrás (backtracking) ou gerenciar escolhas, tornando o processamento da string de entrada linear e extremamente eficiente.\n3. Não. A função de um estado de erro é capturar permanentemente qualquer sequência de entrada que desvie de um padrão válido. Por definição, uma string que leva a um estado de erro deve ser rejeitada. Se o estado de erro fosse também um estado de aceitação, ele aceitaria strings que deveriam ser rejeitadas, contradizendo seu propósito.\n4. Assume-se que as transições não mostradas levam a um estado de erro implícito, não desenhado. A partir desse estado de erro, todas as transições subsequentes (para qualquer símbolo do alfabeto) apontam de volta para ele mesmo. Isso é feito para manter os diagramas mais limpos e legíveis.\n2.5. Sim, é muito comum. Isso é chamado de auto-loop ou laço. Significa que, ao ler um determinado símbolo, a máquina não precisa mudar a informação que está armazenando (representada pelo estado). Exemplo prático: no autômato que reconhece números com paridade par de ‘1’s, a transição delta(q_par,0)=q_par indica que ler um ’0’ não muda a paridade, então a máquina permanece no mesmo estado.\n\n\n21.2.3 Exercícios 3\n1. O diagrama de transições é o seguinte:\n\nTrês vértices: q_A,q_B,q_C.\n\nq_A tem uma seta de entrada “do nada” (estado inicial).\n\nq_C é um círculo duplo (estado de aceitação).\n\nSetas:\n\nDe q_A para q_B com rótulo ‘0’.\n\nUm laço em q_A com rótulo ‘1’.\n\nUm laço em q_B com rótulo ‘0’.\n\nDe q_B para q_C com rótulo ‘1’.\n\nDe q_C para q_B com rótulo ‘0’.\n\nDe q_C para q_A com rótulo ‘1’.\n\n\n2.\n\nQ=q_pp,q_ip,q_pi,q_ii (par-par, ímpar-par, etc.)\n\nSigma=a,b\n\nq_0=q_pp\n\nF=q_pp\n\ndelta é definida por:\n\ndelta(q_pp,a)=q_ip, delta(q_pp,b)=q_pi\n\ndelta(q_ip,a)=q_pp, delta(q_ip,b)=q_ii\n\ndelta(q_pi,a)=q_ii, delta(q_pi,b)=q_pp\n\ndelta(q_ii,a)=q_pi, delta(q_ii,b)=q_ip\n\n\n3.\n\n\n\nEstado\na\nb\nr\ne\nx\n\n\n\n\nrightarrowq_0\nq_1\nq_e\nq_e\nq_e\nq_e\n\n\nq_1\nq_e\nq_2\nq_e\nq_e\nq_e\n\n\nq_2\nq_e\nq_e\nq_3\nq_e\nq_e\n\n\nq_3\nq_e\nq_e\nq_e\nq_4\nq_e\n\n\n\\*q_4\nq_e\nq_e\nq_e\nq_e\nq_e\n\n\nq_e\nq_e\nq_e\nq_e\nq_e\nq_e\n\n\n\n4.\n\nPara depurar: A representação gráfica (diagrama) é geralmente a mais útil, porque oferece uma visualização intuitiva do fluxo de controle. É fácil seguir os caminhos e entender como diferentes strings são processadas.\n\nPara implementar: A representação tabular (tabela de transições) é a mais direta para implementação. Ela mapeia naturalmente para uma estrutura de dados como um array 2D ou um dicionário de dicionários, permitindo acesso O(1) ao próximo estado.\n\n3.5. Para aceitar strings que terminam com “010”, precisamos de um novo estado final. Podemos adicionar um estado q_D e modificar as transições.\n\n\n\nEstado\n0\n1\n\n\n\n\n\\(\\rightarrow q_A\\)\n\\(q_B\\)\n\\(q_A\\)\n\n\n\\(q_C\\)\n\\(\\rightarrow q_D\\)\n\\(q_A\\)\n\n\n\\(\\rightarrow q_D\\)\n\\(q_B\\)\n\\(q_A\\)\n\n\n\n\n\n21.2.4 Exercícios 4\n1.\n\nComeça em \\(q_A\\).\n\nLê ‘0’: \\(q_A \\rightarrow q_B\\). Estado atual: \\(q_B\\).\n\nLê ‘1’: \\(q_B \\rightarrow q_C\\). Estado atual: \\(q_C\\).\n\nLê ‘1’: \\(q_C \\rightarrow q_A\\). Estado atual: \\(q_A\\).\n\nLê ‘0’: \\(q_A \\rightarrow q_B\\). Estado atual: \\(q_B\\).\n\nLê ‘1’: \\(q_B \\rightarrow q_C\\). Estado atual: \\(q_C\\).\nA computação termina em \\(q_C\\), que é um estado de aceitação. Portanto, a string 01101 é aceita.\n\n2.\n\nComeça em \\(q_0\\).\n\nLê ‘&lt;’: \\(q_0 \\rightarrow q_\\lt\\). Estado atual: \\(q_\\lt\\).\n\nLê ‘=’: \\(q_\\lt \\rightarrow q_\\leq\\). Estado atual: \\(q_\\leq\\).\nA computação termina em \\(q_\\leq\\), que é um estado de aceitação. A string \\(&lt;=\\) é aceita.\n\n3.\n\nPara !=: \\(q_0 \\rightarrow q_\\neq\\). Termina em \\(q_\\neq\\) (aceitação). Aceita.\n\nPara =!: \\(q_0 \\rightarrow q_\\neq \\rightarrow q_\\erro\\). Termina em \\(q_\\erro\\) (rejeição).\n\nRejeitada.\n4. Não. A definição formal de aceitação exige que o estado final, após o processamento de toda a string, pertença ao conjunto F. Passar por um estado de aceitação no meio do caminho é irrelevante para a decisão final.\n5.\n\nAceita: ababa. Caminho: \\(q_{pp} \\xrightarrow{a} q_{ip} \\xrightarrow{b} q_{ii} \\xrightarrow{a} q_{pi} \\xrightarrow{b} q_{pp} \\xrightarrow{a} q_{ip}\\). Ops, essa é rejeitada. Vamos tentar aabb.\n\nAceita: aabb. Caminho: \\(q_{pp} \\xrightarrow{a} q_{ip} \\xrightarrow{a} q_{pp} \\xrightarrow{b} q_{pi} \\xrightarrow{b} q_{pp}\\). Termina em \\(q_{pp} \\in F\\). Aceita.\n\n\nRejeitada: ababa. Caminho: \\(q_{pp} \\xrightarrow{a} q_{ip} \\xrightarrow{b} q_{ii} \\xrightarrow{a} q_{pi} \\xrightarrow{b} q_{pp} \\xrightarrow{a} q_{ip}\\). Termina em \\(q_{ip} \\notin F\\). Rejeitada.\n\n\n\n21.2.5 Exercícios 5\n1. Diagrama para “aba”:\n\nq_0xrightarrowbq_0\n\nq_0xrightarrowaq_1\n\nq_1xrightarrowaq_1\n\nq_1xrightarrowbq_2\n\nq_2xrightarrowbq_0\n\nq_2xrightarrowaq_3\n\nq_3 é estado de aceitação (círculo duplo) com laços para ‘a’ e ‘b’ (delta(q_3,a)=q_3,delta(q_3,b)=q_3).\n\n2. Diagrama para múltiplos de 3:\n\nEstados: q_0 (resto 0), q_1 (resto 1), q_2 (resto 2).\n\nq_0 é inicial e de aceitação.\n\nTransições:\n\ndelta(q_0,0)=q_0, delta(q_0,1)=q_1\n\ndelta(q_1,0)=q_2, delta(q_1,1)=q_0\n\ndelta(q_2,0)=q_1, delta(q_2,1)=q_2\n\n\n3. Tabela de transições para //:\n\n\n\nEstado\n/\nc\n\n\n\n\nrightarrowq_0\nq_1\nq_e\n\n\nq_1\n\\*q_2\nq_e\n\n\n\\*q_2\n\\*q_2\n\\*q_2\n\n\nq_e\nq_e\nq_e\n\n\n\n4. Diagrama para comprimento ímpar e termina com ‘a’:\n\nEstados: q_0 (par), q_1 (ímpar, termina ‘a’), q_2 (ímpar, termina ‘b’), q_e (erro).\n\nq_0 é inicial. q_1 é de aceitação.\n\ndelta(q_0,a)=q_1, delta(q_0,b)=q_2\n\ndelta(q_1,a)=q_0, delta(q_1,b)=q_0\n\ndelta(q_2,a)=q_0, delta(q_2,b)=q_0\n(Uma versão mais simples pode combinar os estados de paridade e o último caractere).\n\n5. Diagrama para cat ou car:\n\nq_0xrightarrowcq_1\n\nq_1xrightarrowaq_2\n\nq_2xrightarrowtq_3 (q_3 é de aceitação)\n\nq_2xrightarrowrq_4 (q_4 é de aceitação)\n\nTodas as outras transições levam a um estado de erro q_e.\n\n\n\n21.2.6 Exercícios 6\n1. O autômato união é construído usando o produto cartesiano dos conjuntos de estados: \\[Q = Q_1 \\times Q_2\\]\nPortanto, o número de estados será: \\[|Q| = |Q_1| \\times |Q_2| = 3 \\times 2 = 6 \\text{ estados}\\]\nResposta: O autômato \\(M_1 \\cup M_2\\) terá 6 estados.\n2.\nMudando a condição de aceitação de OU para E, estaríamos realizando a interseção dos autômatos.\n\nUnião (\\(M_1 \\cup M_2\\)): Estado final se \\(p \\in F_1\\) OU \\(q \\in F_2\\)\n\nAceita strings que são aceitas por \\(M_1\\) ou por \\(M_2\\) (ou ambos)\n\nInterseção (\\(M_1 \\cap M_2\\)): Estado final se \\(p \\in F_1\\) E \\(q \\in F_2\\)\n\nAceita strings que são aceitas por \\(M_1\\) e por \\(M_2\\) simultaneamente\n\n\nResposta: Estaríamos realizando a operação de interseção dos autômatos.\n3.\nEspecificação dos autômatos originais:\n\\(M_1\\) aceita apenas “a”: - \\(Q_1 = \\{s_0, s_1, s_2\\}\\) (inicial, aceita “a”, rejeita) - \\(F_1 = \\{s_1\\}\\) - \\(\\delta_1(s_0, a) = s_1\\), \\(\\delta_1(s_0, b) = s_2\\) - \\(\\delta_1(s_1, a) = s_2\\), \\(\\delta_1(s_1, b) = s_2\\) - \\(\\delta_1(s_2, a) = s_2\\), \\(\\delta_1(s_2, b) = s_2\\)\n\\(M_2\\) aceita apenas “b”: - \\(Q_2 = \\{t_0, t_1, t_2\\}\\) (inicial, aceita “b”, rejeita) - \\(F_2 = \\{t_1\\}\\) - \\(\\delta_2(t_0, a) = t_2\\), \\(\\delta_2(t_0, b) = t_1\\) - \\(\\delta_2(t_1, a) = t_2\\), \\(\\delta_2(t_1, b) = t_2\\) - \\(\\delta_2(t_2, a) = t_2\\), \\(\\delta_2(t_2, b) = t_2\\)\nAutômato união \\(M_1 \\cup M_2\\): - Estados finais: \\((s_1, t_0)\\), \\((s_1, t_1)\\), \\((s_1, t_2)\\), \\((s_0, t_1)\\), \\((s_2, t_1)\\)\nTabela de transições:\n\n\n\nEstado\na\nb\n\n\n\n\n→\\((s_0,t_0)\\)\n\\((s_1,t_2)\\)\n\\((s_2,t_1)\\)\n\n\n*\\((s_0,t_1)\\)\n\\((s_1,t_2)\\)\n\\((s_2,t_2)\\)\n\n\n\\((s_0,t_2)\\)\n\\((s_1,t_2)\\)\n\\((s_2,t_2)\\)\n\n\n*\\((s_1,t_0)\\)\n\\((s_2,t_2)\\)\n\\((s_2,t_1)\\)\n\n\n*\\((s_1,t_1)\\)\n\\((s_2,t_2)\\)\n\\((s_2,t_2)\\)\n\n\n*\\((s_1,t_2)\\)\n\\((s_2,t_2)\\)\n\\((s_2,t_2)\\)\n\n\n\\((s_2,t_0)\\)\n\\((s_2,t_2)\\)\n\\((s_2,t_1)\\)\n\n\n*\\((s_2,t_1)\\)\n\\((s_2,t_2)\\)\n\\((s_2,t_2)\\)\n\n\n\\((s_2,t_2)\\)\n\\((s_2,t_2)\\)\n\\((s_2,t_2)\\)\n\n\n\n4.\nEspecificação dos autômatos:\n\\(M_1\\) (termina em ‘01’): - \\(Q_1 = \\{p_0, p_1, p_2\\}\\) onde \\(p_0\\) é inicial, \\(p_1\\) leu ‘0’, \\(p_2\\) leu ‘01’ - \\(F_1 = \\{p_2\\}\\) - Transições: \\(\\delta_1(p_0,0) = p_1\\), \\(\\delta_1(p_0,1) = p_0\\), \\(\\delta_1(p_1,0) = p_1\\), \\(\\delta_1(p_1,1) = p_2\\), \\(\\delta_1(p_2,0) = p_1\\), \\(\\delta_1(p_2,1) = p_0\\)\n\\(M_2\\) (número par de ’1’s): - \\(Q_2 = \\{q_0, q_1\\}\\) onde \\(q_0\\) é par, \\(q_1\\) é ímpar - \\(F_2 = \\{q_0\\}\\) - Transições: \\(\\delta_2(q_0,0) = q_0\\), \\(\\delta_2(q_0,1) = q_1\\), \\(\\delta_2(q_1,0) = q_1\\), \\(\\delta_2(q_1,1) = q_0\\)\nAutômato união \\(M_1 \\cup M_2\\):\nEstados finais: \\((p_2,q_0)\\), \\((p_2,q_1)\\), \\((p_0,q_0)\\), \\((p_1,q_0)\\)\nTabela de transições:\n\n\n\nEstado\n0\n1\n\n\n\n\n→\\((p_0,q_0)\\)\n\\((p_1,q_0)\\)\n\\((p_0,q_1)\\)\n\n\n\\((p_0,q_1)\\)\n\\((p_1,q_1)\\)\n\\((p_0,q_0)\\)\n\n\n*\\((p_1,q_0)\\)\n\\((p_1,q_0)\\)\n\\((p_2,q_1)\\)\n\n\n\\((p_1,q_1)\\)\n\\((p_1,q_1)\\)\n\\((p_2,q_0)\\)\n\n\n*\\((p_2,q_0)\\)\n\\((p_1,q_0)\\)\n\\((p_0,q_1)\\)\n\n\n*\\((p_2,q_1)\\)\n\\((p_1,q_1)\\)\n\\((p_0,q_0)\\)\n\n\n\nA linguagem aceita é \\(L(M_1 \\cup M_2)\\): strings que terminam em ‘01’ OU têm número par de ’1’s.\n5.\nPasso 1: Especificar os autômatos individuais\n\\(M_1\\) (começa com ‘1’): - \\(Q_1 = \\{r_0, r_1\\}\\) onde \\(r_0\\) é inicial, \\(r_1\\) aceita - \\(F_1 = \\{r_1\\}\\) - \\(\\delta_1(r_0,0) = r_0\\), \\(\\delta_1(r_0,1) = r_1\\), \\(\\delta_1(r_1,0) = r_1\\), \\(\\delta_1(r_1,1) = r_1\\)\n\\(M_2\\) (comprimento par): - \\(Q_2 = \\{s_0, s_1\\}\\) onde \\(s_0\\) é par, \\(s_1\\) é ímpar - \\(F_2 = \\{s_0\\}\\) - \\(\\delta_2(s_0,0) = s_1\\), \\(\\delta_2(s_0,1) = s_1\\), \\(\\delta_2(s_1,0) = s_0\\), \\(\\delta_2(s_1,1) = s_0\\)\nPasso 2: Criar conjunto de estados\n\\(Q = Q_1 \\times Q_2 = \\{(r_0,s_0), (r_0,s_1), (r_1,s_0), (r_1,s_1)\\}\\)\nTotal: \\(2 \\times 2 = 4\\) estados\nPasso 3: Determinar estado inicial\nEstado inicial: \\((r_0, s_0)\\)\nPasso 4: Determinar estados finais\nEstados finais onde \\(r_i \\in F_1\\) OU \\(s_j \\in F_2\\): - \\((r_0,s_0)\\): \\(r_0 \\notin F_1\\) mas \\(s_0 \\in F_2\\) → FINAL - \\((r_0,s_1)\\): \\(r_0 \\notin F_1\\) e \\(s_1 \\notin F_2\\) → não final - \\((r_1,s_0)\\): \\(r_1 \\in F_1\\) e \\(s_0 \\in F_2\\) → FINAL - \\((r_1,s_1)\\): \\(r_1 \\in F_1\\) mas \\(s_1 \\notin F_2\\) → FINAL\nEstados finais: \\(F = \\{(r_0,s_0), (r_1,s_0), (r_1,s_1)\\}\\)\nPasso 5: Construir função de transição\n\\(\\delta((r_i,s_j), x) = (\\delta_1(r_i,x), \\delta_2(s_j,x))\\)\nTransições com ‘0’: - \\(\\delta((r_0,s_0), 0) = (r_0,s_1)\\) - \\(\\delta((r_0,s_1), 0) = (r_0,s_0)\\) - \\(\\delta((r_1,s_0), 0) = (r_1,s_1)\\) - \\(\\delta((r_1,s_1), 0) = (r_1,s_0)\\)\nTransições com ‘1’: - \\(\\delta((r_0,s_0), 1) = (r_1,s_1)\\) - \\(\\delta((r_0,s_1), 1) = (r_1,s_0)\\) - \\(\\delta((r_1,s_0), 1) = (r_1,s_1)\\) - \\(\\delta((r_1,s_1), 1) = (r_1,s_0)\\)\nPasso 6: Tabela final\n\n\n\nEstado\n0\n1\n\n\n\n\n→*\\((r_0,s_0)\\)\n\\((r_0,s_1)\\)\n\\((r_1,s_1)\\)\n\n\n\\((r_0,s_1)\\)\n\\((r_0,s_0)\\)\n\\((r_1,s_0)\\)\n\n\n*\\((r_1,s_0)\\)\n\\((r_1,s_1)\\)\n\\((r_1,s_1)\\)\n\n\n*\\((r_1,s_1)\\)\n\\((r_1,s_0)\\)\n\\((r_1,s_0)\\)\n\n\n\nLinguagem aceita: \\(L(M_1 \\cup M_2)\\) são strings que começam com ‘1’ OU têm comprimento par.\nExemplos: \\(\\varepsilon\\), “00”, “1”, “10”, “11”, “0011”, “1010”\n\n\n21.2.7 Exercícios 7\n1. Especificação dos autômatos:\n\\(M_1\\) (termina em ‘10’): - \\(Q_1 = \\{p_0, p_1, p_2\\}\\) onde \\(p_0\\) é inicial, \\(p_1\\) leu ‘1’, \\(p_2\\) leu ‘10’ - \\(F_1 = \\{p_2\\}\\) - Transições: - \\(\\delta_1(p_0, 0) = p_0\\), \\(\\delta_1(p_0, 1) = p_1\\) - \\(\\delta_1(p_1, 0) = p_2\\), \\(\\delta_1(p_1, 1) = p_1\\) - \\(\\delta_1(p_2, 0) = p_0\\), \\(\\delta_1(p_2, 1) = p_1\\)\n\\(M_2\\) (número ímpar de ’0’s): - \\(Q_2 = \\{q_0, q_1\\}\\) onde \\(q_0\\) é par de ’0’s, \\(q_1\\) é ímpar de ’0’s - \\(F_2 = \\{q_1\\}\\) - Transições: - \\(\\delta_2(q_0, 0) = q_1\\), \\(\\delta_2(q_0, 1) = q_0\\) - \\(\\delta_2(q_1, 0) = q_0\\), \\(\\delta_2(q_1, 1) = q_1\\)\nAutômato interseção \\(M_1 \\cap M_2\\):\nEstados: \\(Q = Q_1 \\times Q_2 = \\{(p_0,q_0), (p_0,q_1), (p_1,q_0), (p_1,q_1), (p_2,q_0), (p_2,q_1)\\}\\)\nEstado inicial: \\((p_0, q_0)\\)\nConjunto de estados finais: Para a interseção, um estado \\((p_i, q_j)\\) é final se \\(p_i \\in F_1\\) E \\(q_j \\in F_2\\):\n\n\\((p_0,q_0)\\): \\(p_0 \\notin F_1\\) → não é final\n\\((p_0,q_1)\\): \\(p_0 \\notin F_1\\) → não é final\n\n\\((p_1,q_0)\\): \\(p_1 \\notin F_1\\) → não é final\n\\((p_1,q_1)\\): \\(p_1 \\notin F_1\\) → não é final\n\\((p_2,q_0)\\): \\(p_2 \\in F_1\\) mas \\(q_0 \\notin F_2\\) → não é final\n\\((p_2,q_1)\\): \\(p_2 \\in F_1\\) e \\(q_1 \\in F_2\\) → É FINAL\n\nResposta: \\(F = \\{(p_2, q_1)\\}\\)\nLinguagem aceita: \\(L(M_1 \\cap M_2)\\) são strings que terminam em ‘10’ E têm número ímpar de ’0’s.\nExemplos aceitos: “10”, “010”, “110”, “0010”, “1010”, “0110”\n2.\nSim, é possível que \\(L(M_1 \\cup M_2) = L(M_1 \\cap M_2)\\).\nCondição necessária e suficiente: Isso ocorre se e somente se \\(L(M_1) = L(M_2)\\).\nDemonstração:\nCaso 1: Se \\(L(M_1) = L(M_2)\\), então: - \\(L(M_1 \\cup M_2) = L(M_1) \\cup L(M_2) = L(M_1) \\cup L(M_1) = L(M_1)\\) - \\(L(M_1 \\cap M_2) = L(M_1) \\cap L(M_2) = L(M_1) \\cap L(M_1) = L(M_1)\\)\nPortanto, \\(L(M_1 \\cup M_2) = L(M_1 \\cap M_2)\\).\nCaso 2: Se \\(L(M_1) \\neq L(M_2)\\), então existe alguma string \\(w\\) tal que: - \\(w \\in L(M_1)\\) e \\(w \\notin L(M_2)\\), ou - \\(w \\notin L(M_1)\\) e \\(w \\in L(M_2)\\)\nSem perda de generalidade, suponha \\(w \\in L(M_1)\\) e \\(w \\notin L(M_2)\\).\nEntão: - \\(w \\in L(M_1 \\cup M_2)\\) (porque \\(w \\in L(M_1)\\)) - \\(w \\notin L(M_1 \\cap M_2)\\) (porque \\(w \\notin L(M_2)\\))\nLogo, \\(L(M_1 \\cup M_2) \\neq L(M_1 \\cap M_2)\\).\nExemplos práticos: - Se \\(M_1\\) e \\(M_2\\) ambos aceitam apenas strings que começam com ‘1’, então \\(L(M_1 \\cup M_2) = L(M_1 \\cap M_2)\\) - Se \\(M_1\\) aceita strings pares e \\(M_2\\) aceita strings ímpares, então \\(L(M_1 \\cup M_2) = \\Sigma^*\\) mas \\(L(M_1 \\cap M_2) = \\emptyset\\)\n3. Se \\(F_1 = Q_1\\), então \\(L(M_1 \\cap M_2) = L(M_2)\\).\nDemonstração:\nQuando \\(F_1 = Q_1\\), o autômato \\(M_1\\) aceita todas as strings sobre o alfabeto \\(\\Sigma\\), ou seja, \\(L(M_1) = \\Sigma^*\\).\nAnálise dos estados finais da interseção: No autômato \\(M_1 \\cap M_2\\), um estado \\((p, q)\\) é final se \\(p \\in F_1\\) E \\(q \\in F_2\\).\nComo \\(F_1 = Q_1\\), temos \\(p \\in F_1\\) para todo estado \\(p \\in Q_1\\).\nPortanto, \\((p, q)\\) é final se e somente se \\(q \\in F_2\\).\nComportamento da interseção: - O autômato \\(M_1 \\cap M_2\\) aceita uma string \\(w\\) se e somente se ela leva a um estado \\((p, q)\\) onde \\(q \\in F_2\\) - Isso é equivalente a dizer que \\(M_2\\) aceita \\(w\\), porque a componente \\(q\\) segue exatamente as transições de \\(M_2\\) - A componente \\(p\\) não influencia na aceitação, já que todos os estados de \\(M_1\\) são finais\nResultado: \\[L(M_1 \\cap M_2) = L(M_1) \\cap L(M_2) = \\Sigma^* \\cap L(M_2) = L(M_2)\\]\nInterpretação prática: Quando \\(M_1\\) aceita tudo (\\(L(M_1) = \\Sigma^*\\)), a interseção \\(M_1 \\cap M_2\\) se comporta exatamente como \\(M_2\\), porque a única restrição vem de \\(M_2\\).\nExemplo: - Se \\(M_1\\) tem \\(F_1 = Q_1\\) e \\(M_2\\) aceita strings com número par de ’1’s - Então \\(M_1 \\cap M_2\\) aceita exatamente as strings com número par de ’1’s - Ou seja, \\(L(M_1 \\cap M_2) = L(M_2)\\)\n4.\nEspecificação dos autômatos:\n\\(M_1\\) (termina em ‘ab’): - \\(Q_1 = \\{q_0, q_1, q_2\\}\\) onde \\(q_0\\) é inicial, \\(q_1\\) leu ‘a’, \\(q_2\\) leu ‘ab’ - \\(F_1 = \\{q_2\\}\\)\n\\(M_2\\) (número par de ’a’s): - \\(Q_2 = \\{r_0, r_1\\}\\) onde \\(r_0\\) é par de ’a’s, \\(r_1\\) é ímpar de ’a’s - \\(F_2 = \\{r_0\\}\\)\nAutômato interseção \\(M_1 \\cap M_2\\): - \\(Q = Q_1 \\times Q_2 = \\{(q_0,r_0), (q_0,r_1), (q_1,r_0), (q_1,r_1), (q_2,r_0), (q_2,r_1)\\}\\) - Estado inicial: \\((q_0, r_0)\\) - Estados finais: \\(F = \\{(q_2, r_0)\\}\\) (termina em ‘ab’ E número par de ’a’s)\nLinguagem aceita: \\(L(M_1 \\cap M_2)\\) são strings que terminam em ‘ab’ e têm número par de ’a’s.\nExemplos: “ab”, “aabb”, “baaabb”, “aaab”, “babab”\n5.\nPasso 1: Analisar os autômatos individuais\n\\(M_1\\): Começa com ‘a’ - \\(Q_1 = \\{s_0, s_1, s_2\\}\\) (inicial, aceita, rejeita) - \\(F_1 = \\{s_1\\}\\)\n\\(M_2\\): Comprimento múltiplo de 3 - \\(Q_2 = \\{t_0, t_1, t_2\\}\\) (resto 0, resto 1, resto 2) - \\(F_2 = \\{t_0\\}\\)\n\\(M_3\\): Contém ‘bb’ - \\(Q_3 = \\{u_0, u_1, u_2\\}\\) (inicial, leu ‘b’, encontrou ‘bb’) - \\(F_3 = \\{u_2\\}\\)\nPasso 2: Construir \\(M_1 \\cup M_2\\)\nEstados: \\(Q_{1 \\cup 2} = Q_1 \\times Q_2 = 3 \\times 3 = 9\\) estados\nEstados finais de \\(M_1 \\cup M_2\\): \\((s_i, t_j)\\) onde \\(s_i \\in F_1\\) OU \\(t_j \\in F_2\\) - Estados finais: \\(\\{(s_1, t_0), (s_1, t_1), (s_1, t_2), (s_0, t_0), (s_2, t_0)\\}\\)\nPasso 3: Construir \\((M_1 \\cup M_2) \\cap M_3\\)\nEstados: \\(Q = Q_{1 \\cup 2} \\times Q_3 = 9 \\times 3 = 27\\) estados\nCada estado é da forma \\(((s_i, t_j), u_k)\\)\nPasso 4: Determinar estados finais de \\((M_1 \\cup M_2) \\cap M_3\\)\nUm estado \\(((s_i, t_j), u_k)\\) é final se: - \\((s_i, t_j) \\in F_{1 \\cup 2}\\) E \\(u_k \\in F_3\\) - Ou seja: \\((s_i \\in F_1\\) OU \\(t_j \\in F_2)\\) E \\(u_k = u_2\\)\nEstados finais: - \\(((s_1, t_0), u_2)\\), \\(((s_1, t_1), u_2)\\), \\(((s_1, t_2), u_2)\\), \\(((s_0, t_0), u_2)\\), \\(((s_2, t_0), u_2)\\)\nPasso 5: Função de transição\nPara cada estado \\(((s_i, t_j), u_k)\\) e símbolo \\(x\\): \\[\\delta(((s_i, t_j), u_k), x) = ((\\delta_1(s_i, x), \\delta_2(t_j, x)), \\delta_3(u_k, x))\\]\nResposta final: - Número de estados: 27 estados; - Linguagem aceita: Strings que (começam com ‘a’ OU têm comprimento múltiplo de 3) E contêm ‘bb’; - Exemplos: “abb”, “abba”, “aabbb”, “bbaaa”, “bbbbbb”.\nO autômato resultante aceita strings que satisfazem simultaneamente a condição de união (começa com ‘a’ ou comprimento múltiplo de 3) e a condição de \\(M_3\\) (contém ‘bb’).\n\n\n21.2.8 Exercícios 8\n1.\nAutômato original \\(M\\):\n\n\n\nEstado\na\nb\nFinal?\n\n\n\n\n→\\(q_0\\)\n\\(q_1\\)\n\\(q_0\\)\nSim\n\n\n\\(q_1\\)\n\\(q_0\\)\n\\(q_1\\)\nNão\n\n\n\nConstrução do complemento \\(\\overline{M}\\):\nPara construir \\(\\overline{M}\\), mantemos a mesma estrutura de estados e transições, mas invertemos o conjunto de estados finais: - Estados que eram finais tornam-se não finais - Estados que eram não finais tornam-se finais\nAutômato complemento \\(\\overline{M}\\): - \\(Q_{\\overline{M}} = Q = \\{q_0, q_1\\}\\) - \\(\\delta_{\\overline{M}} = \\delta\\) (mesmas transições) - \\(F_{\\overline{M}} = Q \\setminus F = \\{q_1\\}\\)\nTabela de transições de \\(\\overline{M}\\):\n\n\n\nEstado\na\nb\nFinal?\n\n\n\n\n→\\(q_0\\)\n\\(q_1\\)\n\\(q_0\\)\nNão\n\n\n\\(q_1\\)\n\\(q_0\\)\n\\(q_1\\)\nSim\n\n\n\nVerificação: - \\(M\\) aceita strings com número par de ’a’s - \\(\\overline{M}\\) aceita strings com número ímpar de ’a’s - \\(L(M) \\cup L(\\overline{M}) = \\Sigma^*\\) e \\(L(M) \\cap L(\\overline{M}) = \\emptyset\\)\n2.\nPor que a completude é essencial:\nA completude garante que o autômato tenha uma transição definida para todo par (estado, símbolo). Sem completude, strings que levam a transições indefinidas teriam comportamento ambíguo no complemento.\nProblema sem completude: - Em um autômato incompleto, algumas strings podem “travar” (não ter transição definida) - Strings que travam são implicitamente rejeitadas no autômato original - No complemento, seria necessário decidir se essas strings devem ser aceitas ou rejeitadas - Sem completude, essa decisão não fica clara, tornando o complemento mal definido\nExemplo prático:\nConsidere um autômato incompleto \\(M'\\) sobre \\(\\Sigma = \\{a, b\\}\\): - \\(Q = \\{q_0, q_1\\}\\), estado inicial \\(q_0\\), \\(F = \\{q_1\\}\\) - \\(\\delta(q_0, a) = q_1\\) (transição para ‘a’) - $(q_0, b) = $ indefinido (sem transição para ‘b’) - $(q_1, a) = $ indefinido - $(q_1, b) = $ indefinido\nProblema na construção do complemento: 1. String “a”: aceita por \\(M'\\) → deve ser rejeitada por \\(\\overline{M'}\\) 2. String “b”: trava em \\(M'\\) (rejeitada) → deveria ser aceita por \\(\\overline{M'}\\)? 3. String “aa”: trava em \\(M'\\) após aceitar “a” → comportamento indefinido\nSolução: Completar primeiro o autômato adicionando um estado “lixeira”: - Adicionar estado \\(q_{trap}\\) não final - \\(\\delta(q_0, b) = q_{trap}\\) - \\(\\delta(q_1, a) = q_{trap}\\), \\(\\delta(q_1, b) = q_{trap}\\) - \\(\\delta(q_{trap}, a) = q_{trap}\\), \\(\\delta(q_{trap}, b) = q_{trap}\\)\nAgora o complemento fica bem definido com \\(F_{\\overline{M}} = \\{q_0, q_{trap}\\}\\).\n3.\nTeorema: Se \\(L_1\\) e \\(L_2\\) são linguagens regulares, então \\(L_1 - L_2\\) (diferença de conjuntos) também é regular.\nDemonstração:\nPasso 1: Expressar diferença usando operações básicas \\[L_1 - L_2 = L_1 \\cap \\overline{L_2}\\]\nEsta igualdade é válida porque: - \\(w \\in L_1 - L_2 \\iff w \\in L_1 \\text{ e } w \\notin L_2\\) - \\(w \\in L_1 \\cap \\overline{L_2} \\iff w \\in L_1 \\text{ e } w \\in \\overline{L_2} \\iff w \\in L_1 \\text{ e } w \\notin L_2\\)\nPasso 2: Aplicar propriedades de fechamento\nPropriedade 1: Linguagens regulares são fechadas sob complemento - Se \\(L_2\\) é regular, então \\(\\overline{L_2}\\) também é regular\nPropriedade 2: Linguagens regulares são fechadas sob interseção - Se \\(L_1\\) e \\(\\overline{L_2}\\) são regulares, então \\(L_1 \\cap \\overline{L_2}\\) também é regular\nPasso 3: Conclusão Como: 1. \\(L_1\\) é regular (dado) 2. \\(L_2\\) é regular (dado) 3. \\(\\overline{L_2}\\) é regular (por fechamento sob complemento) 4. \\(L_1 \\cap \\overline{L_2}\\) é regular (por fechamento sob interseção) 5. \\(L_1 - L_2 = L_1 \\cap \\overline{L_2}\\)\nConcluímos que \\(L_1 - L_2\\) é regular. □\nAlgoritmo construtivo: 1. Construir autômato \\(M_2\\) para \\(L_2\\) 2. Construir autômato \\(\\overline{M_2}\\) para \\(\\overline{L_2}\\) (invertendo estados finais) 3. Construir autômato \\(M_1 \\cap \\overline{M_2}\\) usando produto cartesiano 4. O resultado reconhece \\(L_1 - L_2\\)\n4.\nConclusão: Se \\(L\\) é regular e \\(\\overline{L} = \\emptyset\\), então \\(L = \\Sigma^*\\).\nDemonstração:\nPropriedade fundamental: Para qualquer linguagem \\(L\\) sobre alfabeto \\(\\Sigma\\): \\[L \\cup \\overline{L} = \\Sigma^*\\]\nEsta propriedade vale porque toda string pertence a \\(L\\) ou ao seu complemento (mas não a ambos).\nAplicando a hipótese: Se \\(\\overline{L} = \\emptyset\\), então: \\[L \\cup \\overline{L} = L \\cup \\emptyset = L\\]\nCombinando com a propriedade fundamental: \\[L = L \\cup \\overline{L} = \\Sigma^*\\]\nVerificação por contradição: Suponha que existe \\(w \\in \\Sigma^*\\) tal que \\(w \\notin L\\).\nEntão, por definição de complemento, \\(w \\in \\overline{L}\\).\nMas isso contradiz a hipótese \\(\\overline{L} = \\emptyset\\).\nLogo, não existe tal \\(w\\), e portanto \\(L = \\Sigma^*\\).\nInterpretação prática: - Se o complemento de uma linguagem é vazio, significa que não existe nenhuma string que a linguagem não aceite - Portanto, a linguagem aceita todas as strings possíveis - Ou seja, \\(L\\) é a linguagem universal sobre o alfabeto\nExemplo: - Se \\(L = \\{w \\in \\{a,b\\}^* : w \\text{ contém pelo menos 0 símbolos}\\}\\) - Então \\(L = \\{a,b\\}^*\\) e \\(\\overline{L} = \\emptyset\\)\n5.\nLei de De Morgan generalizada: \\[\\overline{L_1 \\cap L_2 \\cap L_3} = \\overline{L_1} \\cup \\overline{L_2} \\cup \\overline{L_3}\\]\nDemonstração passo a passo:\nMétodo 1: Aplicação direta da lei de De Morgan\nPara três conjuntos, a lei de De Morgan estabelece: \\[\\overline{A \\cap B \\cap C} = \\overline{A} \\cup \\overline{B} \\cup \\overline{C}\\]\nAplicando diretamente com \\(A = L_1\\), \\(B = L_2\\), \\(C = L_3\\): \\[\\overline{L_1 \\cap L_2 \\cap L_3} = \\overline{L_1} \\cup \\overline{L_2} \\cup \\overline{L_3}\\]\nMétodo 2: Aplicação iterativa\nPasso 1: Tratar \\((L_1 \\cap L_2 \\cap L_3)\\) como \\((L_1 \\cap L_2) \\cap L_3\\) \\[\\overline{(L_1 \\cap L_2) \\cap L_3} = \\overline{L_1 \\cap L_2} \\cup \\overline{L_3}\\]\nPasso 2: Aplicar De Morgan novamente em \\(\\overline{L_1 \\cap L_2}\\) \\[\\overline{L_1 \\cap L_2} = \\overline{L_1} \\cup \\overline{L_2}\\]\nPasso 3: Substituir de volta \\[\\overline{L_1 \\cap L_2 \\cap L_3} = (\\overline{L_1} \\cup \\overline{L_2}) \\cup \\overline{L_3} = \\overline{L_1} \\cup \\overline{L_2} \\cup \\overline{L_3}\\]\nVerificação semântica:\nUma string \\(w\\) pertence a \\(\\overline{L_1 \\cap L_2 \\cap L_3}\\) se e somente se: \\[w \\notin (L_1 \\cap L_2 \\cap L_3)\\]\nIsso significa que \\(w\\) não pertence simultaneamente a \\(L_1\\), \\(L_2\\) e \\(L_3\\).\nOu seja, \\(w\\) falha em pelo menos uma das três linguagens: \\[w \\notin L_1 \\text{ ou } w \\notin L_2 \\text{ ou } w \\notin L_3\\]\nQue é equivalente a: \\[w \\in \\overline{L_1} \\text{ ou } w \\in \\overline{L_2} \\text{ ou } w \\in \\overline{L_3}\\]\nPortanto: \\[w \\in \\overline{L_1} \\cup \\overline{L_2} \\cup \\overline{L_3}\\]\nResposta final: \\[\\overline{L_1 \\cap L_2 \\cap L_3} = \\overline{L_1} \\cup \\overline{L_2} \\cup \\overline{L_3}\\]\n\n\n21.2.9 Exercícios 10\n\n21.2.9.1 1\na) Classes de equivalência de \\(\\equiv_L\\):\nPara \\(L = \\{w \\mid w \\text{ contém } 00\\}\\), temos três classes de equivalência:\n\nClasse \\(C_0\\): Strings que não terminam em \\(0\\) e não contêm \\(00\\)\n\nRepresentante: \\(\\epsilon\\)\nOutros membros: \\(1\\), \\(11\\), \\(111\\), \\(101\\), \\(1101\\), etc.\n\nClasse \\(C_1\\): Strings que terminam em exatamente um \\(0\\) e não contêm \\(00\\)\n\nRepresentante: \\(0\\)\nOutros membros: \\(10\\), \\(110\\), \\(1110\\), \\(1010\\), etc.\n\nClasse \\(C_2\\): Strings que contêm \\(00\\)\n\nRepresentante: \\(00\\)\nOutros membros: \\(000\\), \\(100\\), \\(001\\), \\(0011\\), qualquer string com \\(00\\)\n\n\nb) Justificativa da equivalência:\n\n\\(C_0\\): Para strings em \\(C_0\\), adicionar \\(0\\) as move para \\(C_1\\), adicionar \\(00\\) as leva para \\(L\\)\n\\(C_1\\): Para strings em \\(C_1\\), adicionar \\(0\\) as leva direto para \\(L\\), adicionar \\(1\\) as retorna para \\(C_0\\)\n\\(C_2\\): Para strings em \\(C_2\\), qualquer sufixo mantém a string em \\(L\\) (porque já contém \\(00\\))\n\nc) Autômato Finito Determinístico mínimo:\nConjunto de estados: \\(Q = \\{q_0, q_1, q_2\\}\\) Alfabeto: \\(\\Sigma = \\{0, 1\\}\\) Estado inicial: \\(q_0\\) Conjunto de estados finais: \\(F = \\{q_2\\}\\) Função de transição: \\(\\delta: Q \\times \\Sigma \\to Q\\) é definida por:\n$$\n\\begin{aligned}\n\\delta(q_0, 0) &= q_1 \\\\\n\\delta(q_0, 1) &= q_0 \\\\\n\\delta(q_1, 0) &= q_2 \\\\\n\\delta(q_1, 1) &= q_0 \\\\\n\\delta(q_2, 0) &= q_2 \\\\\n\\delta(q_2, 1) &= q_2\n\\end{aligned}\n$$\nd) Verificação: O Autômato Finito Determinístico mínimo tem 3 estados, correspondendo exatamente às 3 classes de equivalência de Myhill-Nerode.\n\n\n21.2.9.2 2\nProva de que \\(L = \\{0^i1^j \\mid i &gt; j \\geq 0\\}\\) não é regular:\nConsidere as strings \\(s_n = 0^n\\) para \\(n = 1, 2, 3, ...\\)\nAfirmação: Para \\(m \\neq n\\), temos \\(s_m \\not\\equiv_L s_n\\).\nProva: Considere o sufixo \\(z = 1^m\\). Então: - \\(s_m \\cdot z = 0^m1^m \\notin L\\) (porque \\(m = m\\), não \\(m &gt; m\\)) - \\(s_n \\cdot z = 0^n1^m\\): - Se \\(n &gt; m\\), então \\(0^n1^m \\in L\\) - Se \\(n &lt; m\\), então \\(0^n1^m \\notin L\\)\nComo \\(n \\neq m\\), pelo menos um dos casos acima distingue \\(s_m\\) de \\(s_n\\).\nPortanto, existem infinitas classes de equivalência (uma para cada \\(n \\in \\mathbb{N}\\)), logo \\(L\\) não é regular pelo Teorema de Myhill-Nerode.\n\n\n21.2.9.3 3\nEliminação de estados para encontrar ER:\nEstado inicial: Autômato Finito Determinístico com 3 estados\nPasso 1: Eliminar \\(q_1\\) - Aresta \\(q_0 \\to q_2\\): \\(ab\\) (via \\(q_1\\)) - Aresta \\(q_0 \\to q_0\\): \\(b\\) (direto) - Nova transição: \\(q_0 \\xrightarrow{ab} q_2\\)\nPasso 2: Eliminar \\(q_2\\) (estado final) - De \\(q_0\\) para \\(q_2\\): \\(ab(a|b)^*\\) - Loop em \\(q_0\\): \\(b^*\\)\nExpressão regular final: \\[r = b^*ab(a|b)^*\\]\nEsta ER descreve strings que: 1. Começam com zero ou mais \\(b\\)’s 2. Seguidos por \\(ab\\) 3. Terminam com qualquer sequência de \\(a\\)’s e \\(b\\)’s\n\n\n21.2.9.4 4\na) Teste de equivalência via produto:\nConstruindo \\((L(M_1) - L(M_2)) \\cup (L(M_2) - L(M_1))\\):\nAnálise das linguagens: - \\(L(M_1)\\): strings com número par de \\(a\\)’s - \\(L(M_2)\\): NÃO é equivalente (aceita baseado em posição)\nb) Teste com strings: - \\(\\epsilon\\): \\(M_1\\) aceita (0 \\(a\\)’s = par), \\(M_2\\) aceita → ambos aceitam ✓ - \\(a\\): \\(M_1\\) rejeita (1 \\(a\\) = ímpar), \\(M_2\\) rejeita (posição 1 é ímpar) → ambos rejeitam ✓ - \\(b\\): \\(M_1\\) aceita (0 \\(a\\)’s), \\(M_2\\) aceita → ambos aceitam ✓ - \\(aa\\): \\(M_1\\) aceita (2 \\(a\\)’s = par), \\(M_2\\) rejeita → DIFEREM ✗ - \\(ba\\): \\(M_1\\) rejeita (1 \\(a\\)), \\(M_2\\) aceita (posição 2 é par) → DIFEREM ✗\nConclusão: \\(L(M_1) \\neq L(M_2)\\)\n\n\n21.2.9.5 5\na) Minimização de Hopcroft:\nAutômato Finito Determinístico original para strings terminadas em \\(01\\):\nEstados iniciais: \\(\\{q_0, q_1, ..., q_7\\}\\)\nParticionamento: 1. Separar finais e não-finais 2. Refinar baseado em transições 3. Resultado: Autômato Finito Determinístico mínimo com 3 estados\nAutômato Finito Determinístico mínimo: - \\(s_0\\): não leu nada relevante - \\(s_1\\): último símbolo foi \\(0\\) - \\(s_2\\): últimos símbolos foram \\(01\\) (final)\nb) Complexidade: - Hopcroft: \\(O(k \\cdot n \\log n) = O(2 \\cdot 8 \\log 8) = O(48)\\) operações\nc) Comparação para Autômato Finito Determinísticos com 8 e 10 estados: - Via minimização: \\(O(8 \\log 8) + O(10 \\log 10) + O(1)\\) ≈ \\(O(57)\\) - Via produto: \\(O(8 \\times 10) = O(80)\\)\nPara este caso, minimização é mais eficiente.\n\n\n21.2.9.6 6\na) Autômato Finito Determinísticos individuais:\nEste autômato reconhece sequências não vazias de dígitos.\nRepresentação Algébrica:\n\\(A_{int} = (Q_{int}, \\Sigma_{int}, \\delta_{int}, q_{0,int}, F_{int})\\), onde:\n\nConjunto de estados: \\(Q_{int} = \\{q_0, q_1, q_d\\}\\)\nAlfabeto (classes): \\(\\Sigma_{int} = \\{\\text{dígito}, \\text{outro}\\}\\), onde dígito é qualquer caractere de ‘0’ a ‘9’ e outro é qualquer caractere que não seja um dígito.\nEstado inicial: \\(q_{0,int} = q_0\\)\nConjunto de estados finais: \\(F_{int} = \\{q_1\\}\\)\nFunção de transição \\(\\delta_{int}\\):\n\\[\n\\begin{aligned}\n\\delta(q_0, \\text{dígito}) &= q_1 \\\\\n\\delta(q_0, \\text{outro}) &= q_d \\\\\n\\delta(q_1, \\text{dígito}) &= q_1 \\\\\n\\delta(q_1, \\text{outro}) &= q_d \\\\\n\\delta(q_d, \\text{dígito}) &= q_d \\\\\n\\delta(q_d, \\text{outro}) &= q_d\n\\end{aligned}\n\\]\n\nTabela de Transição:\n\n\n\nEstado\ndígito (0-9)\noutro\n\n\n\n\n\\(\\to q_0\\)\n\\(q_1\\)\n\\(q_d\\)\n\n\n\\(*q_1\\)\n\\(q_1\\)\n\\(q_d\\)\n\n\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\n\n\nEste autômato reconhece tokens que começam com uma letra e são seguidos por zero ou mais letras ou dígitos.\nRepresentação Algébrica:\n\\(A_{id} = (Q_{id}, \\Sigma_{id}, \\delta_{id}, q_{0,id}, F_{id})\\), onde:\n\nConjunto de estados: \\(Q_{id} = \\{q_0, q_1, q_d\\}\\)\nAlfabeto (classes): \\(\\Sigma_{id} = \\{\\text{letra}, \\text{dígito}, \\text{outro}\\}\\), onde letra é ‘a’-‘z’ ou ‘A’-‘Z’, dígito é ‘0’-‘9’, e outro é qualquer caractere que não seja letra nem dígito.\nEstado inicial: \\(q_{0,id} = q_0\\)\nConjunto de estados finais: \\(F_{id} = \\{q_1\\}\\)\nFunção de transição \\(\\delta_{id}\\): \\[\n\\begin{aligned}\n\\delta(q_0, \\text{letra}) &= q_1 \\\\\n\\delta(q_0, \\text{dígito}) &= q_d \\\\\n\\delta(q_0, \\text{outro}) &= q_d \\\\\n\\delta(q_1, \\text{letra}) &= q_1 \\\\\n\\delta(q_1, \\text{dígito}) &= q_1 \\\\\n\\delta(q_1, \\text{outro}) &= q_d \\\\\n\\delta(q_d, \\text{letra}) &= q_d \\\\\n\\delta(q_d, \\text{dígito}) &= q_d \\\\\n\\delta(q_d, \\text{outro}) &= q_d\n\\end{aligned}\n\\]\n\nTabela de Transição:\n\n\n\nEstado\nletra (a-z, A-Z)\ndígito (0-9)\noutro\n\n\n\n\n\\(\\to q_0\\)\n\\(q_1\\)\n\\(q_d\\)\n\\(q_d\\)\n\n\n\\(*q_1\\)\n\\(q_1\\)\n\\(q_1\\)\n\\(q_d\\)\n\n\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\n\n\nEste autômato reconhece os quatro operadores: ++, +=, --, -=.\nRepresentação Algébrica:\n\\(A_{op} = (Q_{op}, \\Sigma_{op}, \\delta_{op}, q_{0,op}, F_{op})\\), onde:\n\nConjunto de estados: \\(Q_{op} = \\{q_0, q_1, q_2, q_3, q_4, q_5, q_6, q_d\\}\\)\nAlfabeto (símbolos): \\(\\Sigma_{op} = \\{+, -, =, \\text{outro}\\}\\)\nEstado inicial: \\(q_{0,op} = q_0\\)\nConjunto de estados finais: \\(F_{op} = \\{q_2, q_3, q_5, q_6\\}\\)\nFunção de transição \\(\\delta_{op}\\): \\[\n\\begin{aligned}\n\\delta(q_0, +) &= q_1 \\quad & \\delta(q_0, -) &= q_4 \\quad & \\delta(q_0, =) &= q_d \\quad & \\delta(q_0, \\text{outro}) &= q_d \\\\\n\\delta(q_1, +) &= q_2 \\quad & \\delta(q_1, =) &= q_3 \\quad & \\delta(q_1, -) &= q_d \\quad & \\delta(q_1, \\text{outro}) &= q_d \\\\\n\\delta(q_4, -) &= q_5 \\quad & \\delta(q_4, =) &= q_6 \\quad & \\delta(q_4, +) &= q_d \\quad & \\delta(q_4, \\text{outro}) &= q_d \\\\\n\\end{aligned}\n\\]\n\nPara todos os outros estados \\(q \\in \\{q_2, q_3, q_5, q_6, q_d\\}\\) e para qualquer símbolo \\(s \\in \\Sigma_{op}\\), a transição é \\(\\delta(q, s) = q_d\\).\nTabela de Transição:\n\n\n\nEstado\n+\n-\n=\noutro\n\n\n\n\n\\(\\to q_0\\)\n\\(q_1\\)\n\\(q_4\\)\n\\(q_d\\)\n\\(q_d\\)\n\n\n\\(q_1\\)\n\\(q_2\\)\n\\(q_d\\)\n\\(q_3\\)\n\\(q_d\\)\n\n\n\\(*q_2\\) (++)\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\n\n\\(*q_3\\) (+=)\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\n\n\\(q_4\\)\n\\(q_d\\)\n\\(q_5\\)\n\\(q_6\\)\n\\(q_d\\)\n\n\n\\(*q_5\\) (--)\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\n\n\\(*q_6\\) (-=)\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\n\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\n\n\nb. Construção do Autômato Finito Determinístico Combinado\nPara criar um único Autômato Finito Determinístico que reconheça a união das linguagens dos três autômatos individuais (\\(A_{int}\\), \\(A_{id}\\), \\(A_{op}\\)), utilizamos o método da construção do produto. Este método gera um novo Autômato Finito Determinístico cujos estados representam a simulação simultânea dos três autômatos originais.\nO novo autômato, \\(A_{comb}\\), é definido pela 5-tupla \\(A_{comb} = (Q_{comb}, \\Sigma_{comb}, \\delta_{comb}, q_{0,comb}, F_{comb})\\).\n1. Alfabeto (\\(\\Sigma_{comb}\\)): o alfabeto combinado é a união de todos os caracteres e classes de caracteres dos autômatos individuais. \\(\\Sigma_{comb} = \\{\\text{letra}, \\text{dígito}, +, -, =, \\text{outro}\\}\\)\n2. Conjunto de Estados (\\(Q_{comb}\\)): o novo conjunto de estados é o produto cartesiano dos conjuntos de estados dos autômatos originais. Cada estado em \\(A_{comb}\\) é uma tupla que rastreia o estado atual de cada autômato individual.\n\\(Q_{comb} = Q_{int} \\times Q_{id} \\times Q_{op}\\)\nO número total de estados seria \\(|Q_{int}| \\times |Q_{id}| \\times |Q_{op}| = 3 \\times 3 \\times 8 = 72\\) estados. Devido ao grande número, a construção explícita da tabela de transição é impraticável, mas o princípio é o seguinte:\n3. Estado Inicial (\\(q_{0,comb}\\)): o estado inicial do autômato combinado é a tupla contendo os estados iniciais de cada autômato individual.\n\\(q_{0,comb} = (q_{0,int}, q_{0,id}, q_{0,op})\\)\n4. Função de Transição (\\(\\delta_{comb}\\)): a função de transição é aplicada componente a componente. Para um estado tupla \\((p, q, r) \\in Q_{comb}\\) e um símbolo de entrada \\(s \\in \\Sigma_{comb}\\), a transição é definida como:\n\\(\\delta_{comb}((p, q, r), s) = (\\delta_{int}(p, s), \\delta_{id}(q, s), \\delta_{op}(r, s))\\)\nExemplos de Transição: vamos calcular a transição a partir do estado inicial para dois símbolos diferentes:\n\nEntrada ‘7’ (um dígito):\n\nEstado atual: \\((q_{0,int}, q_{0,id}, q_{0,op})\\);\n\\(\\delta_{int}(q_{0,int}, \\text{dígito}) = q_{1,int}\\);\n\\(\\delta_{id}(q_{0,id}, \\text{dígito}) = q_{d,id}\\);\n\\(\\delta_{op}(q_{0,op}, \\text{dígito}) = q_{d,op}\\);\nNovo estado: \\((q_{1,int}, q_{d,id}, q_{d,op})\\).\n\nEntrada ‘+’ (o símbolo +):\n\nEstado atual: \\((q_{0,int}, q_{0,id}, q_{0,op})\\);\n\\(\\delta_{int}(q_{0,int}, +) = q_{d,int}\\);\n\\(\\delta_{id}(q_{0,id}, +) = q_{d,id}\\);\n\\(\\delta_{op}(q_{0,op}, +) = q_{1,op}\\);\nNovo estado: \\((q_{d,int}, q_{d,id}, q_{1,op})\\).\n\n\n5. Conjunto de Estados Finais (\\(F_{comb}\\)): um estado no autômato combinado é final se pelo menos um de seus componentes for um estado final no seu respectivo autômato original. Isso corresponde à operação de união.\n\\(F_{comb} = \\{ (p, q, r) \\in Q_{comb} \\mid p \\in F_{int} \\lor q \\in F_{id} \\lor r \\in F_{op} \\}\\)\nPor exemplo, o estado \\((q_{1,int}, q_{d,id}, q_{d,op})\\) seria um estado final em \\(A_{comb}\\) porque \\(q_{1,int}\\) é um estado final em \\(A_{int}\\). Da mesma forma, um estado como \\((q_{d,int}, q_{d,id}, q_{2,op})\\) também seria final, porque \\(q_{2,op}\\) pertence a \\(F_{op}\\).\nc. Distinção entre os Tipos de tokens\nO autômato combinado \\(A_{comb}\\) pode reconhecer qualquer um dos tokens, mas, por si só, não informa qual tipo de token foi encontrado. A implementação de um analisador léxico real precisa dessa distinção. A chave está em analisar o estado final alcançado.\nComo os três autômatos originais reconhecem linguagens disjuntas (um identificador não pode ser um inteiro nem um operador, e vice-versa), nunca haverá ambiguidade. Ao atingir um estado final, apenas um dos componentes da tupla de estado será um estado final de seu autômato original.\nImplementação Prática: A estratégia para distinguir os tokens no analisador léxico seria:\n\nExecutar o Autômato Finito Determinístico: processe a cadeia de entrada usando o \\(A_{comb}\\) até que o final da cadeia seja alcançado ou a transição leve a um estado de erro global (ex: uma tupla onde todos os componentes são estados de erro, como \\((q_{d,int}, q_{d,id}, q_{d,op})\\)).\nVerificar o Estado Final: ao final de um lexema potencial, verifique se o estado atual \\((p, q, r)\\) é um estado final em \\(F_{comb}\\).\nIdentificar o Tipo de Token: se o estado for final, determine o tipo de token inspecionando qual componente da tupla pertence a um conjunto final original:\n\nSe \\(p \\in F_{int}\\) (ex: \\(p=q_{1,int}\\)), o token é um INTEIRO.\nSe \\(q \\in F_{id}\\) (ex: \\(q=q_{1,id}\\)), o token é um IDENTIFICADOR.\nSe \\(r \\in F_{op}\\) (ex: \\(r \\in \\{q_{2,op}, q_{3,op}, q_{5,op}, q_{6,op}\\}\\)), o token é um OPERADOR.\n\nMapear para um Tipo de Token: no código, isso se traduz em uma estrutura de decisão. Em vez de ter um único tipo de estado final, cada estado final no modelo teórico é associado a uma ação ou tipo de token no código prático. A sua sugestão de switch/case é uma excelente forma de representar isso.\n\nPara fins de implementação, não se criam as 72 tuplas explicitamente. Em vez disso, mantém-se três variáveis de estado (uma para cada autômato) e as atualiza em paralelo. O tipo do token é determinado pela primeira máquina que atingir um estado final.\n// Pseudocódigo da lógica do analisador\nenum TokenType { TOKEN_UNKNOWN, TOKEN_INTEGER, TOKEN_IDENTIFIER, TOKEN_OPERATOR_PLUSPLUS, ... };\nTokenType recognize_token(string input) {\n    // Simulação do estado combinado\n    State state_int = q0_int;\n    State state_id  = q0_id;\n    State state_op  = q0_op;\n    for (char c : input) {\n        state_int = delta_int(state_int, c);\n        state_id  = delta_id(state_id, c);\n        state_op  = delta_op(state_op, c);\n    }\n    // Mapeia o estado final para o tipo de token\n    if (is_final_int(state_int)) {\n        return TOKEN_INTEGER;\n    }\n    if (is_final_id(state_id)) {\n        // Em um sistema real, aqui haveria uma verificação de palavras-chave\n        return TOKEN_IDENTIFIER;\n    }\n    if (is_final_op(state_op)) {\n        // O estado específico (q2, q3, q5, q6) diria qual operador é\n        return map_final_op_state_to_token(state_op); // ex: q2 -&gt; TOKEN_OPERATOR_PLUSPLUS\n    }\n    return TOKEN_UNKNOWN; // ou lança um erro léxico\n}\nPrecedência: Embora não haja ambiguidade neste problema, em sistemas mais complexos (ex: palavras-chave vs. identificadores), a regra da maior precedência ou do maior casamento (longest match) é aplicada. Se if pudesse ser um identificador e uma palavra-chave, a Tabela de Símbolos ou a lógica do analisador priorizaria o TOKEN_KEYWORD_IF sobre o TOKEN_IDENTIFIER.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Solução dos Exercícios</span>"
    ]
  },
  {
    "objectID": "sol-exercicios.html#capítulo-sec-lingagens-livres-de-contexto",
    "href": "sol-exercicios.html#capítulo-sec-lingagens-livres-de-contexto",
    "title": "21  Solução dos Exercícios",
    "section": "21.3 Capítulo Chapter 5",
    "text": "21.3 Capítulo Chapter 5\n\n21.3.1 Exercícios de Derivação Section 5.2.2\n\n21.3.1.1 Exercício 1: Palíndromo Ímpar\nSolução: a derivação é feita para construir a string de fora para dentro, finalizando com o caractere central.\n\nPasso 1: Começamos com o símbolo inicial \\(K\\).\nPasso 2: A string 101 começa e termina com 1. Aplicamos a regra \\(K \\rightarrow 1K1\\).\nPasso 3: O símbolo restante no centro da string é 0. Aplicamos a regra de caso base \\(K \\rightarrow 0\\) para finalizar a derivação.\n\nA sequência completa da derivação será:\n\\[\nK \\Rightarrow 1K1 \\Rightarrow 101\n\\]\n\n\n21.3.1.2 Exercício 2: Expressão Aritmética Simples\nSolução: nesta derivação, vamos primeiro gerar a estrutura da adição e deporque resolver os operandos.\n\nPasso 1: iniciar com o símbolo inicial \\(E\\).\nPasso 2: a estrutura principal é uma soma. Aplicamos a regra \\(E \\rightarrow E + E\\).\nPasso 3: o operando à direita da soma é um id. Substituímos o segundo \\(E\\) usando a regra \\(E \\rightarrow id\\).\nPasso 4: o operando à esquerda da soma é uma multiplicação. Substituímos o primeiro \\(E\\) usando a regra \\(E \\rightarrow E * E\\).\nPasso 5: o operando à esquerda da multiplicação é um id. Substituímos o primeiro \\(E\\) da forma sentencial atual por id.\nPasso 6: o operando à direita da multiplicação também é um id. Substituímos o \\(E\\) restante por id para completar a derivação.\n\nA sequência completa da derivação Será:\n\\[\nE \\Rightarrow E + E \\Rightarrow E + id \\Rightarrow E * E + id \\Rightarrow id * E + id \\Rightarrow id * id + id\n\\]\n\n\n21.3.1.3 Exercício 3: Palíndromo de Comprimento Par e Aninhado\nSolução: esta derivação mostra o aninhamento repetido da mesma regra.\n\nPasso 1: Partimos do símbolo inicial \\(K\\).\nPasso 2: A string começa e termina com 0. Usamos \\(K \\rightarrow 0K0\\).\nPasso 3: A subcadeia interna, 1111, começa e termina com 1. Aplicamos a regra \\(K \\rightarrow 1K1\\) ao \\(K\\) interno.\nPasso 4: A nova subcadeia interna, 11, também começa e termina com 1. Aplicamos novamente a regra \\(K \\rightarrow 1K1\\).\nPasso 5: O centro da string agora está vazio. Aplicamos a regra \\(K \\rightarrow \\epsilon\\) para finalizar.\n\nA sequência completa da derivação será:\n\\[\nK \\Rightarrow 0K0 \\Rightarrow 01K10 \\Rightarrow 011K110 \\Rightarrow 011\\epsilon110 = 011110\n\\]\n\n\n21.3.1.4 Exercício 4: Linguagem \\(a^nb^n\\)\nSolução: a derivação aplica a regra recursiva três vezes para gerar os três pares de a e b.\n\nPasso 1: Iniciar com \\(S\\).\nPasso 2: Para o par mais externo, aplicamos \\(S \\rightarrow aSb\\).\nPasso 3: Para o segundo par, aplicamos novamente \\(S \\rightarrow aSb\\) ao \\(S\\) interno.\nPasso 4: Para o terceiro e último par, aplicamos \\(S \\rightarrow aSb\\) mais uma vez.\nPasso 5: Com todos os terminais gerados, substituímos o \\(S\\) final por \\(\\epsilon\\) para concluir.\n\nA sequência completa da derivação é:\n\\[\nS \\Rightarrow aSb \\Rightarrow aaSbb \\Rightarrow aaaSbbb \\Rightarrow aaa\\epsilon bbb = aaabbb\n\\]\n\n\n21.3.1.5 Exercício 5: Comando Condicional if-else\nSolução: esta derivação mostra como diferentes não terminais \\((C, A)\\) colaboram para formar uma estrutura completa.\n\nPasso 1: Começamos com o símbolo inicial \\(C\\).\nPasso 2: A única regra para \\(C\\) define a estrutura if-then-else. A aplicamos: \\(C \\rightarrow \\text{if } id \\text{ then } A \\text{ else } A\\).\nPasso 3: Agora, precisamos derivar o comando no bloco then. Substituímos o primeiro \\(A\\) usando a regra \\(A \\rightarrow id := 0\\).\nPasso 4: Finalmente, derivamos o comando no bloco else. Substituímos o segundo \\(A\\) usando a mesma regra \\(A \\rightarrow id := 0\\).\n\nA sequência completa da derivação será:\n\\[\nC \\Rightarrow \\text{if } id \\text{ then } A \\text{ else } A \\Rightarrow \\text{if } id \\text{ then } id := 0 \\text{ else } A \\Rightarrow \\text{if } id \\text{ then } id := 0 \\text{ else } id := 0\n\\]\n\n\n21.3.1.6 Exercício 6: Parênteses Balanceados\nSolução: esta derivação utiliza a concatenação (\\(BB\\)) e o aninhamento (\\((B)\\)).\n\nPasso 1: Partimos de \\(B\\).\nPasso 2: A string é uma concatenação de () e (()). Aplicamos a regra \\(B \\rightarrow BB\\).\nPasso 3: O primeiro \\(B\\) corresponde a (). Para derivá-lo, usamos a regra \\(B \\rightarrow (B)\\).\nPasso 4: O \\(B\\) interno na primeira parte deve ser a string vazia. Aplicamos \\(B \\rightarrow \\epsilon\\).\nPasso 5: Agora, focamos no segundo \\(B\\) da forma sentencial ()B, que corresponde a (()). Aplicamos a regra \\(B \\rightarrow (B)\\).\nPasso 6: O \\(B\\) interno corresponde a (). Aplicamos novamente \\(B \\rightarrow (B)\\).\nPasso 7: O último \\(B\\) é substituído pela string vazia com \\(B \\rightarrow \\epsilon\\) para finalizar.\n\nA sequência completa da derivação será:\n\\[\nB \\Rightarrow BB \\Rightarrow (B)B \\Rightarrow (\\epsilon)B \\Rightarrow ()B \\Rightarrow ()(B) \\Rightarrow ()( (B) ) \\Rightarrow ()( (\\epsilon) ) = ()(())\n\\]\n\n\n21.3.1.7 Exercício 7: Palíndromo Vazio\nSolução: esta é a derivação mais curta possível e testa o entendimento do caso base para a string vazia.\n\nPasso 1: Começamos com o símbolo inicial \\(K\\).\nPasso 2: A regra \\(K \\rightarrow \\epsilon\\) gera diretamente a string vazia.\n\nA sequência completa da derivação será:\n\\[\nK \\Rightarrow \\epsilon\n\\]",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Solução dos Exercícios</span>"
    ]
  },
  {
    "objectID": "referencias.html",
    "href": "referencias.html",
    "title": "Referências",
    "section": "",
    "text": "[1] LESK, M. E. Lex – A Lexical\nAnalyzer Generator. Murray Hill, NJ: Bell Laboratories,\n1975. \n\n\n[2] PAXSON, V. Flex, a fast\nlexical analyzer generator. [s.l: s.n.]. \n\n\n[3] MOORE, E. F. Gedanken-Experiments\non Sequential Machines. Em: SHANNON, C. E.; MCCARTHY, J. (Eds.).\nAutomata Studies. Princeton: Princeton University\nPress, 1956. p. 129–154. \n\n\n[4] MEALY, G. H. A Method for\nSynthesizing Sequential Circuits. The Bell System Technical\nJournal, v. 34, n. 5, p. 1045–1064, set. 1955. \n\n\n[5] RABIN, M. O.; SCOTT, D. Finite\nAutomata and Their Decision Problems. IBM Journal of\nResearch and Development, v. 3, n. 2, p. 114–125, 1959. \n\n\n[6] TIAN, Y. et al. Experimental\ndemonstration of quantum finite automaton. npj Quantum\nInformation, v. 5, 2019. \n\n\n[7] AHO, A. V. et al. Compilers:\nPrinciples, Techniques, and Tools. 2nd. ed. [s.l.] Pearson;\nAddison-Wesley, 2007. \n\n\n[8] MCCULLOCH, W. S.; PITTS, W. A logical calculus\nof the ideas immanent in nervous activity. The bulletin of\nmathematical biophysics, v. 5, n. 4, p. 115–133, 1943. \n\n\n[9] KLEENE, S. C. Representation of events in nerve\nnets and finite automata. Automata studies, v. 34, p.\n3–41, 1956.",
    "crumbs": [
      "Referências"
    ]
  }
]